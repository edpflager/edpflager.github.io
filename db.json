{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/hueman/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/hueman/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/css/images/logo-header.png","path":"css/images/logo-header.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/css/images/opacity-10.png","path":"css/images/opacity-10.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/css/images/s-left.png","path":"css/images/s-left.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/css/images/thumb-default-small.png","path":"css/images/thumb-default-small.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/css/images/thumb-default.png","path":"css/images/thumb-default.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/justified-gallery/jquery.justifiedGallery.min.js","path":"libs/justified-gallery/jquery.justifiedGallery.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/justified-gallery/justifiedGallery.min.css","path":"libs/justified-gallery/justifiedGallery.min.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/source-code-pro/styles.css","path":"libs/source-code-pro/styles.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/styles.css","path":"libs/titillium-web/styles.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/css/font-awesome.css","path":"libs/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/css/font-awesome.min.css","path":"libs/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/FontAwesome.otf","path":"libs/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.eot","path":"libs/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.svg","path":"libs/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.ttf","path":"libs/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.woff","path":"libs/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.woff2","path":"libs/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/jquery/3.5.0/jquery.min.js","path":"libs/jquery/3.5.0/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.css","path":"libs/lightgallery/css/lg-fb-comment-box.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.css.map","path":"libs/lightgallery/css/lg-fb-comment-box.css.map","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.min.css","path":"libs/lightgallery/css/lg-fb-comment-box.min.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.css","path":"libs/lightgallery/css/lg-transitions.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.css.map","path":"libs/lightgallery/css/lg-transitions.css.map","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.min.css","path":"libs/lightgallery/css/lg-transitions.min.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.css","path":"libs/lightgallery/css/lightgallery.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.css.map","path":"libs/lightgallery/css/lightgallery.css.map","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.min.css","path":"libs/lightgallery/css/lightgallery.min.css","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.eot","path":"libs/lightgallery/fonts/lg.eot","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.svg","path":"libs/lightgallery/fonts/lg.svg","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.ttf","path":"libs/lightgallery/fonts/lg.ttf","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.woff","path":"libs/lightgallery/fonts/lg.woff","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/img/loading.gif","path":"libs/lightgallery/img/loading.gif","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/img/video-play.png","path":"libs/lightgallery/img/video-play.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/img/vimeo-play.png","path":"libs/lightgallery/img/vimeo-play.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/img/youtube-play.png","path":"libs/lightgallery/img/youtube-play.png","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-autoplay.js","path":"libs/lightgallery/js/lg-autoplay.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-autoplay.min.js","path":"libs/lightgallery/js/lg-autoplay.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-fullscreen.js","path":"libs/lightgallery/js/lg-fullscreen.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-fullscreen.min.js","path":"libs/lightgallery/js/lg-fullscreen.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-hash.js","path":"libs/lightgallery/js/lg-hash.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-hash.min.js","path":"libs/lightgallery/js/lg-hash.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-pager.js","path":"libs/lightgallery/js/lg-pager.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-pager.min.js","path":"libs/lightgallery/js/lg-pager.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-share.js","path":"libs/lightgallery/js/lg-share.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-share.min.js","path":"libs/lightgallery/js/lg-share.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-thumbnail.js","path":"libs/lightgallery/js/lg-thumbnail.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-thumbnail.min.js","path":"libs/lightgallery/js/lg-thumbnail.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-video.js","path":"libs/lightgallery/js/lg-video.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-video.min.js","path":"libs/lightgallery/js/lg-video.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-zoom.js","path":"libs/lightgallery/js/lg-zoom.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-zoom.min.js","path":"libs/lightgallery/js/lg-zoom.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lightgallery.js","path":"libs/lightgallery/js/lightgallery.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/lightgallery/js/lightgallery.min.js","path":"libs/lightgallery/js/lightgallery.min.js","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","path":"libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","path":"libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcSo_WB_cotcEMUw1LsIE8mM.woff2","path":"libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcSo_WB_cotcEMUw1LsIE8mM.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcZSnX671uNZIV63UdXh3Mg0.woff2","path":"libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcZSnX671uNZIV63UdXh3Mg0.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr4-67659ICLY8bMrYhtePPA.woff2","path":"libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr4-67659ICLY8bMrYhtePPA.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr46gJz9aNFrmnwBdd69aqzY.woff2","path":"libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr46gJz9aNFrmnwBdd69aqzY.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr9INifKjd1RJ3NxxEi9Cy2w.woff2","path":"libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr9INifKjd1RJ3NxxEi9Cy2w.woff2","modified":1,"renderable":1},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr_SNRT0fZ5CX-AqRkMYgJJo.woff2","path":"libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr_SNRT0fZ5CX-AqRkMYgJJo.woff2","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/access-a-mysql-server-remotely.md","hash":"c9dca1021594963308abd588f068829037293f1f","modified":1598218409857},{"_id":"source/_posts/add-a-pentaho-system-menu-option-in-centos-7.md","hash":"cc87e0d4051a1cd1608f9b6ce0f9f88b9d614446","modified":1598216074950},{"_id":"source/_posts/add-apache-spark-to-cloudera.md","hash":"d73dac1cd4677c6f3d7f27508c4a28d1d98dc4f1","modified":1598216074806},{"_id":"source/_posts/adding-thrift-server-to-a-cloudera-hadoop-cluster.md","hash":"348eea7161051ef00dc28b8ec623f97273c1c389","modified":1598216074778},{"_id":"source/_posts/align-graphics-in-r-markdown-documents.md","hash":"6137b72dae4d43951238f7b3c2839f70e45ba83e","modified":1598216075178},{"_id":"source/_posts/android-not-so-much-for-me-revisited.md","hash":"b154f104dd3723ea47a57706c2a5f30621f391e9","modified":1598216074834},{"_id":"source/_posts/android-not-so-much-for-me.md","hash":"29ebf7814e38f2f2bbe582a3ab0f8d9cbe02a177","modified":1598216074742},{"_id":"source/_posts/apacheds-ldap-on-linux-mint-part-2.md","hash":"0ed0cd66167e731b74656c714f0d9969f52e5f8d","modified":1598216074990},{"_id":"source/_posts/apacheds-ldap-on-linux-mint-part-3.md","hash":"80cebb0ed976380b38d8f708b4eded27011112bb","modified":1598216074994},{"_id":"source/_posts/apacheds-ldap-part-4.md","hash":"aba46fb7a206938730247f66d2ee9279d9c3bf69","modified":1598216074998},{"_id":"source/_posts/austin-mongodb-road-show.md","hash":"99005a191995732a6c75ee543abecff849123532","modified":1598216074858},{"_id":"source/_posts/big-data-redux.md","hash":"6a8c62549fc685a6d16c57f5ad48ceeee87a6330","modified":1598216074786},{"_id":"source/_posts/cannot-connect-to-the-docker-daemon-fixed.md","hash":"10ad51b9e320adad7ad34c34a2f345e9068090fb","modified":1598216075010},{"_id":"source/_posts/cassandra-installation-on-linux-mint.md","hash":"891d704210d5eff37fa5e57791d5b02db5ebae96","modified":1598216075002},{"_id":"source/_posts/centos-show-details-during-boot.md","hash":"3c2b33bd9c110ef3db31f5fb32d0ec7ac9ee7511","modified":1598216074918},{"_id":"source/_posts/change-character-encoding-with-pentaho-select-values.md","hash":"7339b7efafc8ce2e40ad225bff3b8740d4bf30db","modified":1598216074930},{"_id":"source/_posts/check-for-a-file-in-pentaho-kettle.md","hash":"eeb31b96fd76bee322d35813a2350477767204bd","modified":1598216074934},{"_id":"source/_posts/check-for-multiple-files-in-pentaho-kettle-control-flow-components-part-3.md","hash":"f899ad7d4171dd803178dfaf43f4ca36fe9e4c45","modified":1598216074942},{"_id":"source/_posts/configure-your-ubuntu-docker-installation-for-remote-access.md","hash":"9054b6344b977172773d710cf5f7d42f9d7e7a13","modified":1598216075062},{"_id":"source/_posts/connecting-kettle-to-hadoop-hive.md","hash":"944a524b09be589eb77daa027c500d0061221f82","modified":1598216074918},{"_id":"source/_posts/create-a-pentaho-kettle-repository-on-sql-server.md","hash":"7e7c4f931a264cf09c00699b7bef0e803a8cf231","modified":1598216075086},{"_id":"source/_posts/debugging-etl-with-error-output.md","hash":"7c5690a63dc6276b7882394bb13b86d1c4508dfb","modified":1598216074874},{"_id":"source/_posts/dell-inspiron-3400-linux-laptop-first-impressions.md","hash":"15d877c25f0f2b113bdb99dcdfbb96d99acf9e83","modified":1598216074958},{"_id":"source/_posts/diagram-sql-server-graph-databases-in-r-part-1.md","hash":"1dc7128029db8c3ed6e7de5b982932dea5c487c2","modified":1598216075218},{"_id":"source/_posts/diagram-sql-server-graph-databases-in-r-part-2-5.md","hash":"96003a00da423003e0106efe130ff224b4345ca8","modified":1598216075222},{"_id":"source/_posts/diagram-sql-server-graph-databases-in-r-part-2.md","hash":"37f6aaf91cdd28e80268a13ee5c59c8826704071","modified":1598216075218},{"_id":"source/_posts/diagram-sql-server-graph-databases-in-r-part-3.md","hash":"f9dc45c2f1a6c452781c582c4ccbd1db76a88e0a","modified":1598216075226},{"_id":"source/_posts/diagram-sql-server-graph-databases-in-r-part-4.md","hash":"24feab58b12686a6ddaf339933c5f8757c8523d5","modified":1598216075226},{"_id":"source/_posts/disable-selinux-to-install-cloudera.md","hash":"b5c23fd038452cc2699782a18a2cbf6901ba3cbe","modified":1598216074794},{"_id":"source/_posts/docker-admin-cheat-sheet-part-1.md","hash":"26f312a3aa397836a1ee58ce133aeff491d32fbd","modified":1598216075030},{"_id":"source/_posts/docker-fun-play-text-adventures.md","hash":"8c9215ad82807247aaa2bcd4abb58cc087a3e09d","modified":1598216075074},{"_id":"source/_posts/docker-handy-tip-for-frequent-command.md","hash":"4a6e9b6ed1b92820531544627afc0178a55471a2","modified":1598216075066},{"_id":"source/_posts/edpflager-com-is-now-using-https.md","hash":"9ccbc18918a255a59d122b1e27db28923c3aa32b","modified":1598216075254},{"_id":"source/_posts/emails-files-from-kettle.md","hash":"1d56331acd777a0e05cf64f480c561c77f54dd56","modified":1598216074890},{"_id":"source/_posts/embedding-graphics.md","hash":"9da872b978242da3adb4b3107fa40d80b84312f2","modified":1598216075154},{"_id":"source/_posts/enable-linux-sql-server-agent.md","hash":"c30964325b383bff6aab196c5f87a331190efd70","modified":1598216075122},{"_id":"source/_posts/extract-pdf-data-with-tabula.md","hash":"18bc60a1b1e4356210e6642aea9c88e52073b758","modified":1598216074978},{"_id":"source/_posts/floating-and-rotating-embedded-graphics.md","hash":"91b27704c12c3e8f0a028a186c222345748220bc","modified":1598216075162},{"_id":"source/_posts/font-formatting-coloring-and-emphasis.md","hash":"c980cf238d5377537fc52d7045e49b5d55ae1c6c","modified":1598216075146},{"_id":"source/_posts/font-formatting-type-size.md","hash":"4464f5cd8945b2fe8d09ea7f5726b2a9d4a0e15b","modified":1598216075150},{"_id":"source/_posts/formulas-in-r-markdown.md","hash":"e2ed879532a9a17399a30b7c0c8d79b197d47b10","modified":1598216075182},{"_id":"source/_posts/getting-started-with-kettles-insertupdate-function.md","hash":"294b46cabb2b289eb4046fac0fa571b0d18e7026","modified":1598216074790},{"_id":"source/_posts/great-site-for-fonts.md","hash":"dd7713f1438c888ad0063b1cbda397a7534a29a9","modified":1598216074686},{"_id":"source/_posts/graph-database-visualizations-in-r.md","hash":"89b986cc1691ba998eb1a2039a57d0af5fb10018","modified":1598216075230},{"_id":"source/_posts/hadoop-and-hive.md","hash":"b03b5704a0869b3a76e30a62df545f7b54ac0c10","modified":1598216074734},{"_id":"source/_posts/hadoop-whats-the-point.md","hash":"b1940dafd42314e3a126a2ed5eb3fe405a0d76b6","modified":1598216074786},{"_id":"source/_posts/horizontal-lines-or-rules-in-your-documents.md","hash":"6d58416fff8dd9df7e160685188ddf2e90507d81","modified":1598216075166},{"_id":"source/_posts/illustration-of-etl.md","hash":"2b67e6b1554f501c8001abf4c9428210e7b973a7","modified":1598216074846},{"_id":"source/_posts/install-apacheldap-on-linuxmint-part-1.md","hash":"bff1469797460c1071f5d6ca924a12e1d3409739","modified":1598216074990},{"_id":"source/_posts/install-mongodb-as-a-service-in-centos-6-x.md","hash":"398f8c1f426e6040173be83fd34241d3d96ec2ea","modified":1598216074886},{"_id":"source/_posts/install-mysql-workbench-6-2-on-centos.md","hash":"b21599c1fbb2a49058df7f9e77b474d2237404ef","modified":1598216074914},{"_id":"source/_posts/install-rstudios-shiny-server-on-linux-mint.md","hash":"f6551d71e0b1487f9b8083d12c7a863895aab710","modified":1598216075230},{"_id":"source/_posts/install-the-saiku-analytics-plugin-in-pentaho-biserver-ce.md","hash":"0ec10ea7615f53bff9efce856a00747a1f57490f","modified":1598216075058},{"_id":"source/_posts/how-not-to-do-an-update.md","hash":"759677cdf22111a730d0af0f73a5bd30c1ab0728","modified":1598216074754},{"_id":"source/_posts/install-the-tor-browser-on-linux.md","hash":"c8114c9543f496a26c187c305dc983d15ae29d93","modified":1598216075010},{"_id":"source/_posts/interesting-docker-containers-and-some-tips-on-running-them.md","hash":"7fdf2ec005b662e142292d56a213ba42ab53fb71","modified":1598216075066},{"_id":"source/_posts/interesting-stuff-from-around-the-internet.md","hash":"d2d4358609d5a6e42827f687f4ecec9469307a15","modified":1598216074982},{"_id":"source/_posts/just-work.md","hash":"7f777e85815397bcd9015a9180fe3c718b4aff6b","modified":1598216074722},{"_id":"source/_posts/kettle-time-and-date-manipulations-part-1.md","hash":"a1b91bd76b31c1aa66df81a839b5cb917fba212b","modified":1598216074862},{"_id":"source/_posts/launch-pentaho-spoon-from-the-desktop.md","hash":"733e9fb0a3bc23aa4dbec3a5e733ebfcc6654779","modified":1598216074886},{"_id":"source/_posts/linux-bootable-usb-drives-on-mac-os-x-yosemite-solved.md","hash":"ff04088e9f5ba7273b52b1bbde2809d69400a348","modified":1598216074942},{"_id":"source/_posts/linux-partitioning-my-thoughts.md","hash":"ef617299b5162a7c00d3111c93772fb7bd86f6e5","modified":1598216074970},{"_id":"source/_posts/map-windows-host-location-to-linux-container-in-docker.md","hash":"704565fdcc5bff42b407b78f9ae12952b8a70564","modified":1598216075118},{"_id":"source/_posts/mariadbmysql-start-on-demand.md","hash":"2b0614ff2b5ff541de68a010199195ca5151645a","modified":1598216074746},{"_id":"source/_posts/microsoft-sql-server-on-docker-part-2-use-adventureworks.md","hash":"3a913c6258893bf15a3d4c7358afdb4a48951c46","modified":1598216075082},{"_id":"source/_posts/microsoft-sql-server-on-docker.md","hash":"9cc0031e43524c0d814c013577c41a553f80d951","modified":1598216075078},{"_id":"source/_posts/mongodb-data-loading-using-kettle.md","hash":"81133593238adccb62e0d5883cc987fae0eea8d5","modified":1598216074750},{"_id":"source/_posts/more-control-over-footnotes-in-r-markdown.md","hash":"f60cba34c33237191c8b42e09a97dc55a95f06ca","modified":1598216075210},{"_id":"source/_posts/nathns-error-with-docker-sql-server-container-on-windows10.md","hash":"608c9d1f00b7497fa5020faf721ec3f36c296872","modified":1598216075114},{"_id":"source/_posts/nixnote-evernote-client-on-linux.md","hash":"e0721ba4bd46ac10c519d57d5d6b20666d5f825b","modified":1598216074842},{"_id":"source/_posts/not-dead-but-dreaming.md","hash":"c9ac0fa699b7d67c0f74e8a1cd6dcab80cca191e","modified":1598216074966},{"_id":"source/_posts/off-the-grid.md","hash":"621c57cd0508ad034cee82e517f27e296cb41d6a","modified":1598216074702},{"_id":"source/_posts/pdi-kettle-delete-files.md","hash":"b5ddb4e9cd1016da19caf7495abf7f3be1516d5a","modified":1598216074810},{"_id":"source/_posts/pdi-kettle-updated-to-version-5-1.md","hash":"31b42a5fc3faa70494343b179f9a3e1b67727210","modified":1598216074870},{"_id":"source/_posts/pentaho-access-to-sql-server-with-ad-authentication.md","hash":"d0adc0182cadd626974fd742d5fe3658e9b8894c","modified":1598216074890},{"_id":"source/_posts/pentaho-data-extracts-with-lookups.md","hash":"013276b0b21c85b305d98e5a7ef6438a18043a37","modified":1598216074894},{"_id":"source/_posts/pentaho-fuzzy-match.md","hash":"8765f8e32ee802e187d2c970f3488413841ca8ac","modified":1598216075038},{"_id":"source/_posts/pentaho-kettle-and-db2-truncate.md","hash":"921aa7c082bc5e39c119e156d180d7e9438e7042","modified":1598216074878},{"_id":"source/_posts/pentaho-kettle-create-destination-tables-on-the-fly.md","hash":"a2fb390da1d39bf73547995544aa3c2bd2d5d964","modified":1598216074842},{"_id":"source/_posts/pentaho-kettle-spoon-damaged-message-on-mac-osx-mavericks.md","hash":"cc1f59c59d4efef0db504ede4ca8a9cdbf933e0d","modified":1598216074810},{"_id":"source/_posts/pentaho-kettle-time-and-date-manipulations-part-2.md","hash":"93decb68f9d384c54b7d6dbc7c3a71a4b63b9170","modified":1598216074870},{"_id":"source/_posts/pentaho-to-lookup-or-not-to-lookup.md","hash":"d92db1629bebb2268dbfc5486ef98742fb2bbbc8","modified":1598216074926},{"_id":"source/_posts/power-bi-r-packages-an-rvest-example.md","hash":"67876c4ba1469476f8cdcc9690c5c8b919404303","modified":1598216075202},{"_id":"source/_posts/public-data-sets.md","hash":"fb26948db3106f8130528c66c75f1f22fc8fd13f","modified":1598216074850},{"_id":"source/_posts/quick-tips-spacing-and-bulleted-lists.md","hash":"5ad7fbffd69b1b038d50c3b7f5b13d23074d2f99","modified":1598216075210},{"_id":"source/_posts/r-markdown-cookbook-font-formatting.md","hash":"023dc26137f631311a43fd544ee4e6f305007477","modified":1598216075130},{"_id":"source/_posts/r-markdown-cookbook-lorem-ipsum-text.md","hash":"f2843a8203f7fdf3730d3467e5d9d56b4c72af24","modified":1598216075146},{"_id":"source/_posts/r-markdown-cookbook-pdf-page-options.md","hash":"dea0e4eb4cfb4f929e320c7569c2c14525979611","modified":1598216075142},{"_id":"source/_posts/r-markdown-cookbook-table-formatting-for-pdf.md","hash":"d12ae0366703e795a0880275c08a3e3a9da1241f","modified":1598216075134},{"_id":"source/_posts/r-markdown-cookbook-using-inline-code.md","hash":"2e5382e13c3e0e7c6ab909cf15bc821a6eff6b1b","modified":1598216075138},{"_id":"source/_posts/r-pacman-package-manager-with-require.md","hash":"2775264bea7f6e5e04e8d773892693b83c2c512b","modified":1598216075186},{"_id":"source/_posts/r-quick-code-to-install-needed-packages.md","hash":"6911509e82b892d22d581d89b7592b95456a1f8b","modified":1598216075174},{"_id":"source/_posts/remove-evaluator-login-from-pentaho-bi-server.md","hash":"61f5feeeaa837d49d7eece5b57221a958dbfc87c","modified":1598216074986},{"_id":"source/_posts/review-big-data-glossary.md","hash":"d771e0077e912407345059d6ea01422fe5f684ad","modified":1598216074738},{"_id":"source/_posts/review-clean-data-by-megan-squire.md","hash":"c292474350261e4697c23fa3c0876a099a1adb80","modified":1598216074954},{"_id":"source/_posts/rstudio-on-linux-mint-19.md","hash":"d0fc3c9b035e281e3e48272a1d4b518790d4afbc","modified":1598216075206},{"_id":"source/_posts/run-pentaho-biserver-on-linux-demystified.md","hash":"501d288532e8a221b946a31ba7d6fd3c0d7ac23f","modified":1598216074838},{"_id":"source/_posts/run-services-like-mysql-on-demand-with-ubuntu-15-04.md","hash":"8c7b4e20a5508d6868d1dae9da286ff2e99aaf53","modified":1598216074962},{"_id":"source/_posts/run-sqlserver-service-on-ubuntu-16-04-on-demand.md","hash":"4a3eb16cbd35c22c71a996662eceb06edc194d26","modified":1598216075094},{"_id":"source/_posts/running-kettle-pentaho-data-integration-on-mac-osx-10-12-sierra.md","hash":"d2cbf1c82086e0cdae00aab85b5c7ed57056ffa1","modified":1598216075110},{"_id":"source/_posts/running-pentaho-kettle-7-on-windows-10.md","hash":"9bab3e5c068969766bd6c046bcb4bda0dd3ebb37","modified":1598216075082},{"_id":"source/_posts/send-push-notifications-from-your-kettle-jobs-android.md","hash":"94181973a61aeb75358a665534e0222dceab8a55","modified":1598216074894},{"_id":"source/_posts/services-gui-tool-on-centos.md","hash":"a688fad2fe47fc24257b5d501e0a946f56b639e7","modified":1598216074798},{"_id":"source/_posts/set-up-a-hadoop-cluster-part-2.md","hash":"0a1377726df9e05de6ad5ad119f1f90b9f57f7f2","modified":1598216074766},{"_id":"source/_posts/html-hyperlinks-in-r-markdown-pdfs.md","hash":"7072af74786c61977a09109da3364307efaaf859","modified":1598216075194},{"_id":"source/_posts/hyperlinks-in-r-markdown-word-output.md","hash":"5c893ac4dcf1bf3d6037ea940677e124b02e6de3","modified":1598216075194},{"_id":"source/_posts/set-up-a-hadoop-cluster-part-3.md","hash":"e7167d8e2dc0e8605fe75c3623093cc16bdc2a69","modified":1598216074766},{"_id":"source/_posts/set-up-a-hadoop-cluster-part-4.md","hash":"2ace02484d38d19898f4d7efeda73135ad05ffff","modified":1598216074770},{"_id":"source/_posts/set-up-a-hadoop-cluster-part-5.md","hash":"9061f8997a9c8cba1a72468b34ea2e15148382ef","modified":1598216074782},{"_id":"source/_posts/set-up-a-kettle-repository-using-mysql.md","hash":"a3390b9d3f258aecf4e7ac4de458300c9cf4fb00","modified":1598216074746},{"_id":"source/_posts/setting-up-a-hadoop-cluster-part-1.md","hash":"c38a2610278d94fe1d825598f74db0d3e7675870","modified":1598216074762},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-4.md","hash":"4bfd78779a93c93ac6c0bd1c2f8efa9101658750","modified":1598216074826},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-5.md","hash":"aafbe81361f6df5825ee6a00fbad3170fc7583ca","modified":1598216074826},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-2.md","hash":"b6805cd434323da0de6524a963d0b2b571206551","modified":1598216074818},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-6.md","hash":"0d2b046196f4e763b650025f53531871ae5f40ba","modified":1598216074830},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-7.md","hash":"c5efeff1eb78ac5df1625e7f07cdbcf4529e01d4","modified":1598216074834},{"_id":"source/_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-2.md","hash":"fa9f8743103f73f29407bdbe9082bb832cf366bb","modified":1598216074902},{"_id":"source/_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-3.md","hash":"4b7c778fc95ad0c05fb40cd2474025e9a09905bb","modified":1598216074906},{"_id":"source/_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-4.md","hash":"7394762779940adaae13b76c2a43644bda66ea6b","modified":1598216074910},{"_id":"source/_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5.md","hash":"6e58c73fedbbbec54ac648e3c56cbcc4a2380ddd","modified":1598216074898},{"_id":"source/_posts/shell-script-to-control-services-on-linux-mint-especially-sql-server.md","hash":"31174cf9ab06d517f3296862691b03ef74cd1007","modified":1598216075126},{"_id":"source/_posts/smartdiagram-in-r-markdown.md","hash":"8dab7da322992b3d4b3c2148b881f5199b445598","modified":1598216075202},{"_id":"source/_posts/squirrel-client-for-accessing-multiple-database-platforms-part-1.md","hash":"800fffbdf00e26f7696aa380ac9b6516121c2f6b","modified":1598216074946},{"_id":"source/_posts/squirrel-sql-client-for-accessing-different-databases-part-2.md","hash":"1b6fb49f6fa17a7e4e5a1d1d017a827cbf4c9c0f","modified":1598216074958},{"_id":"source/_posts/technical-difficulties-please-stand-by.md","hash":"374b343583703bcc8fa91cd577444fcc6ac02a92","modified":1598216075186},{"_id":"source/_posts/the-vagaries-of-software-updates.md","hash":"73d742da19809278b43a70e11bebae9d8f2e5fe0","modified":1598216074910},{"_id":"source/_posts/tidyverse-installation-on-linux-mint.md","hash":"22d5d52c69f3bd845531d8c8ddfca44819f29c9a","modified":1598216075218},{"_id":"source/_posts/ubuntu-12-10-add-ppa-repositories.md","hash":"da4a9678c9da013522bab2b4e8c930f8a16ad8d4","modified":1598216074726},{"_id":"source/_posts/use-gmailhotmail-with-pentaho-bi-server-community.md","hash":"f94b32c4d8f7c01a5f2607a59d85d6409ed28b68","modified":1598216075002},{"_id":"source/_posts/use-linux-mint-as-a-server.md","hash":"2023cdaeba5e7f66252c502c0eb1bd8cfdb91d41","modified":1598216075006},{"_id":"source/_posts/use-linux-sql-server-with-r-jdbc.md","hash":"33b9f87ec7e4adfa0a8b6b621757ae60bbb51145","modified":1598216075102},{"_id":"source/_posts/use-linux-sql-server-with-r-odbc.md","hash":"a43791453f44947a3f1f4bdedb5a56a8952275e0","modified":1598216075106},{"_id":"source/_posts/use-ms-sql-server-with-pentaho-data-integrator.md","hash":"025c46573e4cdc749e1ef58db16bf90a60193878","modified":1598216074858},{"_id":"source/_posts/using-chrome-with-pentaho-report-designer-6.md","hash":"744d4c0129b6267ecd5dd71f954de1d83ab3135b","modified":1598216074982},{"_id":"source/_posts/using-docker-on-demand-with-linux-mint.md","hash":"0084cd5adf9159b6b512c66826e63809ada20ee1","modified":1598216075038},{"_id":"source/_posts/using-mariadb-jdbc-with-pentaho-kettle-pdi.md","hash":"0489e53b73cd8a97eb30aae0760b5e99dd298ed3","modified":1598216074922},{"_id":"source/_posts/using-pentaho-table-output-and-update-together.md","hash":"0e71e8f390280922815f8aab142cb10b5dfb0586","modified":1598216074898},{"_id":"source/_posts/wait-for-a-file-in-pentaho-kettle-control-flow-components-part-2.md","hash":"b24ad6bac360bd78799207af669b7a5d2d9fd884","modified":1598216074938},{"_id":"source/_posts/what-is-a-data-scientist.md","hash":"a4da1d1ede31a09b6e83518524f003d526ebaf03","modified":1598216074854},{"_id":"source/_posts/what-the-heck-is-big-data.md","hash":"f0a3957d4e196a2455b701ff32f682adf6ac9f1d","modified":1598216074730},{"_id":"source/_posts/wrap-text-around-graphics.md","hash":"5c3bfcc6f8f5c13293295470b833caa2de6f4b4d","modified":1598216075166},{"_id":"source/_trash/photo-break-denver-waterwheel.md","hash":"c78ea058958de4a9d46f5553863900aeb90ed628","modified":1598216075234},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-3.md","hash":"0531067120c594e14b55a6cba41baeec92305c31","modified":1598216074822},{"_id":"source/_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-1.md","hash":"26fe03aabb8aa89913133def8f85258d8e5f7e0c","modified":1598216074814},{"_id":"themes/hueman/LICENSE","hash":"3975b7883caeb33f61fada7c0ef4add7ab189849","modified":1598216749487},{"_id":"themes/hueman/.gitignore","hash":"cd089ae45ce870c45e434019e8f1ed4f066cd425","modified":1598216749487},{"_id":"themes/hueman/_config.yml","hash":"32dc691a594ac54b196807aa770534d2d69cfb84","modified":1598217144518},{"_id":"themes/hueman/README.md","hash":"096d3c5c90b77bb69e5ea5230c60b9fe8c04746b","modified":1598216749487},{"_id":"themes/hueman/_config.yml.example","hash":"32dc691a594ac54b196807aa770534d2d69cfb84","modified":1598216749487},{"_id":"themes/hueman/package.json","hash":"1dda3c209f51d6d6a3b4f112f787915d3add726a","modified":1598216749495},{"_id":"themes/hueman/languages/ca.yml","hash":"deb751cd834e928a218cd4dff25f9b8a4f1597f4","modified":1598216749487},{"_id":"themes/hueman/languages/en.yml","hash":"26a3d8c89a257a4e8b84e3b5214d0f184cb8ad79","modified":1598216749487},{"_id":"themes/hueman/languages/es.yml","hash":"8bfab5d3e889fe2959ee469263c19951aab08fad","modified":1598216749487},{"_id":"themes/hueman/languages/fr.yml","hash":"8ab415076d01d069c24c9bc60e448ef258834dec","modified":1598216749487},{"_id":"themes/hueman/languages/hu.yml","hash":"b612b0d3329aa205e5903576018194e44d5f8d22","modified":1598216749487},{"_id":"themes/hueman/languages/id.yml","hash":"36fa092fe362ffffe1db67afb35d2bab1f04e2f0","modified":1598216749487},{"_id":"themes/hueman/languages/ja.yml","hash":"3695f6c76b48195881419e69fbeb8c25bd985119","modified":1598216749487},{"_id":"themes/hueman/languages/ko.yml","hash":"1f5478612edf72aa7b87a4250e7f27f647bdc575","modified":1598216749487},{"_id":"themes/hueman/languages/pt-BR.yml","hash":"2b65116ff36781cc130138ee8cb209095e5ba5df","modified":1598216749487},{"_id":"themes/hueman/languages/ru.yml","hash":"2878b42646ddea60ce1236f68f4b907c436cf26e","modified":1598216749487},{"_id":"themes/hueman/languages/tr.yml","hash":"c6bd8d946213d0a1dd05c033669513d1ab67a66e","modified":1598216749487},{"_id":"themes/hueman/languages/vi.yml","hash":"c2d23b1034e1e51355f97224ad7bda8066d597a2","modified":1598216749487},{"_id":"themes/hueman/languages/zh-CN.yml","hash":"38c119ff77bb4eaa22403dde4f1303920260e9a3","modified":1598216749487},{"_id":"themes/hueman/languages/zh-TW.yml","hash":"343d3589f6c8449319c872c03e028a92682fce35","modified":1598216749487},{"_id":"themes/hueman/layout/archive.ejs","hash":"8785477232088e09a75da88a0cdcb32fedf4f81f","modified":1598216749491},{"_id":"themes/hueman/layout/author.ejs","hash":"5096d3f019098d9940429152295f6d6161d887ba","modified":1598216749491},{"_id":"themes/hueman/layout/authors.ejs","hash":"8946d0d1a70ac470b3ee56dd36b4f9da2b73ed76","modified":1598216749491},{"_id":"themes/hueman/layout/categories.ejs","hash":"8946d0d1a70ac470b3ee56dd36b4f9da2b73ed76","modified":1598216749491},{"_id":"themes/hueman/layout/category.ejs","hash":"5096d3f019098d9940429152295f6d6161d887ba","modified":1598216749491},{"_id":"themes/hueman/layout/index.ejs","hash":"8785477232088e09a75da88a0cdcb32fedf4f81f","modified":1598216749495},{"_id":"themes/hueman/layout/layout.ejs","hash":"2c8cdd3a857a4b35e89b354931909186eaa91853","modified":1598216749495},{"_id":"themes/hueman/layout/page.ejs","hash":"5afddd6a45fa72beacec8d760487dfe8a667e622","modified":1598216749495},{"_id":"themes/hueman/layout/post.ejs","hash":"5afddd6a45fa72beacec8d760487dfe8a667e622","modified":1598216749495},{"_id":"themes/hueman/layout/tag.ejs","hash":"8785477232088e09a75da88a0cdcb32fedf4f81f","modified":1598216749495},{"_id":"themes/hueman/scripts/excerpt.js","hash":"630f17f2647b2ba5b207dba36fb3c6587a3aa04f","modified":1598216749495},{"_id":"themes/hueman/scripts/md5.js","hash":"7712232a328daf1797a3dd313bfce124f7df67ff","modified":1598216749495},{"_id":"themes/hueman/layout/tags.ejs","hash":"8946d0d1a70ac470b3ee56dd36b4f9da2b73ed76","modified":1598216749495},{"_id":"themes/hueman/scripts/meta.js","hash":"1993754a2f3dffa283fa0538eb8f056385b69ad4","modified":1598216749495},{"_id":"themes/hueman/scripts/thumbnail.js","hash":"e78b1b978fa1acad5409afc685d13f669e80b24f","modified":1598216749495},{"_id":"themes/hueman/layout/comment/changyan.ejs","hash":"4dbfefe8c9ed9b824a6bc4952d2a0a72e9166b61","modified":1598216749491},{"_id":"themes/hueman/layout/comment/counter.ejs","hash":"237fe2f89c128fe4f7c1edb9187f504f42cb5e74","modified":1598216749491},{"_id":"themes/hueman/layout/comment/disqus.ejs","hash":"936ded56d044e5f0a2f4c5d773d41965b2bea145","modified":1598216749491},{"_id":"themes/hueman/layout/comment/duoshuo.ejs","hash":"ce46d7410a99b57704da32e9d09071cef6c9fa93","modified":1598216749491},{"_id":"themes/hueman/layout/comment/facebook.ejs","hash":"5ee16430a4435c2fead0275ff83fc98092d73d4c","modified":1598216749491},{"_id":"themes/hueman/layout/comment/gitalk.ejs","hash":"915124de9806cf92bbe1af1b77e6580d0eaa8ac3","modified":1598216749491},{"_id":"themes/hueman/layout/comment/index.ejs","hash":"058f7dc27f700d083841e3d7a251e57ccbc4706e","modified":1598216749491},{"_id":"themes/hueman/layout/comment/isso.ejs","hash":"c196dbd522a3969d75ce5e61bc7ce7cec52d462f","modified":1598216749491},{"_id":"themes/hueman/layout/comment/livere.ejs","hash":"792e4f0e93b2bdc5abd85d447d804a5c608a9c5c","modified":1598216749491},{"_id":"themes/hueman/layout/comment/scripts.ejs","hash":"48212bc99b046a67a1027024efecda8c79eb7fa9","modified":1598216749491},{"_id":"themes/hueman/layout/comment/valine.ejs","hash":"00502e24843fab3cad0d99d2ac66144685fd39ce","modified":1598216749491},{"_id":"themes/hueman/layout/comment/youyan.ejs","hash":"ef6c37f535d4897679823143ce37c25d556cd729","modified":1598216749491},{"_id":"themes/hueman/layout/common/archive.ejs","hash":"e4d32fa38c969db0d0f66463bc67a3020ed979f7","modified":1598216749491},{"_id":"themes/hueman/layout/common/authors.ejs","hash":"dc9adbd9fa759c80614feaa499433de1b76f8d83","modified":1598216749491},{"_id":"themes/hueman/layout/common/article.ejs","hash":"3aec000ae8c5ea1d1c729214fe5c35c1c620cd0c","modified":1598216749491},{"_id":"themes/hueman/layout/common/categories.ejs","hash":"b408e91026ebbb324b7e4f517242f22903e3992b","modified":1598216749491},{"_id":"themes/hueman/layout/common/content-title.ejs","hash":"4e1a92448207b8e3d1695df855f6be7c70576248","modified":1598216749491},{"_id":"themes/hueman/layout/common/footer.ejs","hash":"d62dc97ec079d8ef5d0af401b56dceec1ffb64f7","modified":1598216749491},{"_id":"themes/hueman/layout/common/head.ejs","hash":"20d27432498ec723a685b989917d9b2d2c3f8b92","modified":1598216749491},{"_id":"themes/hueman/layout/common/header.ejs","hash":"ee7b3d8e28114c9acdd28a37bf57559dbc452307","modified":1598216749491},{"_id":"themes/hueman/layout/common/scripts.ejs","hash":"db677ab336d31292bffc071b2115e25e9c6f198d","modified":1598216749491},{"_id":"themes/hueman/layout/common/sidebar.ejs","hash":"f582cd96ce6fa3b0340a1642695fec1632391e25","modified":1598216749491},{"_id":"themes/hueman/layout/common/summary.ejs","hash":"42d6f99906ae96a6d00ee9a727405e47ec5e7c81","modified":1598216749491},{"_id":"themes/hueman/layout/common/tags.ejs","hash":"20769fc8213091770e5b5dac16e4528f9104e244","modified":1598216749491},{"_id":"themes/hueman/layout/common/thumbnail.ejs","hash":"c5fba5d5287f49e02040c530cd92312b2221a2c1","modified":1598216749491},{"_id":"themes/hueman/layout/plugin/baidu-analytics.ejs","hash":"d99089976258050666208f29000f84496fe1029c","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/cookie-consent.ejs","hash":"6baf0d836b2c1ddc757b047f014c08fbbf008ee8","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/google-adsense.ejs","hash":"8b3870371d6554777b3323389cdeb8a287467ed3","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/google-analytics.ejs","hash":"9ae10a0ff00a5bfe2bb6ad2ce2f882ae9074ad2b","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/scripts.ejs","hash":"7d011d257cabf47c8faacbb1ca98381db4ace71c","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/statcounter.ejs","hash":"7773a7ef15dbaf4ead2485eb7e5c394d20789438","modified":1598216749495},{"_id":"themes/hueman/layout/plugin/twitter-conversion.ejs","hash":"b9bc79727c9790b868e52ce2bd80536a04407a7e","modified":1598216749495},{"_id":"themes/hueman/layout/pwa/index.ejs","hash":"6db63250679a4fdf8113d57f302a50d848a408cf","modified":1598216749495},{"_id":"themes/hueman/layout/search/baidu.ejs","hash":"bcffa60f2d1750ac7499e928f538176e3804393b","modified":1598216749495},{"_id":"themes/hueman/layout/search/index.ejs","hash":"1a6a742727018567f60f8815be0bff5a45294ce5","modified":1598216749495},{"_id":"themes/hueman/layout/search/insight.ejs","hash":"130fe3d33ac71da0b50f7fee6a87979f30938a1b","modified":1598216749495},{"_id":"themes/hueman/layout/search/swiftype.ejs","hash":"379e66d2c13526e72e4120c443f95fccf4edef71","modified":1598216749495},{"_id":"themes/hueman/layout/share/addtoany.ejs","hash":"ac180c4c84b73a04d61b17e7dc18c257e20bf59f","modified":1598216749495},{"_id":"themes/hueman/layout/share/bdshare.ejs","hash":"5fd8c82a40d957e9481540a0b7d3ffca874e14a7","modified":1598216749495},{"_id":"themes/hueman/layout/share/default.ejs","hash":"7492f5b375a56c67a1a1a4f6b893e37f49dc86dc","modified":1598216749495},{"_id":"themes/hueman/layout/share/index.ejs","hash":"029e91aace5a4c0d8387fc7744c477ccc6865c30","modified":1598216749495},{"_id":"themes/hueman/layout/share/jiathis.ejs","hash":"b80332a1e0c8d230fabcf5e696edae349925ae17","modified":1598216749495},{"_id":"themes/hueman/layout/widget/archive.ejs","hash":"c4d303eaaa23768e52ead324c422a8900b1fe448","modified":1598216749495},{"_id":"themes/hueman/layout/widget/catalog.ejs","hash":"b95f91c7b3b26236ea675482b173b29fecf3e7f4","modified":1598216749495},{"_id":"themes/hueman/layout/widget/category.ejs","hash":"2d705df76f2eef7d695a971266fc104e89ca6bcd","modified":1598216749495},{"_id":"themes/hueman/layout/widget/google_adsense.ejs","hash":"c83ea7f18b970c48c90994cb96be2f659f7744fa","modified":1598216749495},{"_id":"themes/hueman/layout/widget/links.ejs","hash":"97dab84d6336a4c926ddc288d5a6c264f54c50c3","modified":1598216749495},{"_id":"themes/hueman/layout/widget/recent_posts.ejs","hash":"16a2bd28bdf520616202670a18c6cc93d1dd3f54","modified":1598216749495},{"_id":"themes/hueman/layout/widget/sticky_posts.ejs","hash":"6bad4126bed652f5f9e93027cbe3ee03b67b034a","modified":1598216749495},{"_id":"themes/hueman/layout/widget/tag.ejs","hash":"bfbc63e675439dcdc35e07dce6948e41500b649c","modified":1598216749495},{"_id":"themes/hueman/layout/widget/tagcloud.ejs","hash":"3ecb048d6098bc3953043a4c25f1f7c4b23397cf","modified":1598216749495},{"_id":"themes/hueman/source/css/_extend.styl","hash":"c2160499dc89c524ff17cd2e3fcbb8df83ebca1a","modified":1598216749495},{"_id":"themes/hueman/source/css/_responsive.styl","hash":"4addaf2f203563accf0479850eda1ab4f4a4c34b","modified":1598216749503},{"_id":"themes/hueman/source/css/_variables.styl","hash":"c077e2412dd38c40e68ecc763c0b8c5e3254808a","modified":1598216749507},{"_id":"themes/hueman/source/css/style.styl","hash":"b76d43839bda01ee791e71093f009ad2c8b093b2","modified":1598216749507},{"_id":"themes/hueman/source/js/insight.js","hash":"6ee84c42c2b230ff9e9bf605a444bd671d44f9e3","modified":1598216749507},{"_id":"themes/hueman/source/js/main.js","hash":"2ef5449478816096cdd0e9dde4d1fc1c2f33ab87","modified":1598216749507},{"_id":"themes/hueman/layout/common/post/author.ejs","hash":"d27178a76c95949f537def3d16ae42ef68437b2b","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/counter.ejs","hash":"708566f1fb93062201e51fc1ce2a251078985437","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/date.ejs","hash":"62600215be93098d88f3ef949cc9d5264167810c","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/gallery.ejs","hash":"659f019761116313169148ec61773e7b84abb739","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/ld_json.ejs","hash":"b33214e00cf9972382ff0c2c93c0fc9e411f921f","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/nav.ejs","hash":"c5f41ebf451cff39eaf116096604ce706a175767","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/tag.ejs","hash":"2e966216256321aa0c76fe1b9be689601c76ef31","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/title.ejs","hash":"6d19c61afb1f5f71c483be2ce37c6820ac2cd8b5","modified":1598216749491},{"_id":"themes/hueman/layout/common/post/valinecounter.ejs","hash":"9d7bb963286520690dc4e0a2ffae44560f013a1e","modified":1598216749491},{"_id":"themes/hueman/.github/ISSUE_TEMPLATE.md","hash":"1ce6c6330e2e37fcf4764d69ed2c43670e30eb23","modified":1598216749487},{"_id":"themes/hueman/source/css/_highlight/androidstudio.styl","hash":"65d09f1b0e81c6a182f549fd3de51e59823c97ae","modified":1598216749495},{"_id":"themes/hueman/source/css/_highlight/arduino-light.styl","hash":"15e8572585cd708221c513dea4bdd89d8fe56c10","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/agate.styl","hash":"601eb70448a16b918df132f6fc41e891ae053653","modified":1598216749495},{"_id":"themes/hueman/source/css/_highlight/arta.styl","hash":"1a5accc115f41d1b669ed708ac6a29abac876599","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/ascetic.styl","hash":"32cff3bef6fac3760fe78f203096477052a90552","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-cave-dark.styl","hash":"bc647b2c1d971d7cc947aa1ed66e9fd115261921","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-cave-light.styl","hash":"a5be0744a7ecf4a08f600ade4cfd555afc67bc15","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-dune-dark.styl","hash":"df50a85a4b14c7ca6e825d665594b91229d0e460","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-dune-light.styl","hash":"931435fbc6f974e8ce9e32722680035d248a9dc1","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-estuary-dark.styl","hash":"d84382bc8298f96730757391d3e761b7e640f406","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-estuary-light.styl","hash":"344276ca9b27e51d4c907f76afe5d13cf8e60bdf","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-forest-light.styl","hash":"95228d9f2102fad425536aac44b80b2cba1f5950","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-forest-dark.styl","hash":"57c154c6045a038dc7df0a25927853e10bf48c4a","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-heath-light.styl","hash":"8c8c2e445abef85273be966d59770e9ced6aac21","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-heath-dark.styl","hash":"b0cf13b2233e7bc38342032d2d7296591a4c2bcf","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-lakeside-dark.styl","hash":"bb0a8c4ad0dd8e3e7de7122ddf268fc42aa94acb","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-lakeside-light.styl","hash":"2c54cb9bdb259ae3b5b29f63ac2469ed34b08578","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-plateau-dark.styl","hash":"09c64f1a7052aec9070c36c0431df25216afaea1","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-plateau-light.styl","hash":"d1a05fdd1ededc9063d181ab25bad55a164aeb4a","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-savanna-dark.styl","hash":"a16c919a1ccf2f845488078fb341381bec46b1f3","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-seaside-dark.styl","hash":"ce233a101daea7124cbfcd34add43ccfe2e1e1c7","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"414b0cfc142f70afe359c16450b651e28bf7325a","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"efa52713efc468abeeb2b9299704371583b857de","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/brown-paper.styl","hash":"c2326ba20a5020a66ca7895258d18833327d4334","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/codepen-embed.styl","hash":"f4dcc84d8e39f9831a5efe80e51923fc3054feb0","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/color-brewer.styl","hash":"2a439d6214430e2f45dd4939b4dfe1fe1a20aa0f","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/dark.styl","hash":"71ce56d311cc2f3a605f6e2c495ccd7236878404","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/darkula.styl","hash":"ad0d5728d21645039c9f199e7a56814170ed3bab","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/docco.styl","hash":"b1c176378bb275f2e8caa759f36294e42d614bf1","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/far.styl","hash":"d9928010ffe71e80b97a5afcba1a4975efdd7372","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/foundation.styl","hash":"bf8ddc94b4ad995b8b8805b5a4cf95004553fdac","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/github-gist.styl","hash":"48211a03d33e7f7ada0b261162bea06676155a71","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/github.styl","hash":"3336aeba324c6d34a6fd41fef9b47bc598f7064c","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/googlecode.styl","hash":"bda816beee7b439814b514e6869dc678822be1bc","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/grayscale.styl","hash":"bf37d8b8d1e602126c51526f0cc28807440228ed","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/hopscotch.styl","hash":"b374c6550b89b4751aedc8fbc3cf98d95bd70ead","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/atelier-savanna-light.styl","hash":"f8244c93711c7cb59dd79d2df966806b30d171ea","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/highlightjs.styl","hash":"0e198b7a59191c7a39b641a4ddd22c948edb9358","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/index.styl","hash":"d421ed06c84f7a561b293f662a670bf132d41c63","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/atelier-seaside-light.styl","hash":"0597342da6e2d0c5bdcc7d42dabb07322b1a4177","modified":1598216749499},{"_id":"themes/hueman/source/css/_highlight/hybrid.styl","hash":"ea8d7ddc258b073308746385f5cb85aabb8bfb83","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/ir-black.styl","hash":"693078bbd72a2091ed30f506cc55949600b717af","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/kimbie.light.styl","hash":"61f8baed25be05288c8604d5070afbcd9f183f49","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/magula.styl","hash":"16d323f989b1420a0f72ef989242ece9bf17a456","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/mono-blue.styl","hash":"4c89a6ae29de67c0700585af82a60607e85df928","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/monokai-sublime.styl","hash":"25aa2fc1dbe38593e7c7ebe525438a39574d9935","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/monokai.styl","hash":"5a4fe9f957fd7a368c21b62a818403db4270452f","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/obsidian.styl","hash":"55572bbcfee1de6c31ac54681bb00336f5ae826d","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/paraiso-dark.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/idea.styl","hash":"a02967cb51c16a34e0ee895d33ded2b823d35b21","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/kimbie.dark.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/paraiso-light.styl","hash":"d224d1df0eb3395d9eea1344cee945c228af2911","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/pojoaque.styl","hash":"77dae9dc41945359d17fe84dbd317f1b40b2ee33","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/rainbow.styl","hash":"ce73b858fc0aba0e57ef9fb136c083082746bc1d","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/railscasts.styl","hash":"acd620f8bb7ff0e3fe5f9a22b4433ceef93a05e6","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/solarized-dark.styl","hash":"702b9299a48c90124e3ac1d45f1591042f2beccc","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/school-book.styl","hash":"d43560fe519a931ce6da7d57416d7aa148441b83","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/solarized-light.styl","hash":"aa0dd3fd25c464183b59c5575c9bee8756b397f2","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/sunburst.styl","hash":"a0b5b5129547a23865d400cfa562ea0ac1ee3958","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/tomorrow-night-bright.styl","hash":"0ac6af6ecb446b5b60d6226748e4a6532db34f57","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/tomorrow-night-blue.styl","hash":"8b3087d4422be6eb800935a22eb11e035341c4ba","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/tomorrow-night-eighties.styl","hash":"fa57b3bb7857a160fc856dbe319b31e30cc5d771","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/tomorrow-night.styl","hash":"19b3080d4b066b40d50d7e7f297472482b5801fd","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/tomorrow.styl","hash":"15779cf6846725c7c35fc56cac39047d7e0aec1c","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/archive.styl","hash":"5e5fb791ab54f0acf33850f586f7aa8cb2782f3a","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/article.styl","hash":"5f93edada8ba08100e1fee6a9f6f5cf4a35b6c7f","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/assets.styl","hash":"3d95417663c5a737f064a31ab4ef52bac7fda8df","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/vs.styl","hash":"959a746f4b37aacb5d1d6ff1d57e0c045289d75d","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/footer.styl","hash":"219f881c937735869ac4a1af1259e0d234aea9f3","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/header.styl","hash":"f94fcb4e2cbda45c8dc910ddb8ff4f19ff0644bf","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/insight.styl","hash":"3d66323e7b75ad197e80d7189a8d9216e1e1ef2f","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/nav.styl","hash":"587a9c9d304ed83eb0331a1d16693461805311c3","modified":1598216749503},{"_id":"themes/hueman/source/css/_partial/sidebar.styl","hash":"2217aa9c746107426d4830058aa67ea73f69f449","modified":1598216749503},{"_id":"themes/hueman/source/css/images/logo-header.png","hash":"a874be8f3e33831614a421d1a74d2c13bd5eba59","modified":1598216749507},{"_id":"themes/hueman/source/css/images/opacity-10.png","hash":"bbc979866c5b50e8adb348419154b28b1ff44d78","modified":1598216749507},{"_id":"themes/hueman/source/css/images/s-left.png","hash":"c8cac4f4e3492606fab93196364bd0f87d93bb98","modified":1598216749507},{"_id":"themes/hueman/source/css/_partial/comment.styl","hash":"d2de8f2c1cf6236ead0800c2a1566e01e7ae0b44","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/zenburn.styl","hash":"fc5ec840435dad80964d04519d3f882ddc03746a","modified":1598216749503},{"_id":"themes/hueman/source/css/_highlight/xcode.styl","hash":"5e8532ae8366dcf6a4ef5e4813dc3d42ab3d0a50","modified":1598216749503},{"_id":"themes/hueman/source/libs/justified-gallery/justifiedGallery.min.css","hash":"13fbcba5e97aa88b748d94d3efc4718475279907","modified":1598216749523},{"_id":"themes/hueman/source/libs/source-code-pro/styles.css","hash":"93c308012738728f906cd4c5cfdb34189e0c712b","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/styles.css","hash":"d98f0c50aae4c922cd0b663fa820fd7dff2dd9b1","modified":1598216749531},{"_id":"themes/hueman/source/css/images/thumb-default-small.png","hash":"e8403b97ed9251f9f5207765b0ce796c5000b4ba","modified":1598216749507},{"_id":"themes/hueman/source/libs/justified-gallery/jquery.justifiedGallery.min.js","hash":"b2683e7a872bc109b1756a65188a37cef7d0bd5c","modified":1598216749523},{"_id":"themes/hueman/source/libs/font-awesome/css/font-awesome.min.css","hash":"7cd5a3384333f95c3d37d9488ad82cd6c4b03761","modified":1598216749507},{"_id":"themes/hueman/source/libs/font-awesome/css/font-awesome.css","hash":"b5020c3860669185ba3f316fa7332cdf5c06f393","modified":1598216749507},{"_id":"themes/hueman/source/css/images/thumb-default.png","hash":"2d0ba175d958d342494241c616a74d37f48059fb","modified":1598216749507},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.css","hash":"844ce27b8488968bccb3e50bb49184ba2aae0625","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.min.css","hash":"05830fadb8454f39dcc98c8686eb4d5c24b71fc0","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-fb-comment-box.css.map","hash":"51e9df39edf0faa3f38c1bab0c1fa6c922b9edcb","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.css.map","hash":"50c3348638b4d82fa08a449c690e8d2bb593005d","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.css","hash":"bef55316a32e512d5a8940e5d0bfe8bf7a9c5c61","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.min.css","hash":"5c22e2073a4c96d6212c72135391b599e8d1359f","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.min.css","hash":"c9a2e19c932b56f4a2ce30c98910d10b74edb38a","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/css/lg-transitions.css","hash":"7871c28498d74451d6aa438c8d3a1817810a1e19","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/img/loading.gif","hash":"607810444094b8619fa4efa6273bc2a7e38dd4b4","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/img/video-play.png","hash":"3ea484cdc04d2e4547f80cbf80001dcf248c94ef","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/img/vimeo-play.png","hash":"6190254f2804904a4a1fa1eb390dfd334e416992","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/img/youtube-play.png","hash":"fea6df9d9d43151f9c9d15f000adb30eb3e26fc4","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-autoplay.min.js","hash":"d845741bcaf961579622880eb2a445257efad1ac","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-autoplay.js","hash":"426bb78b93acfc39d533ea2bab1cec8dc289cf24","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-fullscreen.min.js","hash":"b6b9e4022700b7faf2a5a175ba44a3bd938fdd20","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-fullscreen.js","hash":"65c47ac65362854ba44b00a010bb01e3630209d8","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-hash.min.js","hash":"43f1e1e720ab0e241c19b83aa26bd6848eab8edc","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-hash.js","hash":"15d16516c5642d3de1566ff8fc9160136ccaa405","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-pager.min.js","hash":"25caa6ff65b1c6dee09941e795ae2633bdbab211","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/css/lightgallery.css.map","hash":"3175b4107078674d25798979f7666f4daf31e624","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-share.min.js","hash":"39c615f07c5d3aaa65a2c3068a30fdd6dd5c372d","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-pager.js","hash":"8092c692b244bb26343eb03b91bd97deb9dafc9c","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-thumbnail.min.js","hash":"18dd7d2909d1bfd6852f031d03e774b4428c512b","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-share.js","hash":"b7fb5f6474911060a351b0a6fe9dbb9ac3fb22aa","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1598216749527},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-video.min.js","hash":"032c001ab045a69856f9c3ed4a2a3bf12a8e310f","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-zoom.min.js","hash":"15b49f9728439819ece15e4295cce254c87a4f45","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-thumbnail.js","hash":"3a6476b6df1d2bef4a21861a78776282a7a11ef1","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lightgallery.min.js","hash":"956ef9b706755318da69ad0b5d7786339d831251","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-video.js","hash":"4f99b598f6bb18de9eca8c45c5b4373a03962367","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lg-zoom.js","hash":"a758e2c8fcf710f9ff761da0eea0ab9321f3484d","modified":1598216749531},{"_id":"themes/hueman/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","hash":"942addaec4d3a60af33947a84a3d85f926015947","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcZSnX671uNZIV63UdXh3Mg0.woff2","hash":"78029561e4c2ec565ea11c3f5bbd052b018af8a6","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr4-67659ICLY8bMrYhtePPA.woff2","hash":"4e5557954ec161edc03b6f971ddefee6179c1305","modified":1598216749531},{"_id":"themes/hueman/source/libs/lightgallery/js/lightgallery.js","hash":"3cd19b33ba99efd5ba1d167da91720566d274b2c","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr46gJz9aNFrmnwBdd69aqzY.woff2","hash":"1454a4753468b607c23deac9f5438cd0ed5cb35d","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/7XUFZ5tgS-tD6QamInJTcSo_WB_cotcEMUw1LsIE8mM.woff2","hash":"6d17eac7fcc2866f10d1f2725a08ab749a6e978d","modified":1598216749531},{"_id":"themes/hueman/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","hash":"b0e0bb5ef78db8b15d430d0b9be9d4329289a310","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr9INifKjd1RJ3NxxEi9Cy2w.woff2","hash":"1758c64c8acec4497735ccb5336b1a518d24024c","modified":1598216749531},{"_id":"themes/hueman/source/libs/titillium-web/fonts/anMUvcNT0H1YN4FII8wpr_SNRT0fZ5CX-AqRkMYgJJo.woff2","hash":"e2e2993940fc54ed41f26e39257fdbd824c05e81","modified":1598216749531},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1598216749511},{"_id":"themes/hueman/source/libs/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1598216749511},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1598216749523},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1598216749523},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1598216749523},{"_id":"themes/hueman/source/libs/jquery/3.5.0/jquery.min.js","hash":"206ed314e32bca5b801434367d742c898572aea6","modified":1598216749523},{"_id":"themes/hueman/source/libs/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1598216749519}],"Category":[{"name":"Business Intelligence","_id":"ckeaq99rs0004sdjxe6r22zha"},{"name":"Blog","_id":"ckeaq99s30008sdjxgds0bl3v"},{"name":"Big Data","_id":"ckeaq99s9000csdjx9rtz88dt"},{"name":"Linux","_id":"ckeaq99t5000osdjx4n4m8jtd"},{"name":"Pentaho","_id":"ckeaq99tn0010sdjx1jcnebve"},{"name":"Misc","_id":"ckeaq99wk001vsdjx49pl7hid"},{"name":"Docker","_id":"ckeaq99wv0024sdjxecebh2z8"},{"name":"R","_id":"ckeaq99ya0030sdjxfcbn4mxo"},{"name":"R Markdown Cookbook","_id":"ckeaq9a08004csdjxcp2y48qd"}],"Data":[],"Page":[],"Post":[{"title":"Access a MySQL Server Remotely","id":"3316","comments":0,"date":"2016-05-31T08:49:13.000Z","_content":"\n[![logo-mysql](http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png)](http://edpflager.com/?attachment_id=2588#main)A quick one today: While working on a project, I couldn't access the MySQL server (version 5.7.12) that was on another system. I was in a development environment on a local network with just me on in, so the MySQL server did not have a firewall running. Here is what I did to get my connection to work.\n\n1.  Add an Administrator user account with permissions to connect from any host:\n    \n    **`CREATE USER 'edpflager'@'%' IDENTIFIED BY 'my_password';`** **`GRANT ALL PRIVILEGES ON *.* TO 'edpflager'@'%'`** **`WITH GRANT OPTION;`**\n    \n2.  Next open a terminal prompt on the MySQL server, and navigate to /etc/mysql/mysql.conf.d\n3.  Open a text editor as superuser  and edit mysqld.cnf\n    \n    sudo nano ./mysqld.cnf\n    \n4.  Find the following line and add a # to the beginning to comment it out:\n    \n    bind-address = 127.0.0.1\n    \n5.  Save, exit, and restart MySQL to make it take effect.\n\n","source":"_posts/access-a-mysql-server-remotely.md","raw":"---\ntitle: Access a MySQL Server Remotely\ntags:\n  - cookbook\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - kettle\n  - MySQL\n  - PDI\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3316'\ncategories:\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2016-05-31 04:49:13\n---\n\n[![logo-mysql](http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png)](http://edpflager.com/?attachment_id=2588#main)A quick one today: While working on a project, I couldn't access the MySQL server (version 5.7.12) that was on another system. I was in a development environment on a local network with just me on in, so the MySQL server did not have a firewall running. Here is what I did to get my connection to work.\n\n1.  Add an Administrator user account with permissions to connect from any host:\n    \n    **`CREATE USER 'edpflager'@'%' IDENTIFIED BY 'my_password';`** **`GRANT ALL PRIVILEGES ON *.* TO 'edpflager'@'%'`** **`WITH GRANT OPTION;`**\n    \n2.  Next open a terminal prompt on the MySQL server, and navigate to /etc/mysql/mysql.conf.d\n3.  Open a text editor as superuser  and edit mysqld.cnf\n    \n    sudo nano ./mysqld.cnf\n    \n4.  Find the following line and add a # to the beginning to comment it out:\n    \n    bind-address = 127.0.0.1\n    \n5.  Save, exit, and restart MySQL to make it take effect.\n\n","slug":"access-a-mysql-server-remotely","published":1,"updated":"2020-08-23T21:33:29.857Z","layout":"post","photos":[],"link":"","_id":"ckeaq99qu0000sdjx5fandf4v","content":"<p><a href=\"http://edpflager.com/?attachment_id=2588#main\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png\" alt=\"logo-mysql\"></a>A quick one today: While working on a project, I couldn’t access the MySQL server (version 5.7.12) that was on another system. I was in a development environment on a local network with just me on in, so the MySQL server did not have a firewall running. Here is what I did to get my connection to work.</p>\n<ol>\n<li><p>Add an Administrator user account with permissions to connect from any host:</p>\n<p><strong><code>CREATE USER &#39;edpflager&#39;@&#39;%&#39; IDENTIFIED BY &#39;my_password&#39;;</code></strong> <strong><code>GRANT ALL PRIVILEGES ON *.* TO &#39;edpflager&#39;@&#39;%&#39;</code></strong> <strong><code>WITH GRANT OPTION;</code></strong></p>\n</li>\n<li><p>Next open a terminal prompt on the MySQL server, and navigate to /etc/mysql/mysql.conf.d</p>\n</li>\n<li><p>Open a text editor as superuser  and edit mysqld.cnf</p>\n<p>sudo nano ./mysqld.cnf</p>\n</li>\n<li><p>Find the following line and add a # to the beginning to comment it out:</p>\n<p>bind-address = 127.0.0.1</p>\n</li>\n<li><p>Save, exit, and restart MySQL to make it take effect.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/?attachment_id=2588#main\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png\" alt=\"logo-mysql\"></a>A quick one today: While working on a project, I couldn’t access the MySQL server (version 5.7.12) that was on another system. I was in a development environment on a local network with just me on in, so the MySQL server did not have a firewall running. Here is what I did to get my connection to work.</p>\n<ol>\n<li><p>Add an Administrator user account with permissions to connect from any host:</p>\n<p><strong><code>CREATE USER &#39;edpflager&#39;@&#39;%&#39; IDENTIFIED BY &#39;my_password&#39;;</code></strong> <strong><code>GRANT ALL PRIVILEGES ON *.* TO &#39;edpflager&#39;@&#39;%&#39;</code></strong> <strong><code>WITH GRANT OPTION;</code></strong></p>\n</li>\n<li><p>Next open a terminal prompt on the MySQL server, and navigate to /etc/mysql/mysql.conf.d</p>\n</li>\n<li><p>Open a text editor as superuser  and edit mysqld.cnf</p>\n<p>sudo nano ./mysqld.cnf</p>\n</li>\n<li><p>Find the following line and add a # to the beginning to comment it out:</p>\n<p>bind-address = 127.0.0.1</p>\n</li>\n<li><p>Save, exit, and restart MySQL to make it take effect.</p>\n</li>\n</ol>\n"},{"title":"Add Pentaho to your CentOS Application menu","id":"2787","comments":0,"date":"2015-06-12T23:14:58.000Z","_content":"\n[![menu](http://edpflager.com/wp-content/uploads/2015/06/menu-180x300.jpg)](http://edpflager.com/wp-content/uploads/2015/06/menu.jpg)A while back, I posted on how to get [Pentaho Data Integration to launch from a desktop shortcut.](http://edpflager.com/?p=2322) Recently, I've installed CentOS7 with Gnome and wanted to install a menu item for PDI. While not too difficult, the process isn't streamlined simple either, so I thought it would be good to document it. Start with the same process I've covered before for setting up a \"start-pentaho.sh\" bash script. Once you have it created, copy the file (using the root account) to the folder where you installed Pentaho on your system. In my case,  it is under /opt/pentaho/data-integration, so I copied the \"start-pentaho.sh\" file to the /opt/pentaho folder and renamed it to \"start-spoon.sh\".\n<!-- more -->\nNext, with a text editor, create a file called PentahoDataIntegration.desktop in your home folder with the following contents: \\[Desktop Entry\\] Encoding=UTF-8 Name=PentahoDataIntegration GenericName=PentahoDataIntegration Exec=/opt/pentaho/start-spoon.sh Terminal=true Icon=/opt/pentaho/data-integration/spoon.png Type=Application Save the file. From a terminal, as root, move the .desktop file from your home folder to  /usr/share/applications. Once its in the new location, in your menu bar, click on Applications, and go down to the (new) Other option. In that folder, you will see an entry for Pentaho Data Integration. [![pdi-menu](http://edpflager.com/wp-content/uploads/2015/06/pdi-menu-300x261.png)](http://edpflager.com/wp-content/uploads/2015/06/pdi-menu.png)Click the PDI entry, and a new terminal window will open, and after a few seconds, the PDI splash screen should appear!","source":"_posts/add-a-pentaho-system-menu-option-in-centos-7.md","raw":"---\ntitle: Add Pentaho to your CentOS Application menu\ntags:\n  - centos\n  - ETL\n  - How-to\n  - howto\n  - install\n  - kettle\n  - Linux\n  - technical\nid: '2787'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2015-06-12 19:14:58\n---\n\n[![menu](http://edpflager.com/wp-content/uploads/2015/06/menu-180x300.jpg)](http://edpflager.com/wp-content/uploads/2015/06/menu.jpg)A while back, I posted on how to get [Pentaho Data Integration to launch from a desktop shortcut.](http://edpflager.com/?p=2322) Recently, I've installed CentOS7 with Gnome and wanted to install a menu item for PDI. While not too difficult, the process isn't streamlined simple either, so I thought it would be good to document it. Start with the same process I've covered before for setting up a \"start-pentaho.sh\" bash script. Once you have it created, copy the file (using the root account) to the folder where you installed Pentaho on your system. In my case,  it is under /opt/pentaho/data-integration, so I copied the \"start-pentaho.sh\" file to the /opt/pentaho folder and renamed it to \"start-spoon.sh\".\n<!-- more -->\nNext, with a text editor, create a file called PentahoDataIntegration.desktop in your home folder with the following contents: \\[Desktop Entry\\] Encoding=UTF-8 Name=PentahoDataIntegration GenericName=PentahoDataIntegration Exec=/opt/pentaho/start-spoon.sh Terminal=true Icon=/opt/pentaho/data-integration/spoon.png Type=Application Save the file. From a terminal, as root, move the .desktop file from your home folder to  /usr/share/applications. Once its in the new location, in your menu bar, click on Applications, and go down to the (new) Other option. In that folder, you will see an entry for Pentaho Data Integration. [![pdi-menu](http://edpflager.com/wp-content/uploads/2015/06/pdi-menu-300x261.png)](http://edpflager.com/wp-content/uploads/2015/06/pdi-menu.png)Click the PDI entry, and a new terminal window will open, and after a few seconds, the PDI splash screen should appear!","slug":"add-a-pentaho-system-menu-option-in-centos-7","published":1,"updated":"2020-08-23T20:54:34.950Z","layout":"post","photos":[],"link":"","_id":"ckeaq99re0001sdjx2opubyx3","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/menu.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/menu-180x300.jpg\" alt=\"menu\"></a>A while back, I posted on how to get <a href=\"http://edpflager.com/?p=2322\">Pentaho Data Integration to launch from a desktop shortcut.</a> Recently, I’ve installed CentOS7 with Gnome and wanted to install a menu item for PDI. While not too difficult, the process isn’t streamlined simple either, so I thought it would be good to document it. Start with the same process I’ve covered before for setting up a “start-pentaho.sh” bash script. Once you have it created, copy the file (using the root account) to the folder where you installed Pentaho on your system. In my case,  it is under /opt/pentaho/data-integration, so I copied the “start-pentaho.sh” file to the /opt/pentaho folder and renamed it to “start-spoon.sh”.</p>\n<a id=\"more\"></a>\n<p>Next, with a text editor, create a file called PentahoDataIntegration.desktop in your home folder with the following contents: [Desktop Entry] Encoding=UTF-8 Name=PentahoDataIntegration GenericName=PentahoDataIntegration Exec=/opt/pentaho/start-spoon.sh Terminal=true Icon=/opt/pentaho/data-integration/spoon.png Type=Application Save the file. From a terminal, as root, move the .desktop file from your home folder to  /usr/share/applications. Once its in the new location, in your menu bar, click on Applications, and go down to the (new) Other option. In that folder, you will see an entry for Pentaho Data Integration. <a href=\"http://edpflager.com/wp-content/uploads/2015/06/pdi-menu.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/pdi-menu-300x261.png\" alt=\"pdi-menu\"></a>Click the PDI entry, and a new terminal window will open, and after a few seconds, the PDI splash screen should appear!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/menu.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/menu-180x300.jpg\" alt=\"menu\"></a>A while back, I posted on how to get <a href=\"http://edpflager.com/?p=2322\">Pentaho Data Integration to launch from a desktop shortcut.</a> Recently, I’ve installed CentOS7 with Gnome and wanted to install a menu item for PDI. While not too difficult, the process isn’t streamlined simple either, so I thought it would be good to document it. Start with the same process I’ve covered before for setting up a “start-pentaho.sh” bash script. Once you have it created, copy the file (using the root account) to the folder where you installed Pentaho on your system. In my case,  it is under /opt/pentaho/data-integration, so I copied the “start-pentaho.sh” file to the /opt/pentaho folder and renamed it to “start-spoon.sh”.</p>","more":"<p>Next, with a text editor, create a file called PentahoDataIntegration.desktop in your home folder with the following contents: [Desktop Entry] Encoding=UTF-8 Name=PentahoDataIntegration GenericName=PentahoDataIntegration Exec=/opt/pentaho/start-spoon.sh Terminal=true Icon=/opt/pentaho/data-integration/spoon.png Type=Application Save the file. From a terminal, as root, move the .desktop file from your home folder to  /usr/share/applications. Once its in the new location, in your menu bar, click on Applications, and go down to the (new) Other option. In that folder, you will see an entry for Pentaho Data Integration. <a href=\"http://edpflager.com/wp-content/uploads/2015/06/pdi-menu.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/pdi-menu-300x261.png\" alt=\"pdi-menu\"></a>Click the PDI entry, and a new terminal window will open, and after a few seconds, the PDI splash screen should appear!</p>"},{"title":"Add Apache Spark to Cloudera","id":"1877","comments":0,"date":"2014-03-20T19:32:18.000Z","_content":"\n[![Spark-logo-192x100px](http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png)](http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png)Just a quick note today. A couple of weeks ago, the [Spark](http://spark.apache.org/) project over at Apache graduated to a top-level project and it can now be integrated into your Cloudera environment very easily! Spark is a Hadoop integrated in-memory data analytic framework that uses HDFS (the Hadoop file system) to run programs 100x faster than MapReduce. Speed when using disk isn't quite as fast, just a 10x faster claim than HDFS. It supports a number of different programming languages (Python, Java, Scala), can be used with UC Berkeley's Shark application to see those same speed increases with Hive, and it can read from HBase and Cassandra data sources as well. If you'd like to add Spark to your existing Cloudera cluster, head on over to [Cloudera's website](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/cmig_spark_installation_standalone.html) for instructions on how to install it.","source":"_posts/add-apache-spark-to-cloudera.md","raw":"---\ntitle: Add Apache Spark to Cloudera\ntags:\n  - CDH\n  - Hadoop\nid: '1877'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2014-03-20 15:32:18\n---\n\n[![Spark-logo-192x100px](http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png)](http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png)Just a quick note today. A couple of weeks ago, the [Spark](http://spark.apache.org/) project over at Apache graduated to a top-level project and it can now be integrated into your Cloudera environment very easily! Spark is a Hadoop integrated in-memory data analytic framework that uses HDFS (the Hadoop file system) to run programs 100x faster than MapReduce. Speed when using disk isn't quite as fast, just a 10x faster claim than HDFS. It supports a number of different programming languages (Python, Java, Scala), can be used with UC Berkeley's Shark application to see those same speed increases with Hive, and it can read from HBase and Cassandra data sources as well. If you'd like to add Spark to your existing Cloudera cluster, head on over to [Cloudera's website](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/cmig_spark_installation_standalone.html) for instructions on how to install it.","slug":"add-apache-spark-to-cloudera","published":1,"updated":"2020-08-23T20:54:34.806Z","layout":"post","photos":[],"link":"","_id":"ckeaq99rr0003sdjxctj07si9","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png\" alt=\"Spark-logo-192x100px\"></a>Just a quick note today. A couple of weeks ago, the <a href=\"http://spark.apache.org/\">Spark</a> project over at Apache graduated to a top-level project and it can now be integrated into your Cloudera environment very easily! Spark is a Hadoop integrated in-memory data analytic framework that uses HDFS (the Hadoop file system) to run programs 100x faster than MapReduce. Speed when using disk isn’t quite as fast, just a 10x faster claim than HDFS. It supports a number of different programming languages (Python, Java, Scala), can be used with UC Berkeley’s Shark application to see those same speed increases with Hive, and it can read from HBase and Cassandra data sources as well. If you’d like to add Spark to your existing Cloudera cluster, head on over to <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/cmig_spark_installation_standalone.html\">Cloudera’s website</a> for instructions on how to install it.</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/Spark-logo-192x100px.png\" alt=\"Spark-logo-192x100px\"></a>Just a quick note today. A couple of weeks ago, the <a href=\"http://spark.apache.org/\">Spark</a> project over at Apache graduated to a top-level project and it can now be integrated into your Cloudera environment very easily! Spark is a Hadoop integrated in-memory data analytic framework that uses HDFS (the Hadoop file system) to run programs 100x faster than MapReduce. Speed when using disk isn’t quite as fast, just a 10x faster claim than HDFS. It supports a number of different programming languages (Python, Java, Scala), can be used with UC Berkeley’s Shark application to see those same speed increases with Hive, and it can read from HBase and Cassandra data sources as well. If you’d like to add Spark to your existing Cloudera cluster, head on over to <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/cmig_spark_installation_standalone.html\">Cloudera’s website</a> for instructions on how to install it.</p>\n"},{"title":"Adding Thrift server to a Cloudera Hadoop Cluster","id":"1764","comments":0,"date":"2014-02-10T21:22:16.000Z","_content":"\n[![thriftserver](http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg)](http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg)This is a continuation of my series on setting up a Hadoop Cluster using Cloudera's distribution. When using the HBASE application, or Impala, you may receive errors about the THRIFT service being unavailable. From what I have found this is because Cloudera doesn't install the THRIFT service as part of the automated installation.\n<!-- more -->\nBefore we dig into getting Thift running on your Hadoop cluster, lets take a minute to understand what Thrift is designed to be: Its a light weight bridge across various programming languages allowing applications built with those languages to communicate and pass data. Developed initially [by Facebook](http://thrift.apache.org/static/files/thrift-20070401.pdf), it has been open sourced and is now housed in the Apache Software Foundation library. Because the applications that make up the Hadoop ecosystem were created with many different computer languages, Thrift allows those applications to communicate and pass data back and forth.\n\n## Installation\n\nTo install it, first download and copy the repository file for your distribution from this [page](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_4_4.html#topic_4_4_2_unique_1) and follow the instructions for installing it. Then install the service using your distributions installation command. For CentOS, I entered **yum install hbase-thrift** as root. It will take several minutes to install while all the dependencies are linked in. Once you are returned to the command prompt, enter the following to start the service. **sudo service hbase-thrift start** Now restart your HBase server, and you should be good to go.","source":"_posts/adding-thrift-server-to-a-cloudera-hadoop-cluster.md","raw":"---\ntitle: Adding Thrift server to a Cloudera Hadoop Cluster\ntags:\n  - Big Data\n  - CDH\n  - Hadoop\nid: '1764'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-02-10 16:22:16\n---\n\n[![thriftserver](http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg)](http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg)This is a continuation of my series on setting up a Hadoop Cluster using Cloudera's distribution. When using the HBASE application, or Impala, you may receive errors about the THRIFT service being unavailable. From what I have found this is because Cloudera doesn't install the THRIFT service as part of the automated installation.\n<!-- more -->\nBefore we dig into getting Thift running on your Hadoop cluster, lets take a minute to understand what Thrift is designed to be: Its a light weight bridge across various programming languages allowing applications built with those languages to communicate and pass data. Developed initially [by Facebook](http://thrift.apache.org/static/files/thrift-20070401.pdf), it has been open sourced and is now housed in the Apache Software Foundation library. Because the applications that make up the Hadoop ecosystem were created with many different computer languages, Thrift allows those applications to communicate and pass data back and forth.\n\n## Installation\n\nTo install it, first download and copy the repository file for your distribution from this [page](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_4_4.html#topic_4_4_2_unique_1) and follow the instructions for installing it. Then install the service using your distributions installation command. For CentOS, I entered **yum install hbase-thrift** as root. It will take several minutes to install while all the dependencies are linked in. Once you are returned to the command prompt, enter the following to start the service. **sudo service hbase-thrift start** Now restart your HBase server, and you should be good to go.","slug":"adding-thrift-server-to-a-cloudera-hadoop-cluster","published":1,"updated":"2020-08-23T20:54:34.778Z","layout":"post","photos":[],"link":"","_id":"ckeaq99ru0005sdjx3oyz216e","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg\" alt=\"thriftserver\"></a>This is a continuation of my series on setting up a Hadoop Cluster using Cloudera’s distribution. When using the HBASE application, or Impala, you may receive errors about the THRIFT service being unavailable. From what I have found this is because Cloudera doesn’t install the THRIFT service as part of the automated installation.</p>\n<a id=\"more\"></a>\n<p>Before we dig into getting Thift running on your Hadoop cluster, lets take a minute to understand what Thrift is designed to be: Its a light weight bridge across various programming languages allowing applications built with those languages to communicate and pass data. Developed initially <a href=\"http://thrift.apache.org/static/files/thrift-20070401.pdf\">by Facebook</a>, it has been open sourced and is now housed in the Apache Software Foundation library. Because the applications that make up the Hadoop ecosystem were created with many different computer languages, Thrift allows those applications to communicate and pass data back and forth.</p>\n<h2 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h2><p>To install it, first download and copy the repository file for your distribution from this <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_4_4.html#topic_4_4_2_unique_1\">page</a> and follow the instructions for installing it. Then install the service using your distributions installation command. For CentOS, I entered <strong>yum install hbase-thrift</strong> as root. It will take several minutes to install while all the dependencies are linked in. Once you are returned to the command prompt, enter the following to start the service. <strong>sudo service hbase-thrift start</strong> Now restart your HBase server, and you should be good to go.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/thriftserver.jpg\" alt=\"thriftserver\"></a>This is a continuation of my series on setting up a Hadoop Cluster using Cloudera’s distribution. When using the HBASE application, or Impala, you may receive errors about the THRIFT service being unavailable. From what I have found this is because Cloudera doesn’t install the THRIFT service as part of the automated installation.</p>","more":"<p>Before we dig into getting Thift running on your Hadoop cluster, lets take a minute to understand what Thrift is designed to be: Its a light weight bridge across various programming languages allowing applications built with those languages to communicate and pass data. Developed initially <a href=\"http://thrift.apache.org/static/files/thrift-20070401.pdf\">by Facebook</a>, it has been open sourced and is now housed in the Apache Software Foundation library. Because the applications that make up the Hadoop ecosystem were created with many different computer languages, Thrift allows those applications to communicate and pass data back and forth.</p>\n<h2 id=\"Installation\"><a href=\"#Installation\" class=\"headerlink\" title=\"Installation\"></a>Installation</h2><p>To install it, first download and copy the repository file for your distribution from this <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-Installation-Guide/cdh4ig_topic_4_4.html#topic_4_4_2_unique_1\">page</a> and follow the instructions for installing it. Then install the service using your distributions installation command. For CentOS, I entered <strong>yum install hbase-thrift</strong> as root. It will take several minutes to install while all the dependencies are linked in. Once you are returned to the command prompt, enter the following to start the service. <strong>sudo service hbase-thrift start</strong> Now restart your HBase server, and you should be good to go.</p>"},{"title":"Align Graphics in R Markdown Documents","id":"4115","comments":0,"date":"2018-10-31T07:46:18.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)This time around I am covering aligning graphics in PDF putout for R Markdown. Unfortunately this tip only works in PDF output, not in HTML or Word. Occasionally, perhaps rarely for some, you may want to place two graphics next to each other in your R Markdown documents. By default you can have the bottom edge of your graphics (aka the baseline) align easily, using the [technique](http://edpflager.com/2018/10/09/embedding-graphics/) I've covered previously:\n\n!\\[\\](3inchSquare.png) !\\[\\](1inchSquare.png)\n\nThe output that is produced does have a gap between the images though, because you need a separation in the code between the first image and the second one so the KNITR engine doesn't get confused.\n<!-- more -->\nTo make the images flush against each other, we can use the LaTeX function graphicx by specifying it in the YAML header. Then we call our images in one line of code:\n\nincludegraphics\\[\\]{images/3inchsquare.png}includegraphics\\[\\]{images/1inchsquare.png}\n\nHere is a comparison of the resulting output: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignImages-206x300.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignImages.png) By using the raisebox command that is part of the graphicx function, we get additional control over our image placement. The syntax for the raisebox command is simple: **raisebox{height to raise image}** and follow it with an includegraphics command enclosed in braces. For example if we want to have the images aligned on the center of each image, we can use a calculated height formula, followed by the image, and then a second raisebox command, followed by a height calculation. So for our two boxes, the command would be:\n\nraisebox{-0.5height}{includegraphics\\[\\]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics\\[\\]{images/1inchsquare.png}}\n\nWhich results in this placement of the images: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignCenter-150x150.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignCenter.png) You can also align your images along the topline of the images, using this command:\n\nraisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}\n\nThe resulting placement in your PDF document output would look like this: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignTop-150x150.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignTop.png) There are a number of other placements that are possible using **raisebox**, and I have illustrated some of them in the code for my example R Markdown document below.\n\n\\---\ntitle: \"Aligning Graphics\"\noutput:\n   pdf\\_document\nheader-includes: usepackage{graphicx} \n\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nDefault placement\n\n!\\[\\](3inchSquare.png) !\\[\\](1inchSquare.png)\n\n\\*\\*\\*\n\nincludegraphics\\[\\]{images/3inchsquare.png}includegraphics\\[\\]{images/1inchsquare.png}\n\n\n\\*\\*\\*\n\nPuts the baseline at midway of the first image. The second images bottom will be placed there.\n\nraisebox{-0.5height}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nPuts the baseline at a specific point of the first image. The second image's bottom will be placed there. Use a negative value! \n\nraisebox{-1in}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nIf you use a positive value, the second image's bottom will be placed at that point below the first image's bottom edge. \n\nraisebox{1.5in}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nraisebox{-0.5height}{includegraphics\\[\\]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\* \n\nTop align both figures\n\nraisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}","source":"_posts/align-graphics-in-r-markdown-documents.md","raw":"---\ntitle: Align Graphics in R Markdown Documents\ntags:\n  - howto\n  - R Markdown\n  - technical\nid: '4115'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-10-31 03:46:18\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)This time around I am covering aligning graphics in PDF putout for R Markdown. Unfortunately this tip only works in PDF output, not in HTML or Word. Occasionally, perhaps rarely for some, you may want to place two graphics next to each other in your R Markdown documents. By default you can have the bottom edge of your graphics (aka the baseline) align easily, using the [technique](http://edpflager.com/2018/10/09/embedding-graphics/) I've covered previously:\n\n!\\[\\](3inchSquare.png) !\\[\\](1inchSquare.png)\n\nThe output that is produced does have a gap between the images though, because you need a separation in the code between the first image and the second one so the KNITR engine doesn't get confused.\n<!-- more -->\nTo make the images flush against each other, we can use the LaTeX function graphicx by specifying it in the YAML header. Then we call our images in one line of code:\n\nincludegraphics\\[\\]{images/3inchsquare.png}includegraphics\\[\\]{images/1inchsquare.png}\n\nHere is a comparison of the resulting output: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignImages-206x300.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignImages.png) By using the raisebox command that is part of the graphicx function, we get additional control over our image placement. The syntax for the raisebox command is simple: **raisebox{height to raise image}** and follow it with an includegraphics command enclosed in braces. For example if we want to have the images aligned on the center of each image, we can use a calculated height formula, followed by the image, and then a second raisebox command, followed by a height calculation. So for our two boxes, the command would be:\n\nraisebox{-0.5height}{includegraphics\\[\\]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics\\[\\]{images/1inchsquare.png}}\n\nWhich results in this placement of the images: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignCenter-150x150.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignCenter.png) You can also align your images along the topline of the images, using this command:\n\nraisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}\n\nThe resulting placement in your PDF document output would look like this: [![](http://edpflager.com/wp-content/uploads/2018/10/AlignTop-150x150.png)](http://edpflager.com/wp-content/uploads/2018/10/AlignTop.png) There are a number of other placements that are possible using **raisebox**, and I have illustrated some of them in the code for my example R Markdown document below.\n\n\\---\ntitle: \"Aligning Graphics\"\noutput:\n   pdf\\_document\nheader-includes: usepackage{graphicx} \n\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nDefault placement\n\n!\\[\\](3inchSquare.png) !\\[\\](1inchSquare.png)\n\n\\*\\*\\*\n\nincludegraphics\\[\\]{images/3inchsquare.png}includegraphics\\[\\]{images/1inchsquare.png}\n\n\n\\*\\*\\*\n\nPuts the baseline at midway of the first image. The second images bottom will be placed there.\n\nraisebox{-0.5height}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nPuts the baseline at a specific point of the first image. The second image's bottom will be placed there. Use a negative value! \n\nraisebox{-1in}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nIf you use a positive value, the second image's bottom will be placed at that point below the first image's bottom edge. \n\nraisebox{1.5in}{includegraphics\\[height=3in\\]{images/3inchsquare.png}}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\*\n\nraisebox{-0.5height}{includegraphics\\[\\]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics\\[\\]{images/1inchsquare.png}}\n\n\\*\\*\\* \n\nTop align both figures\n\nraisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}","slug":"align-graphics-in-r-markdown-documents","published":1,"updated":"2020-08-23T20:54:35.178Z","layout":"post","photos":[],"link":"","_id":"ckeaq99s10006sdjxd42i1syq","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>This time around I am covering aligning graphics in PDF putout for R Markdown. Unfortunately this tip only works in PDF output, not in HTML or Word. Occasionally, perhaps rarely for some, you may want to place two graphics next to each other in your R Markdown documents. By default you can have the bottom edge of your graphics (aka the baseline) align easily, using the <a href=\"http://edpflager.com/2018/10/09/embedding-graphics/\">technique</a> I’ve covered previously:</p>\n<p>![](3inchSquare.png) ![](1inchSquare.png)</p>\n<p>The output that is produced does have a gap between the images though, because you need a separation in the code between the first image and the second one so the KNITR engine doesn’t get confused.</p>\n<a id=\"more\"></a>\n<p>To make the images flush against each other, we can use the LaTeX function graphicx by specifying it in the YAML header. Then we call our images in one line of code:</p>\n<p>includegraphics[]{images/3inchsquare.png}includegraphics[]{images/1inchsquare.png}</p>\n<p>Here is a comparison of the resulting output: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignImages.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignImages-206x300.png\"></a> By using the raisebox command that is part of the graphicx function, we get additional control over our image placement. The syntax for the raisebox command is simple: <strong>raisebox{height to raise image}</strong> and follow it with an includegraphics command enclosed in braces. For example if we want to have the images aligned on the center of each image, we can use a calculated height formula, followed by the image, and then a second raisebox command, followed by a height calculation. So for our two boxes, the command would be:</p>\n<p>raisebox{-0.5height}{includegraphics[]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>Which results in this placement of the images: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignCenter.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignCenter-150x150.png\"></a> You can also align your images along the topline of the images, using this command:</p>\n<p>raisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}</p>\n<p>The resulting placement in your PDF document output would look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignTop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignTop-150x150.png\"></a> There are a number of other placements that are possible using <strong>raisebox</strong>, and I have illustrated some of them in the code for my example R Markdown document below.</p>\n<p>-–<br>title: “Aligning Graphics”<br>output:<br>   pdf_document<br>header-includes: usepackage{graphicx} </p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>Default placement</p>\n<p>![](3inchSquare.png) ![](1inchSquare.png)</p>\n<p>***</p>\n<p>includegraphics[]{images/3inchsquare.png}includegraphics[]{images/1inchsquare.png}</p>\n<p>***</p>\n<p>Puts the baseline at midway of the first image. The second images bottom will be placed there.</p>\n<p>raisebox{-0.5height}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>Puts the baseline at a specific point of the first image. The second image’s bottom will be placed there. Use a negative value! </p>\n<p>raisebox{-1in}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>If you use a positive value, the second image’s bottom will be placed at that point below the first image’s bottom edge. </p>\n<p>raisebox{1.5in}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>raisebox{-0.5height}{includegraphics[]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>*** </p>\n<p>Top align both figures</p>\n<p>raisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>This time around I am covering aligning graphics in PDF putout for R Markdown. Unfortunately this tip only works in PDF output, not in HTML or Word. Occasionally, perhaps rarely for some, you may want to place two graphics next to each other in your R Markdown documents. By default you can have the bottom edge of your graphics (aka the baseline) align easily, using the <a href=\"http://edpflager.com/2018/10/09/embedding-graphics/\">technique</a> I’ve covered previously:</p>\n<p>![](3inchSquare.png) ![](1inchSquare.png)</p>\n<p>The output that is produced does have a gap between the images though, because you need a separation in the code between the first image and the second one so the KNITR engine doesn’t get confused.</p>","more":"<p>To make the images flush against each other, we can use the LaTeX function graphicx by specifying it in the YAML header. Then we call our images in one line of code:</p>\n<p>includegraphics[]{images/3inchsquare.png}includegraphics[]{images/1inchsquare.png}</p>\n<p>Here is a comparison of the resulting output: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignImages.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignImages-206x300.png\"></a> By using the raisebox command that is part of the graphicx function, we get additional control over our image placement. The syntax for the raisebox command is simple: <strong>raisebox{height to raise image}</strong> and follow it with an includegraphics command enclosed in braces. For example if we want to have the images aligned on the center of each image, we can use a calculated height formula, followed by the image, and then a second raisebox command, followed by a height calculation. So for our two boxes, the command would be:</p>\n<p>raisebox{-0.5height}{includegraphics[]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>Which results in this placement of the images: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignCenter.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignCenter-150x150.png\"></a> You can also align your images along the topline of the images, using this command:</p>\n<p>raisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}</p>\n<p>The resulting placement in your PDF document output would look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/AlignTop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/AlignTop-150x150.png\"></a> There are a number of other placements that are possible using <strong>raisebox</strong>, and I have illustrated some of them in the code for my example R Markdown document below.</p>\n<p>-–<br>title: “Aligning Graphics”<br>output:<br>   pdf_document<br>header-includes: usepackage{graphicx} </p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>Default placement</p>\n<p>![](3inchSquare.png) ![](1inchSquare.png)</p>\n<p>***</p>\n<p>includegraphics[]{images/3inchsquare.png}includegraphics[]{images/1inchsquare.png}</p>\n<p>***</p>\n<p>Puts the baseline at midway of the first image. The second images bottom will be placed there.</p>\n<p>raisebox{-0.5height}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>Puts the baseline at a specific point of the first image. The second image’s bottom will be placed there. Use a negative value! </p>\n<p>raisebox{-1in}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>If you use a positive value, the second image’s bottom will be placed at that point below the first image’s bottom edge. </p>\n<p>raisebox{1.5in}{includegraphics[height=3in]{images/3inchsquare.png}}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>***</p>\n<p>raisebox{-0.5height}{includegraphics[]{images/3inchsquare.png}}raisebox{-0.5height}{includegraphics[]{images/1inchsquare.png}}</p>\n<p>*** </p>\n<p>Top align both figures</p>\n<p>raisebox{-height}{includegraphics{images/3inchsquare.png}}raisebox{-height}{includegraphics{images/1inchsquare.png}}</p>"},{"title":"Android? Not so much for me? - Revisited","id":"2053","comments":0,"date":"2014-05-06T04:08:21.000Z","_content":"\n![Android_Robot_100](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)In June of last year, I posted about my trials with using an Android tablet, and why I decided at that point to stick with my iPad. Ten months later, I have revisited Android, and I'm happy to report that I am now using a Samsung Galaxy Tab 3 10.1 running Android 4.2 on a daily basis in place of my iPad. So what changed? I did, the IOS and Android operating systems did, and the market did. Read on for more info...\n<!-- more -->\nIn the time frame between my last post on Android and now:\n\n*   Apple updated IOS to version 7.0 and then 7.1. I wasn't crazy about the upgrade, from the visuals to the performance, and there wasn't anything that was a game changer for me. It was the same old, same old. Being a techie, I like new toys, and a span of about two years is a good run for a gadget. The iPad 2 I had was dated and the newer iPads weren't offering anything really radical. In fact most of Apple's products seem to be suffering from the same fate since Steve Jobs passed away. They are still solid performers, but there really isn't anything amazing about the newer iPads. The specs have improved, but essentially its the same device as my old one. So I started looking at Android again.\n*   Several magazine subscriptions I had on the iPad expired, and I chose not to renew them. The magazine store on iPad has a wide variety of magazines, but I realized the cost for a lot of them where you are locked into only being able to use them on an Apple device was too high. The Zinio application allows you to view magazine on multiple devices and through  the web, and several vendors are following that same model now with their periodicals.\n*   I discovered Dropbox and EverNote - two free cloud based applications that allow you to move content between your devices. Now I didn't need to physically hook up my table to get content to it anymore. In fact, in the past six months, I never synced my iPad to my computer at all.\n\nLooking back on that article some of the other reasons I stayed on the iPad are no longer relevant:\n\n*   Size: This time around, I tried out a larger Android tablet. Apple offers tables in two sizes, but with Android, you have more choices. The screen shape can be rectangular or square and the overall size can vary between 6-11 inches. The Samsung tablet I picked up was almost the same size and the iPad I had been using. As a bonus, the vendor I purchased through included a Bluetooth keyboard case with the tablet.\n*   Fragmented app market: The Samsung came bundled with an email app that allowed me to access all three email accounts I use regularly in one application. Plus the fact that I no longer use Yahoo email seems to have helped as well. :)\n*   I did a lot of research into e-reader software for Android and found several choices, that seemed to meet my needs: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few).  MoonReader+ with the PDF reader plugin has been working well for me. The only caveat being adding bookmarks could be easier.\n\nAnd some of the reasons I liked Android still hold true:\n\n*   Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.\n*   Expansion: From what I have found, you can add additional external storage to most Android devices. The Samsung tablet is a 16 GB model, and I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.\n*   Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I'm sure a major factor in the Android market share. The Samsung was about $150 cheaper than the comparable iPad model.\n*   Color: The 1280 x 800 resolution screen on the Samsung looks great. Its clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don't do a lot of gaming, but I intend to look into watching some videos on it soon.\n\nSo there you have it. Who said I was stubborn? _Android is a trademark of Google Inc._","source":"_posts/android-not-so-much-for-me-revisited.md","raw":"---\ntitle: Android? Not so much for me? - Revisited\ntags: []\nid: '2053'\ncategories:\n  - - Blog\ncomments: false\ndate: 2014-05-06 00:08:21\n---\n\n![Android_Robot_100](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)In June of last year, I posted about my trials with using an Android tablet, and why I decided at that point to stick with my iPad. Ten months later, I have revisited Android, and I'm happy to report that I am now using a Samsung Galaxy Tab 3 10.1 running Android 4.2 on a daily basis in place of my iPad. So what changed? I did, the IOS and Android operating systems did, and the market did. Read on for more info...\n<!-- more -->\nIn the time frame between my last post on Android and now:\n\n*   Apple updated IOS to version 7.0 and then 7.1. I wasn't crazy about the upgrade, from the visuals to the performance, and there wasn't anything that was a game changer for me. It was the same old, same old. Being a techie, I like new toys, and a span of about two years is a good run for a gadget. The iPad 2 I had was dated and the newer iPads weren't offering anything really radical. In fact most of Apple's products seem to be suffering from the same fate since Steve Jobs passed away. They are still solid performers, but there really isn't anything amazing about the newer iPads. The specs have improved, but essentially its the same device as my old one. So I started looking at Android again.\n*   Several magazine subscriptions I had on the iPad expired, and I chose not to renew them. The magazine store on iPad has a wide variety of magazines, but I realized the cost for a lot of them where you are locked into only being able to use them on an Apple device was too high. The Zinio application allows you to view magazine on multiple devices and through  the web, and several vendors are following that same model now with their periodicals.\n*   I discovered Dropbox and EverNote - two free cloud based applications that allow you to move content between your devices. Now I didn't need to physically hook up my table to get content to it anymore. In fact, in the past six months, I never synced my iPad to my computer at all.\n\nLooking back on that article some of the other reasons I stayed on the iPad are no longer relevant:\n\n*   Size: This time around, I tried out a larger Android tablet. Apple offers tables in two sizes, but with Android, you have more choices. The screen shape can be rectangular or square and the overall size can vary between 6-11 inches. The Samsung tablet I picked up was almost the same size and the iPad I had been using. As a bonus, the vendor I purchased through included a Bluetooth keyboard case with the tablet.\n*   Fragmented app market: The Samsung came bundled with an email app that allowed me to access all three email accounts I use regularly in one application. Plus the fact that I no longer use Yahoo email seems to have helped as well. :)\n*   I did a lot of research into e-reader software for Android and found several choices, that seemed to meet my needs: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few).  MoonReader+ with the PDF reader plugin has been working well for me. The only caveat being adding bookmarks could be easier.\n\nAnd some of the reasons I liked Android still hold true:\n\n*   Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.\n*   Expansion: From what I have found, you can add additional external storage to most Android devices. The Samsung tablet is a 16 GB model, and I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.\n*   Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I'm sure a major factor in the Android market share. The Samsung was about $150 cheaper than the comparable iPad model.\n*   Color: The 1280 x 800 resolution screen on the Samsung looks great. Its clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don't do a lot of gaming, but I intend to look into watching some videos on it soon.\n\nSo there you have it. Who said I was stubborn? _Android is a trademark of Google Inc._","slug":"android-not-so-much-for-me-revisited","published":1,"updated":"2020-08-23T20:54:34.834Z","layout":"post","photos":[],"link":"","_id":"ckeaq99s40009sdjxei864dku","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\" alt=\"Android_Robot_100\">In June of last year, I posted about my trials with using an Android tablet, and why I decided at that point to stick with my iPad. Ten months later, I have revisited Android, and I’m happy to report that I am now using a Samsung Galaxy Tab 3 10.1 running Android 4.2 on a daily basis in place of my iPad. So what changed? I did, the IOS and Android operating systems did, and the market did. Read on for more info…</p>\n<a id=\"more\"></a>\n<p>In the time frame between my last post on Android and now:</p>\n<ul>\n<li>Apple updated IOS to version 7.0 and then 7.1. I wasn’t crazy about the upgrade, from the visuals to the performance, and there wasn’t anything that was a game changer for me. It was the same old, same old. Being a techie, I like new toys, and a span of about two years is a good run for a gadget. The iPad 2 I had was dated and the newer iPads weren’t offering anything really radical. In fact most of Apple’s products seem to be suffering from the same fate since Steve Jobs passed away. They are still solid performers, but there really isn’t anything amazing about the newer iPads. The specs have improved, but essentially its the same device as my old one. So I started looking at Android again.</li>\n<li>Several magazine subscriptions I had on the iPad expired, and I chose not to renew them. The magazine store on iPad has a wide variety of magazines, but I realized the cost for a lot of them where you are locked into only being able to use them on an Apple device was too high. The Zinio application allows you to view magazine on multiple devices and through  the web, and several vendors are following that same model now with their periodicals.</li>\n<li>I discovered Dropbox and EverNote - two free cloud based applications that allow you to move content between your devices. Now I didn’t need to physically hook up my table to get content to it anymore. In fact, in the past six months, I never synced my iPad to my computer at all.</li>\n</ul>\n<p>Looking back on that article some of the other reasons I stayed on the iPad are no longer relevant:</p>\n<ul>\n<li>Size: This time around, I tried out a larger Android tablet. Apple offers tables in two sizes, but with Android, you have more choices. The screen shape can be rectangular or square and the overall size can vary between 6-11 inches. The Samsung tablet I picked up was almost the same size and the iPad I had been using. As a bonus, the vendor I purchased through included a Bluetooth keyboard case with the tablet.</li>\n<li>Fragmented app market: The Samsung came bundled with an email app that allowed me to access all three email accounts I use regularly in one application. Plus the fact that I no longer use Yahoo email seems to have helped as well. :)</li>\n<li>I did a lot of research into e-reader software for Android and found several choices, that seemed to meet my needs: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few).  MoonReader+ with the PDF reader plugin has been working well for me. The only caveat being adding bookmarks could be easier.</li>\n</ul>\n<p>And some of the reasons I liked Android still hold true:</p>\n<ul>\n<li>Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.</li>\n<li>Expansion: From what I have found, you can add additional external storage to most Android devices. The Samsung tablet is a 16 GB model, and I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.</li>\n<li>Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I’m sure a major factor in the Android market share. The Samsung was about $150 cheaper than the comparable iPad model.</li>\n<li>Color: The 1280 x 800 resolution screen on the Samsung looks great. Its clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don’t do a lot of gaming, but I intend to look into watching some videos on it soon.</li>\n</ul>\n<p>So there you have it. Who said I was stubborn? <em>Android is a trademark of Google Inc.</em></p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\" alt=\"Android_Robot_100\">In June of last year, I posted about my trials with using an Android tablet, and why I decided at that point to stick with my iPad. Ten months later, I have revisited Android, and I’m happy to report that I am now using a Samsung Galaxy Tab 3 10.1 running Android 4.2 on a daily basis in place of my iPad. So what changed? I did, the IOS and Android operating systems did, and the market did. Read on for more info…</p>","more":"<p>In the time frame between my last post on Android and now:</p>\n<ul>\n<li>Apple updated IOS to version 7.0 and then 7.1. I wasn’t crazy about the upgrade, from the visuals to the performance, and there wasn’t anything that was a game changer for me. It was the same old, same old. Being a techie, I like new toys, and a span of about two years is a good run for a gadget. The iPad 2 I had was dated and the newer iPads weren’t offering anything really radical. In fact most of Apple’s products seem to be suffering from the same fate since Steve Jobs passed away. They are still solid performers, but there really isn’t anything amazing about the newer iPads. The specs have improved, but essentially its the same device as my old one. So I started looking at Android again.</li>\n<li>Several magazine subscriptions I had on the iPad expired, and I chose not to renew them. The magazine store on iPad has a wide variety of magazines, but I realized the cost for a lot of them where you are locked into only being able to use them on an Apple device was too high. The Zinio application allows you to view magazine on multiple devices and through  the web, and several vendors are following that same model now with their periodicals.</li>\n<li>I discovered Dropbox and EverNote - two free cloud based applications that allow you to move content between your devices. Now I didn’t need to physically hook up my table to get content to it anymore. In fact, in the past six months, I never synced my iPad to my computer at all.</li>\n</ul>\n<p>Looking back on that article some of the other reasons I stayed on the iPad are no longer relevant:</p>\n<ul>\n<li>Size: This time around, I tried out a larger Android tablet. Apple offers tables in two sizes, but with Android, you have more choices. The screen shape can be rectangular or square and the overall size can vary between 6-11 inches. The Samsung tablet I picked up was almost the same size and the iPad I had been using. As a bonus, the vendor I purchased through included a Bluetooth keyboard case with the tablet.</li>\n<li>Fragmented app market: The Samsung came bundled with an email app that allowed me to access all three email accounts I use regularly in one application. Plus the fact that I no longer use Yahoo email seems to have helped as well. :)</li>\n<li>I did a lot of research into e-reader software for Android and found several choices, that seemed to meet my needs: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few).  MoonReader+ with the PDF reader plugin has been working well for me. The only caveat being adding bookmarks could be easier.</li>\n</ul>\n<p>And some of the reasons I liked Android still hold true:</p>\n<ul>\n<li>Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.</li>\n<li>Expansion: From what I have found, you can add additional external storage to most Android devices. The Samsung tablet is a 16 GB model, and I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.</li>\n<li>Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I’m sure a major factor in the Android market share. The Samsung was about $150 cheaper than the comparable iPad model.</li>\n<li>Color: The 1280 x 800 resolution screen on the Samsung looks great. Its clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don’t do a lot of gaming, but I intend to look into watching some videos on it soon.</li>\n</ul>\n<p>So there you have it. Who said I was stubborn? <em>Android is a trademark of Google Inc.</em></p>"},{"title":"Android? Not so much for me.","id":"1562","comments":0,"date":"2013-06-07T13:32:24.000Z","_content":"\n[![Android_Robot_100](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)Recently, a [news article](http://www.zdnet.com/android-accounts-for-75-percent-market-share-windows-phone-leapfrogs-blackberry-7000015496/) that Android™ had reached a 75% market share of smart phones got my interest. I've been an iPhone user going back several years and I picked up an iPad about a year ago. But I'm also  a supporter of open source software, and Linux in particular. Having never really messed with Android (other than an aborted purchase of an e-reader a few years ago), I wondered: What was I missing? At the same time this news broke, a local big box electronics store had a sale on a name brand 7 inch tablet running Android 4.1, so I pulled the trigger and picked one up. I spent almost a fortnight working with it to see how it stacked up with iOS (and to be able to return it if I decided I didn't like it).\n<!-- more -->\nMy impressions? Its a good platform, but I like iOS better. While iOS is not perfect (and it could be I'm just used to its many foibles now) I don't think that Android measured up as well. Coincidentally, I have the same feeling about OS X over Windows. Want a more detailed analysis? Read on! The Good:\n\n*   Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.\n*   Expansion: From what I have found, you can add additional external storage to most Android devices. While the tablet I was using was an 8 GB model, I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.\n*   Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I'm sure a major factor in the Android market share.\n*   Color: The 1.024 x 600 resolution screen looked great. It was clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don't do a lot of gaming.\n\nThe Bad:\n\n*   Size: Due more to the specific tablet I was testing rather than the operating system itself I think, the 7 inch screen was too small for me for doing any serious work and definitely too small for reading for an extended period. When typing on the screen, the keyboard took up a lot of real estate, and the keys were still fairly small. While the table supported Bluetooth for adding an external keyboard, that does cut down on the portability.\n*   Fragmented app market: One of the things I do a lot of on my iPad and iPhone is access email. My work email and several personal emails integrate well into one application on my iOS devices but on Android they didn't play so well together. The built-in email reader continually had problems with one email account, and occasionally had problems with the email account for this website. I also found that I could install an app to just read Gmail email, another one for Yahoo email, and many others. But I couldn't find a way to integrate all of my email accounts into one of those other apps.\n*   Poor e-reader software choices: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few). On the iPad, the built in iBookstore app allows me to keep all of them organized and available in one place. I couldn't find an app for the Android that would handle all of the different formats, and the organization of the different titles was poor.\n*   Connectivity: Since for the most part I have used Macs, I'd prefer any tablet I use to be able to connect and integrate well with them. Being that an iPad and iPhones are from the same company, connecting them via USB is normally (about 99% of the time) not a problem. The Android tablet I was testing however didn't want to play nice with my Mac. Even after trying several different apps on the tablet and on my computer, I couldn't see the tablet via USB. I had to resort to downloading material to the SDHC card and then popping the card into the tablet. Not good.\n\nConclusions? I think the title of this article spells it out. Android is a solid performer. It has some good points, but not enough to get me to switch. Back the tablet went to  the store after I was done, and back I went to my iPad. _Android is a trademark of Google Inc._","source":"_posts/android-not-so-much-for-me.md","raw":"---\ntitle: Android? Not so much for me.\ntags: []\nid: '1562'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2013-06-07 09:32:24\n---\n\n[![Android_Robot_100](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)](http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png)Recently, a [news article](http://www.zdnet.com/android-accounts-for-75-percent-market-share-windows-phone-leapfrogs-blackberry-7000015496/) that Android™ had reached a 75% market share of smart phones got my interest. I've been an iPhone user going back several years and I picked up an iPad about a year ago. But I'm also  a supporter of open source software, and Linux in particular. Having never really messed with Android (other than an aborted purchase of an e-reader a few years ago), I wondered: What was I missing? At the same time this news broke, a local big box electronics store had a sale on a name brand 7 inch tablet running Android 4.1, so I pulled the trigger and picked one up. I spent almost a fortnight working with it to see how it stacked up with iOS (and to be able to return it if I decided I didn't like it).\n<!-- more -->\nMy impressions? Its a good platform, but I like iOS better. While iOS is not perfect (and it could be I'm just used to its many foibles now) I don't think that Android measured up as well. Coincidentally, I have the same feeling about OS X over Windows. Want a more detailed analysis? Read on! The Good:\n\n*   Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.\n*   Expansion: From what I have found, you can add additional external storage to most Android devices. While the tablet I was using was an 8 GB model, I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.\n*   Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I'm sure a major factor in the Android market share.\n*   Color: The 1.024 x 600 resolution screen looked great. It was clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don't do a lot of gaming.\n\nThe Bad:\n\n*   Size: Due more to the specific tablet I was testing rather than the operating system itself I think, the 7 inch screen was too small for me for doing any serious work and definitely too small for reading for an extended period. When typing on the screen, the keyboard took up a lot of real estate, and the keys were still fairly small. While the table supported Bluetooth for adding an external keyboard, that does cut down on the portability.\n*   Fragmented app market: One of the things I do a lot of on my iPad and iPhone is access email. My work email and several personal emails integrate well into one application on my iOS devices but on Android they didn't play so well together. The built-in email reader continually had problems with one email account, and occasionally had problems with the email account for this website. I also found that I could install an app to just read Gmail email, another one for Yahoo email, and many others. But I couldn't find a way to integrate all of my email accounts into one of those other apps.\n*   Poor e-reader software choices: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few). On the iPad, the built in iBookstore app allows me to keep all of them organized and available in one place. I couldn't find an app for the Android that would handle all of the different formats, and the organization of the different titles was poor.\n*   Connectivity: Since for the most part I have used Macs, I'd prefer any tablet I use to be able to connect and integrate well with them. Being that an iPad and iPhones are from the same company, connecting them via USB is normally (about 99% of the time) not a problem. The Android tablet I was testing however didn't want to play nice with my Mac. Even after trying several different apps on the tablet and on my computer, I couldn't see the tablet via USB. I had to resort to downloading material to the SDHC card and then popping the card into the tablet. Not good.\n\nConclusions? I think the title of this article spells it out. Android is a solid performer. It has some good points, but not enough to get me to switch. Back the tablet went to  the store after I was done, and back I went to my iPad. _Android is a trademark of Google Inc._","slug":"android-not-so-much-for-me","published":1,"updated":"2020-08-23T20:54:34.742Z","layout":"post","photos":[],"link":"","_id":"ckeaq99s6000asdjx5pt97tsd","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\" alt=\"Android_Robot_100\"></a>Recently, a <a href=\"http://www.zdnet.com/android-accounts-for-75-percent-market-share-windows-phone-leapfrogs-blackberry-7000015496/\">news article</a> that Android™ had reached a 75% market share of smart phones got my interest. I’ve been an iPhone user going back several years and I picked up an iPad about a year ago. But I’m also  a supporter of open source software, and Linux in particular. Having never really messed with Android (other than an aborted purchase of an e-reader a few years ago), I wondered: What was I missing? At the same time this news broke, a local big box electronics store had a sale on a name brand 7 inch tablet running Android 4.1, so I pulled the trigger and picked one up. I spent almost a fortnight working with it to see how it stacked up with iOS (and to be able to return it if I decided I didn’t like it).</p>\n<a id=\"more\"></a>\n<p>My impressions? Its a good platform, but I like iOS better. While iOS is not perfect (and it could be I’m just used to its many foibles now) I don’t think that Android measured up as well. Coincidentally, I have the same feeling about OS X over Windows. Want a more detailed analysis? Read on! The Good:</p>\n<ul>\n<li>Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.</li>\n<li>Expansion: From what I have found, you can add additional external storage to most Android devices. While the tablet I was using was an 8 GB model, I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.</li>\n<li>Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I’m sure a major factor in the Android market share.</li>\n<li>Color: The 1.024 x 600 resolution screen looked great. It was clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don’t do a lot of gaming.</li>\n</ul>\n<p>The Bad:</p>\n<ul>\n<li>Size: Due more to the specific tablet I was testing rather than the operating system itself I think, the 7 inch screen was too small for me for doing any serious work and definitely too small for reading for an extended period. When typing on the screen, the keyboard took up a lot of real estate, and the keys were still fairly small. While the table supported Bluetooth for adding an external keyboard, that does cut down on the portability.</li>\n<li>Fragmented app market: One of the things I do a lot of on my iPad and iPhone is access email. My work email and several personal emails integrate well into one application on my iOS devices but on Android they didn’t play so well together. The built-in email reader continually had problems with one email account, and occasionally had problems with the email account for this website. I also found that I could install an app to just read Gmail email, another one for Yahoo email, and many others. But I couldn’t find a way to integrate all of my email accounts into one of those other apps.</li>\n<li>Poor e-reader software choices: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few). On the iPad, the built in iBookstore app allows me to keep all of them organized and available in one place. I couldn’t find an app for the Android that would handle all of the different formats, and the organization of the different titles was poor.</li>\n<li>Connectivity: Since for the most part I have used Macs, I’d prefer any tablet I use to be able to connect and integrate well with them. Being that an iPad and iPhones are from the same company, connecting them via USB is normally (about 99% of the time) not a problem. The Android tablet I was testing however didn’t want to play nice with my Mac. Even after trying several different apps on the tablet and on my computer, I couldn’t see the tablet via USB. I had to resort to downloading material to the SDHC card and then popping the card into the tablet. Not good.</li>\n</ul>\n<p>Conclusions? I think the title of this article spells it out. Android is a solid performer. It has some good points, but not enough to get me to switch. Back the tablet went to  the store after I was done, and back I went to my iPad. <em>Android is a trademark of Google Inc.</em></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/06/Android_Robot_100.png\" alt=\"Android_Robot_100\"></a>Recently, a <a href=\"http://www.zdnet.com/android-accounts-for-75-percent-market-share-windows-phone-leapfrogs-blackberry-7000015496/\">news article</a> that Android™ had reached a 75% market share of smart phones got my interest. I’ve been an iPhone user going back several years and I picked up an iPad about a year ago. But I’m also  a supporter of open source software, and Linux in particular. Having never really messed with Android (other than an aborted purchase of an e-reader a few years ago), I wondered: What was I missing? At the same time this news broke, a local big box electronics store had a sale on a name brand 7 inch tablet running Android 4.1, so I pulled the trigger and picked one up. I spent almost a fortnight working with it to see how it stacked up with iOS (and to be able to return it if I decided I didn’t like it).</p>","more":"<p>My impressions? Its a good platform, but I like iOS better. While iOS is not perfect (and it could be I’m just used to its many foibles now) I don’t think that Android measured up as well. Coincidentally, I have the same feeling about OS X over Windows. Want a more detailed analysis? Read on! The Good:</p>\n<ul>\n<li>Speed. No doubt about it, Android is fast. Starting an app generally takes only a second or two and switching between them is just as fast. The small amount of memory in smartphones requires more efficient coding and it shows in how well apps perform. Plus because Android is open source, a whole lot of programmers have looked at it and tweaked it to run as well as possible.</li>\n<li>Expansion: From what I have found, you can add additional external storage to most Android devices. While the tablet I was using was an 8 GB model, I was able to add an additional 8 GB of storage by inserting an SDHC card. After a quick format of the card, the system recognized it and I was able to store data on it. Contrast that with an iOS device where you are locked into the amount of storage it came with.</li>\n<li>Price: Much like Microsoft concentrated on the software and left the hardware to others, Google is not trying to control the hardware end of the smartphone and tablet market for Android devices. The result is Android devices at prices ranging from $50 up to several hundred depending on the quality of the hardware. What a boon for consumers and I’m sure a major factor in the Android market share.</li>\n<li>Color: The 1.024 x 600 resolution screen looked great. It was clear and crisp, with no fuzzy edges. Perfect for gaming. Unfortunately, I don’t do a lot of gaming.</li>\n</ul>\n<p>The Bad:</p>\n<ul>\n<li>Size: Due more to the specific tablet I was testing rather than the operating system itself I think, the 7 inch screen was too small for me for doing any serious work and definitely too small for reading for an extended period. When typing on the screen, the keyboard took up a lot of real estate, and the keys were still fairly small. While the table supported Bluetooth for adding an external keyboard, that does cut down on the portability.</li>\n<li>Fragmented app market: One of the things I do a lot of on my iPad and iPhone is access email. My work email and several personal emails integrate well into one application on my iOS devices but on Android they didn’t play so well together. The built-in email reader continually had problems with one email account, and occasionally had problems with the email account for this website. I also found that I could install an app to just read Gmail email, another one for Yahoo email, and many others. But I couldn’t find a way to integrate all of my email accounts into one of those other apps.</li>\n<li>Poor e-reader software choices: I do a lot of reading on my iPad and usually have a couple of dozen books stored on it at any one time (computer books for reference, several fiction titles, photography books). They come in a variety of formats (PDF, ePub, Mobi for a few). On the iPad, the built in iBookstore app allows me to keep all of them organized and available in one place. I couldn’t find an app for the Android that would handle all of the different formats, and the organization of the different titles was poor.</li>\n<li>Connectivity: Since for the most part I have used Macs, I’d prefer any tablet I use to be able to connect and integrate well with them. Being that an iPad and iPhones are from the same company, connecting them via USB is normally (about 99% of the time) not a problem. The Android tablet I was testing however didn’t want to play nice with my Mac. Even after trying several different apps on the tablet and on my computer, I couldn’t see the tablet via USB. I had to resort to downloading material to the SDHC card and then popping the card into the tablet. Not good.</li>\n</ul>\n<p>Conclusions? I think the title of this article spells it out. Android is a solid performer. It has some good points, but not enough to get me to switch. Back the tablet went to  the store after I was done, and back I went to my iPad. <em>Android is a trademark of Google Inc.</em></p>"},{"title":"ApacheDS (LDAP) - Part 2","id":"3055","comments":0,"date":"2015-11-23T07:26:30.000Z","_content":"\nIn the first part of this series, I walked through installing Apache Directory Server (ApacheDS), an LDAP server, on Linux Mint. In this part, we'll cover how to install and configure the management tool for ApacheDS, called Apache Directory Studio. Download and extract Apache Directory Studio from the [Apache website](http://directory.apache.org/studio/download/download-linux.html). If you are the only person on your system who will be using it, its OK to extract it to your Home folder. For shared systems, you may wish to extract it to the /opt folder, create a group with access to it, and make each user a member of that group. Apache Directory Studio is built on the Eclipse framework, so if you are familiar with Eclipse it will be fairly easy to navigate.\n<!-- more -->\n1.  Start the Apache Directory Studio application by double clicking it in Nemo (file manager for Mint), or from a Terminal prompt, type: ./ApacheDirectoryStudio followed by <ENTER>\n2.  When APS first starts, you'll see a Welcome window.![DirStudio1stStart](http://edpflager.com/wp-content/uploads/2015/11/DirStudio1stStart-300x202.png)\n3.  Go ahead and close that by clicking the X on the Welcome tab, and you will see the LDAP perspective.[![LDAP Perspective](http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective-300x194.png)](http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective.png)\n4.  In the lower left side of the main window, you will see the Connections panel. Hover your cursor over the LDAP icon and you'll see a tool tip that says \"New Connection\".[![NewConnection](http://edpflager.com/wp-content/uploads/2015/11/NewConnection1-300x246.png)](http://edpflager.com/wp-content/uploads/2015/11/NewConnection1.png)\n5.  Click on the LDAP icon and a New Connection window will open. On the first page, Network Parameter, for the **Connection Name**, provide a user friendly name. For the **Hostname** field, enter the server name for the LDAP box. ApacheDS default to using **port** 10389 (the common port for LDAP is 389). For now, you can use \"No encryption\" for the **Encryption Method**. Make sure the **Provider** drop down has \"Apache Directory LDAP Client API\" selected. Click the **Check Network Parameter** button and a test connection will be attempted. If everything is OK, you'll see a popup window letting you know. Click OK to close that popup, and then click NEXT on the Network Parameter window.![LDAPConnectionWindowFilledOut](http://edpflager.com/wp-content/uploads/2015/11/LDAPConnectionWindowFilledOut-250x300.png)\n6.  You'll now see the **Authentication** window where you can enter your credentials for accessing the Apache DS server. Leave the **Authentication Method** set to \"Simple Authentication\".  In the **Bind DN or user** field, enter \"uid=admin; ou=system\".This will let the server know that you want to login with user ID(uid): **admin** from the **system** organization unit (ou). Click the SAVE password checkbox, and then click the Check Authentication button. If you have entered everything correctly, you'll see a message that authentication was successful. Click OK to dismiss the message, and then NEXT on the Authentication window.![ApacheDSConnectionAuthentication](http://edpflager.com/wp-content/uploads/2015/11/ApacheDSConnectionAuthentication-249x300.png)\n7.  In the **Browser Options** window, leave the defaults as they are, and click the **Fetch Base DNS** button. If everything is working well, one DNS entry will be retrieved and you'll see a detail message showing what was retrieved. Click OK to dismiss it and then NEXT to advance to the next screen.![BrowserOptions](http://edpflager.com/wp-content/uploads/2015/11/BrowserOptions1-300x214.png)\n8.  The last window you see will be the **Edit Options** window. You can accept the values here, and click the FINISH button to save your work.![ApacheDSStudio Edit Options](http://edpflager.com/wp-content/uploads/2015/11/ApacheDSStudio-Edit-Options-247x300.png)\n9.  The LDAP perspective will update, and you'll see the Connection tab has a new entry and the LDAP browser will show an DIT tree. Congratulations! You have configured Apache Directory Studio to connect to your server.\n\n![Screenshot from 2015-11-22 20:42:21](http://edpflager.com/wp-content/uploads/2015/11/Screenshot-from-2015-11-22-204221-117x300.png)\n\nIn the third part of this series, I'll cover some basic editing tasks for your new LDAP system.","source":"_posts/apacheds-ldap-on-linux-mint-part-2.md","raw":"---\ntitle: ApacheDS (LDAP) - Part 2\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - LDAP\n  - Mint\n  - SysAdmin\n  - technical\nid: '3055'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-11-23 02:26:30\n---\n\nIn the first part of this series, I walked through installing Apache Directory Server (ApacheDS), an LDAP server, on Linux Mint. In this part, we'll cover how to install and configure the management tool for ApacheDS, called Apache Directory Studio. Download and extract Apache Directory Studio from the [Apache website](http://directory.apache.org/studio/download/download-linux.html). If you are the only person on your system who will be using it, its OK to extract it to your Home folder. For shared systems, you may wish to extract it to the /opt folder, create a group with access to it, and make each user a member of that group. Apache Directory Studio is built on the Eclipse framework, so if you are familiar with Eclipse it will be fairly easy to navigate.\n<!-- more -->\n1.  Start the Apache Directory Studio application by double clicking it in Nemo (file manager for Mint), or from a Terminal prompt, type: ./ApacheDirectoryStudio followed by <ENTER>\n2.  When APS first starts, you'll see a Welcome window.![DirStudio1stStart](http://edpflager.com/wp-content/uploads/2015/11/DirStudio1stStart-300x202.png)\n3.  Go ahead and close that by clicking the X on the Welcome tab, and you will see the LDAP perspective.[![LDAP Perspective](http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective-300x194.png)](http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective.png)\n4.  In the lower left side of the main window, you will see the Connections panel. Hover your cursor over the LDAP icon and you'll see a tool tip that says \"New Connection\".[![NewConnection](http://edpflager.com/wp-content/uploads/2015/11/NewConnection1-300x246.png)](http://edpflager.com/wp-content/uploads/2015/11/NewConnection1.png)\n5.  Click on the LDAP icon and a New Connection window will open. On the first page, Network Parameter, for the **Connection Name**, provide a user friendly name. For the **Hostname** field, enter the server name for the LDAP box. ApacheDS default to using **port** 10389 (the common port for LDAP is 389). For now, you can use \"No encryption\" for the **Encryption Method**. Make sure the **Provider** drop down has \"Apache Directory LDAP Client API\" selected. Click the **Check Network Parameter** button and a test connection will be attempted. If everything is OK, you'll see a popup window letting you know. Click OK to close that popup, and then click NEXT on the Network Parameter window.![LDAPConnectionWindowFilledOut](http://edpflager.com/wp-content/uploads/2015/11/LDAPConnectionWindowFilledOut-250x300.png)\n6.  You'll now see the **Authentication** window where you can enter your credentials for accessing the Apache DS server. Leave the **Authentication Method** set to \"Simple Authentication\".  In the **Bind DN or user** field, enter \"uid=admin; ou=system\".This will let the server know that you want to login with user ID(uid): **admin** from the **system** organization unit (ou). Click the SAVE password checkbox, and then click the Check Authentication button. If you have entered everything correctly, you'll see a message that authentication was successful. Click OK to dismiss the message, and then NEXT on the Authentication window.![ApacheDSConnectionAuthentication](http://edpflager.com/wp-content/uploads/2015/11/ApacheDSConnectionAuthentication-249x300.png)\n7.  In the **Browser Options** window, leave the defaults as they are, and click the **Fetch Base DNS** button. If everything is working well, one DNS entry will be retrieved and you'll see a detail message showing what was retrieved. Click OK to dismiss it and then NEXT to advance to the next screen.![BrowserOptions](http://edpflager.com/wp-content/uploads/2015/11/BrowserOptions1-300x214.png)\n8.  The last window you see will be the **Edit Options** window. You can accept the values here, and click the FINISH button to save your work.![ApacheDSStudio Edit Options](http://edpflager.com/wp-content/uploads/2015/11/ApacheDSStudio-Edit-Options-247x300.png)\n9.  The LDAP perspective will update, and you'll see the Connection tab has a new entry and the LDAP browser will show an DIT tree. Congratulations! You have configured Apache Directory Studio to connect to your server.\n\n![Screenshot from 2015-11-22 20:42:21](http://edpflager.com/wp-content/uploads/2015/11/Screenshot-from-2015-11-22-204221-117x300.png)\n\nIn the third part of this series, I'll cover some basic editing tasks for your new LDAP system.","slug":"apacheds-ldap-on-linux-mint-part-2","published":1,"updated":"2020-08-23T20:54:34.990Z","layout":"post","photos":[],"link":"","_id":"ckeaq99sa000dsdjxfs0edmxr","content":"<p>In the first part of this series, I walked through installing Apache Directory Server (ApacheDS), an LDAP server, on Linux Mint. In this part, we’ll cover how to install and configure the management tool for ApacheDS, called Apache Directory Studio. Download and extract Apache Directory Studio from the <a href=\"http://directory.apache.org/studio/download/download-linux.html\">Apache website</a>. If you are the only person on your system who will be using it, its OK to extract it to your Home folder. For shared systems, you may wish to extract it to the /opt folder, create a group with access to it, and make each user a member of that group. Apache Directory Studio is built on the Eclipse framework, so if you are familiar with Eclipse it will be fairly easy to navigate.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Start the Apache Directory Studio application by double clicking it in Nemo (file manager for Mint), or from a Terminal prompt, type: ./ApacheDirectoryStudio followed by <ENTER></li>\n<li>When APS first starts, you’ll see a Welcome window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/DirStudio1stStart-300x202.png\" alt=\"DirStudio1stStart\"></li>\n<li>Go ahead and close that by clicking the X on the Welcome tab, and you will see the LDAP perspective.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective-300x194.png\" alt=\"LDAP Perspective\"></a></li>\n<li>In the lower left side of the main window, you will see the Connections panel. Hover your cursor over the LDAP icon and you’ll see a tool tip that says “New Connection”.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/NewConnection1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/NewConnection1-300x246.png\" alt=\"NewConnection\"></a></li>\n<li>Click on the LDAP icon and a New Connection window will open. On the first page, Network Parameter, for the <strong>Connection Name</strong>, provide a user friendly name. For the <strong>Hostname</strong> field, enter the server name for the LDAP box. ApacheDS default to using <strong>port</strong> 10389 (the common port for LDAP is 389). For now, you can use “No encryption” for the <strong>Encryption Method</strong>. Make sure the <strong>Provider</strong> drop down has “Apache Directory LDAP Client API” selected. Click the <strong>Check Network Parameter</strong> button and a test connection will be attempted. If everything is OK, you’ll see a popup window letting you know. Click OK to close that popup, and then click NEXT on the Network Parameter window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/LDAPConnectionWindowFilledOut-250x300.png\" alt=\"LDAPConnectionWindowFilledOut\"></li>\n<li>You’ll now see the <strong>Authentication</strong> window where you can enter your credentials for accessing the Apache DS server. Leave the <strong>Authentication Method</strong> set to “Simple Authentication”.  In the <strong>Bind DN or user</strong> field, enter “uid=admin; ou=system”.This will let the server know that you want to login with user ID(uid): <strong>admin</strong> from the <strong>system</strong> organization unit (ou). Click the SAVE password checkbox, and then click the Check Authentication button. If you have entered everything correctly, you’ll see a message that authentication was successful. Click OK to dismiss the message, and then NEXT on the Authentication window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/ApacheDSConnectionAuthentication-249x300.png\" alt=\"ApacheDSConnectionAuthentication\"></li>\n<li>In the <strong>Browser Options</strong> window, leave the defaults as they are, and click the <strong>Fetch Base DNS</strong> button. If everything is working well, one DNS entry will be retrieved and you’ll see a detail message showing what was retrieved. Click OK to dismiss it and then NEXT to advance to the next screen.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/BrowserOptions1-300x214.png\" alt=\"BrowserOptions\"></li>\n<li>The last window you see will be the <strong>Edit Options</strong> window. You can accept the values here, and click the FINISH button to save your work.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/ApacheDSStudio-Edit-Options-247x300.png\" alt=\"ApacheDSStudio Edit Options\"></li>\n<li>The LDAP perspective will update, and you’ll see the Connection tab has a new entry and the LDAP browser will show an DIT tree. Congratulations! You have configured Apache Directory Studio to connect to your server.</li>\n</ol>\n<p><img src=\"http://edpflager.com/wp-content/uploads/2015/11/Screenshot-from-2015-11-22-204221-117x300.png\" alt=\"Screenshot from 2015-11-22 20:42:21\"></p>\n<p>In the third part of this series, I’ll cover some basic editing tasks for your new LDAP system.</p>\n","site":{"data":{}},"excerpt":"<p>In the first part of this series, I walked through installing Apache Directory Server (ApacheDS), an LDAP server, on Linux Mint. In this part, we’ll cover how to install and configure the management tool for ApacheDS, called Apache Directory Studio. Download and extract Apache Directory Studio from the <a href=\"http://directory.apache.org/studio/download/download-linux.html\">Apache website</a>. If you are the only person on your system who will be using it, its OK to extract it to your Home folder. For shared systems, you may wish to extract it to the /opt folder, create a group with access to it, and make each user a member of that group. Apache Directory Studio is built on the Eclipse framework, so if you are familiar with Eclipse it will be fairly easy to navigate.</p>","more":"<ol>\n<li>Start the Apache Directory Studio application by double clicking it in Nemo (file manager for Mint), or from a Terminal prompt, type: ./ApacheDirectoryStudio followed by <ENTER></li>\n<li>When APS first starts, you’ll see a Welcome window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/DirStudio1stStart-300x202.png\" alt=\"DirStudio1stStart\"></li>\n<li>Go ahead and close that by clicking the X on the Welcome tab, and you will see the LDAP perspective.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/LDAP-Perspective-300x194.png\" alt=\"LDAP Perspective\"></a></li>\n<li>In the lower left side of the main window, you will see the Connections panel. Hover your cursor over the LDAP icon and you’ll see a tool tip that says “New Connection”.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/NewConnection1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/NewConnection1-300x246.png\" alt=\"NewConnection\"></a></li>\n<li>Click on the LDAP icon and a New Connection window will open. On the first page, Network Parameter, for the <strong>Connection Name</strong>, provide a user friendly name. For the <strong>Hostname</strong> field, enter the server name for the LDAP box. ApacheDS default to using <strong>port</strong> 10389 (the common port for LDAP is 389). For now, you can use “No encryption” for the <strong>Encryption Method</strong>. Make sure the <strong>Provider</strong> drop down has “Apache Directory LDAP Client API” selected. Click the <strong>Check Network Parameter</strong> button and a test connection will be attempted. If everything is OK, you’ll see a popup window letting you know. Click OK to close that popup, and then click NEXT on the Network Parameter window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/LDAPConnectionWindowFilledOut-250x300.png\" alt=\"LDAPConnectionWindowFilledOut\"></li>\n<li>You’ll now see the <strong>Authentication</strong> window where you can enter your credentials for accessing the Apache DS server. Leave the <strong>Authentication Method</strong> set to “Simple Authentication”.  In the <strong>Bind DN or user</strong> field, enter “uid=admin; ou=system”.This will let the server know that you want to login with user ID(uid): <strong>admin</strong> from the <strong>system</strong> organization unit (ou). Click the SAVE password checkbox, and then click the Check Authentication button. If you have entered everything correctly, you’ll see a message that authentication was successful. Click OK to dismiss the message, and then NEXT on the Authentication window.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/ApacheDSConnectionAuthentication-249x300.png\" alt=\"ApacheDSConnectionAuthentication\"></li>\n<li>In the <strong>Browser Options</strong> window, leave the defaults as they are, and click the <strong>Fetch Base DNS</strong> button. If everything is working well, one DNS entry will be retrieved and you’ll see a detail message showing what was retrieved. Click OK to dismiss it and then NEXT to advance to the next screen.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/BrowserOptions1-300x214.png\" alt=\"BrowserOptions\"></li>\n<li>The last window you see will be the <strong>Edit Options</strong> window. You can accept the values here, and click the FINISH button to save your work.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/ApacheDSStudio-Edit-Options-247x300.png\" alt=\"ApacheDSStudio Edit Options\"></li>\n<li>The LDAP perspective will update, and you’ll see the Connection tab has a new entry and the LDAP browser will show an DIT tree. Congratulations! You have configured Apache Directory Studio to connect to your server.</li>\n</ol>\n<p><img src=\"http://edpflager.com/wp-content/uploads/2015/11/Screenshot-from-2015-11-22-204221-117x300.png\" alt=\"Screenshot from 2015-11-22 20:42:21\"></p>\n<p>In the third part of this series, I’ll cover some basic editing tasks for your new LDAP system.</p>"},{"title":"ApacheDS (LDAP) - Part 3","id":"3081","comments":0,"date":"2015-11-29T22:45:01.000Z","_content":"\n![ExampleTree](http://edpflager.com/wp-content/uploads/2015/11/ExampleTree.png)In [part 1 of this series](http://edpflager.com/?p=3029), I covered installing the Apache Directory Server (ApacheDS), an open source LDAP server and in [part 2](http://edpflager.com/?p=3055), I discussed configuring the Apache Directory Studio application to connect to your ApacheDS instance. In this third part, I will look at replacing the default Example schema in ApacheDS with a new one, and adding a new organizational unit to the directory. When you first open Apache Directory Studio, on the left side of the screen under the LDAP browser tab, you'll see a DIT (Directory Information Tree) hierarchy like the one illustrated here. The ROOT DSE (Directory System Agent Server Entry) is the start of the tree and has no associated properties. Under that you will see four nodes. The first is the top level entry for  a specific domain, denoted by \"dc=\". In this case, there are two dc (domain component) parameters, that together indicate that is domain is: **example.com.** Below that are three more nodes which define system and schema settings (ou-config, ou=schema and ou=system). There is a significant amount of detail contained here, and its outside the scope of what I am trying to cover. Please check the [ApacheDS website](http://directory.apache.org/apacheds/basic-user-guide.html) for more explanations.\n<!-- more -->\nCHANGE THE DIT entry In the bottom left panel of the Directory Studio window, click on the **Connections** tab if its not active. Then right click the ApacheDS entry and choose \"Open Configuration\" from the menu that appears. The center pane will change and display the server information.![Configuration](http://edpflager.com/wp-content/uploads/2015/11/Configuration-300x151.png) Click the **Partitions** tab at the bottom of the panel. In the window, click on the EXAMPLE partition on the left to make it active. The properties of the partition will be filled in on the right. [![ExamplePartion](http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion-300x148.png)](http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion.png) Change the ID box to the name you want to use. For my purposes, I'll use my name. Then in the Suffix box, change the DC setting to your domain name. Click FILE -> SAVE to save your updated Configuration file. Restart your server.\n\n##### ADD NEW ENTRY - ORGANIZATIONAL UNIT\n\nAfter your server restarts, reconnect to it using Apache Directory Studio. Open the DIT tree, and right click on the new dc entry you created above. Chose New -> New Entry from the menu that appears. You'll be prompted to select : Create entry from scratch or Use existing entry as a template. For right  now, leave New Entry from Scratch selected, and click NEXT at the bottom of the window. Scroll through the list of available object classes on the left, until you find organizationalUnit. (Organizational Units are distinct groups of related entities in the organization. For this example, I am creating an OU of employees.) Click organizationalUnit once to highlight it, and then click the ADD button in the middle. Two entries will be added to the right side of the screen: organizationalUnit and top. Click NEXT at the bottom of the screen.[![ObjectClasses](http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses-276x300.png)](http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses.png) The window will change to the Distinguished Name screen. The parent field will be filled out with the area of the tree where you clicked to access the add a New Entry menu. If you have the wrong spot in the tree, click the BROWSE button and you will see a new window allowing you to navigate through the existing tree. For my purposes I am working at the top of the tree which is fine. In the RDN section, drop the list down and choose **ou** from the list of available attributes.  In the field after the equals sign I entered \"employees\"[![disting_name](http://edpflager.com/wp-content/uploads/2015/11/disting_name-279x300.png)](http://edpflager.com/wp-content/uploads/2015/11/disting_name.png) From the drop down list you can see there are a number of additional attributes you can add. In my case, only OU was currently needed. If you accidentally add a line to the RDN section, the DN preview line will say \"RDN is invalid\" and you will not be able to proceed. To fix it, click the Minus button next to the blank line. If the RDN line populates successfully, click NEXT at the bottom of the window. The window will update and be replaced with the Attributes section. For the Attributes here, make sure there are values entered on the right. The window description here says you have to \"Enter at least the MUST attributes\". While not defined here, MUST attributes are those that are in bold text. Generally MUST attributes are those that a child object has in common with its parent (inherits them from the parent). [![ou_entry](http://edpflager.com/wp-content/uploads/2015/11/ou_entry-276x300.png)](http://edpflager.com/wp-content/uploads/2015/11/ou_entry.png) In this case all MUST attributes are filled in, so go ahead and click FINISH. A short message will appear in the window indicating the LDAP tree is being updated, and once completed, the window will close. Your LDAP tree will show the new organizationalUnit of \"employees\".[![OUTree](http://edpflager.com/wp-content/uploads/2015/11/OUTree.png)](http://edpflager.com/wp-content/uploads/2015/11/OUTree.png) Congratulations! You've created your first Entry in your tree. In the final part of  this series, we'll walk through adding users to your LDAP tree and setting passwords for them.","source":"_posts/apacheds-ldap-on-linux-mint-part-3.md","raw":"---\ntitle: ApacheDS (LDAP) - Part 3\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - LDAP\n  - Mint\n  - SysAdmin\n  - technical\nid: '3081'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-11-29 17:45:01\n---\n\n![ExampleTree](http://edpflager.com/wp-content/uploads/2015/11/ExampleTree.png)In [part 1 of this series](http://edpflager.com/?p=3029), I covered installing the Apache Directory Server (ApacheDS), an open source LDAP server and in [part 2](http://edpflager.com/?p=3055), I discussed configuring the Apache Directory Studio application to connect to your ApacheDS instance. In this third part, I will look at replacing the default Example schema in ApacheDS with a new one, and adding a new organizational unit to the directory. When you first open Apache Directory Studio, on the left side of the screen under the LDAP browser tab, you'll see a DIT (Directory Information Tree) hierarchy like the one illustrated here. The ROOT DSE (Directory System Agent Server Entry) is the start of the tree and has no associated properties. Under that you will see four nodes. The first is the top level entry for  a specific domain, denoted by \"dc=\". In this case, there are two dc (domain component) parameters, that together indicate that is domain is: **example.com.** Below that are three more nodes which define system and schema settings (ou-config, ou=schema and ou=system). There is a significant amount of detail contained here, and its outside the scope of what I am trying to cover. Please check the [ApacheDS website](http://directory.apache.org/apacheds/basic-user-guide.html) for more explanations.\n<!-- more -->\nCHANGE THE DIT entry In the bottom left panel of the Directory Studio window, click on the **Connections** tab if its not active. Then right click the ApacheDS entry and choose \"Open Configuration\" from the menu that appears. The center pane will change and display the server information.![Configuration](http://edpflager.com/wp-content/uploads/2015/11/Configuration-300x151.png) Click the **Partitions** tab at the bottom of the panel. In the window, click on the EXAMPLE partition on the left to make it active. The properties of the partition will be filled in on the right. [![ExamplePartion](http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion-300x148.png)](http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion.png) Change the ID box to the name you want to use. For my purposes, I'll use my name. Then in the Suffix box, change the DC setting to your domain name. Click FILE -> SAVE to save your updated Configuration file. Restart your server.\n\n##### ADD NEW ENTRY - ORGANIZATIONAL UNIT\n\nAfter your server restarts, reconnect to it using Apache Directory Studio. Open the DIT tree, and right click on the new dc entry you created above. Chose New -> New Entry from the menu that appears. You'll be prompted to select : Create entry from scratch or Use existing entry as a template. For right  now, leave New Entry from Scratch selected, and click NEXT at the bottom of the window. Scroll through the list of available object classes on the left, until you find organizationalUnit. (Organizational Units are distinct groups of related entities in the organization. For this example, I am creating an OU of employees.) Click organizationalUnit once to highlight it, and then click the ADD button in the middle. Two entries will be added to the right side of the screen: organizationalUnit and top. Click NEXT at the bottom of the screen.[![ObjectClasses](http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses-276x300.png)](http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses.png) The window will change to the Distinguished Name screen. The parent field will be filled out with the area of the tree where you clicked to access the add a New Entry menu. If you have the wrong spot in the tree, click the BROWSE button and you will see a new window allowing you to navigate through the existing tree. For my purposes I am working at the top of the tree which is fine. In the RDN section, drop the list down and choose **ou** from the list of available attributes.  In the field after the equals sign I entered \"employees\"[![disting_name](http://edpflager.com/wp-content/uploads/2015/11/disting_name-279x300.png)](http://edpflager.com/wp-content/uploads/2015/11/disting_name.png) From the drop down list you can see there are a number of additional attributes you can add. In my case, only OU was currently needed. If you accidentally add a line to the RDN section, the DN preview line will say \"RDN is invalid\" and you will not be able to proceed. To fix it, click the Minus button next to the blank line. If the RDN line populates successfully, click NEXT at the bottom of the window. The window will update and be replaced with the Attributes section. For the Attributes here, make sure there are values entered on the right. The window description here says you have to \"Enter at least the MUST attributes\". While not defined here, MUST attributes are those that are in bold text. Generally MUST attributes are those that a child object has in common with its parent (inherits them from the parent). [![ou_entry](http://edpflager.com/wp-content/uploads/2015/11/ou_entry-276x300.png)](http://edpflager.com/wp-content/uploads/2015/11/ou_entry.png) In this case all MUST attributes are filled in, so go ahead and click FINISH. A short message will appear in the window indicating the LDAP tree is being updated, and once completed, the window will close. Your LDAP tree will show the new organizationalUnit of \"employees\".[![OUTree](http://edpflager.com/wp-content/uploads/2015/11/OUTree.png)](http://edpflager.com/wp-content/uploads/2015/11/OUTree.png) Congratulations! You've created your first Entry in your tree. In the final part of  this series, we'll walk through adding users to your LDAP tree and setting passwords for them.","slug":"apacheds-ldap-on-linux-mint-part-3","published":1,"updated":"2020-08-23T20:54:34.994Z","layout":"post","photos":[],"link":"","_id":"ckeaq99sc000esdjx59exegqq","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ExampleTree.png\" alt=\"ExampleTree\">In <a href=\"http://edpflager.com/?p=3029\">part 1 of this series</a>, I covered installing the Apache Directory Server (ApacheDS), an open source LDAP server and in <a href=\"http://edpflager.com/?p=3055\">part 2</a>, I discussed configuring the Apache Directory Studio application to connect to your ApacheDS instance. In this third part, I will look at replacing the default Example schema in ApacheDS with a new one, and adding a new organizational unit to the directory. When you first open Apache Directory Studio, on the left side of the screen under the LDAP browser tab, you’ll see a DIT (Directory Information Tree) hierarchy like the one illustrated here. The ROOT DSE (Directory System Agent Server Entry) is the start of the tree and has no associated properties. Under that you will see four nodes. The first is the top level entry for  a specific domain, denoted by “dc=”. In this case, there are two dc (domain component) parameters, that together indicate that is domain is: <strong>example.com.</strong> Below that are three more nodes which define system and schema settings (ou-config, ou=schema and ou=system). There is a significant amount of detail contained here, and its outside the scope of what I am trying to cover. Please check the <a href=\"http://directory.apache.org/apacheds/basic-user-guide.html\">ApacheDS website</a> for more explanations.</p>\n<a id=\"more\"></a>\n<p>CHANGE THE DIT entry In the bottom left panel of the Directory Studio window, click on the <strong>Connections</strong> tab if its not active. Then right click the ApacheDS entry and choose “Open Configuration” from the menu that appears. The center pane will change and display the server information.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/Configuration-300x151.png\" alt=\"Configuration\"> Click the <strong>Partitions</strong> tab at the bottom of the panel. In the window, click on the EXAMPLE partition on the left to make it active. The properties of the partition will be filled in on the right. <a href=\"http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion-300x148.png\" alt=\"ExamplePartion\"></a> Change the ID box to the name you want to use. For my purposes, I’ll use my name. Then in the Suffix box, change the DC setting to your domain name. Click FILE -&gt; SAVE to save your updated Configuration file. Restart your server.</p>\n<h5 id=\"ADD-NEW-ENTRY-ORGANIZATIONAL-UNIT\"><a href=\"#ADD-NEW-ENTRY-ORGANIZATIONAL-UNIT\" class=\"headerlink\" title=\"ADD NEW ENTRY - ORGANIZATIONAL UNIT\"></a>ADD NEW ENTRY - ORGANIZATIONAL UNIT</h5><p>After your server restarts, reconnect to it using Apache Directory Studio. Open the DIT tree, and right click on the new dc entry you created above. Chose New -&gt; New Entry from the menu that appears. You’ll be prompted to select : Create entry from scratch or Use existing entry as a template. For right  now, leave New Entry from Scratch selected, and click NEXT at the bottom of the window. Scroll through the list of available object classes on the left, until you find organizationalUnit. (Organizational Units are distinct groups of related entities in the organization. For this example, I am creating an OU of employees.) Click organizationalUnit once to highlight it, and then click the ADD button in the middle. Two entries will be added to the right side of the screen: organizationalUnit and top. Click NEXT at the bottom of the screen.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses-276x300.png\" alt=\"ObjectClasses\"></a> The window will change to the Distinguished Name screen. The parent field will be filled out with the area of the tree where you clicked to access the add a New Entry menu. If you have the wrong spot in the tree, click the BROWSE button and you will see a new window allowing you to navigate through the existing tree. For my purposes I am working at the top of the tree which is fine. In the RDN section, drop the list down and choose <strong>ou</strong> from the list of available attributes.  In the field after the equals sign I entered “employees”<a href=\"http://edpflager.com/wp-content/uploads/2015/11/disting_name.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/disting_name-279x300.png\" alt=\"disting_name\"></a> From the drop down list you can see there are a number of additional attributes you can add. In my case, only OU was currently needed. If you accidentally add a line to the RDN section, the DN preview line will say “RDN is invalid” and you will not be able to proceed. To fix it, click the Minus button next to the blank line. If the RDN line populates successfully, click NEXT at the bottom of the window. The window will update and be replaced with the Attributes section. For the Attributes here, make sure there are values entered on the right. The window description here says you have to “Enter at least the MUST attributes”. While not defined here, MUST attributes are those that are in bold text. Generally MUST attributes are those that a child object has in common with its parent (inherits them from the parent). <a href=\"http://edpflager.com/wp-content/uploads/2015/11/ou_entry.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ou_entry-276x300.png\" alt=\"ou_entry\"></a> In this case all MUST attributes are filled in, so go ahead and click FINISH. A short message will appear in the window indicating the LDAP tree is being updated, and once completed, the window will close. Your LDAP tree will show the new organizationalUnit of “employees”.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/OUTree.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/OUTree.png\" alt=\"OUTree\"></a> Congratulations! You’ve created your first Entry in your tree. In the final part of  this series, we’ll walk through adding users to your LDAP tree and setting passwords for them.</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ExampleTree.png\" alt=\"ExampleTree\">In <a href=\"http://edpflager.com/?p=3029\">part 1 of this series</a>, I covered installing the Apache Directory Server (ApacheDS), an open source LDAP server and in <a href=\"http://edpflager.com/?p=3055\">part 2</a>, I discussed configuring the Apache Directory Studio application to connect to your ApacheDS instance. In this third part, I will look at replacing the default Example schema in ApacheDS with a new one, and adding a new organizational unit to the directory. When you first open Apache Directory Studio, on the left side of the screen under the LDAP browser tab, you’ll see a DIT (Directory Information Tree) hierarchy like the one illustrated here. The ROOT DSE (Directory System Agent Server Entry) is the start of the tree and has no associated properties. Under that you will see four nodes. The first is the top level entry for  a specific domain, denoted by “dc=”. In this case, there are two dc (domain component) parameters, that together indicate that is domain is: <strong>example.com.</strong> Below that are three more nodes which define system and schema settings (ou-config, ou=schema and ou=system). There is a significant amount of detail contained here, and its outside the scope of what I am trying to cover. Please check the <a href=\"http://directory.apache.org/apacheds/basic-user-guide.html\">ApacheDS website</a> for more explanations.</p>","more":"<p>CHANGE THE DIT entry In the bottom left panel of the Directory Studio window, click on the <strong>Connections</strong> tab if its not active. Then right click the ApacheDS entry and choose “Open Configuration” from the menu that appears. The center pane will change and display the server information.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/Configuration-300x151.png\" alt=\"Configuration\"> Click the <strong>Partitions</strong> tab at the bottom of the panel. In the window, click on the EXAMPLE partition on the left to make it active. The properties of the partition will be filled in on the right. <a href=\"http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ExamplePartion-300x148.png\" alt=\"ExamplePartion\"></a> Change the ID box to the name you want to use. For my purposes, I’ll use my name. Then in the Suffix box, change the DC setting to your domain name. Click FILE -&gt; SAVE to save your updated Configuration file. Restart your server.</p>\n<h5 id=\"ADD-NEW-ENTRY-ORGANIZATIONAL-UNIT\"><a href=\"#ADD-NEW-ENTRY-ORGANIZATIONAL-UNIT\" class=\"headerlink\" title=\"ADD NEW ENTRY - ORGANIZATIONAL UNIT\"></a>ADD NEW ENTRY - ORGANIZATIONAL UNIT</h5><p>After your server restarts, reconnect to it using Apache Directory Studio. Open the DIT tree, and right click on the new dc entry you created above. Chose New -&gt; New Entry from the menu that appears. You’ll be prompted to select : Create entry from scratch or Use existing entry as a template. For right  now, leave New Entry from Scratch selected, and click NEXT at the bottom of the window. Scroll through the list of available object classes on the left, until you find organizationalUnit. (Organizational Units are distinct groups of related entities in the organization. For this example, I am creating an OU of employees.) Click organizationalUnit once to highlight it, and then click the ADD button in the middle. Two entries will be added to the right side of the screen: organizationalUnit and top. Click NEXT at the bottom of the screen.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ObjectClasses-276x300.png\" alt=\"ObjectClasses\"></a> The window will change to the Distinguished Name screen. The parent field will be filled out with the area of the tree where you clicked to access the add a New Entry menu. If you have the wrong spot in the tree, click the BROWSE button and you will see a new window allowing you to navigate through the existing tree. For my purposes I am working at the top of the tree which is fine. In the RDN section, drop the list down and choose <strong>ou</strong> from the list of available attributes.  In the field after the equals sign I entered “employees”<a href=\"http://edpflager.com/wp-content/uploads/2015/11/disting_name.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/disting_name-279x300.png\" alt=\"disting_name\"></a> From the drop down list you can see there are a number of additional attributes you can add. In my case, only OU was currently needed. If you accidentally add a line to the RDN section, the DN preview line will say “RDN is invalid” and you will not be able to proceed. To fix it, click the Minus button next to the blank line. If the RDN line populates successfully, click NEXT at the bottom of the window. The window will update and be replaced with the Attributes section. For the Attributes here, make sure there are values entered on the right. The window description here says you have to “Enter at least the MUST attributes”. While not defined here, MUST attributes are those that are in bold text. Generally MUST attributes are those that a child object has in common with its parent (inherits them from the parent). <a href=\"http://edpflager.com/wp-content/uploads/2015/11/ou_entry.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/ou_entry-276x300.png\" alt=\"ou_entry\"></a> In this case all MUST attributes are filled in, so go ahead and click FINISH. A short message will appear in the window indicating the LDAP tree is being updated, and once completed, the window will close. Your LDAP tree will show the new organizationalUnit of “employees”.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/OUTree.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/OUTree.png\" alt=\"OUTree\"></a> Congratulations! You’ve created your first Entry in your tree. In the final part of  this series, we’ll walk through adding users to your LDAP tree and setting passwords for them.</p>"},{"title":"ApacheDS (LDAP) - Part 4","id":"3127","comments":0,"date":"2015-12-03T17:57:56.000Z","_content":"\n[![users](http://edpflager.com/wp-content/uploads/2015/12/users-300x213.png)](http://edpflager.com/wp-content/uploads/2015/12/users.png)This is part 4 (the last part) of my series on using Apache Directory Server (ApacheDS), an open source LDAP implementation. Parts [1](http://edpflager.com/?p=3029)\\-[2](http://edpflager.com/?p=3055) covered getting ApacheDS installed, as well as connecting the configuration application to it to make changes to the structure of the directory. In [part 3](http://edpflager.com/?p=3081), I walked through adding a new Organizational Unit from scratch to your LDAP tree. In this last part I'll cover adding a new user, using an existing user object as a template.\n\n##### ADD A NEW USER\n\nIf you have been following these tutorials in order, you should have an DIT tree with an OU called Employees under your DC entry. Right click on the Employees OU you just created and chose New -> New Entry from the menu that appears. You'll be prompted to select : \"Create entry from scratch\" or \"Use existing entry as a template\". Choose the option to Use an Existing entry as a template. Then click the BROWSE button on the right.\n<!-- more -->\n[![Browse for Object](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png)](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png)A window titled **Select DN** will open. Toggle the arrows until you find the entry labeled \"uid=admin\". Click it to highlight it, and then click the OK button at the bottom of the window. [![Browse for Object](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png)](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png) The window will close, and you will be returned to the New Entry window. The result should look like this: [![entry from existing](http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing-278x300.png)](http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing.png) Click the NEXT button to continue. The window will update and show the available Object Classes on the left. There will be several already entered on the right. Click the tlsKeyInfo item on the right to select it, and click the REMOVE button. You should be left with these four items:[![orgperson](http://edpflager.com/wp-content/uploads/2015/11/orgperson-275x300.png)](http://edpflager.com/wp-content/uploads/2015/11/orgperson.png) Click the NEXT button to continue. In the Distinguished Name window, change the uid= to the name of your new object. In my case that would be \"biadmin\". Now add one attribute: cn = \"biadmin\".[![entry_disting](http://edpflager.com/wp-content/uploads/2015/12/entry_disting-279x300.png)](http://edpflager.com/wp-content/uploads/2015/12/entry_disting.png) If the DN Preview is populated successfully click NEXT to advance to the next screen. On the Attributes page you'll see an error message indicating that several attributes are not allowed or are not filled in with a valid entry. [![biadmin_attributes_remove](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove-246x300.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove.png) Delete these values by highlighting them with a click and then clicking the red X that will appear at the top right:\n\n*   the second CN with a value of \"system administrator\"\n*   displayName\n*   keyAlgorithm\n*   privateKey,\n*   privateKeyFormat,\n*   publicKey,\n*   publicKeyFormat\n*   userCertificate.\n\nClick OK when prompted to see if you are sure.  Click the Value field next to SN and change the text to \"bi administrator\". When completed, you should be left with the values below.[![biadmin_attributes_endresult](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult-249x300.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult.png)Now we need to enter a user password. Double click the value field next to userPassword that currently says Empty Password and a Password Editor window will appear. Click the New Password tab to work with it. Enter a password for the user in the Enter New Password field, and then again in the Confirm New Password field.The password is stored with a HASH method, the default being \"plaintext\". If you'd like to use a different Hash Method to encrypt your password, choose the option from the drop down list. You can check the box labeled **Show new password details** and the mask on the password fields will be replaced by the values you entered.[![password_editor_change_filled](http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled-300x227.png)](http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled.png) Click the OK button and the window will close and return you to the New Entry - Attributes window. Click OK again, and after a few seconds, your new user will be created! [![biadmin_added](http://edpflager.com/wp-content/uploads/2015/12/biadmin_added-300x147.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_added.png)   This is the conclusion of my series on setting up Apache Directory Server.","source":"_posts/apacheds-ldap-part-4.md","raw":"---\ntitle: ApacheDS (LDAP) - Part 4\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - LDAP\n  - Mint\n  - SysAdmin\n  - technical\nid: '3127'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-12-03 12:57:56\n---\n\n[![users](http://edpflager.com/wp-content/uploads/2015/12/users-300x213.png)](http://edpflager.com/wp-content/uploads/2015/12/users.png)This is part 4 (the last part) of my series on using Apache Directory Server (ApacheDS), an open source LDAP implementation. Parts [1](http://edpflager.com/?p=3029)\\-[2](http://edpflager.com/?p=3055) covered getting ApacheDS installed, as well as connecting the configuration application to it to make changes to the structure of the directory. In [part 3](http://edpflager.com/?p=3081), I walked through adding a new Organizational Unit from scratch to your LDAP tree. In this last part I'll cover adding a new user, using an existing user object as a template.\n\n##### ADD A NEW USER\n\nIf you have been following these tutorials in order, you should have an DIT tree with an OU called Employees under your DC entry. Right click on the Employees OU you just created and chose New -> New Entry from the menu that appears. You'll be prompted to select : \"Create entry from scratch\" or \"Use existing entry as a template\". Choose the option to Use an Existing entry as a template. Then click the BROWSE button on the right.\n<!-- more -->\n[![Browse for Object](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png)](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png)A window titled **Select DN** will open. Toggle the arrows until you find the entry labeled \"uid=admin\". Click it to highlight it, and then click the OK button at the bottom of the window. [![Browse for Object](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png)](http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png) The window will close, and you will be returned to the New Entry window. The result should look like this: [![entry from existing](http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing-278x300.png)](http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing.png) Click the NEXT button to continue. The window will update and show the available Object Classes on the left. There will be several already entered on the right. Click the tlsKeyInfo item on the right to select it, and click the REMOVE button. You should be left with these four items:[![orgperson](http://edpflager.com/wp-content/uploads/2015/11/orgperson-275x300.png)](http://edpflager.com/wp-content/uploads/2015/11/orgperson.png) Click the NEXT button to continue. In the Distinguished Name window, change the uid= to the name of your new object. In my case that would be \"biadmin\". Now add one attribute: cn = \"biadmin\".[![entry_disting](http://edpflager.com/wp-content/uploads/2015/12/entry_disting-279x300.png)](http://edpflager.com/wp-content/uploads/2015/12/entry_disting.png) If the DN Preview is populated successfully click NEXT to advance to the next screen. On the Attributes page you'll see an error message indicating that several attributes are not allowed or are not filled in with a valid entry. [![biadmin_attributes_remove](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove-246x300.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove.png) Delete these values by highlighting them with a click and then clicking the red X that will appear at the top right:\n\n*   the second CN with a value of \"system administrator\"\n*   displayName\n*   keyAlgorithm\n*   privateKey,\n*   privateKeyFormat,\n*   publicKey,\n*   publicKeyFormat\n*   userCertificate.\n\nClick OK when prompted to see if you are sure.  Click the Value field next to SN and change the text to \"bi administrator\". When completed, you should be left with the values below.[![biadmin_attributes_endresult](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult-249x300.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult.png)Now we need to enter a user password. Double click the value field next to userPassword that currently says Empty Password and a Password Editor window will appear. Click the New Password tab to work with it. Enter a password for the user in the Enter New Password field, and then again in the Confirm New Password field.The password is stored with a HASH method, the default being \"plaintext\". If you'd like to use a different Hash Method to encrypt your password, choose the option from the drop down list. You can check the box labeled **Show new password details** and the mask on the password fields will be replaced by the values you entered.[![password_editor_change_filled](http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled-300x227.png)](http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled.png) Click the OK button and the window will close and return you to the New Entry - Attributes window. Click OK again, and after a few seconds, your new user will be created! [![biadmin_added](http://edpflager.com/wp-content/uploads/2015/12/biadmin_added-300x147.png)](http://edpflager.com/wp-content/uploads/2015/12/biadmin_added.png)   This is the conclusion of my series on setting up Apache Directory Server.","slug":"apacheds-ldap-part-4","published":1,"updated":"2020-08-23T20:54:34.998Z","layout":"post","photos":[],"link":"","_id":"ckeaq99sh000hsdjxb6ag3mss","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/12/users.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/users-300x213.png\" alt=\"users\"></a>This is part 4 (the last part) of my series on using Apache Directory Server (ApacheDS), an open source LDAP implementation. Parts <a href=\"http://edpflager.com/?p=3029\">1</a>-<a href=\"http://edpflager.com/?p=3055\">2</a> covered getting ApacheDS installed, as well as connecting the configuration application to it to make changes to the structure of the directory. In <a href=\"http://edpflager.com/?p=3081\">part 3</a>, I walked through adding a new Organizational Unit from scratch to your LDAP tree. In this last part I’ll cover adding a new user, using an existing user object as a template.</p>\n<h5 id=\"ADD-A-NEW-USER\"><a href=\"#ADD-A-NEW-USER\" class=\"headerlink\" title=\"ADD A NEW USER\"></a>ADD A NEW USER</h5><p>If you have been following these tutorials in order, you should have an DIT tree with an OU called Employees under your DC entry. Right click on the Employees OU you just created and chose New -&gt; New Entry from the menu that appears. You’ll be prompted to select : “Create entry from scratch” or “Use existing entry as a template”. Choose the option to Use an Existing entry as a template. Then click the BROWSE button on the right.</p>\n<a id=\"more\"></a>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png\" alt=\"Browse for Object\"></a>A window titled <strong>Select DN</strong> will open. Toggle the arrows until you find the entry labeled “uid=admin”. Click it to highlight it, and then click the OK button at the bottom of the window. <a href=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png\" alt=\"Browse for Object\"></a> The window will close, and you will be returned to the New Entry window. The result should look like this: <a href=\"http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing-278x300.png\" alt=\"entry from existing\"></a> Click the NEXT button to continue. The window will update and show the available Object Classes on the left. There will be several already entered on the right. Click the tlsKeyInfo item on the right to select it, and click the REMOVE button. You should be left with these four items:<a href=\"http://edpflager.com/wp-content/uploads/2015/11/orgperson.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/orgperson-275x300.png\" alt=\"orgperson\"></a> Click the NEXT button to continue. In the Distinguished Name window, change the uid= to the name of your new object. In my case that would be “biadmin”. Now add one attribute: cn = “biadmin”.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/entry_disting.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/entry_disting-279x300.png\" alt=\"entry_disting\"></a> If the DN Preview is populated successfully click NEXT to advance to the next screen. On the Attributes page you’ll see an error message indicating that several attributes are not allowed or are not filled in with a valid entry. <a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove-246x300.png\" alt=\"biadmin_attributes_remove\"></a> Delete these values by highlighting them with a click and then clicking the red X that will appear at the top right:</p>\n<ul>\n<li>the second CN with a value of “system administrator”</li>\n<li>displayName</li>\n<li>keyAlgorithm</li>\n<li>privateKey,</li>\n<li>privateKeyFormat,</li>\n<li>publicKey,</li>\n<li>publicKeyFormat</li>\n<li>userCertificate.</li>\n</ul>\n<p>Click OK when prompted to see if you are sure.  Click the Value field next to SN and change the text to “bi administrator”. When completed, you should be left with the values below.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult-249x300.png\" alt=\"biadmin_attributes_endresult\"></a>Now we need to enter a user password. Double click the value field next to userPassword that currently says Empty Password and a Password Editor window will appear. Click the New Password tab to work with it. Enter a password for the user in the Enter New Password field, and then again in the Confirm New Password field.The password is stored with a HASH method, the default being “plaintext”. If you’d like to use a different Hash Method to encrypt your password, choose the option from the drop down list. You can check the box labeled <strong>Show new password details</strong> and the mask on the password fields will be replaced by the values you entered.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled-300x227.png\" alt=\"password_editor_change_filled\"></a> Click the OK button and the window will close and return you to the New Entry - Attributes window. Click OK again, and after a few seconds, your new user will be created! <a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_added.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_added-300x147.png\" alt=\"biadmin_added\"></a>   This is the conclusion of my series on setting up Apache Directory Server.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/12/users.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/users-300x213.png\" alt=\"users\"></a>This is part 4 (the last part) of my series on using Apache Directory Server (ApacheDS), an open source LDAP implementation. Parts <a href=\"http://edpflager.com/?p=3029\">1</a>-<a href=\"http://edpflager.com/?p=3055\">2</a> covered getting ApacheDS installed, as well as connecting the configuration application to it to make changes to the structure of the directory. In <a href=\"http://edpflager.com/?p=3081\">part 3</a>, I walked through adding a new Organizational Unit from scratch to your LDAP tree. In this last part I’ll cover adding a new user, using an existing user object as a template.</p>\n<h5 id=\"ADD-A-NEW-USER\"><a href=\"#ADD-A-NEW-USER\" class=\"headerlink\" title=\"ADD A NEW USER\"></a>ADD A NEW USER</h5><p>If you have been following these tutorials in order, you should have an DIT tree with an OU called Employees under your DC entry. Right click on the Employees OU you just created and chose New -&gt; New Entry from the menu that appears. You’ll be prompted to select : “Create entry from scratch” or “Use existing entry as a template”. Choose the option to Use an Existing entry as a template. Then click the BROWSE button on the right.</p>","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png\" alt=\"Browse for Object\"></a>A window titled <strong>Select DN</strong> will open. Toggle the arrows until you find the entry labeled “uid=admin”. Click it to highlight it, and then click the OK button at the bottom of the window. <a href=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Browse-for-Object-252x300.png\" alt=\"Browse for Object\"></a> The window will close, and you will be returned to the New Entry window. The result should look like this: <a href=\"http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/entry-from-existing-278x300.png\" alt=\"entry from existing\"></a> Click the NEXT button to continue. The window will update and show the available Object Classes on the left. There will be several already entered on the right. Click the tlsKeyInfo item on the right to select it, and click the REMOVE button. You should be left with these four items:<a href=\"http://edpflager.com/wp-content/uploads/2015/11/orgperson.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/orgperson-275x300.png\" alt=\"orgperson\"></a> Click the NEXT button to continue. In the Distinguished Name window, change the uid= to the name of your new object. In my case that would be “biadmin”. Now add one attribute: cn = “biadmin”.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/entry_disting.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/entry_disting-279x300.png\" alt=\"entry_disting\"></a> If the DN Preview is populated successfully click NEXT to advance to the next screen. On the Attributes page you’ll see an error message indicating that several attributes are not allowed or are not filled in with a valid entry. <a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_remove-246x300.png\" alt=\"biadmin_attributes_remove\"></a> Delete these values by highlighting them with a click and then clicking the red X that will appear at the top right:</p>\n<ul>\n<li>the second CN with a value of “system administrator”</li>\n<li>displayName</li>\n<li>keyAlgorithm</li>\n<li>privateKey,</li>\n<li>privateKeyFormat,</li>\n<li>publicKey,</li>\n<li>publicKeyFormat</li>\n<li>userCertificate.</li>\n</ul>\n<p>Click OK when prompted to see if you are sure.  Click the Value field next to SN and change the text to “bi administrator”. When completed, you should be left with the values below.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_attributes_endresult-249x300.png\" alt=\"biadmin_attributes_endresult\"></a>Now we need to enter a user password. Double click the value field next to userPassword that currently says Empty Password and a Password Editor window will appear. Click the New Password tab to work with it. Enter a password for the user in the Enter New Password field, and then again in the Confirm New Password field.The password is stored with a HASH method, the default being “plaintext”. If you’d like to use a different Hash Method to encrypt your password, choose the option from the drop down list. You can check the box labeled <strong>Show new password details</strong> and the mask on the password fields will be replaced by the values you entered.<a href=\"http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/password_editor_change_filled-300x227.png\" alt=\"password_editor_change_filled\"></a> Click the OK button and the window will close and return you to the New Entry - Attributes window. Click OK again, and after a few seconds, your new user will be created! <a href=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_added.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/biadmin_added-300x147.png\" alt=\"biadmin_added\"></a>   This is the conclusion of my series on setting up Apache Directory Server.</p>"},{"title":"Austin MongoDB Road Show","id":"2150","comments":0,"date":"2014-06-23T19:17:17.000Z","_content":"\n![image](http://edpflager.com/wp-content/uploads/2014/06/wpid-mongodb_roadshow_emailhero.png \"MongoDB_RoadShow_EmailHero.png\") Last week I was on vacation in Texas, bouncing around San Antonio, Austin, Houston and several points in between, and I was lucky enough to be able to attend the MongoDB Road Show on Thursday afternoon. Being in IT for over a decade, I attend these events with a skeptical eye.  Go for the Swag, Stay for the Information is my honest opinion. Generally if its good a presentation, I can get something out of it, but if not, at least I got a T-Shirt or some other branded item I can use. I'm happy to report this was a successful trip. I've only touched briefly before on MongoDB here, not because I am unfamiliar with it, just that in my day to day work, it doesn't come up. I made an unsuccessful attempt a couple of years ago to get our development team interested in NoSQL in general and MongoDB specifically, but there was little interest. Despite that, I've still paid attention to what's been going on in this arena, though, and I'm going to attempt to post more on MongoDB in the future.\n<!-- more -->\nMongoDB is industry leader in the NoSQL world according to the initial presentation. There is a large number of job opportunities for developers familiar with it,  a large and growing installed user base, the company is attracting large amounts of investment dollars from various well known venture capital groups,  and its one of the most frequently searched for database systems. All of that seems pretty positive, but I wonder is it attracting attention because they have the best product, or is it because its just good enough? MongoDB certainly has one of the best marketing machines out there, far superior to other NoSQL vendors like Hortonworks (Hadoop), DataStax (Cassandra) and CouchBase. And as they gain market share, they have partnered with other vendors to expand the ecosystem. [Pentaho](http://www.pentaho.com) reps were present at the Road Show, providing free copies of a book on integrating their products with MongoDB and presenting on how easy it is to pull data to and from MongoDB. Pentaho Analytics now supports MongoDB across their analytics platform, not just in Kettle. I'll be testing and writing about that in future entries here. Another vendor - [Pure Storage](http://www.purestorage.com/) was also present (giving away tshirts and coozies) and talked about how their flash disk arrays improve disk read/write speed by several orders of magnitude. One of the biggest bottlenecks for large scale data sharding and redundancy is the amount of time it takes to write the data to multiple machines. By using flash storage, they are cutting  that latency down significantly. By partnering with them, the MongoDB team is addressing some concerns that have been voiced in the field that data was not synced and verified across multiple nodes before MongoDB moved on. Some of the  other enhancements that were covered indicate that the MongoDB team is actively listening to the user base and working to improve their product. The latest release (MongoDB 2.6.) introduced the MMS - management system that allows you to monitor your cluster from another machine. And some of the biggest roadblocks to larger enterprise adoptions are being worked on and rolled out including  enhancements in Security, BI and Analytics, and Text Search. Cramming all of this information into a couple of hours on a hot June afternoon gave me some new insight into MongoDB and the direction they are working towards. While I'm still not sold on them, I am ready to take another look at their product and try it out. More to come....","source":"_posts/austin-mongodb-road-show.md","raw":"---\ntitle: Austin MongoDB Road Show\ntags:\n  - Big Data\nid: '2150'\ncategories:\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2014-06-23 15:17:17\n---\n\n![image](http://edpflager.com/wp-content/uploads/2014/06/wpid-mongodb_roadshow_emailhero.png \"MongoDB_RoadShow_EmailHero.png\") Last week I was on vacation in Texas, bouncing around San Antonio, Austin, Houston and several points in between, and I was lucky enough to be able to attend the MongoDB Road Show on Thursday afternoon. Being in IT for over a decade, I attend these events with a skeptical eye.  Go for the Swag, Stay for the Information is my honest opinion. Generally if its good a presentation, I can get something out of it, but if not, at least I got a T-Shirt or some other branded item I can use. I'm happy to report this was a successful trip. I've only touched briefly before on MongoDB here, not because I am unfamiliar with it, just that in my day to day work, it doesn't come up. I made an unsuccessful attempt a couple of years ago to get our development team interested in NoSQL in general and MongoDB specifically, but there was little interest. Despite that, I've still paid attention to what's been going on in this arena, though, and I'm going to attempt to post more on MongoDB in the future.\n<!-- more -->\nMongoDB is industry leader in the NoSQL world according to the initial presentation. There is a large number of job opportunities for developers familiar with it,  a large and growing installed user base, the company is attracting large amounts of investment dollars from various well known venture capital groups,  and its one of the most frequently searched for database systems. All of that seems pretty positive, but I wonder is it attracting attention because they have the best product, or is it because its just good enough? MongoDB certainly has one of the best marketing machines out there, far superior to other NoSQL vendors like Hortonworks (Hadoop), DataStax (Cassandra) and CouchBase. And as they gain market share, they have partnered with other vendors to expand the ecosystem. [Pentaho](http://www.pentaho.com) reps were present at the Road Show, providing free copies of a book on integrating their products with MongoDB and presenting on how easy it is to pull data to and from MongoDB. Pentaho Analytics now supports MongoDB across their analytics platform, not just in Kettle. I'll be testing and writing about that in future entries here. Another vendor - [Pure Storage](http://www.purestorage.com/) was also present (giving away tshirts and coozies) and talked about how their flash disk arrays improve disk read/write speed by several orders of magnitude. One of the biggest bottlenecks for large scale data sharding and redundancy is the amount of time it takes to write the data to multiple machines. By using flash storage, they are cutting  that latency down significantly. By partnering with them, the MongoDB team is addressing some concerns that have been voiced in the field that data was not synced and verified across multiple nodes before MongoDB moved on. Some of the  other enhancements that were covered indicate that the MongoDB team is actively listening to the user base and working to improve their product. The latest release (MongoDB 2.6.) introduced the MMS - management system that allows you to monitor your cluster from another machine. And some of the biggest roadblocks to larger enterprise adoptions are being worked on and rolled out including  enhancements in Security, BI and Analytics, and Text Search. Cramming all of this information into a couple of hours on a hot June afternoon gave me some new insight into MongoDB and the direction they are working towards. While I'm still not sold on them, I am ready to take another look at their product and try it out. More to come....","slug":"austin-mongodb-road-show","published":1,"updated":"2020-08-23T20:54:34.858Z","layout":"post","photos":[],"link":"","_id":"ckeaq99sx000jsdjxcjms1oci","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/06/wpid-mongodb_roadshow_emailhero.png\" alt=\"image\" title=\"MongoDB_RoadShow_EmailHero.png\"> Last week I was on vacation in Texas, bouncing around San Antonio, Austin, Houston and several points in between, and I was lucky enough to be able to attend the MongoDB Road Show on Thursday afternoon. Being in IT for over a decade, I attend these events with a skeptical eye.  Go for the Swag, Stay for the Information is my honest opinion. Generally if its good a presentation, I can get something out of it, but if not, at least I got a T-Shirt or some other branded item I can use. I’m happy to report this was a successful trip. I’ve only touched briefly before on MongoDB here, not because I am unfamiliar with it, just that in my day to day work, it doesn’t come up. I made an unsuccessful attempt a couple of years ago to get our development team interested in NoSQL in general and MongoDB specifically, but there was little interest. Despite that, I’ve still paid attention to what’s been going on in this arena, though, and I’m going to attempt to post more on MongoDB in the future.</p>\n<a id=\"more\"></a>\n<p>MongoDB is industry leader in the NoSQL world according to the initial presentation. There is a large number of job opportunities for developers familiar with it,  a large and growing installed user base, the company is attracting large amounts of investment dollars from various well known venture capital groups,  and its one of the most frequently searched for database systems. All of that seems pretty positive, but I wonder is it attracting attention because they have the best product, or is it because its just good enough? MongoDB certainly has one of the best marketing machines out there, far superior to other NoSQL vendors like Hortonworks (Hadoop), DataStax (Cassandra) and CouchBase. And as they gain market share, they have partnered with other vendors to expand the ecosystem. <a href=\"http://www.pentaho.com/\">Pentaho</a> reps were present at the Road Show, providing free copies of a book on integrating their products with MongoDB and presenting on how easy it is to pull data to and from MongoDB. Pentaho Analytics now supports MongoDB across their analytics platform, not just in Kettle. I’ll be testing and writing about that in future entries here. Another vendor - <a href=\"http://www.purestorage.com/\">Pure Storage</a> was also present (giving away tshirts and coozies) and talked about how their flash disk arrays improve disk read/write speed by several orders of magnitude. One of the biggest bottlenecks for large scale data sharding and redundancy is the amount of time it takes to write the data to multiple machines. By using flash storage, they are cutting  that latency down significantly. By partnering with them, the MongoDB team is addressing some concerns that have been voiced in the field that data was not synced and verified across multiple nodes before MongoDB moved on. Some of the  other enhancements that were covered indicate that the MongoDB team is actively listening to the user base and working to improve their product. The latest release (MongoDB 2.6.) introduced the MMS - management system that allows you to monitor your cluster from another machine. And some of the biggest roadblocks to larger enterprise adoptions are being worked on and rolled out including  enhancements in Security, BI and Analytics, and Text Search. Cramming all of this information into a couple of hours on a hot June afternoon gave me some new insight into MongoDB and the direction they are working towards. While I’m still not sold on them, I am ready to take another look at their product and try it out. More to come….</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/06/wpid-mongodb_roadshow_emailhero.png\" alt=\"image\" title=\"MongoDB_RoadShow_EmailHero.png\"> Last week I was on vacation in Texas, bouncing around San Antonio, Austin, Houston and several points in between, and I was lucky enough to be able to attend the MongoDB Road Show on Thursday afternoon. Being in IT for over a decade, I attend these events with a skeptical eye.  Go for the Swag, Stay for the Information is my honest opinion. Generally if its good a presentation, I can get something out of it, but if not, at least I got a T-Shirt or some other branded item I can use. I’m happy to report this was a successful trip. I’ve only touched briefly before on MongoDB here, not because I am unfamiliar with it, just that in my day to day work, it doesn’t come up. I made an unsuccessful attempt a couple of years ago to get our development team interested in NoSQL in general and MongoDB specifically, but there was little interest. Despite that, I’ve still paid attention to what’s been going on in this arena, though, and I’m going to attempt to post more on MongoDB in the future.</p>","more":"<p>MongoDB is industry leader in the NoSQL world according to the initial presentation. There is a large number of job opportunities for developers familiar with it,  a large and growing installed user base, the company is attracting large amounts of investment dollars from various well known venture capital groups,  and its one of the most frequently searched for database systems. All of that seems pretty positive, but I wonder is it attracting attention because they have the best product, or is it because its just good enough? MongoDB certainly has one of the best marketing machines out there, far superior to other NoSQL vendors like Hortonworks (Hadoop), DataStax (Cassandra) and CouchBase. And as they gain market share, they have partnered with other vendors to expand the ecosystem. <a href=\"http://www.pentaho.com/\">Pentaho</a> reps were present at the Road Show, providing free copies of a book on integrating their products with MongoDB and presenting on how easy it is to pull data to and from MongoDB. Pentaho Analytics now supports MongoDB across their analytics platform, not just in Kettle. I’ll be testing and writing about that in future entries here. Another vendor - <a href=\"http://www.purestorage.com/\">Pure Storage</a> was also present (giving away tshirts and coozies) and talked about how their flash disk arrays improve disk read/write speed by several orders of magnitude. One of the biggest bottlenecks for large scale data sharding and redundancy is the amount of time it takes to write the data to multiple machines. By using flash storage, they are cutting  that latency down significantly. By partnering with them, the MongoDB team is addressing some concerns that have been voiced in the field that data was not synced and verified across multiple nodes before MongoDB moved on. Some of the  other enhancements that were covered indicate that the MongoDB team is actively listening to the user base and working to improve their product. The latest release (MongoDB 2.6.) introduced the MMS - management system that allows you to monitor your cluster from another machine. And some of the biggest roadblocks to larger enterprise adoptions are being worked on and rolled out including  enhancements in Security, BI and Analytics, and Text Search. Cramming all of this information into a couple of hours on a hot June afternoon gave me some new insight into MongoDB and the direction they are working towards. While I’m still not sold on them, I am ready to take another look at their product and try it out. More to come….</p>"},{"title":"Big Data Redux","id":"1803","comments":0,"date":"2014-02-13T20:30:32.000Z","_content":"\n[![bigdata2](http://edpflager.com/wp-content/uploads/2013/04/bigdata2-200x300.jpg)](http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg)After re-reading my initial post on [What is Big Data?](http://edpflager.com/?p=1473 \"What the Heck is BIG DATA?\") I decided a little more clarification was in order. Humans as a people are generating more data now than at any point in our history. Figures thrown out are that 90% of human data has been created in the past few years. I think that may be misleading because a lot of data that is generated isn't really human created data, but machine generated data. Human data may be a blog post like this one or a book from your local library.\n<!-- more -->\nMachine generated data could be a computer's system and application logs. The amount of data in those logs can be pretty extensive, but they are generated by monitoring the computer and interactions with it, from external sources (other systems and people) and internally (hard drive reads/writes, CPU activity, system temperature, etc). Machines can generate significantly more data in a short amount of time than a human can. To take all of that accumulated data and sift through it to find something meaningful will depend a lot on what you are trying to find. From the computer logs, you can look for errors if the system is malfunctioning. If the system was hacked, you might look for failed log-in attempts. A typical PC log might be several megabytes long which can be filtered pretty quickly. The biggest issue is how what to filter on to look through it quickly and efficiently to find what you are looking for. But when you have logs from a web server farm like Amazon or Google uses, sifting through those logs quickly and efficiently becomes a major challenge. Companies, organizations and governments are producing massive amounts of data because they are monitoring and measuring so many different aspects of their activities. But once they have that information, they need to come up with a way to search it, manipulate it, sort it, and codify what is valuable from what isn't. (And something that isn't necessary valuable today, may be valuable tomorrow.) That is the concept behind BIG DATA.","source":"_posts/big-data-redux.md","raw":"---\ntitle: Big Data Redux\ntags: []\nid: '1803'\ncategories:\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2014-02-13 15:30:32\n---\n\n[![bigdata2](http://edpflager.com/wp-content/uploads/2013/04/bigdata2-200x300.jpg)](http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg)After re-reading my initial post on [What is Big Data?](http://edpflager.com/?p=1473 \"What the Heck is BIG DATA?\") I decided a little more clarification was in order. Humans as a people are generating more data now than at any point in our history. Figures thrown out are that 90% of human data has been created in the past few years. I think that may be misleading because a lot of data that is generated isn't really human created data, but machine generated data. Human data may be a blog post like this one or a book from your local library.\n<!-- more -->\nMachine generated data could be a computer's system and application logs. The amount of data in those logs can be pretty extensive, but they are generated by monitoring the computer and interactions with it, from external sources (other systems and people) and internally (hard drive reads/writes, CPU activity, system temperature, etc). Machines can generate significantly more data in a short amount of time than a human can. To take all of that accumulated data and sift through it to find something meaningful will depend a lot on what you are trying to find. From the computer logs, you can look for errors if the system is malfunctioning. If the system was hacked, you might look for failed log-in attempts. A typical PC log might be several megabytes long which can be filtered pretty quickly. The biggest issue is how what to filter on to look through it quickly and efficiently to find what you are looking for. But when you have logs from a web server farm like Amazon or Google uses, sifting through those logs quickly and efficiently becomes a major challenge. Companies, organizations and governments are producing massive amounts of data because they are monitoring and measuring so many different aspects of their activities. But once they have that information, they need to come up with a way to search it, manipulate it, sort it, and codify what is valuable from what isn't. (And something that isn't necessary valuable today, may be valuable tomorrow.) That is the concept behind BIG DATA.","slug":"big-data-redux","published":1,"updated":"2020-08-23T20:54:34.786Z","layout":"post","photos":[],"link":"","_id":"ckeaq99t0000msdjx5qjh4dxx","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2-200x300.jpg\" alt=\"bigdata2\"></a>After re-reading my initial post on <a href=\"http://edpflager.com/?p=1473\" title=\"What the Heck is BIG DATA?\">What is Big Data?</a> I decided a little more clarification was in order. Humans as a people are generating more data now than at any point in our history. Figures thrown out are that 90% of human data has been created in the past few years. I think that may be misleading because a lot of data that is generated isn’t really human created data, but machine generated data. Human data may be a blog post like this one or a book from your local library.</p>\n<a id=\"more\"></a>\n<p>Machine generated data could be a computer’s system and application logs. The amount of data in those logs can be pretty extensive, but they are generated by monitoring the computer and interactions with it, from external sources (other systems and people) and internally (hard drive reads/writes, CPU activity, system temperature, etc). Machines can generate significantly more data in a short amount of time than a human can. To take all of that accumulated data and sift through it to find something meaningful will depend a lot on what you are trying to find. From the computer logs, you can look for errors if the system is malfunctioning. If the system was hacked, you might look for failed log-in attempts. A typical PC log might be several megabytes long which can be filtered pretty quickly. The biggest issue is how what to filter on to look through it quickly and efficiently to find what you are looking for. But when you have logs from a web server farm like Amazon or Google uses, sifting through those logs quickly and efficiently becomes a major challenge. Companies, organizations and governments are producing massive amounts of data because they are monitoring and measuring so many different aspects of their activities. But once they have that information, they need to come up with a way to search it, manipulate it, sort it, and codify what is valuable from what isn’t. (And something that isn’t necessary valuable today, may be valuable tomorrow.) That is the concept behind BIG DATA.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2-200x300.jpg\" alt=\"bigdata2\"></a>After re-reading my initial post on <a href=\"http://edpflager.com/?p=1473\" title=\"What the Heck is BIG DATA?\">What is Big Data?</a> I decided a little more clarification was in order. Humans as a people are generating more data now than at any point in our history. Figures thrown out are that 90% of human data has been created in the past few years. I think that may be misleading because a lot of data that is generated isn’t really human created data, but machine generated data. Human data may be a blog post like this one or a book from your local library.</p>","more":"<p>Machine generated data could be a computer’s system and application logs. The amount of data in those logs can be pretty extensive, but they are generated by monitoring the computer and interactions with it, from external sources (other systems and people) and internally (hard drive reads/writes, CPU activity, system temperature, etc). Machines can generate significantly more data in a short amount of time than a human can. To take all of that accumulated data and sift through it to find something meaningful will depend a lot on what you are trying to find. From the computer logs, you can look for errors if the system is malfunctioning. If the system was hacked, you might look for failed log-in attempts. A typical PC log might be several megabytes long which can be filtered pretty quickly. The biggest issue is how what to filter on to look through it quickly and efficiently to find what you are looking for. But when you have logs from a web server farm like Amazon or Google uses, sifting through those logs quickly and efficiently becomes a major challenge. Companies, organizations and governments are producing massive amounts of data because they are monitoring and measuring so many different aspects of their activities. But once they have that information, they need to come up with a way to search it, manipulate it, sort it, and codify what is valuable from what isn’t. (And something that isn’t necessary valuable today, may be valuable tomorrow.) That is the concept behind BIG DATA.</p>"},{"title":"Cannot connect to the Docker daemon - fixed!","id":"3256","comments":0,"date":"2016-02-07T00:10:08.000Z","_content":"\n[![containers](http://edpflager.com/wp-content/uploads/2016/02/containers-1529075-638x480-300x226.jpg)](http://edpflager.com/?attachment_id=3258#main)A quick note this time around: After installing Docker on my Linux Mint laptop, I wanted to be able to run it with my normal user account and not have to SUDO every time I wanted to start a container. (Please be advised that doing so can be considered a HUGE security hole, but since I am just testing Docker, I was willing to risk it). In order to do this, all of the documentation I found said to create a Docker group, and then add my user account to that group, and restart. This is fairly simple to accomplish with the GUI Users and Groups tool. The Docker daemon then uses that group to see if the user account has permissions to start Docker and connect to the daemon. The result? I would get this error: _Cannot connect to the Docker daemon. Is 'docker daemon' running on this host?_ After trying several things, I finally found the solution.\n\n1.  From a terminal prompt run this command:\n    \n    echo $DOCKER\\_HOST\n    \n    You should see a line indicating your Docker host is set to an IP address. You will need to clear it.\n2.  At the terminal prompt, switch  to your home folder:\n    \n    cd ~\n    \n3.  Now, edit your shell profile:\n    \n     nano ./.profile\n    \n4.  Look for a line like this:\n    \n     export DOCKER\\_HOST=tcp://127.0.0.1:4243\n    \n5.  Add a comment marker (#) before it or delete the line completely. Save your profile file and exit.\n6.  Restart your system, and you should now be able to run Docker using your normal user account.","source":"_posts/cannot-connect-to-the-docker-daemon-fixed.md","raw":"---\ntitle: Cannot connect to the Docker daemon - fixed!\ntags:\n  - guides\n  - How-to\n  - howto\n  - Mint\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3256'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-02-06 19:10:08\n---\n\n[![containers](http://edpflager.com/wp-content/uploads/2016/02/containers-1529075-638x480-300x226.jpg)](http://edpflager.com/?attachment_id=3258#main)A quick note this time around: After installing Docker on my Linux Mint laptop, I wanted to be able to run it with my normal user account and not have to SUDO every time I wanted to start a container. (Please be advised that doing so can be considered a HUGE security hole, but since I am just testing Docker, I was willing to risk it). In order to do this, all of the documentation I found said to create a Docker group, and then add my user account to that group, and restart. This is fairly simple to accomplish with the GUI Users and Groups tool. The Docker daemon then uses that group to see if the user account has permissions to start Docker and connect to the daemon. The result? I would get this error: _Cannot connect to the Docker daemon. Is 'docker daemon' running on this host?_ After trying several things, I finally found the solution.\n\n1.  From a terminal prompt run this command:\n    \n    echo $DOCKER\\_HOST\n    \n    You should see a line indicating your Docker host is set to an IP address. You will need to clear it.\n2.  At the terminal prompt, switch  to your home folder:\n    \n    cd ~\n    \n3.  Now, edit your shell profile:\n    \n     nano ./.profile\n    \n4.  Look for a line like this:\n    \n     export DOCKER\\_HOST=tcp://127.0.0.1:4243\n    \n5.  Add a comment marker (#) before it or delete the line completely. Save your profile file and exit.\n6.  Restart your system, and you should now be able to run Docker using your normal user account.","slug":"cannot-connect-to-the-docker-daemon-fixed","published":1,"updated":"2020-08-23T20:54:35.010Z","layout":"post","photos":[],"link":"","_id":"ckeaq99t3000nsdjxdenk3vb1","content":"<p><a href=\"http://edpflager.com/?attachment_id=3258#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/02/containers-1529075-638x480-300x226.jpg\" alt=\"containers\"></a>A quick note this time around: After installing Docker on my Linux Mint laptop, I wanted to be able to run it with my normal user account and not have to SUDO every time I wanted to start a container. (Please be advised that doing so can be considered a HUGE security hole, but since I am just testing Docker, I was willing to risk it). In order to do this, all of the documentation I found said to create a Docker group, and then add my user account to that group, and restart. This is fairly simple to accomplish with the GUI Users and Groups tool. The Docker daemon then uses that group to see if the user account has permissions to start Docker and connect to the daemon. The result? I would get this error: <em>Cannot connect to the Docker daemon. Is ‘docker daemon’ running on this host?</em> After trying several things, I finally found the solution.</p>\n<ol>\n<li><p>From a terminal prompt run this command:</p>\n<p>echo $DOCKER_HOST</p>\n<p>You should see a line indicating your Docker host is set to an IP address. You will need to clear it.</p>\n</li>\n<li><p>At the terminal prompt, switch  to your home folder:</p>\n<p>cd ~</p>\n</li>\n<li><p>Now, edit your shell profile:</p>\n<p> nano ./.profile</p>\n</li>\n<li><p>Look for a line like this:</p>\n<p> export DOCKER_HOST=tcp://127.0.0.1:4243</p>\n</li>\n<li><p>Add a comment marker (#) before it or delete the line completely. Save your profile file and exit.</p>\n</li>\n<li><p>Restart your system, and you should now be able to run Docker using your normal user account.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/?attachment_id=3258#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/02/containers-1529075-638x480-300x226.jpg\" alt=\"containers\"></a>A quick note this time around: After installing Docker on my Linux Mint laptop, I wanted to be able to run it with my normal user account and not have to SUDO every time I wanted to start a container. (Please be advised that doing so can be considered a HUGE security hole, but since I am just testing Docker, I was willing to risk it). In order to do this, all of the documentation I found said to create a Docker group, and then add my user account to that group, and restart. This is fairly simple to accomplish with the GUI Users and Groups tool. The Docker daemon then uses that group to see if the user account has permissions to start Docker and connect to the daemon. The result? I would get this error: <em>Cannot connect to the Docker daemon. Is ‘docker daemon’ running on this host?</em> After trying several things, I finally found the solution.</p>\n<ol>\n<li><p>From a terminal prompt run this command:</p>\n<p>echo $DOCKER_HOST</p>\n<p>You should see a line indicating your Docker host is set to an IP address. You will need to clear it.</p>\n</li>\n<li><p>At the terminal prompt, switch  to your home folder:</p>\n<p>cd ~</p>\n</li>\n<li><p>Now, edit your shell profile:</p>\n<p> nano ./.profile</p>\n</li>\n<li><p>Look for a line like this:</p>\n<p> export DOCKER_HOST=tcp://127.0.0.1:4243</p>\n</li>\n<li><p>Add a comment marker (#) before it or delete the line completely. Save your profile file and exit.</p>\n</li>\n<li><p>Restart your system, and you should now be able to run Docker using your normal user account.</p>\n</li>\n</ol>\n"},{"title":"Cassandra Installation on Linux Mint","id":"3168","comments":0,"date":"2015-12-20T23:46:18.000Z","_content":"\n[![Cassandra_logo](http://edpflager.com/wp-content/uploads/2015/12/Cassandra_logo.png)](http://edpflager.com/?attachment_id=3216#main)NoSQL databases are multiplying faster that anyone could have imagined when Google released their Google File System and MapReduce papers in 2003 and 2004 - the inspiration for a host of NoSQL database developers. The NoSQL project that became Apache Cassandra was originally developed at Facebook as the back-end for their Inbox Search feature. It was released as open source in July 2008 and later moved to the Apache Foundation. Gaining in popularity in the intervening time,  the website [db-engines.com](http://db-engines.com/en/ranking) now ranks Cassandra the eighth most popular data management system. Setting up a development version of Cassandra on Linux Mint is pretty straightforward, and this tutorial will walk through the process. At the time of this writing, Cassandra 3.0.1 had just been released. It requires Java 8 or later and at least 8GB of RAM. Version 2.2.4 is available as the most current stable version, which will run with Java 6 or later with 4GB or RAM. If you intend to use [DataStax's OpsCenter](https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/opscCompatibility.html) management tool be aware that it does not currently support either version. To illustrate the installation of OpsCenter I will be covering the older Cassandra version 2.1 which IS supported by OpsCenter. DataStax notes on their website that the next version of OpsCenter will support Cassandra 2.2 and 3.0. The difference between installing version 2.2 or 3.0 is minimal and I'll note that as we process. Also, be aware that the Linux Mint update center may try to upgrade your system to 3.0 once you are complete.\n<!-- more -->\n##### Cassandra Installation\n\nBefore starting, make sure your Linux Mint system is configured properly. See my previous post about [setting up a Linux Mint server.](http://edpflager.com/?p=3173) Once that is complete, continue with the instructions below.\n\n1\\. Edit the source list file to add the Cassandra repository information. In a terminal:\n\nsudo nano /etc/apt/sources.list\n\n2\\. On Linux Mint, this file is likely to only have one line in it. Go to the bottom of the file and add these two lines:\n\ndeb http://www.apache.org/dist/cassandra/debian 21x main\ndeb-src http://www.apache.org/dist/cassandra/debian 21x main\n\n3\\. To install the newer 3.0.1 version , change the 21x to 30x or if you want to install version 2.2.4 change it to 22x.\n\n4\\. Now, we need to add the public keys to authenticate the Cassandra repository:\n\ngpg --keyserver pgp.mit.edu --recv-keys F758CE318D77295D\ngpg --export --armor F758CE318D77295D | sudo apt-key add -\n\ngpg --keyserver pgp.mit.edu --recv-keys 2B5C1B00\ngpg --export --armor 2B5C1B00 | sudo apt-key add -\n\ngpg --keyserver pgp.mit.edu --recv-keys 0353B12C\ngpg --export --armor 0353B12C | sudo apt-key add -\n\n5\\. Update your system's cached repository information with:\n\nsudo apt-get update\n\n6\\. Now enter  this command to install Cassandra:\n\nsudo apt-get install cassandra\n\n7\\. Any dependent packages will be identified and you'll be prompted to install all of them, along with Cassandra. Touch Y to do so. Once installation completes, you'll be returned to the terminal prompt. At this point you may want to restart the server, just to make sure all is well with your setup.\n\n8\\. From a command prompt, start Cassandra with the command:\n\nsudo cassandra\n\n9.After a few seconds, your terminal window will fill with a large amount of text as Cassandra starts up. Once it completes,  and you are returned to a command prompt (or if it appears to hang, touch ENTER), you can access the interactive shell for Cassandra by entering this:\n\ncqlsh\n\n10\\. The cqlsh shell (Cassandra SQL Shell)  will display its version number and the version of your Cassandra installation and then return to a > prompt for input of commands.\n\n[![cqlsh](http://edpflager.com/wp-content/uploads/2015/12/cqlsh-300x48.png)](http://edpflager.com/?attachment_id=3212#main)\n\n11\\. Type \"quit\" (without the quotes) at the prompt and you'll exit the cqlsh application.\n\n \n\n##### DataStax OpsCenter Installation\n\n1\\. Edit the source list file to add the Cassandra repository information. In a terminal:\n\n1.  sudo nano /etc/apt/sources.list\n    \n    2\\. Go to the bottom of the file and add this line:\n    \n    deb http://debian.datastax.com/community stable main\n    \n    3\\. Download the Add the DataStax repository key:\n2.  $ sudo wget https://debian.datastax.com/debian/repo\\_key\n    \n\n4\\. Install the key you just downloaded with:\n\nsudo apt-key add ./repo\\_key\n\n5\\. Install the DataStax OpsCenter package using the APT Package Manager:\n\n$ sudo apt-get update\n$ sudo apt-get install opscenter\n\n6\\. Start the OpsCenter software with the command:\n\nsudo service opscenterd start\n\n7\\. In order for the OpsCenter software to communicate with your Cassandra installation, you'll need to install the Datastax agent:\n\nsudo apt-get install datastax-agent\n\n8\\. You'll need to make one configuration change to the DataStax **address.yaml** file.\n\nsudo nano /var/lib/datastax-agent/conf/address.yaml \n\n9\\. The file is probably empty at this point. Add this line to it, putting your IP address in place of the <phrase>:\n\nstomp\\_interface: <ip\\_address of the server>\n\n10. Now from a web browser access your Cassandra system on port 8888, i.e.: http://cassandra.yourdomain.com:8888. If all has gone well, OpsCenter will load and you'll see the dashboard indicating your systems health.\n\nCassandra logo By Apache Software Foundation used under Apache License 2.0 (http://www.apache.org/licenses/LICENSE-2.0)\\], via Wikimedia Commons","source":"_posts/cassandra-installation-on-linux-mint.md","raw":"---\ntitle: Cassandra Installation on Linux Mint\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - Mint\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3168'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-12-20 18:46:18\n---\n\n[![Cassandra_logo](http://edpflager.com/wp-content/uploads/2015/12/Cassandra_logo.png)](http://edpflager.com/?attachment_id=3216#main)NoSQL databases are multiplying faster that anyone could have imagined when Google released their Google File System and MapReduce papers in 2003 and 2004 - the inspiration for a host of NoSQL database developers. The NoSQL project that became Apache Cassandra was originally developed at Facebook as the back-end for their Inbox Search feature. It was released as open source in July 2008 and later moved to the Apache Foundation. Gaining in popularity in the intervening time,  the website [db-engines.com](http://db-engines.com/en/ranking) now ranks Cassandra the eighth most popular data management system. Setting up a development version of Cassandra on Linux Mint is pretty straightforward, and this tutorial will walk through the process. At the time of this writing, Cassandra 3.0.1 had just been released. It requires Java 8 or later and at least 8GB of RAM. Version 2.2.4 is available as the most current stable version, which will run with Java 6 or later with 4GB or RAM. If you intend to use [DataStax's OpsCenter](https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/opscCompatibility.html) management tool be aware that it does not currently support either version. To illustrate the installation of OpsCenter I will be covering the older Cassandra version 2.1 which IS supported by OpsCenter. DataStax notes on their website that the next version of OpsCenter will support Cassandra 2.2 and 3.0. The difference between installing version 2.2 or 3.0 is minimal and I'll note that as we process. Also, be aware that the Linux Mint update center may try to upgrade your system to 3.0 once you are complete.\n<!-- more -->\n##### Cassandra Installation\n\nBefore starting, make sure your Linux Mint system is configured properly. See my previous post about [setting up a Linux Mint server.](http://edpflager.com/?p=3173) Once that is complete, continue with the instructions below.\n\n1\\. Edit the source list file to add the Cassandra repository information. In a terminal:\n\nsudo nano /etc/apt/sources.list\n\n2\\. On Linux Mint, this file is likely to only have one line in it. Go to the bottom of the file and add these two lines:\n\ndeb http://www.apache.org/dist/cassandra/debian 21x main\ndeb-src http://www.apache.org/dist/cassandra/debian 21x main\n\n3\\. To install the newer 3.0.1 version , change the 21x to 30x or if you want to install version 2.2.4 change it to 22x.\n\n4\\. Now, we need to add the public keys to authenticate the Cassandra repository:\n\ngpg --keyserver pgp.mit.edu --recv-keys F758CE318D77295D\ngpg --export --armor F758CE318D77295D | sudo apt-key add -\n\ngpg --keyserver pgp.mit.edu --recv-keys 2B5C1B00\ngpg --export --armor 2B5C1B00 | sudo apt-key add -\n\ngpg --keyserver pgp.mit.edu --recv-keys 0353B12C\ngpg --export --armor 0353B12C | sudo apt-key add -\n\n5\\. Update your system's cached repository information with:\n\nsudo apt-get update\n\n6\\. Now enter  this command to install Cassandra:\n\nsudo apt-get install cassandra\n\n7\\. Any dependent packages will be identified and you'll be prompted to install all of them, along with Cassandra. Touch Y to do so. Once installation completes, you'll be returned to the terminal prompt. At this point you may want to restart the server, just to make sure all is well with your setup.\n\n8\\. From a command prompt, start Cassandra with the command:\n\nsudo cassandra\n\n9.After a few seconds, your terminal window will fill with a large amount of text as Cassandra starts up. Once it completes,  and you are returned to a command prompt (or if it appears to hang, touch ENTER), you can access the interactive shell for Cassandra by entering this:\n\ncqlsh\n\n10\\. The cqlsh shell (Cassandra SQL Shell)  will display its version number and the version of your Cassandra installation and then return to a > prompt for input of commands.\n\n[![cqlsh](http://edpflager.com/wp-content/uploads/2015/12/cqlsh-300x48.png)](http://edpflager.com/?attachment_id=3212#main)\n\n11\\. Type \"quit\" (without the quotes) at the prompt and you'll exit the cqlsh application.\n\n \n\n##### DataStax OpsCenter Installation\n\n1\\. Edit the source list file to add the Cassandra repository information. In a terminal:\n\n1.  sudo nano /etc/apt/sources.list\n    \n    2\\. Go to the bottom of the file and add this line:\n    \n    deb http://debian.datastax.com/community stable main\n    \n    3\\. Download the Add the DataStax repository key:\n2.  $ sudo wget https://debian.datastax.com/debian/repo\\_key\n    \n\n4\\. Install the key you just downloaded with:\n\nsudo apt-key add ./repo\\_key\n\n5\\. Install the DataStax OpsCenter package using the APT Package Manager:\n\n$ sudo apt-get update\n$ sudo apt-get install opscenter\n\n6\\. Start the OpsCenter software with the command:\n\nsudo service opscenterd start\n\n7\\. In order for the OpsCenter software to communicate with your Cassandra installation, you'll need to install the Datastax agent:\n\nsudo apt-get install datastax-agent\n\n8\\. You'll need to make one configuration change to the DataStax **address.yaml** file.\n\nsudo nano /var/lib/datastax-agent/conf/address.yaml \n\n9\\. The file is probably empty at this point. Add this line to it, putting your IP address in place of the <phrase>:\n\nstomp\\_interface: <ip\\_address of the server>\n\n10. Now from a web browser access your Cassandra system on port 8888, i.e.: http://cassandra.yourdomain.com:8888. If all has gone well, OpsCenter will load and you'll see the dashboard indicating your systems health.\n\nCassandra logo By Apache Software Foundation used under Apache License 2.0 (http://www.apache.org/licenses/LICENSE-2.0)\\], via Wikimedia Commons","slug":"cassandra-installation-on-linux-mint","published":1,"updated":"2020-08-23T20:54:35.002Z","layout":"post","photos":[],"link":"","_id":"ckeaq99t5000psdjx3dr1cbpm","content":"<p><a href=\"http://edpflager.com/?attachment_id=3216#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Cassandra_logo.png\" alt=\"Cassandra_logo\"></a>NoSQL databases are multiplying faster that anyone could have imagined when Google released their Google File System and MapReduce papers in 2003 and 2004 - the inspiration for a host of NoSQL database developers. The NoSQL project that became Apache Cassandra was originally developed at Facebook as the back-end for their Inbox Search feature. It was released as open source in July 2008 and later moved to the Apache Foundation. Gaining in popularity in the intervening time,  the website <a href=\"http://db-engines.com/en/ranking\">db-engines.com</a> now ranks Cassandra the eighth most popular data management system. Setting up a development version of Cassandra on Linux Mint is pretty straightforward, and this tutorial will walk through the process. At the time of this writing, Cassandra 3.0.1 had just been released. It requires Java 8 or later and at least 8GB of RAM. Version 2.2.4 is available as the most current stable version, which will run with Java 6 or later with 4GB or RAM. If you intend to use <a href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/opscCompatibility.html\">DataStax’s OpsCenter</a> management tool be aware that it does not currently support either version. To illustrate the installation of OpsCenter I will be covering the older Cassandra version 2.1 which IS supported by OpsCenter. DataStax notes on their website that the next version of OpsCenter will support Cassandra 2.2 and 3.0. The difference between installing version 2.2 or 3.0 is minimal and I’ll note that as we process. Also, be aware that the Linux Mint update center may try to upgrade your system to 3.0 once you are complete.</p>\n<a id=\"more\"></a>\n<h5 id=\"Cassandra-Installation\"><a href=\"#Cassandra-Installation\" class=\"headerlink\" title=\"Cassandra Installation\"></a>Cassandra Installation</h5><p>Before starting, make sure your Linux Mint system is configured properly. See my previous post about <a href=\"http://edpflager.com/?p=3173\">setting up a Linux Mint server.</a> Once that is complete, continue with the instructions below.</p>\n<p>1. Edit the source list file to add the Cassandra repository information. In a terminal:</p>\n<p>sudo nano /etc/apt/sources.list</p>\n<p>2. On Linux Mint, this file is likely to only have one line in it. Go to the bottom of the file and add these two lines:</p>\n<p>deb <a href=\"http://www.apache.org/dist/cassandra/debian\">http://www.apache.org/dist/cassandra/debian</a> 21x main<br>deb-src <a href=\"http://www.apache.org/dist/cassandra/debian\">http://www.apache.org/dist/cassandra/debian</a> 21x main</p>\n<p>3. To install the newer 3.0.1 version , change the 21x to 30x or if you want to install version 2.2.4 change it to 22x.</p>\n<p>4. Now, we need to add the public keys to authenticate the Cassandra repository:</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys F758CE318D77295D<br>gpg –export –armor F758CE318D77295D | sudo apt-key add -</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys 2B5C1B00<br>gpg –export –armor 2B5C1B00 | sudo apt-key add -</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys 0353B12C<br>gpg –export –armor 0353B12C | sudo apt-key add -</p>\n<p>5. Update your system’s cached repository information with:</p>\n<p>sudo apt-get update</p>\n<p>6. Now enter  this command to install Cassandra:</p>\n<p>sudo apt-get install cassandra</p>\n<p>7. Any dependent packages will be identified and you’ll be prompted to install all of them, along with Cassandra. Touch Y to do so. Once installation completes, you’ll be returned to the terminal prompt. At this point you may want to restart the server, just to make sure all is well with your setup.</p>\n<p>8. From a command prompt, start Cassandra with the command:</p>\n<p>sudo cassandra</p>\n<p>9.After a few seconds, your terminal window will fill with a large amount of text as Cassandra starts up. Once it completes,  and you are returned to a command prompt (or if it appears to hang, touch ENTER), you can access the interactive shell for Cassandra by entering this:</p>\n<p>cqlsh</p>\n<p>10. The cqlsh shell (Cassandra SQL Shell)  will display its version number and the version of your Cassandra installation and then return to a &gt; prompt for input of commands.</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3212#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/cqlsh-300x48.png\" alt=\"cqlsh\"></a></p>\n<p>11. Type “quit” (without the quotes) at the prompt and you’ll exit the cqlsh application.</p>\n<p> </p>\n<h5 id=\"DataStax-OpsCenter-Installation\"><a href=\"#DataStax-OpsCenter-Installation\" class=\"headerlink\" title=\"DataStax OpsCenter Installation\"></a>DataStax OpsCenter Installation</h5><p>1. Edit the source list file to add the Cassandra repository information. In a terminal:</p>\n<ol>\n<li><p>sudo nano /etc/apt/sources.list</p>\n<p>2. Go to the bottom of the file and add this line:</p>\n<p>deb <a href=\"http://debian.datastax.com/community\">http://debian.datastax.com/community</a> stable main</p>\n<p>3. Download the Add the DataStax repository key:</p>\n</li>\n<li><p>$ sudo wget <a href=\"https://debian.datastax.com/debian/repo/_key\">https://debian.datastax.com/debian/repo\\_key</a></p>\n</li>\n</ol>\n<p>4. Install the key you just downloaded with:</p>\n<p>sudo apt-key add ./repo_key</p>\n<p>5. Install the DataStax OpsCenter package using the APT Package Manager:</p>\n<p>$ sudo apt-get update<br>$ sudo apt-get install opscenter</p>\n<p>6. Start the OpsCenter software with the command:</p>\n<p>sudo service opscenterd start</p>\n<p>7. In order for the OpsCenter software to communicate with your Cassandra installation, you’ll need to install the Datastax agent:</p>\n<p>sudo apt-get install datastax-agent</p>\n<p>8. You’ll need to make one configuration change to the DataStax <strong>address.yaml</strong> file.</p>\n<p>sudo nano /var/lib/datastax-agent/conf/address.yaml </p>\n<p>9. The file is probably empty at this point. Add this line to it, putting your IP address in place of the <phrase>:</p>\n<p>stomp_interface: &lt;ip_address of the server&gt;</p>\n<p>10. Now from a web browser access your Cassandra system on port 8888, i.e.: <a href=\"http://cassandra.yourdomain.com:8888/\">http://cassandra.yourdomain.com:8888</a>. If all has gone well, OpsCenter will load and you’ll see the dashboard indicating your systems health.</p>\n<p>Cassandra logo By Apache Software Foundation used under Apache License 2.0 (<a href=\"http://www.apache.org/licenses/LICENSE-2.0)/]\">http://www.apache.org/licenses/LICENSE-2.0)\\]</a>, via Wikimedia Commons</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3216#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/Cassandra_logo.png\" alt=\"Cassandra_logo\"></a>NoSQL databases are multiplying faster that anyone could have imagined when Google released their Google File System and MapReduce papers in 2003 and 2004 - the inspiration for a host of NoSQL database developers. The NoSQL project that became Apache Cassandra was originally developed at Facebook as the back-end for their Inbox Search feature. It was released as open source in July 2008 and later moved to the Apache Foundation. Gaining in popularity in the intervening time,  the website <a href=\"http://db-engines.com/en/ranking\">db-engines.com</a> now ranks Cassandra the eighth most popular data management system. Setting up a development version of Cassandra on Linux Mint is pretty straightforward, and this tutorial will walk through the process. At the time of this writing, Cassandra 3.0.1 had just been released. It requires Java 8 or later and at least 8GB of RAM. Version 2.2.4 is available as the most current stable version, which will run with Java 6 or later with 4GB or RAM. If you intend to use <a href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/opscCompatibility.html\">DataStax’s OpsCenter</a> management tool be aware that it does not currently support either version. To illustrate the installation of OpsCenter I will be covering the older Cassandra version 2.1 which IS supported by OpsCenter. DataStax notes on their website that the next version of OpsCenter will support Cassandra 2.2 and 3.0. The difference between installing version 2.2 or 3.0 is minimal and I’ll note that as we process. Also, be aware that the Linux Mint update center may try to upgrade your system to 3.0 once you are complete.</p>","more":"<h5 id=\"Cassandra-Installation\"><a href=\"#Cassandra-Installation\" class=\"headerlink\" title=\"Cassandra Installation\"></a>Cassandra Installation</h5><p>Before starting, make sure your Linux Mint system is configured properly. See my previous post about <a href=\"http://edpflager.com/?p=3173\">setting up a Linux Mint server.</a> Once that is complete, continue with the instructions below.</p>\n<p>1. Edit the source list file to add the Cassandra repository information. In a terminal:</p>\n<p>sudo nano /etc/apt/sources.list</p>\n<p>2. On Linux Mint, this file is likely to only have one line in it. Go to the bottom of the file and add these two lines:</p>\n<p>deb <a href=\"http://www.apache.org/dist/cassandra/debian\">http://www.apache.org/dist/cassandra/debian</a> 21x main<br>deb-src <a href=\"http://www.apache.org/dist/cassandra/debian\">http://www.apache.org/dist/cassandra/debian</a> 21x main</p>\n<p>3. To install the newer 3.0.1 version , change the 21x to 30x or if you want to install version 2.2.4 change it to 22x.</p>\n<p>4. Now, we need to add the public keys to authenticate the Cassandra repository:</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys F758CE318D77295D<br>gpg –export –armor F758CE318D77295D | sudo apt-key add -</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys 2B5C1B00<br>gpg –export –armor 2B5C1B00 | sudo apt-key add -</p>\n<p>gpg –keyserver pgp.mit.edu –recv-keys 0353B12C<br>gpg –export –armor 0353B12C | sudo apt-key add -</p>\n<p>5. Update your system’s cached repository information with:</p>\n<p>sudo apt-get update</p>\n<p>6. Now enter  this command to install Cassandra:</p>\n<p>sudo apt-get install cassandra</p>\n<p>7. Any dependent packages will be identified and you’ll be prompted to install all of them, along with Cassandra. Touch Y to do so. Once installation completes, you’ll be returned to the terminal prompt. At this point you may want to restart the server, just to make sure all is well with your setup.</p>\n<p>8. From a command prompt, start Cassandra with the command:</p>\n<p>sudo cassandra</p>\n<p>9.After a few seconds, your terminal window will fill with a large amount of text as Cassandra starts up. Once it completes,  and you are returned to a command prompt (or if it appears to hang, touch ENTER), you can access the interactive shell for Cassandra by entering this:</p>\n<p>cqlsh</p>\n<p>10. The cqlsh shell (Cassandra SQL Shell)  will display its version number and the version of your Cassandra installation and then return to a &gt; prompt for input of commands.</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3212#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/cqlsh-300x48.png\" alt=\"cqlsh\"></a></p>\n<p>11. Type “quit” (without the quotes) at the prompt and you’ll exit the cqlsh application.</p>\n<p> </p>\n<h5 id=\"DataStax-OpsCenter-Installation\"><a href=\"#DataStax-OpsCenter-Installation\" class=\"headerlink\" title=\"DataStax OpsCenter Installation\"></a>DataStax OpsCenter Installation</h5><p>1. Edit the source list file to add the Cassandra repository information. In a terminal:</p>\n<ol>\n<li><p>sudo nano /etc/apt/sources.list</p>\n<p>2. Go to the bottom of the file and add this line:</p>\n<p>deb <a href=\"http://debian.datastax.com/community\">http://debian.datastax.com/community</a> stable main</p>\n<p>3. Download the Add the DataStax repository key:</p>\n</li>\n<li><p>$ sudo wget <a href=\"https://debian.datastax.com/debian/repo/_key\">https://debian.datastax.com/debian/repo\\_key</a></p>\n</li>\n</ol>\n<p>4. Install the key you just downloaded with:</p>\n<p>sudo apt-key add ./repo_key</p>\n<p>5. Install the DataStax OpsCenter package using the APT Package Manager:</p>\n<p>$ sudo apt-get update<br>$ sudo apt-get install opscenter</p>\n<p>6. Start the OpsCenter software with the command:</p>\n<p>sudo service opscenterd start</p>\n<p>7. In order for the OpsCenter software to communicate with your Cassandra installation, you’ll need to install the Datastax agent:</p>\n<p>sudo apt-get install datastax-agent</p>\n<p>8. You’ll need to make one configuration change to the DataStax <strong>address.yaml</strong> file.</p>\n<p>sudo nano /var/lib/datastax-agent/conf/address.yaml </p>\n<p>9. The file is probably empty at this point. Add this line to it, putting your IP address in place of the <phrase>:</p>\n<p>stomp_interface: &lt;ip_address of the server&gt;</p>\n<p>10. Now from a web browser access your Cassandra system on port 8888, i.e.: <a href=\"http://cassandra.yourdomain.com:8888/\">http://cassandra.yourdomain.com:8888</a>. If all has gone well, OpsCenter will load and you’ll see the dashboard indicating your systems health.</p>\n<p>Cassandra logo By Apache Software Foundation used under Apache License 2.0 (<a href=\"http://www.apache.org/licenses/LICENSE-2.0)/]\">http://www.apache.org/licenses/LICENSE-2.0)\\]</a>, via Wikimedia Commons</p>"},{"title":"Centos - Show details during boot","id":"2646","comments":0,"date":"2015-01-10T00:02:51.000Z","_content":"\n[![startup](http://edpflager.com/wp-content/uploads/2015/01/startup-300x167.png)](http://edpflager.com/wp-content/uploads/2015/01/startup.png)By default CentOS 6 shows an animation while the system boots up, indicating its progress with either a rotating ring or a progress bar (in my experience physical machine installs show the rings, and VM installs show the progress bar). However, if you are from a sysadmin background or are responsible for monitoring one or more CentOS boxes, you may want to see what's happening while the system comes up, rather than a simple animation.\n<!-- more -->\nIf you want to see the startup progress one time, just hit the up arrow key on your keyboard, and the animation will disappear and be replaced with scrolling text showing you what is happening on the box. If you want the text mode to display every time you start the system, the process is pretty straightforward, and only takes a couple of minutes to enable.\n\n1.  Open a terminal and switch to Super User mode with:\n    \n    ```\n    su -\n    ```\n    \n    When prompted, enter your password.\n2.  Type this command:\n    \n    ```\n    plymouth-set-default-theme details <ENTER>\n    ```\n    \n3.  Next type this command:\n    \n    ```\n    /usr/libexec/plymouth/plymouth-update-initrd <ENTER>\n    ```\n    \n    The system will process for a few minutes (on my VM it took about a minute), and eventually the command prompt will be returned.\n4.  That's it. The next time you start the system the animation will be replaced with a scrolling screen indicating the progress of various system processes as they start. If you want to switch back to the rings/progress bar, reenter the commands, and replace \"details\" in the first command with \"rings\" .\n\nSo what did the commands above do? The first command tells the plymouth application to use the details theme as its default. Plymouth is the program that shows the animation at boot up and it interacts with  the initrd process while the system starts. For a more detailed explanation of plymouth, check out this [link](http://www.freedesktop.org/wiki/Software/Plymouth/). The second command updates the plymouth process to use the new default. There are other plymouth themes available and you can even create your own if you are so inclined.","source":"_posts/centos-show-details-during-boot.md","raw":"---\ntitle: Centos - Show details during boot\ntags:\n  - goofy\n  - How-to\n  - howto\n  - SysAdmin\n  - technical\nid: '2646'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-01-09 19:02:51\n---\n\n[![startup](http://edpflager.com/wp-content/uploads/2015/01/startup-300x167.png)](http://edpflager.com/wp-content/uploads/2015/01/startup.png)By default CentOS 6 shows an animation while the system boots up, indicating its progress with either a rotating ring or a progress bar (in my experience physical machine installs show the rings, and VM installs show the progress bar). However, if you are from a sysadmin background or are responsible for monitoring one or more CentOS boxes, you may want to see what's happening while the system comes up, rather than a simple animation.\n<!-- more -->\nIf you want to see the startup progress one time, just hit the up arrow key on your keyboard, and the animation will disappear and be replaced with scrolling text showing you what is happening on the box. If you want the text mode to display every time you start the system, the process is pretty straightforward, and only takes a couple of minutes to enable.\n\n1.  Open a terminal and switch to Super User mode with:\n    \n    ```\n    su -\n    ```\n    \n    When prompted, enter your password.\n2.  Type this command:\n    \n    ```\n    plymouth-set-default-theme details <ENTER>\n    ```\n    \n3.  Next type this command:\n    \n    ```\n    /usr/libexec/plymouth/plymouth-update-initrd <ENTER>\n    ```\n    \n    The system will process for a few minutes (on my VM it took about a minute), and eventually the command prompt will be returned.\n4.  That's it. The next time you start the system the animation will be replaced with a scrolling screen indicating the progress of various system processes as they start. If you want to switch back to the rings/progress bar, reenter the commands, and replace \"details\" in the first command with \"rings\" .\n\nSo what did the commands above do? The first command tells the plymouth application to use the details theme as its default. Plymouth is the program that shows the animation at boot up and it interacts with  the initrd process while the system starts. For a more detailed explanation of plymouth, check out this [link](http://www.freedesktop.org/wiki/Software/Plymouth/). The second command updates the plymouth process to use the new default. There are other plymouth themes available and you can even create your own if you are so inclined.","slug":"centos-show-details-during-boot","published":1,"updated":"2020-08-23T20:54:34.918Z","layout":"post","photos":[],"link":"","_id":"ckeaq99tc000rsdjx9fxme6p6","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/01/startup.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/01/startup-300x167.png\" alt=\"startup\"></a>By default CentOS 6 shows an animation while the system boots up, indicating its progress with either a rotating ring or a progress bar (in my experience physical machine installs show the rings, and VM installs show the progress bar). However, if you are from a sysadmin background or are responsible for monitoring one or more CentOS boxes, you may want to see what’s happening while the system comes up, rather than a simple animation.</p>\n<a id=\"more\"></a>\n<p>If you want to see the startup progress one time, just hit the up arrow key on your keyboard, and the animation will disappear and be replaced with scrolling text showing you what is happening on the box. If you want the text mode to display every time you start the system, the process is pretty straightforward, and only takes a couple of minutes to enable.</p>\n<ol>\n<li><p>Open a terminal and switch to Super User mode with:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su -</span><br></pre></td></tr></table></figure>\n\n<p>When prompted, enter your password.</p>\n</li>\n<li><p>Type this command:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plymouth-set-default-theme details &lt;ENTER&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Next type this command:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;usr&#x2F;libexec&#x2F;plymouth&#x2F;plymouth-update-initrd &lt;ENTER&gt;</span><br></pre></td></tr></table></figure>\n\n<p>The system will process for a few minutes (on my VM it took about a minute), and eventually the command prompt will be returned.</p>\n</li>\n<li><p>That’s it. The next time you start the system the animation will be replaced with a scrolling screen indicating the progress of various system processes as they start. If you want to switch back to the rings/progress bar, reenter the commands, and replace “details” in the first command with “rings” .</p>\n</li>\n</ol>\n<p>So what did the commands above do? The first command tells the plymouth application to use the details theme as its default. Plymouth is the program that shows the animation at boot up and it interacts with  the initrd process while the system starts. For a more detailed explanation of plymouth, check out this <a href=\"http://www.freedesktop.org/wiki/Software/Plymouth/\">link</a>. The second command updates the plymouth process to use the new default. There are other plymouth themes available and you can even create your own if you are so inclined.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/01/startup.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/01/startup-300x167.png\" alt=\"startup\"></a>By default CentOS 6 shows an animation while the system boots up, indicating its progress with either a rotating ring or a progress bar (in my experience physical machine installs show the rings, and VM installs show the progress bar). However, if you are from a sysadmin background or are responsible for monitoring one or more CentOS boxes, you may want to see what’s happening while the system comes up, rather than a simple animation.</p>","more":"<p>If you want to see the startup progress one time, just hit the up arrow key on your keyboard, and the animation will disappear and be replaced with scrolling text showing you what is happening on the box. If you want the text mode to display every time you start the system, the process is pretty straightforward, and only takes a couple of minutes to enable.</p>\n<ol>\n<li><p>Open a terminal and switch to Super User mode with:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">su -</span><br></pre></td></tr></table></figure>\n\n<p>When prompted, enter your password.</p>\n</li>\n<li><p>Type this command:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plymouth-set-default-theme details &lt;ENTER&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Next type this command:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;usr&#x2F;libexec&#x2F;plymouth&#x2F;plymouth-update-initrd &lt;ENTER&gt;</span><br></pre></td></tr></table></figure>\n\n<p>The system will process for a few minutes (on my VM it took about a minute), and eventually the command prompt will be returned.</p>\n</li>\n<li><p>That’s it. The next time you start the system the animation will be replaced with a scrolling screen indicating the progress of various system processes as they start. If you want to switch back to the rings/progress bar, reenter the commands, and replace “details” in the first command with “rings” .</p>\n</li>\n</ol>\n<p>So what did the commands above do? The first command tells the plymouth application to use the details theme as its default. Plymouth is the program that shows the animation at boot up and it interacts with  the initrd process while the system starts. For a more detailed explanation of plymouth, check out this <a href=\"http://www.freedesktop.org/wiki/Software/Plymouth/\">link</a>. The second command updates the plymouth process to use the new default. There are other plymouth themes available and you can even create your own if you are so inclined.</p>"},{"title":"Change character encoding with Pentaho Select Values","id":"2696","comments":0,"date":"2015-04-01T23:08:33.000Z","_content":"\n[![468319_70689252](http://edpflager.com/wp-content/uploads/2015/04/468319_70689252-300x225.jpg)](http://edpflager.com/wp-content/uploads/2015/04/468319_70689252.jpg)Moving data between different systems often requires converting between different character encoding specifications. If you aren't familiar with the term, it means how characters are stored programmatically. In Latin character based languages like English, there are fewer characters, and they require a smaller amount of code to represent them. When computers had a much smaller amount of memory, processing power and disk space the smaller foot print of the ASCII and Windows 1252 characters sets were widely used to conserve resources. However, in other non-Latin character based languages, there can be a significantly larger amount of characters that make them up. Consequently, more system resources are required to represent them. Today, with larger amounts of data than ever before moving between different counties, moving that information between systems with different character schemes  can result in scrambled data if the underlying encoding isn't taken into account. UTF-8 and UTF-16 encoding schemas are the most prevalent and widely accepted specifications, and should be used in place of the older ASCII and Windows 1252 schemes. Moving data to this format with Pentaho Data Integrator (aka Kettle) can be handled with a transform component called Select Values, although the method to perform this process is somewhat hidden.\n<!-- more -->\nThe example here involves moving some data from an older RDBMS system that is using the Windows 1252 scheme to one using UTF-8.\n\n1.  Start a new transformation in Pentaho with a connection to your source system.\n2.  On the workspace, drag out a Table Input component, and define your initial query. For my purposes, I was accessing a table with countries from around the world with their currency names.\n3.  Open the Transform node in the Design pallet and drag out the Select Values component.\n4.  Attach Table Input to Select Values and double click it. There are three tabs, but you can only use one at a time. If you need to perform operations from two or more tabs, you'll need to string them together in successive steps. The functions are:\n    *   Select & Alter allows you to rename fields and also change the Length or Precision of the field.\n    *   Remove allows you to remove a field that is no longer needed from the processing stream.\n    *   Meta-data allows you to change the information about the fields in the processing stream and includes many of the Select & Alter options.\n5.  Click the Meta-data tab, and then click the \"Get fields to change\" button if you want all of the fields to be added. If you only want one or a few fields to be added, click on the down arrow button on the right side of the field name column  and choose the column you want to add. ![fieldnames](http://edpflager.com/wp-content/uploads/2015/04/fieldnames-300x293.png)\n6.  Moving to the right, you can:\n    *   rename the field, \n    *   change the type with the length and precision columns (similar to a CAST statement)\n    *   switch a binary field to a normal value\n    *   supply a different format for the field\n    *   indicate if the date format is lenient, change the date locale and date time zone \n    *   indicate if the number conversion is lenient as well, \n    *   change the encoding type of the data, and\n    *   make changes to the decimal format, grouping and currency settings\n7.  To avoid confusion, its best to rename the field to when changing the encoding. Supply a new name in the rename column.\n8.  Click on the down arrow to the right of the encoding column, and you will be presented with a long list of different encoding schemes. Select UTF-8 from the list, and then click OK at the bottom.[![encoding_types](http://edpflager.com/wp-content/uploads/2015/04/encoding_types-233x300.png)](http://edpflager.com/wp-content/uploads/2015/04/encoding_types.png)\n9.  Open the table output component and provide the necessary connection information for your output table. Check the Specify database fields box, and then click the Database fields tab.\n10.  Click the Get fields button and the fields from the previous step will be populated. If you renamed your column for the encoding change, the new field name will be here. Click OK at the bottom of the window.\n11.  Save your transformation and run it. Check the destination database, and the fields' encoding type should now match the new type you selected.","source":"_posts/change-character-encoding-with-pentaho-select-values.md","raw":"---\ntitle: Change character encoding with Pentaho Select Values\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2696'\ncategories:\n  - - Blog\n  - - Pentaho\ncomments: false\ndate: 2015-04-01 19:08:33\n---\n\n[![468319_70689252](http://edpflager.com/wp-content/uploads/2015/04/468319_70689252-300x225.jpg)](http://edpflager.com/wp-content/uploads/2015/04/468319_70689252.jpg)Moving data between different systems often requires converting between different character encoding specifications. If you aren't familiar with the term, it means how characters are stored programmatically. In Latin character based languages like English, there are fewer characters, and they require a smaller amount of code to represent them. When computers had a much smaller amount of memory, processing power and disk space the smaller foot print of the ASCII and Windows 1252 characters sets were widely used to conserve resources. However, in other non-Latin character based languages, there can be a significantly larger amount of characters that make them up. Consequently, more system resources are required to represent them. Today, with larger amounts of data than ever before moving between different counties, moving that information between systems with different character schemes  can result in scrambled data if the underlying encoding isn't taken into account. UTF-8 and UTF-16 encoding schemas are the most prevalent and widely accepted specifications, and should be used in place of the older ASCII and Windows 1252 schemes. Moving data to this format with Pentaho Data Integrator (aka Kettle) can be handled with a transform component called Select Values, although the method to perform this process is somewhat hidden.\n<!-- more -->\nThe example here involves moving some data from an older RDBMS system that is using the Windows 1252 scheme to one using UTF-8.\n\n1.  Start a new transformation in Pentaho with a connection to your source system.\n2.  On the workspace, drag out a Table Input component, and define your initial query. For my purposes, I was accessing a table with countries from around the world with their currency names.\n3.  Open the Transform node in the Design pallet and drag out the Select Values component.\n4.  Attach Table Input to Select Values and double click it. There are three tabs, but you can only use one at a time. If you need to perform operations from two or more tabs, you'll need to string them together in successive steps. The functions are:\n    *   Select & Alter allows you to rename fields and also change the Length or Precision of the field.\n    *   Remove allows you to remove a field that is no longer needed from the processing stream.\n    *   Meta-data allows you to change the information about the fields in the processing stream and includes many of the Select & Alter options.\n5.  Click the Meta-data tab, and then click the \"Get fields to change\" button if you want all of the fields to be added. If you only want one or a few fields to be added, click on the down arrow button on the right side of the field name column  and choose the column you want to add. ![fieldnames](http://edpflager.com/wp-content/uploads/2015/04/fieldnames-300x293.png)\n6.  Moving to the right, you can:\n    *   rename the field, \n    *   change the type with the length and precision columns (similar to a CAST statement)\n    *   switch a binary field to a normal value\n    *   supply a different format for the field\n    *   indicate if the date format is lenient, change the date locale and date time zone \n    *   indicate if the number conversion is lenient as well, \n    *   change the encoding type of the data, and\n    *   make changes to the decimal format, grouping and currency settings\n7.  To avoid confusion, its best to rename the field to when changing the encoding. Supply a new name in the rename column.\n8.  Click on the down arrow to the right of the encoding column, and you will be presented with a long list of different encoding schemes. Select UTF-8 from the list, and then click OK at the bottom.[![encoding_types](http://edpflager.com/wp-content/uploads/2015/04/encoding_types-233x300.png)](http://edpflager.com/wp-content/uploads/2015/04/encoding_types.png)\n9.  Open the table output component and provide the necessary connection information for your output table. Check the Specify database fields box, and then click the Database fields tab.\n10.  Click the Get fields button and the fields from the previous step will be populated. If you renamed your column for the encoding change, the new field name will be here. Click OK at the bottom of the window.\n11.  Save your transformation and run it. Check the destination database, and the fields' encoding type should now match the new type you selected.","slug":"change-character-encoding-with-pentaho-select-values","published":1,"updated":"2020-08-23T20:54:34.930Z","layout":"post","photos":[],"link":"","_id":"ckeaq99tf000tsdjxeb2scz44","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/468319_70689252.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/468319_70689252-300x225.jpg\" alt=\"468319_70689252\"></a>Moving data between different systems often requires converting between different character encoding specifications. If you aren’t familiar with the term, it means how characters are stored programmatically. In Latin character based languages like English, there are fewer characters, and they require a smaller amount of code to represent them. When computers had a much smaller amount of memory, processing power and disk space the smaller foot print of the ASCII and Windows 1252 characters sets were widely used to conserve resources. However, in other non-Latin character based languages, there can be a significantly larger amount of characters that make them up. Consequently, more system resources are required to represent them. Today, with larger amounts of data than ever before moving between different counties, moving that information between systems with different character schemes  can result in scrambled data if the underlying encoding isn’t taken into account. UTF-8 and UTF-16 encoding schemas are the most prevalent and widely accepted specifications, and should be used in place of the older ASCII and Windows 1252 schemes. Moving data to this format with Pentaho Data Integrator (aka Kettle) can be handled with a transform component called Select Values, although the method to perform this process is somewhat hidden.</p>\n<a id=\"more\"></a>\n<p>The example here involves moving some data from an older RDBMS system that is using the Windows 1252 scheme to one using UTF-8.</p>\n<ol>\n<li>Start a new transformation in Pentaho with a connection to your source system.</li>\n<li>On the workspace, drag out a Table Input component, and define your initial query. For my purposes, I was accessing a table with countries from around the world with their currency names.</li>\n<li>Open the Transform node in the Design pallet and drag out the Select Values component.</li>\n<li>Attach Table Input to Select Values and double click it. There are three tabs, but you can only use one at a time. If you need to perform operations from two or more tabs, you’ll need to string them together in successive steps. The functions are:<ul>\n<li>Select &amp; Alter allows you to rename fields and also change the Length or Precision of the field.</li>\n<li>Remove allows you to remove a field that is no longer needed from the processing stream.</li>\n<li>Meta-data allows you to change the information about the fields in the processing stream and includes many of the Select &amp; Alter options.</li>\n</ul>\n</li>\n<li>Click the Meta-data tab, and then click the “Get fields to change” button if you want all of the fields to be added. If you only want one or a few fields to be added, click on the down arrow button on the right side of the field name column  and choose the column you want to add. <img src=\"http://edpflager.com/wp-content/uploads/2015/04/fieldnames-300x293.png\" alt=\"fieldnames\"></li>\n<li>Moving to the right, you can:<ul>\n<li>rename the field, </li>\n<li>change the type with the length and precision columns (similar to a CAST statement)</li>\n<li>switch a binary field to a normal value</li>\n<li>supply a different format for the field</li>\n<li>indicate if the date format is lenient, change the date locale and date time zone </li>\n<li>indicate if the number conversion is lenient as well, </li>\n<li>change the encoding type of the data, and</li>\n<li>make changes to the decimal format, grouping and currency settings</li>\n</ul>\n</li>\n<li>To avoid confusion, its best to rename the field to when changing the encoding. Supply a new name in the rename column.</li>\n<li>Click on the down arrow to the right of the encoding column, and you will be presented with a long list of different encoding schemes. Select UTF-8 from the list, and then click OK at the bottom.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/encoding_types.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/encoding_types-233x300.png\" alt=\"encoding_types\"></a></li>\n<li>Open the table output component and provide the necessary connection information for your output table. Check the Specify database fields box, and then click the Database fields tab.</li>\n<li>Click the Get fields button and the fields from the previous step will be populated. If you renamed your column for the encoding change, the new field name will be here. Click OK at the bottom of the window.</li>\n<li>Save your transformation and run it. Check the destination database, and the fields’ encoding type should now match the new type you selected.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/468319_70689252.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/468319_70689252-300x225.jpg\" alt=\"468319_70689252\"></a>Moving data between different systems often requires converting between different character encoding specifications. If you aren’t familiar with the term, it means how characters are stored programmatically. In Latin character based languages like English, there are fewer characters, and they require a smaller amount of code to represent them. When computers had a much smaller amount of memory, processing power and disk space the smaller foot print of the ASCII and Windows 1252 characters sets were widely used to conserve resources. However, in other non-Latin character based languages, there can be a significantly larger amount of characters that make them up. Consequently, more system resources are required to represent them. Today, with larger amounts of data than ever before moving between different counties, moving that information between systems with different character schemes  can result in scrambled data if the underlying encoding isn’t taken into account. UTF-8 and UTF-16 encoding schemas are the most prevalent and widely accepted specifications, and should be used in place of the older ASCII and Windows 1252 schemes. Moving data to this format with Pentaho Data Integrator (aka Kettle) can be handled with a transform component called Select Values, although the method to perform this process is somewhat hidden.</p>","more":"<p>The example here involves moving some data from an older RDBMS system that is using the Windows 1252 scheme to one using UTF-8.</p>\n<ol>\n<li>Start a new transformation in Pentaho with a connection to your source system.</li>\n<li>On the workspace, drag out a Table Input component, and define your initial query. For my purposes, I was accessing a table with countries from around the world with their currency names.</li>\n<li>Open the Transform node in the Design pallet and drag out the Select Values component.</li>\n<li>Attach Table Input to Select Values and double click it. There are three tabs, but you can only use one at a time. If you need to perform operations from two or more tabs, you’ll need to string them together in successive steps. The functions are:<ul>\n<li>Select &amp; Alter allows you to rename fields and also change the Length or Precision of the field.</li>\n<li>Remove allows you to remove a field that is no longer needed from the processing stream.</li>\n<li>Meta-data allows you to change the information about the fields in the processing stream and includes many of the Select &amp; Alter options.</li>\n</ul>\n</li>\n<li>Click the Meta-data tab, and then click the “Get fields to change” button if you want all of the fields to be added. If you only want one or a few fields to be added, click on the down arrow button on the right side of the field name column  and choose the column you want to add. <img src=\"http://edpflager.com/wp-content/uploads/2015/04/fieldnames-300x293.png\" alt=\"fieldnames\"></li>\n<li>Moving to the right, you can:<ul>\n<li>rename the field, </li>\n<li>change the type with the length and precision columns (similar to a CAST statement)</li>\n<li>switch a binary field to a normal value</li>\n<li>supply a different format for the field</li>\n<li>indicate if the date format is lenient, change the date locale and date time zone </li>\n<li>indicate if the number conversion is lenient as well, </li>\n<li>change the encoding type of the data, and</li>\n<li>make changes to the decimal format, grouping and currency settings</li>\n</ul>\n</li>\n<li>To avoid confusion, its best to rename the field to when changing the encoding. Supply a new name in the rename column.</li>\n<li>Click on the down arrow to the right of the encoding column, and you will be presented with a long list of different encoding schemes. Select UTF-8 from the list, and then click OK at the bottom.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/encoding_types.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/encoding_types-233x300.png\" alt=\"encoding_types\"></a></li>\n<li>Open the table output component and provide the necessary connection information for your output table. Check the Specify database fields box, and then click the Database fields tab.</li>\n<li>Click the Get fields button and the fields from the previous step will be populated. If you renamed your column for the encoding change, the new field name will be here. Click OK at the bottom of the window.</li>\n<li>Save your transformation and run it. Check the destination database, and the fields’ encoding type should now match the new type you selected.</li>\n</ol>"},{"title":"Check for a file in Pentaho Kettle - Control Flow components - Part 1","id":"2716","comments":0,"date":"2015-04-27T21:54:43.000Z","_content":"\n[![fileexists](http://edpflager.com/wp-content/uploads/2015/04/fileexists-300x166.png)](http://edpflager.com/wp-content/uploads/2015/04/fileexists.png)While working on a recent ETL project at my day job, I needed to include a step to check for a file before processing could begin. Pretty simple concept, but one which the tool I are using doesn't have. You have to write an external code snippet (in one of two languages), and then call that code from within the job. It just re-enforced for me why I like Pentaho Data Integration. Below I'll demonstrate how you can look for a file in Kettle, and quit the job if the file isn't found, easily with no external code. In Kettle, both the transformation palette and the job palette have a FileExists component. The Job palette also includes a component to check for multiple files. In my case, I needed to check for one file for job control purposes, so this example will demonstrate that process.\n<!-- more -->\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.\n3.  Open the Utility node, and drag an Abort job component to the canvas.\n4.  Finally, add a File Exists component from the Conditions node.\n5.  Add an unconditional hop between the Start component to File Exists.\n6.  From File Exists, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  \"Follow when result it true.\" [![TrueCondition](http://edpflager.com/wp-content/uploads/2015/04/TrueCondition-300x110.png)](http://edpflager.com/wp-content/uploads/2015/04/TrueCondition.png)\n7.  Add another hop from File Exists to the Abort component. Right click the hop and from the Evaluation option, click on \"Follow when result is false\".[![FalseCondition](http://edpflager.com/wp-content/uploads/2015/04/FalseCondition-300x117.png)](http://edpflager.com/wp-content/uploads/2015/04/FalseCondition.png)\n8.  Double click the File Exists component to open it for the incredibly simple configuration. You can supply a name for the Job entry (I suggest supplying the file name you are looking for - i.e. Check\\_for\\_file\\_name) in the first text box.The second box is where you enter the file name you want to check for. If you want to access a file localto the machine you are working on, just enter the path to it, preceded by \"file:///\"  (for example \"file:///C:/Users/root/file\\_name.txt\" on a Windows machine, or on a Mac or Linux machine enter \"file:///users/username/file\\_name.txt\"). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder\\_name/filename.csv). If you'd prefer to browse for the file or aren't sure of the path to where the file will be stored, just click the Browse button to access the Open File window.[![CheckFile](http://edpflager.com/wp-content/uploads/2015/04/CheckFile-300x268.png)](http://edpflager.com/wp-content/uploads/2015/04/CheckFile.png)\n9.  In this window, you can browse your local system, or choose to look for your file in the Hadoop file system(HDFS), the MapR Hadoop distribution's file system (MapRFS), or Amazon's S3 file system. You'll be prompted to enter credentials and system information for any non-local systems . Once you have the location specified, click the OK button to return to the component. Click OK to close the component.![FileExists](http://edpflager.com/wp-content/uploads/2015/04/FileExists-300x186.png)\n10.  At this point the job should look like this image. Save your work and run the job. If you entered a correct path to the file, the log will indicate the file was found and the job completed successfully.\n11.  Go back into the Check Files component and alter the file name to a non-existent file. Save the job and run it again. The log will indicate the file was not found, and the Abort step was performed.\n\nIn order to use this in your own workflows, you could add an email step to the file not found branch, to let whomever is responsible for monitoring the system know that the expected file was not found. Any processing dependent on a file being found can be added to the file found branch.","source":"_posts/check-for-a-file-in-pentaho-kettle.md","raw":"---\ntitle: Check for a file in Pentaho Kettle - Control Flow components - Part 1\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - Linux\n  - PDI\n  - technical\nid: '2716'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2015-04-27 17:54:43\n---\n\n[![fileexists](http://edpflager.com/wp-content/uploads/2015/04/fileexists-300x166.png)](http://edpflager.com/wp-content/uploads/2015/04/fileexists.png)While working on a recent ETL project at my day job, I needed to include a step to check for a file before processing could begin. Pretty simple concept, but one which the tool I are using doesn't have. You have to write an external code snippet (in one of two languages), and then call that code from within the job. It just re-enforced for me why I like Pentaho Data Integration. Below I'll demonstrate how you can look for a file in Kettle, and quit the job if the file isn't found, easily with no external code. In Kettle, both the transformation palette and the job palette have a FileExists component. The Job palette also includes a component to check for multiple files. In my case, I needed to check for one file for job control purposes, so this example will demonstrate that process.\n<!-- more -->\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.\n3.  Open the Utility node, and drag an Abort job component to the canvas.\n4.  Finally, add a File Exists component from the Conditions node.\n5.  Add an unconditional hop between the Start component to File Exists.\n6.  From File Exists, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  \"Follow when result it true.\" [![TrueCondition](http://edpflager.com/wp-content/uploads/2015/04/TrueCondition-300x110.png)](http://edpflager.com/wp-content/uploads/2015/04/TrueCondition.png)\n7.  Add another hop from File Exists to the Abort component. Right click the hop and from the Evaluation option, click on \"Follow when result is false\".[![FalseCondition](http://edpflager.com/wp-content/uploads/2015/04/FalseCondition-300x117.png)](http://edpflager.com/wp-content/uploads/2015/04/FalseCondition.png)\n8.  Double click the File Exists component to open it for the incredibly simple configuration. You can supply a name for the Job entry (I suggest supplying the file name you are looking for - i.e. Check\\_for\\_file\\_name) in the first text box.The second box is where you enter the file name you want to check for. If you want to access a file localto the machine you are working on, just enter the path to it, preceded by \"file:///\"  (for example \"file:///C:/Users/root/file\\_name.txt\" on a Windows machine, or on a Mac or Linux machine enter \"file:///users/username/file\\_name.txt\"). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder\\_name/filename.csv). If you'd prefer to browse for the file or aren't sure of the path to where the file will be stored, just click the Browse button to access the Open File window.[![CheckFile](http://edpflager.com/wp-content/uploads/2015/04/CheckFile-300x268.png)](http://edpflager.com/wp-content/uploads/2015/04/CheckFile.png)\n9.  In this window, you can browse your local system, or choose to look for your file in the Hadoop file system(HDFS), the MapR Hadoop distribution's file system (MapRFS), or Amazon's S3 file system. You'll be prompted to enter credentials and system information for any non-local systems . Once you have the location specified, click the OK button to return to the component. Click OK to close the component.![FileExists](http://edpflager.com/wp-content/uploads/2015/04/FileExists-300x186.png)\n10.  At this point the job should look like this image. Save your work and run the job. If you entered a correct path to the file, the log will indicate the file was found and the job completed successfully.\n11.  Go back into the Check Files component and alter the file name to a non-existent file. Save the job and run it again. The log will indicate the file was not found, and the Abort step was performed.\n\nIn order to use this in your own workflows, you could add an email step to the file not found branch, to let whomever is responsible for monitoring the system know that the expected file was not found. Any processing dependent on a file being found can be added to the file found branch.","slug":"check-for-a-file-in-pentaho-kettle","published":1,"updated":"2020-08-23T20:54:34.934Z","layout":"post","photos":[],"link":"","_id":"ckeaq99tk000xsdjxgczwe103","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/fileexists.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/fileexists-300x166.png\" alt=\"fileexists\"></a>While working on a recent ETL project at my day job, I needed to include a step to check for a file before processing could begin. Pretty simple concept, but one which the tool I are using doesn’t have. You have to write an external code snippet (in one of two languages), and then call that code from within the job. It just re-enforced for me why I like Pentaho Data Integration. Below I’ll demonstrate how you can look for a file in Kettle, and quit the job if the file isn’t found, easily with no external code. In Kettle, both the transformation palette and the job palette have a FileExists component. The Job palette also includes a component to check for multiple files. In my case, I needed to check for one file for job control purposes, so this example will demonstrate that process.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.</li>\n<li>Open the Utility node, and drag an Abort job component to the canvas.</li>\n<li>Finally, add a File Exists component from the Conditions node.</li>\n<li>Add an unconditional hop between the Start component to File Exists.</li>\n<li>From File Exists, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.” <a href=\"http://edpflager.com/wp-content/uploads/2015/04/TrueCondition.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/TrueCondition-300x110.png\" alt=\"TrueCondition\"></a></li>\n<li>Add another hop from File Exists to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/FalseCondition.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/FalseCondition-300x117.png\" alt=\"FalseCondition\"></a></li>\n<li>Double click the File Exists component to open it for the incredibly simple configuration. You can supply a name for the Job entry (I suggest supplying the file name you are looking for - i.e. Check_for_file_name) in the first text box.The second box is where you enter the file name you want to check for. If you want to access a file localto the machine you are working on, just enter the path to it, preceded by “file:///“  (for example “file:///C:/Users/root/file_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/CheckFile.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/CheckFile-300x268.png\" alt=\"CheckFile\"></a></li>\n<li>In this window, you can browse your local system, or choose to look for your file in the Hadoop file system(HDFS), the MapR Hadoop distribution’s file system (MapRFS), or Amazon’s S3 file system. You’ll be prompted to enter credentials and system information for any non-local systems . Once you have the location specified, click the OK button to return to the component. Click OK to close the component.<img src=\"http://edpflager.com/wp-content/uploads/2015/04/FileExists-300x186.png\" alt=\"FileExists\"></li>\n<li>At this point the job should look like this image. Save your work and run the job. If you entered a correct path to the file, the log will indicate the file was found and the job completed successfully.</li>\n<li>Go back into the Check Files component and alter the file name to a non-existent file. Save the job and run it again. The log will indicate the file was not found, and the Abort step was performed.</li>\n</ol>\n<p>In order to use this in your own workflows, you could add an email step to the file not found branch, to let whomever is responsible for monitoring the system know that the expected file was not found. Any processing dependent on a file being found can be added to the file found branch.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/fileexists.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/fileexists-300x166.png\" alt=\"fileexists\"></a>While working on a recent ETL project at my day job, I needed to include a step to check for a file before processing could begin. Pretty simple concept, but one which the tool I are using doesn’t have. You have to write an external code snippet (in one of two languages), and then call that code from within the job. It just re-enforced for me why I like Pentaho Data Integration. Below I’ll demonstrate how you can look for a file in Kettle, and quit the job if the file isn’t found, easily with no external code. In Kettle, both the transformation palette and the job palette have a FileExists component. The Job palette also includes a component to check for multiple files. In my case, I needed to check for one file for job control purposes, so this example will demonstrate that process.</p>","more":"<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.</li>\n<li>Open the Utility node, and drag an Abort job component to the canvas.</li>\n<li>Finally, add a File Exists component from the Conditions node.</li>\n<li>Add an unconditional hop between the Start component to File Exists.</li>\n<li>From File Exists, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.” <a href=\"http://edpflager.com/wp-content/uploads/2015/04/TrueCondition.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/TrueCondition-300x110.png\" alt=\"TrueCondition\"></a></li>\n<li>Add another hop from File Exists to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/FalseCondition.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/FalseCondition-300x117.png\" alt=\"FalseCondition\"></a></li>\n<li>Double click the File Exists component to open it for the incredibly simple configuration. You can supply a name for the Job entry (I suggest supplying the file name you are looking for - i.e. Check_for_file_name) in the first text box.The second box is where you enter the file name you want to check for. If you want to access a file localto the machine you are working on, just enter the path to it, preceded by “file:///“  (for example “file:///C:/Users/root/file_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.<a href=\"http://edpflager.com/wp-content/uploads/2015/04/CheckFile.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/CheckFile-300x268.png\" alt=\"CheckFile\"></a></li>\n<li>In this window, you can browse your local system, or choose to look for your file in the Hadoop file system(HDFS), the MapR Hadoop distribution’s file system (MapRFS), or Amazon’s S3 file system. You’ll be prompted to enter credentials and system information for any non-local systems . Once you have the location specified, click the OK button to return to the component. Click OK to close the component.<img src=\"http://edpflager.com/wp-content/uploads/2015/04/FileExists-300x186.png\" alt=\"FileExists\"></li>\n<li>At this point the job should look like this image. Save your work and run the job. If you entered a correct path to the file, the log will indicate the file was found and the job completed successfully.</li>\n<li>Go back into the Check Files component and alter the file name to a non-existent file. Save the job and run it again. The log will indicate the file was not found, and the Abort step was performed.</li>\n</ol>\n<p>In order to use this in your own workflows, you could add an email step to the file not found branch, to let whomever is responsible for monitoring the system know that the expected file was not found. Any processing dependent on a file being found can be added to the file found branch.</p>"},{"title":"Check for multiple files in Pentaho Kettle - Control Flow components - Part 3","id":"2749","comments":0,"date":"2015-05-09T22:10:57.000Z","_content":"\n[![arrows](http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg)](http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg)A couple of posts back, I covered how to use the control flow component - \"File Exists\" to look for a file, and if its not found, to take appropriate actions. While useful, that component does have the limitation that it only looks for one file. If you need to check for multiple files on the local machine or somewhere on your network, you can use the \"Check if files exist\" component instead. This comes in handy if your workflow requires more than one file, or if different actions can be taken if a file is missing or not.  Below I'll walk through creating a simple workflow to illustrate the usage of this step.\n<!-- more -->\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a \"Start\" and a \"Success\" component on to the canvas.\n3.  Open the Utility node, and drag an \"Abort job\" component to the canvas.\n4.  Finally, add a \"Check if files Exist\" component from the Conditions node.\n5.  Add an unconditional hop between the \"Start\" component to \"Check if files Exist\".\n6.  From \"Check if files Exist\", add a hop to the \"Success\" component. Right click the hop, and a menu will appear. Click the Evaluation option to set  \"Follow when result it true.\"\n7.  At this point, your workflow should look similar to this:[![CheckIfExist](http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist-300x127.png)](http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist.png)\n8.  Double click the \"Check if files exist\" component to configure it. You be presented with this window:[![CheckFilesExist2](http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2-300x117.png)](http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2.png)\n9.  Click the FILE button to navigate to where the file will be located. If the file is on a different computer than the one Kettle (PDI) will be running on, and you are running Linux or Mac OS X, you'll need to mount the file location as a Volume. If you are using Windows, you can map a drive, or use an UNC path to the file.  (NOTE -  I've tried using file:\\\\\\\\,  cifs:\\\\,  smb:\\\\ , afb:\\\\ and vfs:\\\\ along with a number of other permutations with no success trying to access files over a network under Mac OS X and Linux, and the only way I could get it to work was using a mounted location as a volume, so it appears to be part of the local filesystem).\n10.  Once you have a location entered in the File/Folder name box, click the ADD button to move it to the grid at the bottom of the window.\n11.  Repeat steps 9 and 10 until you have all of the files listed that you want to look for in your workflow. At that point, click OK to close the window and return to the canvas.\n12.  Save your work and run the job to test it. If all the files you entered are found, the job will take the path to the Dummy step. If even one of the files are not found, the Abort Job step will be called.","source":"_posts/check-for-multiple-files-in-pentaho-kettle-control-flow-components-part-3.md","raw":"---\ntitle: Check for multiple files in Pentaho Kettle - Control Flow components - Part 3\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2749'\ncategories:\n  - - Blog\n  - - Pentaho\ncomments: false\ndate: 2015-05-09 18:10:57\n---\n\n[![arrows](http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg)](http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg)A couple of posts back, I covered how to use the control flow component - \"File Exists\" to look for a file, and if its not found, to take appropriate actions. While useful, that component does have the limitation that it only looks for one file. If you need to check for multiple files on the local machine or somewhere on your network, you can use the \"Check if files exist\" component instead. This comes in handy if your workflow requires more than one file, or if different actions can be taken if a file is missing or not.  Below I'll walk through creating a simple workflow to illustrate the usage of this step.\n<!-- more -->\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a \"Start\" and a \"Success\" component on to the canvas.\n3.  Open the Utility node, and drag an \"Abort job\" component to the canvas.\n4.  Finally, add a \"Check if files Exist\" component from the Conditions node.\n5.  Add an unconditional hop between the \"Start\" component to \"Check if files Exist\".\n6.  From \"Check if files Exist\", add a hop to the \"Success\" component. Right click the hop, and a menu will appear. Click the Evaluation option to set  \"Follow when result it true.\"\n7.  At this point, your workflow should look similar to this:[![CheckIfExist](http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist-300x127.png)](http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist.png)\n8.  Double click the \"Check if files exist\" component to configure it. You be presented with this window:[![CheckFilesExist2](http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2-300x117.png)](http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2.png)\n9.  Click the FILE button to navigate to where the file will be located. If the file is on a different computer than the one Kettle (PDI) will be running on, and you are running Linux or Mac OS X, you'll need to mount the file location as a Volume. If you are using Windows, you can map a drive, or use an UNC path to the file.  (NOTE -  I've tried using file:\\\\\\\\,  cifs:\\\\,  smb:\\\\ , afb:\\\\ and vfs:\\\\ along with a number of other permutations with no success trying to access files over a network under Mac OS X and Linux, and the only way I could get it to work was using a mounted location as a volume, so it appears to be part of the local filesystem).\n10.  Once you have a location entered in the File/Folder name box, click the ADD button to move it to the grid at the bottom of the window.\n11.  Repeat steps 9 and 10 until you have all of the files listed that you want to look for in your workflow. At that point, click OK to close the window and return to the canvas.\n12.  Save your work and run the job to test it. If all the files you entered are found, the job will take the path to the Dummy step. If even one of the files are not found, the Abort Job step will be called.","slug":"check-for-multiple-files-in-pentaho-kettle-control-flow-components-part-3","published":1,"updated":"2020-08-23T20:54:34.942Z","layout":"post","photos":[],"link":"","_id":"ckeaq99tm000zsdjxgf95ax4u","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg\" alt=\"arrows\"></a>A couple of posts back, I covered how to use the control flow component - “File Exists” to look for a file, and if its not found, to take appropriate actions. While useful, that component does have the limitation that it only looks for one file. If you need to check for multiple files on the local machine or somewhere on your network, you can use the “Check if files exist” component instead. This comes in handy if your workflow requires more than one file, or if different actions can be taken if a file is missing or not.  Below I’ll walk through creating a simple workflow to illustrate the usage of this step.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a “Start” and a “Success” component on to the canvas.</li>\n<li>Open the Utility node, and drag an “Abort job” component to the canvas.</li>\n<li>Finally, add a “Check if files Exist” component from the Conditions node.</li>\n<li>Add an unconditional hop between the “Start” component to “Check if files Exist”.</li>\n<li>From “Check if files Exist”, add a hop to the “Success” component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”</li>\n<li>At this point, your workflow should look similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist-300x127.png\" alt=\"CheckIfExist\"></a></li>\n<li>Double click the “Check if files exist” component to configure it. You be presented with this window:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2-300x117.png\" alt=\"CheckFilesExist2\"></a></li>\n<li>Click the FILE button to navigate to where the file will be located. If the file is on a different computer than the one Kettle (PDI) will be running on, and you are running Linux or Mac OS X, you’ll need to mount the file location as a Volume. If you are using Windows, you can map a drive, or use an UNC path to the file.  (NOTE -  I’ve tried using file:\\\\,  cifs:\\,  smb:\\ , afb:\\ and vfs:\\ along with a number of other permutations with no success trying to access files over a network under Mac OS X and Linux, and the only way I could get it to work was using a mounted location as a volume, so it appears to be part of the local filesystem).</li>\n<li>Once you have a location entered in the File/Folder name box, click the ADD button to move it to the grid at the bottom of the window.</li>\n<li>Repeat steps 9 and 10 until you have all of the files listed that you want to look for in your workflow. At that point, click OK to close the window and return to the canvas.</li>\n<li>Save your work and run the job to test it. If all the files you entered are found, the job will take the path to the Dummy step. If even one of the files are not found, the Abort Job step will be called.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/arrows.jpg\" alt=\"arrows\"></a>A couple of posts back, I covered how to use the control flow component - “File Exists” to look for a file, and if its not found, to take appropriate actions. While useful, that component does have the limitation that it only looks for one file. If you need to check for multiple files on the local machine or somewhere on your network, you can use the “Check if files exist” component instead. This comes in handy if your workflow requires more than one file, or if different actions can be taken if a file is missing or not.  Below I’ll walk through creating a simple workflow to illustrate the usage of this step.</p>","more":"<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a “Start” and a “Success” component on to the canvas.</li>\n<li>Open the Utility node, and drag an “Abort job” component to the canvas.</li>\n<li>Finally, add a “Check if files Exist” component from the Conditions node.</li>\n<li>Add an unconditional hop between the “Start” component to “Check if files Exist”.</li>\n<li>From “Check if files Exist”, add a hop to the “Success” component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”</li>\n<li>At this point, your workflow should look similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/CheckIfExist-300x127.png\" alt=\"CheckIfExist\"></a></li>\n<li>Double click the “Check if files exist” component to configure it. You be presented with this window:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/CheckFilesExist2-300x117.png\" alt=\"CheckFilesExist2\"></a></li>\n<li>Click the FILE button to navigate to where the file will be located. If the file is on a different computer than the one Kettle (PDI) will be running on, and you are running Linux or Mac OS X, you’ll need to mount the file location as a Volume. If you are using Windows, you can map a drive, or use an UNC path to the file.  (NOTE -  I’ve tried using file:\\\\,  cifs:\\,  smb:\\ , afb:\\ and vfs:\\ along with a number of other permutations with no success trying to access files over a network under Mac OS X and Linux, and the only way I could get it to work was using a mounted location as a volume, so it appears to be part of the local filesystem).</li>\n<li>Once you have a location entered in the File/Folder name box, click the ADD button to move it to the grid at the bottom of the window.</li>\n<li>Repeat steps 9 and 10 until you have all of the files listed that you want to look for in your workflow. At that point, click OK to close the window and return to the canvas.</li>\n<li>Save your work and run the job to test it. If all the files you entered are found, the job will take the path to the Dummy step. If even one of the files are not found, the Abort Job step will be called.</li>\n</ol>"},{"title":"Configure your Ubuntu Docker installation for remote access","id":"3347","comments":0,"date":"2016-07-03T12:16:36.000Z","_content":"\n[![container](http://edpflager.com/wp-content/uploads/2016/07/container-300x185.jpg)](http://edpflager.com/?attachment_id=3359#main)The default installation for Docker on Ubuntu server (16.04) configures the daemon (service) to listen on a local socket. But what if you want to access your Docker daemon remotely, from another box, If you are using the default configuration, you would need to open a Secure Shell to the server, and access Docker that way. But there is a way to setup Docker to allow remote access. First, lets verify that Docker is only working with a local socket.  On the server,  run from this command line:\n\nls -l /run\n\nThe results show have an entry with the docker.sock entry like this: [![dockersocket](http://edpflager.com/wp-content/uploads/2016/07/dockersocket-1024x178.png)](http://edpflager.com/?attachment_id=3348#main) And you can check that the daemon is not listening on any ports on your system by running this command:\n\nsudo netstat -tlp\n\nNo Docker application should be listed in the results. [![NoDocker_port](http://edpflager.com/wp-content/uploads/2016/07/NoDocker_port-1024x107.png)](http://edpflager.com/?attachment_id=3355#main)\n<!-- more -->\n \n\n1.  Now change to **/etc/default**.\n2.  As root, open the docker file in the folder with a text editor:\n    \n    nano ./docker\n    \n3.  Find the line that starts **#DOCKER\\_OPTS=** and add a new line below it.\n4.  The new line should be:\n    \n     DOCKER\\_OPTS=\"-H tcp://192.168.0.22:2375\"\n    \n    substitute your server's IP address for 192.168.0.22. The port specified **:2375** is the conventional one used for unencrypted communications to the docker daemon. Use **:2376** for an encrypted port.\n5.  Save the file and exit.\n6.  Restart your Docker service:\n    \n     sudo service docker restart\n    \n7.  Verify that Docker is now listening on the IP address specified, by rerunning this command:\n    \n     sudo netstat -tlp\n    \n8.  The results should show that Docker is indeed listening on the specified port[![docker_port](http://edpflager.com/wp-content/uploads/2016/07/docker_port-1024x130.png)](http://edpflager.com/?attachment_id=3351#main)\n9.  Docker is now setup to listen for remote communications. To test it, form your Docker server, run this command, again substituting your server's IP address:\n    \n    docker -H tcp://192.168.0.22:2375 version\n    \n10.  You should see results similar to this: [![dockerversion](http://edpflager.com/wp-content/uploads/2016/07/dockerversion-300x185.png)](http://edpflager.com/?attachment_id=3367#main)\n11.  Docker is now responding to requests on the address and port you specified! When accessing your Docker machine remotely, be careful of mixing release candidate clients with stable Docker daemons. They don't play nice together.","source":"_posts/configure-your-ubuntu-docker-installation-for-remote-access.md","raw":"---\ntitle: Configure your Ubuntu Docker installation for remote access\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3347'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-07-03 08:16:36\n---\n\n[![container](http://edpflager.com/wp-content/uploads/2016/07/container-300x185.jpg)](http://edpflager.com/?attachment_id=3359#main)The default installation for Docker on Ubuntu server (16.04) configures the daemon (service) to listen on a local socket. But what if you want to access your Docker daemon remotely, from another box, If you are using the default configuration, you would need to open a Secure Shell to the server, and access Docker that way. But there is a way to setup Docker to allow remote access. First, lets verify that Docker is only working with a local socket.  On the server,  run from this command line:\n\nls -l /run\n\nThe results show have an entry with the docker.sock entry like this: [![dockersocket](http://edpflager.com/wp-content/uploads/2016/07/dockersocket-1024x178.png)](http://edpflager.com/?attachment_id=3348#main) And you can check that the daemon is not listening on any ports on your system by running this command:\n\nsudo netstat -tlp\n\nNo Docker application should be listed in the results. [![NoDocker_port](http://edpflager.com/wp-content/uploads/2016/07/NoDocker_port-1024x107.png)](http://edpflager.com/?attachment_id=3355#main)\n<!-- more -->\n \n\n1.  Now change to **/etc/default**.\n2.  As root, open the docker file in the folder with a text editor:\n    \n    nano ./docker\n    \n3.  Find the line that starts **#DOCKER\\_OPTS=** and add a new line below it.\n4.  The new line should be:\n    \n     DOCKER\\_OPTS=\"-H tcp://192.168.0.22:2375\"\n    \n    substitute your server's IP address for 192.168.0.22. The port specified **:2375** is the conventional one used for unencrypted communications to the docker daemon. Use **:2376** for an encrypted port.\n5.  Save the file and exit.\n6.  Restart your Docker service:\n    \n     sudo service docker restart\n    \n7.  Verify that Docker is now listening on the IP address specified, by rerunning this command:\n    \n     sudo netstat -tlp\n    \n8.  The results should show that Docker is indeed listening on the specified port[![docker_port](http://edpflager.com/wp-content/uploads/2016/07/docker_port-1024x130.png)](http://edpflager.com/?attachment_id=3351#main)\n9.  Docker is now setup to listen for remote communications. To test it, form your Docker server, run this command, again substituting your server's IP address:\n    \n    docker -H tcp://192.168.0.22:2375 version\n    \n10.  You should see results similar to this: [![dockerversion](http://edpflager.com/wp-content/uploads/2016/07/dockerversion-300x185.png)](http://edpflager.com/?attachment_id=3367#main)\n11.  Docker is now responding to requests on the address and port you specified! When accessing your Docker machine remotely, be careful of mixing release candidate clients with stable Docker daemons. They don't play nice together.","slug":"configure-your-ubuntu-docker-installation-for-remote-access","published":1,"updated":"2020-08-23T20:54:35.062Z","layout":"post","photos":[],"link":"","_id":"ckeaq99u60013sdjx87lx0yym","content":"<p><a href=\"http://edpflager.com/?attachment_id=3359#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/container-300x185.jpg\" alt=\"container\"></a>The default installation for Docker on Ubuntu server (16.04) configures the daemon (service) to listen on a local socket. But what if you want to access your Docker daemon remotely, from another box, If you are using the default configuration, you would need to open a Secure Shell to the server, and access Docker that way. But there is a way to setup Docker to allow remote access. First, lets verify that Docker is only working with a local socket.  On the server,  run from this command line:</p>\n<p>ls -l /run</p>\n<p>The results show have an entry with the docker.sock entry like this: <a href=\"http://edpflager.com/?attachment_id=3348#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/dockersocket-1024x178.png\" alt=\"dockersocket\"></a> And you can check that the daemon is not listening on any ports on your system by running this command:</p>\n<p>sudo netstat -tlp</p>\n<p>No Docker application should be listed in the results. <a href=\"http://edpflager.com/?attachment_id=3355#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/NoDocker_port-1024x107.png\" alt=\"NoDocker_port\"></a></p>\n<a id=\"more\"></a>\n<p> </p>\n<ol>\n<li><p>Now change to <strong>/etc/default</strong>.</p>\n</li>\n<li><p>As root, open the docker file in the folder with a text editor:</p>\n<p>nano ./docker</p>\n</li>\n<li><p>Find the line that starts <strong>#DOCKER_OPTS=</strong> and add a new line below it.</p>\n</li>\n<li><p>The new line should be:</p>\n<p> DOCKER_OPTS=”-H tcp://192.168.0.22:2375”</p>\n<p>substitute your server’s IP address for 192.168.0.22. The port specified <strong>:2375</strong> is the conventional one used for unencrypted communications to the docker daemon. Use <strong>:2376</strong> for an encrypted port.</p>\n</li>\n<li><p>Save the file and exit.</p>\n</li>\n<li><p>Restart your Docker service:</p>\n<p> sudo service docker restart</p>\n</li>\n<li><p>Verify that Docker is now listening on the IP address specified, by rerunning this command:</p>\n<p> sudo netstat -tlp</p>\n</li>\n<li><p>The results should show that Docker is indeed listening on the specified port<a href=\"http://edpflager.com/?attachment_id=3351#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/docker_port-1024x130.png\" alt=\"docker_port\"></a></p>\n</li>\n<li><p>Docker is now setup to listen for remote communications. To test it, form your Docker server, run this command, again substituting your server’s IP address:</p>\n<p>docker -H tcp://192.168.0.22:2375 version</p>\n</li>\n<li><p>You should see results similar to this: <a href=\"http://edpflager.com/?attachment_id=3367#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/dockerversion-300x185.png\" alt=\"dockerversion\"></a></p>\n</li>\n<li><p>Docker is now responding to requests on the address and port you specified! When accessing your Docker machine remotely, be careful of mixing release candidate clients with stable Docker daemons. They don’t play nice together.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3359#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/container-300x185.jpg\" alt=\"container\"></a>The default installation for Docker on Ubuntu server (16.04) configures the daemon (service) to listen on a local socket. But what if you want to access your Docker daemon remotely, from another box, If you are using the default configuration, you would need to open a Secure Shell to the server, and access Docker that way. But there is a way to setup Docker to allow remote access. First, lets verify that Docker is only working with a local socket.  On the server,  run from this command line:</p>\n<p>ls -l /run</p>\n<p>The results show have an entry with the docker.sock entry like this: <a href=\"http://edpflager.com/?attachment_id=3348#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/dockersocket-1024x178.png\" alt=\"dockersocket\"></a> And you can check that the daemon is not listening on any ports on your system by running this command:</p>\n<p>sudo netstat -tlp</p>\n<p>No Docker application should be listed in the results. <a href=\"http://edpflager.com/?attachment_id=3355#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/NoDocker_port-1024x107.png\" alt=\"NoDocker_port\"></a></p>","more":"<p> </p>\n<ol>\n<li><p>Now change to <strong>/etc/default</strong>.</p>\n</li>\n<li><p>As root, open the docker file in the folder with a text editor:</p>\n<p>nano ./docker</p>\n</li>\n<li><p>Find the line that starts <strong>#DOCKER_OPTS=</strong> and add a new line below it.</p>\n</li>\n<li><p>The new line should be:</p>\n<p> DOCKER_OPTS=”-H tcp://192.168.0.22:2375”</p>\n<p>substitute your server’s IP address for 192.168.0.22. The port specified <strong>:2375</strong> is the conventional one used for unencrypted communications to the docker daemon. Use <strong>:2376</strong> for an encrypted port.</p>\n</li>\n<li><p>Save the file and exit.</p>\n</li>\n<li><p>Restart your Docker service:</p>\n<p> sudo service docker restart</p>\n</li>\n<li><p>Verify that Docker is now listening on the IP address specified, by rerunning this command:</p>\n<p> sudo netstat -tlp</p>\n</li>\n<li><p>The results should show that Docker is indeed listening on the specified port<a href=\"http://edpflager.com/?attachment_id=3351#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/docker_port-1024x130.png\" alt=\"docker_port\"></a></p>\n</li>\n<li><p>Docker is now setup to listen for remote communications. To test it, form your Docker server, run this command, again substituting your server’s IP address:</p>\n<p>docker -H tcp://192.168.0.22:2375 version</p>\n</li>\n<li><p>You should see results similar to this: <a href=\"http://edpflager.com/?attachment_id=3367#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/dockerversion-300x185.png\" alt=\"dockerversion\"></a></p>\n</li>\n<li><p>Docker is now responding to requests on the address and port you specified! When accessing your Docker machine remotely, be careful of mixing release candidate clients with stable Docker daemons. They don’t play nice together.</p>\n</li>\n</ol>"},{"title":"Connecting Kettle to Cloudera Hadoop Impala","id":"2592","comments":0,"date":"2014-12-05T23:44:35.000Z","_content":"\n[![hadoop-elephant](http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant-300x214.png)](http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant.png)As Big Data platforms like Hadoop and its ecosystem of related applications has matured, they have moved beyond the original key-value model to embrace data processing of more traditional structured data. But a big problem for DBAs and Data Analysts wanting to use the power of these new platforms to analyze data from RDBMS systems like MySQL, SQL Server, is getting data moved between them. Using CSV or flat files is one way, but it adds additional processing. Data has to be extracted from the source system to an intermediary format and then imported into the destination. Its far more efficient and less prone to error if the data can be passed without that middle step. In this first article of a series where I'll be looking at interactivity between Hadoop and other database systems, I'll cover setting up a database connection to Hadoop via Cloudera's Impala JDBC driver to Pentaho's Kettle ETL system.\n<!-- more -->\n##### Prerequisites:\n\n*   If you want to follow along, and you don't have a Hadoop environment setup to play with, download a [Cloudera Quick Start VM](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-2-x.html) to work with. There are versions available for VMWare/Fusion, Oracle's VirtualBox and KVM.\n*   Additionally, you'll need a recent copy of the community version of [Pentaho Data Integration](http://sourceforge.net/projects/pentaho/files/Data%20Integration/5.2/pdi-ce-5.2.0.0-209.zip/download) (aka Kettle). I don't recommend installing Kettle in the VM that you are running Hadoop in, but if your host machine is sufficiently powerful, you can run it there alongside the VM.\n*   Once you have those applications in place, download the Cloudera JDBC driver for Impala [here](https://downloads.cloudera.com/impala-jdbc/impala-jdbc-0.5-2.zip). The current version as of this writing is 0.5.2. The driver is O/S independent, so it will work with Linux/MacOSX/Windows.\n    \n\n1.  Extract the JDBC ZIP file to somewhere you can access, and then copy the nine JAR files to the data-integration/lib folder for Pentaho. Because there is already a log4j jar file present in the LIB folder, you may get a prompt to overwrite it. Go ahead.\n2.  Start up the Hadoop VM. Open a terminal prompt and enter the command: **ifconfig** to get the IP address assigned to the VM.![IPAddress](http://edpflager.com/wp-content/uploads/2014/12/IPAddress-300x112.png)\n3.  Switch to the PC where you have Pentaho installed. Open a terminal window on that machine and enter this command: ping <ip address>, substituting the address you got in step 2 above. If the results are not successful you'll need to determine what is causing the breakdown in communications. Unfortunately that is outside the scope of this article but two possibilities may be the firewall/ip tables is turned on in your VM, or the network connection in your VM platform may need to be configured differently.![Ping](http://edpflager.com/wp-content/uploads/2014/12/Ping-300x74.png)\n4.  Start Pentaho Data Integration and start a new transformation. Switch to the View tab in the left panel of the application. Expand the Transformation folder by toggling the folder arrow, and then the Transformation. Right click the Database connections option and click NEW in the menu that appears.\n5.  The Database Connection window will appear.\n6.  Enter a name in the Connection Name box to identify it.\n7.  Scroll down in Connection Type and choose Impala.\n8.  In the Access panel, the Native (JDBC) should be selected since its the only option.\n9.  In the Settings panel, enter your VM's IP address, the database you want to connect to (for our purposes we'll use the default database, called default) and the user name: cloudera and password: cloudera in the appropriate fields. The port will default to 21500. Unless you have modified the Impala port, you can leave it set to 21500.\n10.  Once you are finished, the window should like the image below. Click the test button, and you should receive a message that the connection was successful.[![DBConnection](http://edpflager.com/wp-content/uploads/2014/12/DBConnection-300x279.png)](http://edpflager.com/wp-content/uploads/2014/12/DBConnection.png)[](http://edpflager.com/wp-content/uploads/2014/12/connection.png)\n11.  Congratulations! You have made a database connection to your test Hadoop environment!","source":"_posts/connecting-kettle-to-hadoop-hive.md","raw":"---\ntitle: Connecting Kettle to Cloudera Hadoop Impala\ntags:\n  - CDH\n  - Cloudera\n  - ETL\n  - Hadoop\n  - How-to\n  - howto\n  - impala\n  - kettle\n  - PDI\n  - SysAdmin\n  - technical\nid: '2592'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-12-05 18:44:35\n---\n\n[![hadoop-elephant](http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant-300x214.png)](http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant.png)As Big Data platforms like Hadoop and its ecosystem of related applications has matured, they have moved beyond the original key-value model to embrace data processing of more traditional structured data. But a big problem for DBAs and Data Analysts wanting to use the power of these new platforms to analyze data from RDBMS systems like MySQL, SQL Server, is getting data moved between them. Using CSV or flat files is one way, but it adds additional processing. Data has to be extracted from the source system to an intermediary format and then imported into the destination. Its far more efficient and less prone to error if the data can be passed without that middle step. In this first article of a series where I'll be looking at interactivity between Hadoop and other database systems, I'll cover setting up a database connection to Hadoop via Cloudera's Impala JDBC driver to Pentaho's Kettle ETL system.\n<!-- more -->\n##### Prerequisites:\n\n*   If you want to follow along, and you don't have a Hadoop environment setup to play with, download a [Cloudera Quick Start VM](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-2-x.html) to work with. There are versions available for VMWare/Fusion, Oracle's VirtualBox and KVM.\n*   Additionally, you'll need a recent copy of the community version of [Pentaho Data Integration](http://sourceforge.net/projects/pentaho/files/Data%20Integration/5.2/pdi-ce-5.2.0.0-209.zip/download) (aka Kettle). I don't recommend installing Kettle in the VM that you are running Hadoop in, but if your host machine is sufficiently powerful, you can run it there alongside the VM.\n*   Once you have those applications in place, download the Cloudera JDBC driver for Impala [here](https://downloads.cloudera.com/impala-jdbc/impala-jdbc-0.5-2.zip). The current version as of this writing is 0.5.2. The driver is O/S independent, so it will work with Linux/MacOSX/Windows.\n    \n\n1.  Extract the JDBC ZIP file to somewhere you can access, and then copy the nine JAR files to the data-integration/lib folder for Pentaho. Because there is already a log4j jar file present in the LIB folder, you may get a prompt to overwrite it. Go ahead.\n2.  Start up the Hadoop VM. Open a terminal prompt and enter the command: **ifconfig** to get the IP address assigned to the VM.![IPAddress](http://edpflager.com/wp-content/uploads/2014/12/IPAddress-300x112.png)\n3.  Switch to the PC where you have Pentaho installed. Open a terminal window on that machine and enter this command: ping <ip address>, substituting the address you got in step 2 above. If the results are not successful you'll need to determine what is causing the breakdown in communications. Unfortunately that is outside the scope of this article but two possibilities may be the firewall/ip tables is turned on in your VM, or the network connection in your VM platform may need to be configured differently.![Ping](http://edpflager.com/wp-content/uploads/2014/12/Ping-300x74.png)\n4.  Start Pentaho Data Integration and start a new transformation. Switch to the View tab in the left panel of the application. Expand the Transformation folder by toggling the folder arrow, and then the Transformation. Right click the Database connections option and click NEW in the menu that appears.\n5.  The Database Connection window will appear.\n6.  Enter a name in the Connection Name box to identify it.\n7.  Scroll down in Connection Type and choose Impala.\n8.  In the Access panel, the Native (JDBC) should be selected since its the only option.\n9.  In the Settings panel, enter your VM's IP address, the database you want to connect to (for our purposes we'll use the default database, called default) and the user name: cloudera and password: cloudera in the appropriate fields. The port will default to 21500. Unless you have modified the Impala port, you can leave it set to 21500.\n10.  Once you are finished, the window should like the image below. Click the test button, and you should receive a message that the connection was successful.[![DBConnection](http://edpflager.com/wp-content/uploads/2014/12/DBConnection-300x279.png)](http://edpflager.com/wp-content/uploads/2014/12/DBConnection.png)[](http://edpflager.com/wp-content/uploads/2014/12/connection.png)\n11.  Congratulations! You have made a database connection to your test Hadoop environment!","slug":"connecting-kettle-to-hadoop-hive","published":1,"updated":"2020-08-23T20:54:34.918Z","layout":"post","photos":[],"link":"","_id":"ckeaq99ud0015sdjxe3bicarp","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant-300x214.png\" alt=\"hadoop-elephant\"></a>As Big Data platforms like Hadoop and its ecosystem of related applications has matured, they have moved beyond the original key-value model to embrace data processing of more traditional structured data. But a big problem for DBAs and Data Analysts wanting to use the power of these new platforms to analyze data from RDBMS systems like MySQL, SQL Server, is getting data moved between them. Using CSV or flat files is one way, but it adds additional processing. Data has to be extracted from the source system to an intermediary format and then imported into the destination. Its far more efficient and less prone to error if the data can be passed without that middle step. In this first article of a series where I’ll be looking at interactivity between Hadoop and other database systems, I’ll cover setting up a database connection to Hadoop via Cloudera’s Impala JDBC driver to Pentaho’s Kettle ETL system.</p>\n<a id=\"more\"></a>\n<h5 id=\"Prerequisites\"><a href=\"#Prerequisites\" class=\"headerlink\" title=\"Prerequisites:\"></a>Prerequisites:</h5><ul>\n<li>If you want to follow along, and you don’t have a Hadoop environment setup to play with, download a <a href=\"http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-2-x.html\">Cloudera Quick Start VM</a> to work with. There are versions available for VMWare/Fusion, Oracle’s VirtualBox and KVM.</li>\n<li>Additionally, you’ll need a recent copy of the community version of <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/5.2/pdi-ce-5.2.0.0-209.zip/download\">Pentaho Data Integration</a> (aka Kettle). I don’t recommend installing Kettle in the VM that you are running Hadoop in, but if your host machine is sufficiently powerful, you can run it there alongside the VM.</li>\n<li>Once you have those applications in place, download the Cloudera JDBC driver for Impala <a href=\"https://downloads.cloudera.com/impala-jdbc/impala-jdbc-0.5-2.zip\">here</a>. The current version as of this writing is 0.5.2. The driver is O/S independent, so it will work with Linux/MacOSX/Windows.</li>\n</ul>\n<ol>\n<li>Extract the JDBC ZIP file to somewhere you can access, and then copy the nine JAR files to the data-integration/lib folder for Pentaho. Because there is already a log4j jar file present in the LIB folder, you may get a prompt to overwrite it. Go ahead.</li>\n<li>Start up the Hadoop VM. Open a terminal prompt and enter the command: <strong>ifconfig</strong> to get the IP address assigned to the VM.<img src=\"http://edpflager.com/wp-content/uploads/2014/12/IPAddress-300x112.png\" alt=\"IPAddress\"></li>\n<li>Switch to the PC where you have Pentaho installed. Open a terminal window on that machine and enter this command: ping <ip address>, substituting the address you got in step 2 above. If the results are not successful you’ll need to determine what is causing the breakdown in communications. Unfortunately that is outside the scope of this article but two possibilities may be the firewall/ip tables is turned on in your VM, or the network connection in your VM platform may need to be configured differently.<img src=\"http://edpflager.com/wp-content/uploads/2014/12/Ping-300x74.png\" alt=\"Ping\"></li>\n<li>Start Pentaho Data Integration and start a new transformation. Switch to the View tab in the left panel of the application. Expand the Transformation folder by toggling the folder arrow, and then the Transformation. Right click the Database connections option and click NEW in the menu that appears.</li>\n<li>The Database Connection window will appear.</li>\n<li>Enter a name in the Connection Name box to identify it.</li>\n<li>Scroll down in Connection Type and choose Impala.</li>\n<li>In the Access panel, the Native (JDBC) should be selected since its the only option.</li>\n<li>In the Settings panel, enter your VM’s IP address, the database you want to connect to (for our purposes we’ll use the default database, called default) and the user name: cloudera and password: cloudera in the appropriate fields. The port will default to 21500. Unless you have modified the Impala port, you can leave it set to 21500.</li>\n<li>Once you are finished, the window should like the image below. Click the test button, and you should receive a message that the connection was successful.<a href=\"http://edpflager.com/wp-content/uploads/2014/12/DBConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/12/DBConnection-300x279.png\" alt=\"DBConnection\"></a><a href=\"http://edpflager.com/wp-content/uploads/2014/12/connection.png\"></a></li>\n<li>Congratulations! You have made a database connection to your test Hadoop environment!</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/12/hadoop-elephant-300x214.png\" alt=\"hadoop-elephant\"></a>As Big Data platforms like Hadoop and its ecosystem of related applications has matured, they have moved beyond the original key-value model to embrace data processing of more traditional structured data. But a big problem for DBAs and Data Analysts wanting to use the power of these new platforms to analyze data from RDBMS systems like MySQL, SQL Server, is getting data moved between them. Using CSV or flat files is one way, but it adds additional processing. Data has to be extracted from the source system to an intermediary format and then imported into the destination. Its far more efficient and less prone to error if the data can be passed without that middle step. In this first article of a series where I’ll be looking at interactivity between Hadoop and other database systems, I’ll cover setting up a database connection to Hadoop via Cloudera’s Impala JDBC driver to Pentaho’s Kettle ETL system.</p>","more":"<h5 id=\"Prerequisites\"><a href=\"#Prerequisites\" class=\"headerlink\" title=\"Prerequisites:\"></a>Prerequisites:</h5><ul>\n<li>If you want to follow along, and you don’t have a Hadoop environment setup to play with, download a <a href=\"http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-2-x.html\">Cloudera Quick Start VM</a> to work with. There are versions available for VMWare/Fusion, Oracle’s VirtualBox and KVM.</li>\n<li>Additionally, you’ll need a recent copy of the community version of <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/5.2/pdi-ce-5.2.0.0-209.zip/download\">Pentaho Data Integration</a> (aka Kettle). I don’t recommend installing Kettle in the VM that you are running Hadoop in, but if your host machine is sufficiently powerful, you can run it there alongside the VM.</li>\n<li>Once you have those applications in place, download the Cloudera JDBC driver for Impala <a href=\"https://downloads.cloudera.com/impala-jdbc/impala-jdbc-0.5-2.zip\">here</a>. The current version as of this writing is 0.5.2. The driver is O/S independent, so it will work with Linux/MacOSX/Windows.</li>\n</ul>\n<ol>\n<li>Extract the JDBC ZIP file to somewhere you can access, and then copy the nine JAR files to the data-integration/lib folder for Pentaho. Because there is already a log4j jar file present in the LIB folder, you may get a prompt to overwrite it. Go ahead.</li>\n<li>Start up the Hadoop VM. Open a terminal prompt and enter the command: <strong>ifconfig</strong> to get the IP address assigned to the VM.<img src=\"http://edpflager.com/wp-content/uploads/2014/12/IPAddress-300x112.png\" alt=\"IPAddress\"></li>\n<li>Switch to the PC where you have Pentaho installed. Open a terminal window on that machine and enter this command: ping <ip address>, substituting the address you got in step 2 above. If the results are not successful you’ll need to determine what is causing the breakdown in communications. Unfortunately that is outside the scope of this article but two possibilities may be the firewall/ip tables is turned on in your VM, or the network connection in your VM platform may need to be configured differently.<img src=\"http://edpflager.com/wp-content/uploads/2014/12/Ping-300x74.png\" alt=\"Ping\"></li>\n<li>Start Pentaho Data Integration and start a new transformation. Switch to the View tab in the left panel of the application. Expand the Transformation folder by toggling the folder arrow, and then the Transformation. Right click the Database connections option and click NEW in the menu that appears.</li>\n<li>The Database Connection window will appear.</li>\n<li>Enter a name in the Connection Name box to identify it.</li>\n<li>Scroll down in Connection Type and choose Impala.</li>\n<li>In the Access panel, the Native (JDBC) should be selected since its the only option.</li>\n<li>In the Settings panel, enter your VM’s IP address, the database you want to connect to (for our purposes we’ll use the default database, called default) and the user name: cloudera and password: cloudera in the appropriate fields. The port will default to 21500. Unless you have modified the Impala port, you can leave it set to 21500.</li>\n<li>Once you are finished, the window should like the image below. Click the test button, and you should receive a message that the connection was successful.<a href=\"http://edpflager.com/wp-content/uploads/2014/12/DBConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/12/DBConnection-300x279.png\" alt=\"DBConnection\"></a><a href=\"http://edpflager.com/wp-content/uploads/2014/12/connection.png\"></a></li>\n<li>Congratulations! You have made a database connection to your test Hadoop environment!</li>\n</ol>"},{"title":"Create a Pentaho Kettle Repository on SQL Server","id":"3492","comments":0,"date":"2017-01-31T20:43:29.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2017/01/black-tea-kettle-pv-300x225.jpg)](http://edpflager.com/?attachment_id=3516#main)As I have stated previously when creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. With the community version 7.0 of PENTAHO® DATA INTEGRATION (PDI), I am happy to report that you can finally create a repository for your ETL code on Microsoft SQL Server. Previously, you could [setup a repository on MySQL](http://edpflager.com/?p=1622) or PostgreSQL with the community edition but there were compatibility problems with the code that Kettle used that didn't work with SQL Server. After downloading the latest version I was attempting to make a connection to SQL Server, and decided to test setting up a repository again. I am happy to say it works so the remainder of this article will walk through the process of setting up a Pentaho repository on SQL Server 2016 from a Windows 10 machine. Prerequisites:\n\n*   Download the [jTDS open source SQL Server JDBC driver](http://jtds.sourceforge.net/). Extract the ZIP file, and copy the jtds-1.3.1.jar file from your download and save it into the data-integrationlib folder of your Pentaho application. Although Microsoft provides a JDBC driver, it did not work for me.\n*   Create an empty database on your Microsoft SQL Server. I created one called \"PentahoRepository\"\n*   Setup a SQL Server user account (not an Active Directory account) on your database server and give the account  DBO (owner) permissions on the database. Using a DDLADMIN level does not work. I created my account and called it \"repository\". I also set the default database for this account to the new database.\n\nNow that we have our prerequisites setup, we can start the PDI client.\n<!-- more -->\n1.  Within the PDI client, look at the upper right side of the screen for the work CONNECT. Click on it. [![](http://edpflager.com/wp-content/uploads/2017/01/connect-300x86.png)](http://edpflager.com/?attachment_id=3503#main)\n2.  The New Repository window will appear. Under the graphic that is in the middle of the screen, click the Other Repositories link.\n3.  The Other Repositories window will appear. Click the top option - Database Repository to select it it, and then click the Get Started button at the bottom.[![](http://edpflager.com/wp-content/uploads/2017/01/databaserepository-300x300.png)](http://edpflager.com/?attachment_id=3510#main)\n4.  You'll now see the Connection Details window. Supply a Display Name in the appropriate field, and if you would like a description fill one in that field as well. To have the client open the connection to the repository when you start every time, check the box labeled Launch Connection on startup. At this point, you need to make a connection to the SQL Server. Click the Database Connection field that currently is labeled _None._ \n5.  [![](http://edpflager.com/wp-content/uploads/2017/01/connection-300x300.png)](http://edpflager.com/?attachment_id=3504#main)The window will update and show you that you currently do not have a database connection for this repository. Let's fix that.  Click the Create New Connection button.\n6.  [![](http://edpflager.com/wp-content/uploads/2017/01/selectconnection-300x300.png)](http://edpflager.com/?attachment_id=3514#main)The Database Connection window will appear. Enter a name for your connection at the top. In the Connection Type list, scroll up and select MS SQL Server. In the Access list, make sure JDBC is selected. In the host name, enter the host name of your SQL Server or the IP address. In the database name field, enter the database you created previously - \"PentahoRepository\" in my case. Instance name can be left blank if your SQL Server was installed without one, otherwise supply the appropriate text. The default port for SQL Server is 1433 and will already be filled in. Change it if your DBA informs you that they setup SQL Server with a different port. Finally enter the user name - \"repository\"  and password that were created earlier on your SQL Server. Finally click the TEST button at the bottom. If everything is working, you will see a small success window appear.\n7.  Click OK in the Database Connection Test window, and then OK in the Database Connection window.[![](http://edpflager.com/wp-content/uploads/2017/01/connectiondefined-300x259.png)](http://edpflager.com/?attachment_id=3505#main)\n8.  The New Repository Connection window will reappear, this time with the connection you just set up listed. Click the BACK button.\n9.  The Connection Details window will reappear. The Database Connection field should now have your newly created Connection entered. Click the FINISH button. An Almost Finished message will display for a short time. Then a Congratulations message will appear, telling you that your connection has been created and is ready to use. (If you examine the database in SQL Server Management Studio you'll see a number of tables have been added to the database.)[![](http://edpflager.com/wp-content/uploads/2017/01/congrats-300x300.png)](http://edpflager.com/?attachment_id=3502#main)\n10.  Click CONNECT NOW. The screen updates again and a login screen will appear for your Repository. The user account will already be filled in: admin. For the initial password, enter:admin and click the CONNECT button.[![](http://edpflager.com/wp-content/uploads/2017/01/login-300x300.png)](http://edpflager.com/?attachment_id=3509#main)\n11.  You'll see a brief message that the system is connection, and then the window will disappear, leaving you back at the PDI GUI screen.  The upper right of the screen will now indicate that you are connected to your repository.[![](http://edpflager.com/wp-content/uploads/2017/01/pentahoconnected-300x51.png)](http://edpflager.com/?attachment_id=3512#main)\n12.  At this point you can proceed to use your repository, but its a good idea to change the Admin password, and add a user for yourself to the repository.\n13.  Click on TOOLS in the PDI menu bar, go down to Repository, and over to Explore. Click on it.[![](http://edpflager.com/wp-content/uploads/2017/01/respository-explore-300x115.png)](http://edpflager.com/?attachment_id=3513#main)\n14.  The Repository Explorer window will appear. Click on the security tab to see the defined users.\n15.  Click on ADMIN in the Available user list on the left to highlight it, then click on  the Pencil icon to edit it. The EDIT USER window appears. User names can't be edited once entered so the user name field is grayed out. In the password field, enter a new password, and then click OK.[![](http://edpflager.com/wp-content/uploads/2017/01/explorer-300x227.png)](http://edpflager.com/?attachment_id=3507#main)\n16.  Click on GUEST in the Available user list on the left to highlight it, then click on the X icon to delete it. You'll be prompted to verify the request. Click YES in the security window.[![](http://edpflager.com/wp-content/uploads/2017/01/guest-account-300x227.png)](http://edpflager.com/?attachment_id=3508#main)\n17.  Finally, we need to add ourselves as a user of the repository. Click the **+** icon, and the Add User window will appear. Fill in the user name, a password, and you can add a description if you like. Click OK to be returned to the Repository Explorer window. Click CLOSE in the lower right to exit and return to the PDI GUI.[![](http://edpflager.com/wp-content/uploads/2017/01/addnewrepositoryuser-300x228.png)](http://edpflager.com/?attachment_id=3501#main)\n18.  Congratulations! You have setup your Pentaho Repository on SQL Server. Now when working in the client, when you save or open jobs and transformations, the client will pull them from the repository database.\n\nPENTAHO DATA INTEGRATION is a registered trademark of Pentaho, Inc.","source":"_posts/create-a-pentaho-kettle-repository-on-sql-server.md","raw":"---\ntitle: Create a Pentaho Kettle Repository on SQL Server\ntags:\n  - cookbook\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - install\n  - kettle\n  - SQL Server\n  - SysAdmin\n  - technical\n  - Windows\nid: '3492'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2017-01-31 15:43:29\n---\n\n[![](http://edpflager.com/wp-content/uploads/2017/01/black-tea-kettle-pv-300x225.jpg)](http://edpflager.com/?attachment_id=3516#main)As I have stated previously when creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. With the community version 7.0 of PENTAHO® DATA INTEGRATION (PDI), I am happy to report that you can finally create a repository for your ETL code on Microsoft SQL Server. Previously, you could [setup a repository on MySQL](http://edpflager.com/?p=1622) or PostgreSQL with the community edition but there were compatibility problems with the code that Kettle used that didn't work with SQL Server. After downloading the latest version I was attempting to make a connection to SQL Server, and decided to test setting up a repository again. I am happy to say it works so the remainder of this article will walk through the process of setting up a Pentaho repository on SQL Server 2016 from a Windows 10 machine. Prerequisites:\n\n*   Download the [jTDS open source SQL Server JDBC driver](http://jtds.sourceforge.net/). Extract the ZIP file, and copy the jtds-1.3.1.jar file from your download and save it into the data-integrationlib folder of your Pentaho application. Although Microsoft provides a JDBC driver, it did not work for me.\n*   Create an empty database on your Microsoft SQL Server. I created one called \"PentahoRepository\"\n*   Setup a SQL Server user account (not an Active Directory account) on your database server and give the account  DBO (owner) permissions on the database. Using a DDLADMIN level does not work. I created my account and called it \"repository\". I also set the default database for this account to the new database.\n\nNow that we have our prerequisites setup, we can start the PDI client.\n<!-- more -->\n1.  Within the PDI client, look at the upper right side of the screen for the work CONNECT. Click on it. [![](http://edpflager.com/wp-content/uploads/2017/01/connect-300x86.png)](http://edpflager.com/?attachment_id=3503#main)\n2.  The New Repository window will appear. Under the graphic that is in the middle of the screen, click the Other Repositories link.\n3.  The Other Repositories window will appear. Click the top option - Database Repository to select it it, and then click the Get Started button at the bottom.[![](http://edpflager.com/wp-content/uploads/2017/01/databaserepository-300x300.png)](http://edpflager.com/?attachment_id=3510#main)\n4.  You'll now see the Connection Details window. Supply a Display Name in the appropriate field, and if you would like a description fill one in that field as well. To have the client open the connection to the repository when you start every time, check the box labeled Launch Connection on startup. At this point, you need to make a connection to the SQL Server. Click the Database Connection field that currently is labeled _None._ \n5.  [![](http://edpflager.com/wp-content/uploads/2017/01/connection-300x300.png)](http://edpflager.com/?attachment_id=3504#main)The window will update and show you that you currently do not have a database connection for this repository. Let's fix that.  Click the Create New Connection button.\n6.  [![](http://edpflager.com/wp-content/uploads/2017/01/selectconnection-300x300.png)](http://edpflager.com/?attachment_id=3514#main)The Database Connection window will appear. Enter a name for your connection at the top. In the Connection Type list, scroll up and select MS SQL Server. In the Access list, make sure JDBC is selected. In the host name, enter the host name of your SQL Server or the IP address. In the database name field, enter the database you created previously - \"PentahoRepository\" in my case. Instance name can be left blank if your SQL Server was installed without one, otherwise supply the appropriate text. The default port for SQL Server is 1433 and will already be filled in. Change it if your DBA informs you that they setup SQL Server with a different port. Finally enter the user name - \"repository\"  and password that were created earlier on your SQL Server. Finally click the TEST button at the bottom. If everything is working, you will see a small success window appear.\n7.  Click OK in the Database Connection Test window, and then OK in the Database Connection window.[![](http://edpflager.com/wp-content/uploads/2017/01/connectiondefined-300x259.png)](http://edpflager.com/?attachment_id=3505#main)\n8.  The New Repository Connection window will reappear, this time with the connection you just set up listed. Click the BACK button.\n9.  The Connection Details window will reappear. The Database Connection field should now have your newly created Connection entered. Click the FINISH button. An Almost Finished message will display for a short time. Then a Congratulations message will appear, telling you that your connection has been created and is ready to use. (If you examine the database in SQL Server Management Studio you'll see a number of tables have been added to the database.)[![](http://edpflager.com/wp-content/uploads/2017/01/congrats-300x300.png)](http://edpflager.com/?attachment_id=3502#main)\n10.  Click CONNECT NOW. The screen updates again and a login screen will appear for your Repository. The user account will already be filled in: admin. For the initial password, enter:admin and click the CONNECT button.[![](http://edpflager.com/wp-content/uploads/2017/01/login-300x300.png)](http://edpflager.com/?attachment_id=3509#main)\n11.  You'll see a brief message that the system is connection, and then the window will disappear, leaving you back at the PDI GUI screen.  The upper right of the screen will now indicate that you are connected to your repository.[![](http://edpflager.com/wp-content/uploads/2017/01/pentahoconnected-300x51.png)](http://edpflager.com/?attachment_id=3512#main)\n12.  At this point you can proceed to use your repository, but its a good idea to change the Admin password, and add a user for yourself to the repository.\n13.  Click on TOOLS in the PDI menu bar, go down to Repository, and over to Explore. Click on it.[![](http://edpflager.com/wp-content/uploads/2017/01/respository-explore-300x115.png)](http://edpflager.com/?attachment_id=3513#main)\n14.  The Repository Explorer window will appear. Click on the security tab to see the defined users.\n15.  Click on ADMIN in the Available user list on the left to highlight it, then click on  the Pencil icon to edit it. The EDIT USER window appears. User names can't be edited once entered so the user name field is grayed out. In the password field, enter a new password, and then click OK.[![](http://edpflager.com/wp-content/uploads/2017/01/explorer-300x227.png)](http://edpflager.com/?attachment_id=3507#main)\n16.  Click on GUEST in the Available user list on the left to highlight it, then click on the X icon to delete it. You'll be prompted to verify the request. Click YES in the security window.[![](http://edpflager.com/wp-content/uploads/2017/01/guest-account-300x227.png)](http://edpflager.com/?attachment_id=3508#main)\n17.  Finally, we need to add ourselves as a user of the repository. Click the **+** icon, and the Add User window will appear. Fill in the user name, a password, and you can add a description if you like. Click OK to be returned to the Repository Explorer window. Click CLOSE in the lower right to exit and return to the PDI GUI.[![](http://edpflager.com/wp-content/uploads/2017/01/addnewrepositoryuser-300x228.png)](http://edpflager.com/?attachment_id=3501#main)\n18.  Congratulations! You have setup your Pentaho Repository on SQL Server. Now when working in the client, when you save or open jobs and transformations, the client will pull them from the repository database.\n\nPENTAHO DATA INTEGRATION is a registered trademark of Pentaho, Inc.","slug":"create-a-pentaho-kettle-repository-on-sql-server","published":1,"updated":"2020-08-23T20:54:35.086Z","layout":"post","photos":[],"link":"","_id":"ckeaq99uh0019sdjxah2t8k96","content":"<p><a href=\"http://edpflager.com/?attachment_id=3516#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/black-tea-kettle-pv-300x225.jpg\"></a>As I have stated previously when creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. With the community version 7.0 of PENTAHO® DATA INTEGRATION (PDI), I am happy to report that you can finally create a repository for your ETL code on Microsoft SQL Server. Previously, you could <a href=\"http://edpflager.com/?p=1622\">setup a repository on MySQL</a> or PostgreSQL with the community edition but there were compatibility problems with the code that Kettle used that didn’t work with SQL Server. After downloading the latest version I was attempting to make a connection to SQL Server, and decided to test setting up a repository again. I am happy to say it works so the remainder of this article will walk through the process of setting up a Pentaho repository on SQL Server 2016 from a Windows 10 machine. Prerequisites:</p>\n<ul>\n<li>Download the <a href=\"http://jtds.sourceforge.net/\">jTDS open source SQL Server JDBC driver</a>. Extract the ZIP file, and copy the jtds-1.3.1.jar file from your download and save it into the data-integrationlib folder of your Pentaho application. Although Microsoft provides a JDBC driver, it did not work for me.</li>\n<li>Create an empty database on your Microsoft SQL Server. I created one called “PentahoRepository”</li>\n<li>Setup a SQL Server user account (not an Active Directory account) on your database server and give the account  DBO (owner) permissions on the database. Using a DDLADMIN level does not work. I created my account and called it “repository”. I also set the default database for this account to the new database.</li>\n</ul>\n<p>Now that we have our prerequisites setup, we can start the PDI client.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Within the PDI client, look at the upper right side of the screen for the work CONNECT. Click on it. <a href=\"http://edpflager.com/?attachment_id=3503#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connect-300x86.png\"></a></li>\n<li>The New Repository window will appear. Under the graphic that is in the middle of the screen, click the Other Repositories link.</li>\n<li>The Other Repositories window will appear. Click the top option - Database Repository to select it it, and then click the Get Started button at the bottom.<a href=\"http://edpflager.com/?attachment_id=3510#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/databaserepository-300x300.png\"></a></li>\n<li>You’ll now see the Connection Details window. Supply a Display Name in the appropriate field, and if you would like a description fill one in that field as well. To have the client open the connection to the repository when you start every time, check the box labeled Launch Connection on startup. At this point, you need to make a connection to the SQL Server. Click the Database Connection field that currently is labeled <em>None.</em> </li>\n<li><a href=\"http://edpflager.com/?attachment_id=3504#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connection-300x300.png\"></a>The window will update and show you that you currently do not have a database connection for this repository. Let’s fix that.  Click the Create New Connection button.</li>\n<li><a href=\"http://edpflager.com/?attachment_id=3514#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/selectconnection-300x300.png\"></a>The Database Connection window will appear. Enter a name for your connection at the top. In the Connection Type list, scroll up and select MS SQL Server. In the Access list, make sure JDBC is selected. In the host name, enter the host name of your SQL Server or the IP address. In the database name field, enter the database you created previously - “PentahoRepository” in my case. Instance name can be left blank if your SQL Server was installed without one, otherwise supply the appropriate text. The default port for SQL Server is 1433 and will already be filled in. Change it if your DBA informs you that they setup SQL Server with a different port. Finally enter the user name - “repository”  and password that were created earlier on your SQL Server. Finally click the TEST button at the bottom. If everything is working, you will see a small success window appear.</li>\n<li>Click OK in the Database Connection Test window, and then OK in the Database Connection window.<a href=\"http://edpflager.com/?attachment_id=3505#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connectiondefined-300x259.png\"></a></li>\n<li>The New Repository Connection window will reappear, this time with the connection you just set up listed. Click the BACK button.</li>\n<li>The Connection Details window will reappear. The Database Connection field should now have your newly created Connection entered. Click the FINISH button. An Almost Finished message will display for a short time. Then a Congratulations message will appear, telling you that your connection has been created and is ready to use. (If you examine the database in SQL Server Management Studio you’ll see a number of tables have been added to the database.)<a href=\"http://edpflager.com/?attachment_id=3502#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/congrats-300x300.png\"></a></li>\n<li>Click CONNECT NOW. The screen updates again and a login screen will appear for your Repository. The user account will already be filled in: admin. For the initial password, enter:admin and click the CONNECT button.<a href=\"http://edpflager.com/?attachment_id=3509#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/login-300x300.png\"></a></li>\n<li>You’ll see a brief message that the system is connection, and then the window will disappear, leaving you back at the PDI GUI screen.  The upper right of the screen will now indicate that you are connected to your repository.<a href=\"http://edpflager.com/?attachment_id=3512#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/pentahoconnected-300x51.png\"></a></li>\n<li>At this point you can proceed to use your repository, but its a good idea to change the Admin password, and add a user for yourself to the repository.</li>\n<li>Click on TOOLS in the PDI menu bar, go down to Repository, and over to Explore. Click on it.<a href=\"http://edpflager.com/?attachment_id=3513#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/respository-explore-300x115.png\"></a></li>\n<li>The Repository Explorer window will appear. Click on the security tab to see the defined users.</li>\n<li>Click on ADMIN in the Available user list on the left to highlight it, then click on  the Pencil icon to edit it. The EDIT USER window appears. User names can’t be edited once entered so the user name field is grayed out. In the password field, enter a new password, and then click OK.<a href=\"http://edpflager.com/?attachment_id=3507#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/explorer-300x227.png\"></a></li>\n<li>Click on GUEST in the Available user list on the left to highlight it, then click on the X icon to delete it. You’ll be prompted to verify the request. Click YES in the security window.<a href=\"http://edpflager.com/?attachment_id=3508#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/guest-account-300x227.png\"></a></li>\n<li>Finally, we need to add ourselves as a user of the repository. Click the <strong>+</strong> icon, and the Add User window will appear. Fill in the user name, a password, and you can add a description if you like. Click OK to be returned to the Repository Explorer window. Click CLOSE in the lower right to exit and return to the PDI GUI.<a href=\"http://edpflager.com/?attachment_id=3501#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/addnewrepositoryuser-300x228.png\"></a></li>\n<li>Congratulations! You have setup your Pentaho Repository on SQL Server. Now when working in the client, when you save or open jobs and transformations, the client will pull them from the repository database.</li>\n</ol>\n<p>PENTAHO DATA INTEGRATION is a registered trademark of Pentaho, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3516#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/black-tea-kettle-pv-300x225.jpg\"></a>As I have stated previously when creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. With the community version 7.0 of PENTAHO® DATA INTEGRATION (PDI), I am happy to report that you can finally create a repository for your ETL code on Microsoft SQL Server. Previously, you could <a href=\"http://edpflager.com/?p=1622\">setup a repository on MySQL</a> or PostgreSQL with the community edition but there were compatibility problems with the code that Kettle used that didn’t work with SQL Server. After downloading the latest version I was attempting to make a connection to SQL Server, and decided to test setting up a repository again. I am happy to say it works so the remainder of this article will walk through the process of setting up a Pentaho repository on SQL Server 2016 from a Windows 10 machine. Prerequisites:</p>\n<ul>\n<li>Download the <a href=\"http://jtds.sourceforge.net/\">jTDS open source SQL Server JDBC driver</a>. Extract the ZIP file, and copy the jtds-1.3.1.jar file from your download and save it into the data-integrationlib folder of your Pentaho application. Although Microsoft provides a JDBC driver, it did not work for me.</li>\n<li>Create an empty database on your Microsoft SQL Server. I created one called “PentahoRepository”</li>\n<li>Setup a SQL Server user account (not an Active Directory account) on your database server and give the account  DBO (owner) permissions on the database. Using a DDLADMIN level does not work. I created my account and called it “repository”. I also set the default database for this account to the new database.</li>\n</ul>\n<p>Now that we have our prerequisites setup, we can start the PDI client.</p>","more":"<ol>\n<li>Within the PDI client, look at the upper right side of the screen for the work CONNECT. Click on it. <a href=\"http://edpflager.com/?attachment_id=3503#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connect-300x86.png\"></a></li>\n<li>The New Repository window will appear. Under the graphic that is in the middle of the screen, click the Other Repositories link.</li>\n<li>The Other Repositories window will appear. Click the top option - Database Repository to select it it, and then click the Get Started button at the bottom.<a href=\"http://edpflager.com/?attachment_id=3510#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/databaserepository-300x300.png\"></a></li>\n<li>You’ll now see the Connection Details window. Supply a Display Name in the appropriate field, and if you would like a description fill one in that field as well. To have the client open the connection to the repository when you start every time, check the box labeled Launch Connection on startup. At this point, you need to make a connection to the SQL Server. Click the Database Connection field that currently is labeled <em>None.</em> </li>\n<li><a href=\"http://edpflager.com/?attachment_id=3504#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connection-300x300.png\"></a>The window will update and show you that you currently do not have a database connection for this repository. Let’s fix that.  Click the Create New Connection button.</li>\n<li><a href=\"http://edpflager.com/?attachment_id=3514#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/selectconnection-300x300.png\"></a>The Database Connection window will appear. Enter a name for your connection at the top. In the Connection Type list, scroll up and select MS SQL Server. In the Access list, make sure JDBC is selected. In the host name, enter the host name of your SQL Server or the IP address. In the database name field, enter the database you created previously - “PentahoRepository” in my case. Instance name can be left blank if your SQL Server was installed without one, otherwise supply the appropriate text. The default port for SQL Server is 1433 and will already be filled in. Change it if your DBA informs you that they setup SQL Server with a different port. Finally enter the user name - “repository”  and password that were created earlier on your SQL Server. Finally click the TEST button at the bottom. If everything is working, you will see a small success window appear.</li>\n<li>Click OK in the Database Connection Test window, and then OK in the Database Connection window.<a href=\"http://edpflager.com/?attachment_id=3505#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/connectiondefined-300x259.png\"></a></li>\n<li>The New Repository Connection window will reappear, this time with the connection you just set up listed. Click the BACK button.</li>\n<li>The Connection Details window will reappear. The Database Connection field should now have your newly created Connection entered. Click the FINISH button. An Almost Finished message will display for a short time. Then a Congratulations message will appear, telling you that your connection has been created and is ready to use. (If you examine the database in SQL Server Management Studio you’ll see a number of tables have been added to the database.)<a href=\"http://edpflager.com/?attachment_id=3502#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/congrats-300x300.png\"></a></li>\n<li>Click CONNECT NOW. The screen updates again and a login screen will appear for your Repository. The user account will already be filled in: admin. For the initial password, enter:admin and click the CONNECT button.<a href=\"http://edpflager.com/?attachment_id=3509#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/login-300x300.png\"></a></li>\n<li>You’ll see a brief message that the system is connection, and then the window will disappear, leaving you back at the PDI GUI screen.  The upper right of the screen will now indicate that you are connected to your repository.<a href=\"http://edpflager.com/?attachment_id=3512#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/pentahoconnected-300x51.png\"></a></li>\n<li>At this point you can proceed to use your repository, but its a good idea to change the Admin password, and add a user for yourself to the repository.</li>\n<li>Click on TOOLS in the PDI menu bar, go down to Repository, and over to Explore. Click on it.<a href=\"http://edpflager.com/?attachment_id=3513#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/respository-explore-300x115.png\"></a></li>\n<li>The Repository Explorer window will appear. Click on the security tab to see the defined users.</li>\n<li>Click on ADMIN in the Available user list on the left to highlight it, then click on  the Pencil icon to edit it. The EDIT USER window appears. User names can’t be edited once entered so the user name field is grayed out. In the password field, enter a new password, and then click OK.<a href=\"http://edpflager.com/?attachment_id=3507#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/explorer-300x227.png\"></a></li>\n<li>Click on GUEST in the Available user list on the left to highlight it, then click on the X icon to delete it. You’ll be prompted to verify the request. Click YES in the security window.<a href=\"http://edpflager.com/?attachment_id=3508#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/guest-account-300x227.png\"></a></li>\n<li>Finally, we need to add ourselves as a user of the repository. Click the <strong>+</strong> icon, and the Add User window will appear. Fill in the user name, a password, and you can add a description if you like. Click OK to be returned to the Repository Explorer window. Click CLOSE in the lower right to exit and return to the PDI GUI.<a href=\"http://edpflager.com/?attachment_id=3501#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/addnewrepositoryuser-300x228.png\"></a></li>\n<li>Congratulations! You have setup your Pentaho Repository on SQL Server. Now when working in the client, when you save or open jobs and transformations, the client will pull them from the repository database.</li>\n</ol>\n<p>PENTAHO DATA INTEGRATION is a registered trademark of Pentaho, Inc.</p>"},{"title":"Debugging ETL with Error Output","id":"2208","comments":0,"date":"2014-08-15T23:05:08.000Z","_content":"\n[![sys-error](http://edpflager.com/wp-content/uploads/2014/07/sys-error-300x124.png)](http://edpflager.com/wp-content/uploads/2014/07/sys-error.png)When developing new ETL flows, at an early stage you should include steps for error output so you can more easily locate and fix problems, especially when going between different data platforms. As an example, I have recently been working on data transformations that move records on a regular basis between PostgreSQL, Microsoft SQL Server and a DB2 mainframe system. All of these systems use similar data types, but not always the same ones and the method they use to handle data types may vary as well. Generally what I do is create an On Error step at the destination points in the transformations, and dump any bad records to a text file. This allows me to review if the error is affecting all of the records ( which would indicate a problem with the configuration) or only some of the records (which may indicate a problem with how the step is coded). And in some cases the error output indicates an issue with the source data that wasn't foreseen! The biggest benefit however is that you get to run the complete sample of records through your workflow to see how well it processes different values, rather than having it fail on the first error.\n<!-- more -->\nHere is an illustration of what I've stated above using Pentaho Kettle. Its a simple workflow with a table input and table output. Your ETL workflows are likely to be more involved then this, but for this discussion its sufficient.![errorout](http://edpflager.com/wp-content/uploads/2014/08/errorout-300x77.png) After the Table Output step, create another step that writes data to a text file. (You could use a table in your destination database just as easily.) From the Table Output step, drag a hop to the Text file output step. At this point a small menu will appear allowing  you to set this as the Main output  of step (the normal option) or Error Handling of step. Choose this option, and the hop looks like the one above, with the red line connecting the step steps. [![setup](http://edpflager.com/wp-content/uploads/2014/08/setup-300x67.png)](http://edpflager.com/wp-content/uploads/2014/08/setup.png) Now you are ready to test your ETL workflow. Execute the workflow and watch the Step Metric tab in the results panel. You can see how many records are moved to the destination and how many are written to the error handling step. If I see that most of the records process, I know I am on the right track, and if most or all of the records fail, then I have some serious issues I need to investigate.\n\n* * *\n\n  Pentaho is a trademark of PENTAHO, Inc.","source":"_posts/debugging-etl-with-error-output.md","raw":"---\ntitle: Debugging ETL with Error Output\ntags:\n  - ETL\n  - How-to\n  - kettle\n  - Linux\n  - PDI\nid: '2208'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-08-15 19:05:08\n---\n\n[![sys-error](http://edpflager.com/wp-content/uploads/2014/07/sys-error-300x124.png)](http://edpflager.com/wp-content/uploads/2014/07/sys-error.png)When developing new ETL flows, at an early stage you should include steps for error output so you can more easily locate and fix problems, especially when going between different data platforms. As an example, I have recently been working on data transformations that move records on a regular basis between PostgreSQL, Microsoft SQL Server and a DB2 mainframe system. All of these systems use similar data types, but not always the same ones and the method they use to handle data types may vary as well. Generally what I do is create an On Error step at the destination points in the transformations, and dump any bad records to a text file. This allows me to review if the error is affecting all of the records ( which would indicate a problem with the configuration) or only some of the records (which may indicate a problem with how the step is coded). And in some cases the error output indicates an issue with the source data that wasn't foreseen! The biggest benefit however is that you get to run the complete sample of records through your workflow to see how well it processes different values, rather than having it fail on the first error.\n<!-- more -->\nHere is an illustration of what I've stated above using Pentaho Kettle. Its a simple workflow with a table input and table output. Your ETL workflows are likely to be more involved then this, but for this discussion its sufficient.![errorout](http://edpflager.com/wp-content/uploads/2014/08/errorout-300x77.png) After the Table Output step, create another step that writes data to a text file. (You could use a table in your destination database just as easily.) From the Table Output step, drag a hop to the Text file output step. At this point a small menu will appear allowing  you to set this as the Main output  of step (the normal option) or Error Handling of step. Choose this option, and the hop looks like the one above, with the red line connecting the step steps. [![setup](http://edpflager.com/wp-content/uploads/2014/08/setup-300x67.png)](http://edpflager.com/wp-content/uploads/2014/08/setup.png) Now you are ready to test your ETL workflow. Execute the workflow and watch the Step Metric tab in the results panel. You can see how many records are moved to the destination and how many are written to the error handling step. If I see that most of the records process, I know I am on the right track, and if most or all of the records fail, then I have some serious issues I need to investigate.\n\n* * *\n\n  Pentaho is a trademark of PENTAHO, Inc.","slug":"debugging-etl-with-error-output","published":1,"updated":"2020-08-23T20:54:34.874Z","layout":"post","photos":[],"link":"","_id":"ckeaq99uj001bsdjx1f3mbsya","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/sys-error.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/sys-error-300x124.png\" alt=\"sys-error\"></a>When developing new ETL flows, at an early stage you should include steps for error output so you can more easily locate and fix problems, especially when going between different data platforms. As an example, I have recently been working on data transformations that move records on a regular basis between PostgreSQL, Microsoft SQL Server and a DB2 mainframe system. All of these systems use similar data types, but not always the same ones and the method they use to handle data types may vary as well. Generally what I do is create an On Error step at the destination points in the transformations, and dump any bad records to a text file. This allows me to review if the error is affecting all of the records ( which would indicate a problem with the configuration) or only some of the records (which may indicate a problem with how the step is coded). And in some cases the error output indicates an issue with the source data that wasn’t foreseen! The biggest benefit however is that you get to run the complete sample of records through your workflow to see how well it processes different values, rather than having it fail on the first error.</p>\n<a id=\"more\"></a>\n<p>Here is an illustration of what I’ve stated above using Pentaho Kettle. Its a simple workflow with a table input and table output. Your ETL workflows are likely to be more involved then this, but for this discussion its sufficient.<img src=\"http://edpflager.com/wp-content/uploads/2014/08/errorout-300x77.png\" alt=\"errorout\"> After the Table Output step, create another step that writes data to a text file. (You could use a table in your destination database just as easily.) From the Table Output step, drag a hop to the Text file output step. At this point a small menu will appear allowing  you to set this as the Main output  of step (the normal option) or Error Handling of step. Choose this option, and the hop looks like the one above, with the red line connecting the step steps. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/setup.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/setup-300x67.png\" alt=\"setup\"></a> Now you are ready to test your ETL workflow. Execute the workflow and watch the Step Metric tab in the results panel. You can see how many records are moved to the destination and how many are written to the error handling step. If I see that most of the records process, I know I am on the right track, and if most or all of the records fail, then I have some serious issues I need to investigate.</p>\n<hr>\n<p>  Pentaho is a trademark of PENTAHO, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/sys-error.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/sys-error-300x124.png\" alt=\"sys-error\"></a>When developing new ETL flows, at an early stage you should include steps for error output so you can more easily locate and fix problems, especially when going between different data platforms. As an example, I have recently been working on data transformations that move records on a regular basis between PostgreSQL, Microsoft SQL Server and a DB2 mainframe system. All of these systems use similar data types, but not always the same ones and the method they use to handle data types may vary as well. Generally what I do is create an On Error step at the destination points in the transformations, and dump any bad records to a text file. This allows me to review if the error is affecting all of the records ( which would indicate a problem with the configuration) or only some of the records (which may indicate a problem with how the step is coded). And in some cases the error output indicates an issue with the source data that wasn’t foreseen! The biggest benefit however is that you get to run the complete sample of records through your workflow to see how well it processes different values, rather than having it fail on the first error.</p>","more":"<p>Here is an illustration of what I’ve stated above using Pentaho Kettle. Its a simple workflow with a table input and table output. Your ETL workflows are likely to be more involved then this, but for this discussion its sufficient.<img src=\"http://edpflager.com/wp-content/uploads/2014/08/errorout-300x77.png\" alt=\"errorout\"> After the Table Output step, create another step that writes data to a text file. (You could use a table in your destination database just as easily.) From the Table Output step, drag a hop to the Text file output step. At this point a small menu will appear allowing  you to set this as the Main output  of step (the normal option) or Error Handling of step. Choose this option, and the hop looks like the one above, with the red line connecting the step steps. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/setup.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/setup-300x67.png\" alt=\"setup\"></a> Now you are ready to test your ETL workflow. Execute the workflow and watch the Step Metric tab in the results panel. You can see how many records are moved to the destination and how many are written to the error handling step. If I see that most of the records process, I know I am on the right track, and if most or all of the records fail, then I have some serious issues I need to investigate.</p>\n<hr>\n<p>  Pentaho is a trademark of PENTAHO, Inc.</p>"},{"title":"Dell Inspiron 3400 Linux laptop - First Impressions","id":"2883","comments":0,"date":"2015-07-18T23:05:28.000Z","_content":"\n## [![Dell Inspiron 14 3451](http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451-300x162.jpg)](http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451.jpg)Dell Inspiron 3400 Linux laptop - First Impressions\n\nFor years, many of us who wanted to run Linux on reasonably priced hardware had to buy systems with Windows already installed on it, and then wipe the system to put our chosen distro on it. While some manufacturers have offered systems with Linux, they tend to be higher end machines, with a matching higher end price tag. You also had the option to build your own system, but that was time consuming for a lot of us. Earlier this year, Dell announced two laptop lines with a lower price tag, the Inspiron 14 inch 3400 Ubuntu Edition and Inspiron 15 inch 3500 Ubuntu Edition series. All of them come with a slightly modified Ubuntu 14.04 LTS operating system and with options for Intel Celeron or Pentium processors, 2 or 4 GB of RAM and a 500 GB hard drive. All of the systems are very affordable at $225-$350 dollars. **UPDATE: After some research, I found that the Pentium processor in the 14 inch model does support up to 8GB of RAM. I swapped out the 4GB stick with an 8GB one I had from a ZBOX machine, and I am happy to report that it functions fine with 8GB! ** With that information in hand, I ordered a 14 inch system with the Intel Pentium  N3540 processor and 4 GB of RAM and received it just before the Independence Day holiday. I've spent the last couple of weeks poking and prodding it, and putting it through its paces.\n<!-- more -->\nIts been noted online that the modified Ubuntu installation has some issues, and I experienced many of the same problems. The biggest issue is with the steps that are supposed to allow you to create a restore diskette. Eventually I gave up on that, and decided to swap out the original hard drive with another one so I would have the original one if necessary to fall back on. I dropped a 120 GB SSD drive in place of stock 500 GB drive and went to work trying out different live DVDs CentOS 7 worked fine, detecting the hardware with no issues. Although I'm a fan of CentOS 6, the newer version has some usability changes that I don't care for. Other live ISOs worked OK as well, except for the touchpad. Mint 17, Fedora 22 workstation and Ubuntu 15.04 all failed to recognize the touchpad. While not a huge issue for me because I prefer to use a mouse, it still could be seen as a very serious issue for others. I tried a 14.04 Ubuntu Live disk and it also had the same issue. Strange! Eventually I found a [post by Australian blogger Cain Hill](http://www.cainhill.com.au/blog/fixing-my-dell-inspiron-3451-touchpad-on-ubuntu-14.10/) that pointed to a fix for this issue. I have since verified that it does fix the issue using multiple Linux installations, not just Ubuntu.\n\n1.  Open your system's blacklist file by running the following command in Ubuntu's Terminal application: **sudo gedit /etc/modprobe.d/blacklist.conf**\n2.  Add the following line to the file: **blacklist i2c-hid**\n3.  Save and restart, the touchpad will begin working instantly.\n\nAfter some research, from what I can piece together, the touchpad has two different control interfaces. Because these Linux distros detect both interfaces, the two drivers conflict and neither one works correctly. By adding the line to the blacklist.conf file, you are effectively telling the OS not to use one of the drivers. Since getting that resolved, I've installed Ubuntu 15.04 with a number of different applications (Gimp, Chromium instead of FireFox, MySQL server and Workbench, SQuirreL SQL client, and Pentaho Data Integration to name a few).\n\n##### Impressions?\n\n*   I wish Dell offered an SSD drive instead of the 500 GB drive that it came with, but the slower drive does allow Dell to keep prices down. But with the SSD, the time it takes from pressing the power button to when the Ubuntu login screen appears is less than 14 seconds.\n*   Starting at 3.9 pounds, the 14 inch model is fairly lightweight (although still heavier than the XPS 13 inch Ubuntu developer laptop that weighs in around 2.6 pounds). By replacing the stock hard drive with an SSD drive reduces the weight a little bit.\n*   The max screen resolution of 1366 x 768 resolution is adequate and the Truelife LED-Backlit display looks good, for development or watching a video. Overall the display is not great, but not bad.\n*   The keyboard is full size with good spacing, and responsiveness. My only quibble is that the keyboard includes a Windows key. Couldn't they put a Tux picture on there instead of that? :)\n*   Sporting a 40 WHr, 4-Cell Battery Battery, the system says it has  6.5 hours on a full charge, but a lot of that depends on what you are running. Expect 3-4 hours when running graphics or processor intensive applications.\n*   Some miscellaneous specs are: the system is less than an inch high when closed, making it a very low form factor. It has 2 USB 2.0 ports on the right side, 1 USB 3.0 port on the left, an HDMI out, a head phone jack , and Card reader slot. Finally it has a HD 720p capable webcam and a microphone for web conferences and video recordings.\n\nAll in all, the system is a great value for the cost conscious user looking to purchase a Linux based laptop. I recommend it.","source":"_posts/dell-inspiron-3400-linux-laptop-first-impressions.md","raw":"---\ntitle: Dell Inspiron 3400 Linux laptop - First Impressions\ntags:\n  - SysAdmin\n  - Ubuntu\nid: '2883'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-07-18 19:05:28\n---\n\n## [![Dell Inspiron 14 3451](http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451-300x162.jpg)](http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451.jpg)Dell Inspiron 3400 Linux laptop - First Impressions\n\nFor years, many of us who wanted to run Linux on reasonably priced hardware had to buy systems with Windows already installed on it, and then wipe the system to put our chosen distro on it. While some manufacturers have offered systems with Linux, they tend to be higher end machines, with a matching higher end price tag. You also had the option to build your own system, but that was time consuming for a lot of us. Earlier this year, Dell announced two laptop lines with a lower price tag, the Inspiron 14 inch 3400 Ubuntu Edition and Inspiron 15 inch 3500 Ubuntu Edition series. All of them come with a slightly modified Ubuntu 14.04 LTS operating system and with options for Intel Celeron or Pentium processors, 2 or 4 GB of RAM and a 500 GB hard drive. All of the systems are very affordable at $225-$350 dollars. **UPDATE: After some research, I found that the Pentium processor in the 14 inch model does support up to 8GB of RAM. I swapped out the 4GB stick with an 8GB one I had from a ZBOX machine, and I am happy to report that it functions fine with 8GB! ** With that information in hand, I ordered a 14 inch system with the Intel Pentium  N3540 processor and 4 GB of RAM and received it just before the Independence Day holiday. I've spent the last couple of weeks poking and prodding it, and putting it through its paces.\n<!-- more -->\nIts been noted online that the modified Ubuntu installation has some issues, and I experienced many of the same problems. The biggest issue is with the steps that are supposed to allow you to create a restore diskette. Eventually I gave up on that, and decided to swap out the original hard drive with another one so I would have the original one if necessary to fall back on. I dropped a 120 GB SSD drive in place of stock 500 GB drive and went to work trying out different live DVDs CentOS 7 worked fine, detecting the hardware with no issues. Although I'm a fan of CentOS 6, the newer version has some usability changes that I don't care for. Other live ISOs worked OK as well, except for the touchpad. Mint 17, Fedora 22 workstation and Ubuntu 15.04 all failed to recognize the touchpad. While not a huge issue for me because I prefer to use a mouse, it still could be seen as a very serious issue for others. I tried a 14.04 Ubuntu Live disk and it also had the same issue. Strange! Eventually I found a [post by Australian blogger Cain Hill](http://www.cainhill.com.au/blog/fixing-my-dell-inspiron-3451-touchpad-on-ubuntu-14.10/) that pointed to a fix for this issue. I have since verified that it does fix the issue using multiple Linux installations, not just Ubuntu.\n\n1.  Open your system's blacklist file by running the following command in Ubuntu's Terminal application: **sudo gedit /etc/modprobe.d/blacklist.conf**\n2.  Add the following line to the file: **blacklist i2c-hid**\n3.  Save and restart, the touchpad will begin working instantly.\n\nAfter some research, from what I can piece together, the touchpad has two different control interfaces. Because these Linux distros detect both interfaces, the two drivers conflict and neither one works correctly. By adding the line to the blacklist.conf file, you are effectively telling the OS not to use one of the drivers. Since getting that resolved, I've installed Ubuntu 15.04 with a number of different applications (Gimp, Chromium instead of FireFox, MySQL server and Workbench, SQuirreL SQL client, and Pentaho Data Integration to name a few).\n\n##### Impressions?\n\n*   I wish Dell offered an SSD drive instead of the 500 GB drive that it came with, but the slower drive does allow Dell to keep prices down. But with the SSD, the time it takes from pressing the power button to when the Ubuntu login screen appears is less than 14 seconds.\n*   Starting at 3.9 pounds, the 14 inch model is fairly lightweight (although still heavier than the XPS 13 inch Ubuntu developer laptop that weighs in around 2.6 pounds). By replacing the stock hard drive with an SSD drive reduces the weight a little bit.\n*   The max screen resolution of 1366 x 768 resolution is adequate and the Truelife LED-Backlit display looks good, for development or watching a video. Overall the display is not great, but not bad.\n*   The keyboard is full size with good spacing, and responsiveness. My only quibble is that the keyboard includes a Windows key. Couldn't they put a Tux picture on there instead of that? :)\n*   Sporting a 40 WHr, 4-Cell Battery Battery, the system says it has  6.5 hours on a full charge, but a lot of that depends on what you are running. Expect 3-4 hours when running graphics or processor intensive applications.\n*   Some miscellaneous specs are: the system is less than an inch high when closed, making it a very low form factor. It has 2 USB 2.0 ports on the right side, 1 USB 3.0 port on the left, an HDMI out, a head phone jack , and Card reader slot. Finally it has a HD 720p capable webcam and a microphone for web conferences and video recordings.\n\nAll in all, the system is a great value for the cost conscious user looking to purchase a Linux based laptop. I recommend it.","slug":"dell-inspiron-3400-linux-laptop-first-impressions","published":1,"updated":"2020-08-23T20:54:34.958Z","layout":"post","photos":[],"link":"","_id":"ckeaq99um001esdjx82f6alcm","content":"<h2 id=\"Dell-Inspiron-3400-Linux-laptop-First-Impressions\"><a href=\"#Dell-Inspiron-3400-Linux-laptop-First-Impressions\" class=\"headerlink\" title=\"Dell Inspiron 3400 Linux laptop - First Impressions\"></a><a href=\"http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451-300x162.jpg\" alt=\"Dell Inspiron 14 3451\"></a>Dell Inspiron 3400 Linux laptop - First Impressions</h2><p>For years, many of us who wanted to run Linux on reasonably priced hardware had to buy systems with Windows already installed on it, and then wipe the system to put our chosen distro on it. While some manufacturers have offered systems with Linux, they tend to be higher end machines, with a matching higher end price tag. You also had the option to build your own system, but that was time consuming for a lot of us. Earlier this year, Dell announced two laptop lines with a lower price tag, the Inspiron 14 inch 3400 Ubuntu Edition and Inspiron 15 inch 3500 Ubuntu Edition series. All of them come with a slightly modified Ubuntu 14.04 LTS operating system and with options for Intel Celeron or Pentium processors, 2 or 4 GB of RAM and a 500 GB hard drive. All of the systems are very affordable at $225-$350 dollars. **UPDATE: After some research, I found that the Pentium processor in the 14 inch model does support up to 8GB of RAM. I swapped out the 4GB stick with an 8GB one I had from a ZBOX machine, and I am happy to report that it functions fine with 8GB! ** With that information in hand, I ordered a 14 inch system with the Intel Pentium  N3540 processor and 4 GB of RAM and received it just before the Independence Day holiday. I’ve spent the last couple of weeks poking and prodding it, and putting it through its paces.</p>\n<a id=\"more\"></a>\n<p>Its been noted online that the modified Ubuntu installation has some issues, and I experienced many of the same problems. The biggest issue is with the steps that are supposed to allow you to create a restore diskette. Eventually I gave up on that, and decided to swap out the original hard drive with another one so I would have the original one if necessary to fall back on. I dropped a 120 GB SSD drive in place of stock 500 GB drive and went to work trying out different live DVDs CentOS 7 worked fine, detecting the hardware with no issues. Although I’m a fan of CentOS 6, the newer version has some usability changes that I don’t care for. Other live ISOs worked OK as well, except for the touchpad. Mint 17, Fedora 22 workstation and Ubuntu 15.04 all failed to recognize the touchpad. While not a huge issue for me because I prefer to use a mouse, it still could be seen as a very serious issue for others. I tried a 14.04 Ubuntu Live disk and it also had the same issue. Strange! Eventually I found a <a href=\"http://www.cainhill.com.au/blog/fixing-my-dell-inspiron-3451-touchpad-on-ubuntu-14.10/\">post by Australian blogger Cain Hill</a> that pointed to a fix for this issue. I have since verified that it does fix the issue using multiple Linux installations, not just Ubuntu.</p>\n<ol>\n<li>Open your system’s blacklist file by running the following command in Ubuntu’s Terminal application: <strong>sudo gedit /etc/modprobe.d/blacklist.conf</strong></li>\n<li>Add the following line to the file: <strong>blacklist i2c-hid</strong></li>\n<li>Save and restart, the touchpad will begin working instantly.</li>\n</ol>\n<p>After some research, from what I can piece together, the touchpad has two different control interfaces. Because these Linux distros detect both interfaces, the two drivers conflict and neither one works correctly. By adding the line to the blacklist.conf file, you are effectively telling the OS not to use one of the drivers. Since getting that resolved, I’ve installed Ubuntu 15.04 with a number of different applications (Gimp, Chromium instead of FireFox, MySQL server and Workbench, SQuirreL SQL client, and Pentaho Data Integration to name a few).</p>\n<h5 id=\"Impressions\"><a href=\"#Impressions\" class=\"headerlink\" title=\"Impressions?\"></a>Impressions?</h5><ul>\n<li>I wish Dell offered an SSD drive instead of the 500 GB drive that it came with, but the slower drive does allow Dell to keep prices down. But with the SSD, the time it takes from pressing the power button to when the Ubuntu login screen appears is less than 14 seconds.</li>\n<li>Starting at 3.9 pounds, the 14 inch model is fairly lightweight (although still heavier than the XPS 13 inch Ubuntu developer laptop that weighs in around 2.6 pounds). By replacing the stock hard drive with an SSD drive reduces the weight a little bit.</li>\n<li>The max screen resolution of 1366 x 768 resolution is adequate and the Truelife LED-Backlit display looks good, for development or watching a video. Overall the display is not great, but not bad.</li>\n<li>The keyboard is full size with good spacing, and responsiveness. My only quibble is that the keyboard includes a Windows key. Couldn’t they put a Tux picture on there instead of that? :)</li>\n<li>Sporting a 40 WHr, 4-Cell Battery Battery, the system says it has  6.5 hours on a full charge, but a lot of that depends on what you are running. Expect 3-4 hours when running graphics or processor intensive applications.</li>\n<li>Some miscellaneous specs are: the system is less than an inch high when closed, making it a very low form factor. It has 2 USB 2.0 ports on the right side, 1 USB 3.0 port on the left, an HDMI out, a head phone jack , and Card reader slot. Finally it has a HD 720p capable webcam and a microphone for web conferences and video recordings.</li>\n</ul>\n<p>All in all, the system is a great value for the cost conscious user looking to purchase a Linux based laptop. I recommend it.</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Dell-Inspiron-3400-Linux-laptop-First-Impressions\"><a href=\"#Dell-Inspiron-3400-Linux-laptop-First-Impressions\" class=\"headerlink\" title=\"Dell Inspiron 3400 Linux laptop - First Impressions\"></a><a href=\"http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/Dell-Inspiron-14-3451-300x162.jpg\" alt=\"Dell Inspiron 14 3451\"></a>Dell Inspiron 3400 Linux laptop - First Impressions</h2><p>For years, many of us who wanted to run Linux on reasonably priced hardware had to buy systems with Windows already installed on it, and then wipe the system to put our chosen distro on it. While some manufacturers have offered systems with Linux, they tend to be higher end machines, with a matching higher end price tag. You also had the option to build your own system, but that was time consuming for a lot of us. Earlier this year, Dell announced two laptop lines with a lower price tag, the Inspiron 14 inch 3400 Ubuntu Edition and Inspiron 15 inch 3500 Ubuntu Edition series. All of them come with a slightly modified Ubuntu 14.04 LTS operating system and with options for Intel Celeron or Pentium processors, 2 or 4 GB of RAM and a 500 GB hard drive. All of the systems are very affordable at $225-$350 dollars. **UPDATE: After some research, I found that the Pentium processor in the 14 inch model does support up to 8GB of RAM. I swapped out the 4GB stick with an 8GB one I had from a ZBOX machine, and I am happy to report that it functions fine with 8GB! ** With that information in hand, I ordered a 14 inch system with the Intel Pentium  N3540 processor and 4 GB of RAM and received it just before the Independence Day holiday. I’ve spent the last couple of weeks poking and prodding it, and putting it through its paces.</p>","more":"<p>Its been noted online that the modified Ubuntu installation has some issues, and I experienced many of the same problems. The biggest issue is with the steps that are supposed to allow you to create a restore diskette. Eventually I gave up on that, and decided to swap out the original hard drive with another one so I would have the original one if necessary to fall back on. I dropped a 120 GB SSD drive in place of stock 500 GB drive and went to work trying out different live DVDs CentOS 7 worked fine, detecting the hardware with no issues. Although I’m a fan of CentOS 6, the newer version has some usability changes that I don’t care for. Other live ISOs worked OK as well, except for the touchpad. Mint 17, Fedora 22 workstation and Ubuntu 15.04 all failed to recognize the touchpad. While not a huge issue for me because I prefer to use a mouse, it still could be seen as a very serious issue for others. I tried a 14.04 Ubuntu Live disk and it also had the same issue. Strange! Eventually I found a <a href=\"http://www.cainhill.com.au/blog/fixing-my-dell-inspiron-3451-touchpad-on-ubuntu-14.10/\">post by Australian blogger Cain Hill</a> that pointed to a fix for this issue. I have since verified that it does fix the issue using multiple Linux installations, not just Ubuntu.</p>\n<ol>\n<li>Open your system’s blacklist file by running the following command in Ubuntu’s Terminal application: <strong>sudo gedit /etc/modprobe.d/blacklist.conf</strong></li>\n<li>Add the following line to the file: <strong>blacklist i2c-hid</strong></li>\n<li>Save and restart, the touchpad will begin working instantly.</li>\n</ol>\n<p>After some research, from what I can piece together, the touchpad has two different control interfaces. Because these Linux distros detect both interfaces, the two drivers conflict and neither one works correctly. By adding the line to the blacklist.conf file, you are effectively telling the OS not to use one of the drivers. Since getting that resolved, I’ve installed Ubuntu 15.04 with a number of different applications (Gimp, Chromium instead of FireFox, MySQL server and Workbench, SQuirreL SQL client, and Pentaho Data Integration to name a few).</p>\n<h5 id=\"Impressions\"><a href=\"#Impressions\" class=\"headerlink\" title=\"Impressions?\"></a>Impressions?</h5><ul>\n<li>I wish Dell offered an SSD drive instead of the 500 GB drive that it came with, but the slower drive does allow Dell to keep prices down. But with the SSD, the time it takes from pressing the power button to when the Ubuntu login screen appears is less than 14 seconds.</li>\n<li>Starting at 3.9 pounds, the 14 inch model is fairly lightweight (although still heavier than the XPS 13 inch Ubuntu developer laptop that weighs in around 2.6 pounds). By replacing the stock hard drive with an SSD drive reduces the weight a little bit.</li>\n<li>The max screen resolution of 1366 x 768 resolution is adequate and the Truelife LED-Backlit display looks good, for development or watching a video. Overall the display is not great, but not bad.</li>\n<li>The keyboard is full size with good spacing, and responsiveness. My only quibble is that the keyboard includes a Windows key. Couldn’t they put a Tux picture on there instead of that? :)</li>\n<li>Sporting a 40 WHr, 4-Cell Battery Battery, the system says it has  6.5 hours on a full charge, but a lot of that depends on what you are running. Expect 3-4 hours when running graphics or processor intensive applications.</li>\n<li>Some miscellaneous specs are: the system is less than an inch high when closed, making it a very low form factor. It has 2 USB 2.0 ports on the right side, 1 USB 3.0 port on the left, an HDMI out, a head phone jack , and Card reader slot. Finally it has a HD 720p capable webcam and a microphone for web conferences and video recordings.</li>\n</ul>\n<p>All in all, the system is a great value for the cost conscious user looking to purchase a Linux based laptop. I recommend it.</p>"},{"title":"Diagram SQL Server Graph Databases in R - Part 1","id":"4338","comments":0,"date":"2019-01-29T20:46:52.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2019/01/student-class.png)](http://edpflager.com/wp-content/uploads/2019/01/student-class.png)\n\nWelcome to the first of several articles covering generating diagrams in R of Microsoft SQL Server graph databases. A little theoretical background is in order before diving into the mechanics. In relational databases, tables typically connect or join based on a unique value in one table to another table where the same value may appear one or more times. Called a one-to-one or one-to-many relationship, it functions very well for most purposes. Dealing with real world situations with many-to-many relationship representation has been more awkward. As an example, the image above shows a representation of a classic many-to-many situation: class enrollment. A single student may be enrolled in many classes, and a single class may contain many students. In order to model this in a relational database, you would need to use an intermediary table, with one row for each student and each class they are enrolled in. By doing it this way, you can query the tables in either direction to get a list of all the students with the classes they are enrolled in, or all of the classes and which students are enrolled in those classes.\n<!-- more -->\nWhile graph databases have existed almost as long as relational databases, its only since the advent of NoSQL databases that they have become more widespread. An introduction to Graph databases with all of its nuances is beyond the scope of this series, but for those who are interested, Neo4J, Inc (manufacturer of the most popular graph databases systems) provides a [free ebook](https://neo4j.com/graph-databases-book/?ref=home) covering many Graph database concepts. [![](http://edpflager.com/wp-content/uploads/2019/01/class_graph-300x132.png)](http://edpflager.com/wp-content/uploads/2019/01/class_graph.png)For purposes of these articles, its enough to say that Graph databases seek to model the information by expressing the relationship as part of the model. So using the student class example, a graph database design could be visualized like the figure here: Note that the enrollment relationship shows arrows on both ends, meaning that its bidirectional. Students can be enrolled in a class, and a classes can have many students enrolled. Some situations don't make sense to have a bidirectional relationship, say for example ownership of a book. A person may own many books in their lifetime, but an individual books doesn't belong to many people. For bidirectional relationships, use arrows on both ends of the relationship, otherwise just arrows in one direction.\n\n##### USING MICROSOFT SQL SERVER\n\nIn [SQL Server 2017](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-overview?view=sql-server-2017), Microsoft started incorporating graph processing technology. While still new and not without some warts, it does hold some promise and the basic functionality is sufficient for what I wanted to accomplish here. Often times when working on a project, I don't have access to the underlying OLTP database model and have to do a lot of discovery to understand how data is processed through the system.  A visual representation of the graph tables is a quick way to understand the relationships and the entities in a graph database. The initial genesis of this series was to show how to generate a visualization of a SQL Server graph database to provide that understanding. It grew into producing a reusable process to generate visualizations from any Microsoft graph database just from knowing the name of the database. For initial testing, here is the code to create a simple graph database, based on the Student <- enrolled -> Class model.\n\nCREATE DATABASE StudentClass;\nGO\n\nUSE StudentClass;\nGO\n\n--Create Node tables and populate them\nCREATE TABLE Student(\nStudentID int PRIMARY KEY,\nStudentName varchar (100) \n) as NODE;\n\nINSERT INTO Student Values (1, 'Will Shakespeare');\nINSERT INTO Student Values (2, 'Percy Shelley');\nINSERT INTO Student Values (3, 'Chuck Dickens');\nINSERT INTO Student Values (4, 'Art Doyle');\nINSERT INTO Student Values (5, 'Gina Wolfe');\n\nCREATE TABLE Class(\nClassID int PRIMARY KEY,\nClassName varchar(100)\n) as NODE;\n\nINSERT INTO Class Values (1, 'History of Surfing');\nINSERT INTO Class Values (2, 'Joy of Garbage');\nINSERT INTO Class Values (3, 'Art of Walking');\nINSERT INTO Class Values (4, 'Street Fighting Mathematics');\nINSERT INTO Class Values (5, 'Fermentation Studies');\n\n--Create EDGE table and populate it.\nCREATE TABLE enrolledIn as Edge;\n\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 1));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 2));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 3));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 2),\n(Select $node\\_id from Class WHERE ClassID = 3));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 2),\n(Select $node\\_id from Class WHERE ClassID = 4));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 1));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 4));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 5));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 4),\n(Select $node\\_id from Class WHERE ClassID = 2));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 4),\n(Select $node\\_id from Class WHERE ClassID = 5));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 5),\n(Select $node\\_id from Class WHERE ClassID = 3));\n\nThis code creates two Node tables (Student and Class), and the Edge table that bridges them (enrolledIn). Sample data for five students, five classes, and who is enrolled in each class is added. **UPDATE: I had forgotten to include the data to make  the edge going in the opposite direction from the above. Here is the additional code to set that up:**\n\nCREATE TABLE enrolledStudent as Edge;\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\n\nWe now have a functional, but simple graph database. Next time, I'll cover briefly some of the new system functions that were added to SQL Server to allow you to query for the structure of graph databases, and present code I created that will show that information in a handy format for review.","source":"_posts/diagram-sql-server-graph-databases-in-r-part-1.md","raw":"---\ntitle: Diagram SQL Server Graph Databases in R - Part 1\ntags:\n  - Graph Databases\n  - howto\n  - metadata\n  - SQL Server\n  - SysAdmin\n  - technical\nid: '4338'\ncategories:\n  - - Business Intelligence\n  - - Linux\n  - - Misc\n  - - R\ncomments: false\ndate: 2019-01-29 15:46:52\n---\n\n[![](http://edpflager.com/wp-content/uploads/2019/01/student-class.png)](http://edpflager.com/wp-content/uploads/2019/01/student-class.png)\n\nWelcome to the first of several articles covering generating diagrams in R of Microsoft SQL Server graph databases. A little theoretical background is in order before diving into the mechanics. In relational databases, tables typically connect or join based on a unique value in one table to another table where the same value may appear one or more times. Called a one-to-one or one-to-many relationship, it functions very well for most purposes. Dealing with real world situations with many-to-many relationship representation has been more awkward. As an example, the image above shows a representation of a classic many-to-many situation: class enrollment. A single student may be enrolled in many classes, and a single class may contain many students. In order to model this in a relational database, you would need to use an intermediary table, with one row for each student and each class they are enrolled in. By doing it this way, you can query the tables in either direction to get a list of all the students with the classes they are enrolled in, or all of the classes and which students are enrolled in those classes.\n<!-- more -->\nWhile graph databases have existed almost as long as relational databases, its only since the advent of NoSQL databases that they have become more widespread. An introduction to Graph databases with all of its nuances is beyond the scope of this series, but for those who are interested, Neo4J, Inc (manufacturer of the most popular graph databases systems) provides a [free ebook](https://neo4j.com/graph-databases-book/?ref=home) covering many Graph database concepts. [![](http://edpflager.com/wp-content/uploads/2019/01/class_graph-300x132.png)](http://edpflager.com/wp-content/uploads/2019/01/class_graph.png)For purposes of these articles, its enough to say that Graph databases seek to model the information by expressing the relationship as part of the model. So using the student class example, a graph database design could be visualized like the figure here: Note that the enrollment relationship shows arrows on both ends, meaning that its bidirectional. Students can be enrolled in a class, and a classes can have many students enrolled. Some situations don't make sense to have a bidirectional relationship, say for example ownership of a book. A person may own many books in their lifetime, but an individual books doesn't belong to many people. For bidirectional relationships, use arrows on both ends of the relationship, otherwise just arrows in one direction.\n\n##### USING MICROSOFT SQL SERVER\n\nIn [SQL Server 2017](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-overview?view=sql-server-2017), Microsoft started incorporating graph processing technology. While still new and not without some warts, it does hold some promise and the basic functionality is sufficient for what I wanted to accomplish here. Often times when working on a project, I don't have access to the underlying OLTP database model and have to do a lot of discovery to understand how data is processed through the system.  A visual representation of the graph tables is a quick way to understand the relationships and the entities in a graph database. The initial genesis of this series was to show how to generate a visualization of a SQL Server graph database to provide that understanding. It grew into producing a reusable process to generate visualizations from any Microsoft graph database just from knowing the name of the database. For initial testing, here is the code to create a simple graph database, based on the Student <- enrolled -> Class model.\n\nCREATE DATABASE StudentClass;\nGO\n\nUSE StudentClass;\nGO\n\n--Create Node tables and populate them\nCREATE TABLE Student(\nStudentID int PRIMARY KEY,\nStudentName varchar (100) \n) as NODE;\n\nINSERT INTO Student Values (1, 'Will Shakespeare');\nINSERT INTO Student Values (2, 'Percy Shelley');\nINSERT INTO Student Values (3, 'Chuck Dickens');\nINSERT INTO Student Values (4, 'Art Doyle');\nINSERT INTO Student Values (5, 'Gina Wolfe');\n\nCREATE TABLE Class(\nClassID int PRIMARY KEY,\nClassName varchar(100)\n) as NODE;\n\nINSERT INTO Class Values (1, 'History of Surfing');\nINSERT INTO Class Values (2, 'Joy of Garbage');\nINSERT INTO Class Values (3, 'Art of Walking');\nINSERT INTO Class Values (4, 'Street Fighting Mathematics');\nINSERT INTO Class Values (5, 'Fermentation Studies');\n\n--Create EDGE table and populate it.\nCREATE TABLE enrolledIn as Edge;\n\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 1));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 2));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 1),\n(Select $node\\_id from Class WHERE ClassID = 3));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 2),\n(Select $node\\_id from Class WHERE ClassID = 3));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 2),\n(Select $node\\_id from Class WHERE ClassID = 4));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 1));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 4));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 3),\n(Select $node\\_id from Class WHERE ClassID = 5));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 4),\n(Select $node\\_id from Class WHERE ClassID = 2));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 4),\n(Select $node\\_id from Class WHERE ClassID = 5));\nINSERT INTO enrolledIn VALUES ((SELECT $node\\_id FROM Student WHERE StudentID = 5),\n(Select $node\\_id from Class WHERE ClassID = 3));\n\nThis code creates two Node tables (Student and Class), and the Edge table that bridges them (enrolledIn). Sample data for five students, five classes, and who is enrolled in each class is added. **UPDATE: I had forgotten to include the data to make  the edge going in the opposite direction from the above. Here is the additional code to set that up:**\n\nCREATE TABLE enrolledStudent as Edge;\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\n\nWe now have a functional, but simple graph database. Next time, I'll cover briefly some of the new system functions that were added to SQL Server to allow you to query for the structure of graph databases, and present code I created that will show that information in a handy format for review.","slug":"diagram-sql-server-graph-databases-in-r-part-1","published":1,"updated":"2020-08-23T20:54:35.218Z","layout":"post","photos":[],"link":"","_id":"ckeaq99us001hsdjxh1123mjy","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/01/student-class.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/student-class.png\"></a></p>\n<p>Welcome to the first of several articles covering generating diagrams in R of Microsoft SQL Server graph databases. A little theoretical background is in order before diving into the mechanics. In relational databases, tables typically connect or join based on a unique value in one table to another table where the same value may appear one or more times. Called a one-to-one or one-to-many relationship, it functions very well for most purposes. Dealing with real world situations with many-to-many relationship representation has been more awkward. As an example, the image above shows a representation of a classic many-to-many situation: class enrollment. A single student may be enrolled in many classes, and a single class may contain many students. In order to model this in a relational database, you would need to use an intermediary table, with one row for each student and each class they are enrolled in. By doing it this way, you can query the tables in either direction to get a list of all the students with the classes they are enrolled in, or all of the classes and which students are enrolled in those classes.</p>\n<a id=\"more\"></a>\n<p>While graph databases have existed almost as long as relational databases, its only since the advent of NoSQL databases that they have become more widespread. An introduction to Graph databases with all of its nuances is beyond the scope of this series, but for those who are interested, Neo4J, Inc (manufacturer of the most popular graph databases systems) provides a <a href=\"https://neo4j.com/graph-databases-book/?ref=home\">free ebook</a> covering many Graph database concepts. <a href=\"http://edpflager.com/wp-content/uploads/2019/01/class_graph.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/class_graph-300x132.png\"></a>For purposes of these articles, its enough to say that Graph databases seek to model the information by expressing the relationship as part of the model. So using the student class example, a graph database design could be visualized like the figure here: Note that the enrollment relationship shows arrows on both ends, meaning that its bidirectional. Students can be enrolled in a class, and a classes can have many students enrolled. Some situations don’t make sense to have a bidirectional relationship, say for example ownership of a book. A person may own many books in their lifetime, but an individual books doesn’t belong to many people. For bidirectional relationships, use arrows on both ends of the relationship, otherwise just arrows in one direction.</p>\n<h5 id=\"USING-MICROSOFT-SQL-SERVER\"><a href=\"#USING-MICROSOFT-SQL-SERVER\" class=\"headerlink\" title=\"USING MICROSOFT SQL SERVER\"></a>USING MICROSOFT SQL SERVER</h5><p>In <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-overview?view=sql-server-2017\">SQL Server 2017</a>, Microsoft started incorporating graph processing technology. While still new and not without some warts, it does hold some promise and the basic functionality is sufficient for what I wanted to accomplish here. Often times when working on a project, I don’t have access to the underlying OLTP database model and have to do a lot of discovery to understand how data is processed through the system.  A visual representation of the graph tables is a quick way to understand the relationships and the entities in a graph database. The initial genesis of this series was to show how to generate a visualization of a SQL Server graph database to provide that understanding. It grew into producing a reusable process to generate visualizations from any Microsoft graph database just from knowing the name of the database. For initial testing, here is the code to create a simple graph database, based on the Student &lt;- enrolled -&gt; Class model.</p>\n<p>CREATE DATABASE StudentClass;<br>GO</p>\n<p>USE StudentClass;<br>GO</p>\n<p>–Create Node tables and populate them<br>CREATE TABLE Student(<br>StudentID int PRIMARY KEY,<br>StudentName varchar (100)<br>) as NODE;</p>\n<p>INSERT INTO Student Values (1, ‘Will Shakespeare’);<br>INSERT INTO Student Values (2, ‘Percy Shelley’);<br>INSERT INTO Student Values (3, ‘Chuck Dickens’);<br>INSERT INTO Student Values (4, ‘Art Doyle’);<br>INSERT INTO Student Values (5, ‘Gina Wolfe’);</p>\n<p>CREATE TABLE Class(<br>ClassID int PRIMARY KEY,<br>ClassName varchar(100)<br>) as NODE;</p>\n<p>INSERT INTO Class Values (1, ‘History of Surfing’);<br>INSERT INTO Class Values (2, ‘Joy of Garbage’);<br>INSERT INTO Class Values (3, ‘Art of Walking’);<br>INSERT INTO Class Values (4, ‘Street Fighting Mathematics’);<br>INSERT INTO Class Values (5, ‘Fermentation Studies’);</p>\n<p>–Create EDGE table and populate it.<br>CREATE TABLE enrolledIn as Edge;</p>\n<p>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 1));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 2));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 3));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 2),<br>(Select $node_id from Class WHERE ClassID = 3));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 2),<br>(Select $node_id from Class WHERE ClassID = 4));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 1));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 4));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 5));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 4),<br>(Select $node_id from Class WHERE ClassID = 2));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 4),<br>(Select $node_id from Class WHERE ClassID = 5));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 5),<br>(Select $node_id from Class WHERE ClassID = 3));</p>\n<p>This code creates two Node tables (Student and Class), and the Edge table that bridges them (enrolledIn). Sample data for five students, five classes, and who is enrolled in each class is added. <strong>UPDATE: I had forgotten to include the data to make  the edge going in the opposite direction from the above. Here is the additional code to set that up:</strong></p>\n<p>CREATE TABLE enrolledStudent as Edge;<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));</p>\n<p>We now have a functional, but simple graph database. Next time, I’ll cover briefly some of the new system functions that were added to SQL Server to allow you to query for the structure of graph databases, and present code I created that will show that information in a handy format for review.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/01/student-class.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/student-class.png\"></a></p>\n<p>Welcome to the first of several articles covering generating diagrams in R of Microsoft SQL Server graph databases. A little theoretical background is in order before diving into the mechanics. In relational databases, tables typically connect or join based on a unique value in one table to another table where the same value may appear one or more times. Called a one-to-one or one-to-many relationship, it functions very well for most purposes. Dealing with real world situations with many-to-many relationship representation has been more awkward. As an example, the image above shows a representation of a classic many-to-many situation: class enrollment. A single student may be enrolled in many classes, and a single class may contain many students. In order to model this in a relational database, you would need to use an intermediary table, with one row for each student and each class they are enrolled in. By doing it this way, you can query the tables in either direction to get a list of all the students with the classes they are enrolled in, or all of the classes and which students are enrolled in those classes.</p>","more":"<p>While graph databases have existed almost as long as relational databases, its only since the advent of NoSQL databases that they have become more widespread. An introduction to Graph databases with all of its nuances is beyond the scope of this series, but for those who are interested, Neo4J, Inc (manufacturer of the most popular graph databases systems) provides a <a href=\"https://neo4j.com/graph-databases-book/?ref=home\">free ebook</a> covering many Graph database concepts. <a href=\"http://edpflager.com/wp-content/uploads/2019/01/class_graph.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/class_graph-300x132.png\"></a>For purposes of these articles, its enough to say that Graph databases seek to model the information by expressing the relationship as part of the model. So using the student class example, a graph database design could be visualized like the figure here: Note that the enrollment relationship shows arrows on both ends, meaning that its bidirectional. Students can be enrolled in a class, and a classes can have many students enrolled. Some situations don’t make sense to have a bidirectional relationship, say for example ownership of a book. A person may own many books in their lifetime, but an individual books doesn’t belong to many people. For bidirectional relationships, use arrows on both ends of the relationship, otherwise just arrows in one direction.</p>\n<h5 id=\"USING-MICROSOFT-SQL-SERVER\"><a href=\"#USING-MICROSOFT-SQL-SERVER\" class=\"headerlink\" title=\"USING MICROSOFT SQL SERVER\"></a>USING MICROSOFT SQL SERVER</h5><p>In <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-overview?view=sql-server-2017\">SQL Server 2017</a>, Microsoft started incorporating graph processing technology. While still new and not without some warts, it does hold some promise and the basic functionality is sufficient for what I wanted to accomplish here. Often times when working on a project, I don’t have access to the underlying OLTP database model and have to do a lot of discovery to understand how data is processed through the system.  A visual representation of the graph tables is a quick way to understand the relationships and the entities in a graph database. The initial genesis of this series was to show how to generate a visualization of a SQL Server graph database to provide that understanding. It grew into producing a reusable process to generate visualizations from any Microsoft graph database just from knowing the name of the database. For initial testing, here is the code to create a simple graph database, based on the Student &lt;- enrolled -&gt; Class model.</p>\n<p>CREATE DATABASE StudentClass;<br>GO</p>\n<p>USE StudentClass;<br>GO</p>\n<p>–Create Node tables and populate them<br>CREATE TABLE Student(<br>StudentID int PRIMARY KEY,<br>StudentName varchar (100)<br>) as NODE;</p>\n<p>INSERT INTO Student Values (1, ‘Will Shakespeare’);<br>INSERT INTO Student Values (2, ‘Percy Shelley’);<br>INSERT INTO Student Values (3, ‘Chuck Dickens’);<br>INSERT INTO Student Values (4, ‘Art Doyle’);<br>INSERT INTO Student Values (5, ‘Gina Wolfe’);</p>\n<p>CREATE TABLE Class(<br>ClassID int PRIMARY KEY,<br>ClassName varchar(100)<br>) as NODE;</p>\n<p>INSERT INTO Class Values (1, ‘History of Surfing’);<br>INSERT INTO Class Values (2, ‘Joy of Garbage’);<br>INSERT INTO Class Values (3, ‘Art of Walking’);<br>INSERT INTO Class Values (4, ‘Street Fighting Mathematics’);<br>INSERT INTO Class Values (5, ‘Fermentation Studies’);</p>\n<p>–Create EDGE table and populate it.<br>CREATE TABLE enrolledIn as Edge;</p>\n<p>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 1));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 2));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 1),<br>(Select $node_id from Class WHERE ClassID = 3));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 2),<br>(Select $node_id from Class WHERE ClassID = 3));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 2),<br>(Select $node_id from Class WHERE ClassID = 4));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 1));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 4));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 3),<br>(Select $node_id from Class WHERE ClassID = 5));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 4),<br>(Select $node_id from Class WHERE ClassID = 2));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 4),<br>(Select $node_id from Class WHERE ClassID = 5));<br>INSERT INTO enrolledIn VALUES ((SELECT $node_id FROM Student WHERE StudentID = 5),<br>(Select $node_id from Class WHERE ClassID = 3));</p>\n<p>This code creates two Node tables (Student and Class), and the Edge table that bridges them (enrolledIn). Sample data for five students, five classes, and who is enrolled in each class is added. <strong>UPDATE: I had forgotten to include the data to make  the edge going in the opposite direction from the above. Here is the additional code to set that up:</strong></p>\n<p>CREATE TABLE enrolledStudent as Edge;<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));</p>\n<p>We now have a functional, but simple graph database. Next time, I’ll cover briefly some of the new system functions that were added to SQL Server to allow you to query for the structure of graph databases, and present code I created that will show that information in a handy format for review.</p>"},{"title":"Diagram SQL Server Graph Databases in R - Part 2.5","id":"4403","comments":0,"date":"2019-02-06T23:40:26.000Z","_content":"\n![](http://edpflager.com/wp-content/uploads/2019/02/cursorwithschema-300x55.png)Quick post tonight. While working on some examples to test out the cursor I posted in Part 2, I realized that I had only coded the cursor to retrieve information for the dbo schema. So here I present the revised cursor which pulls from any schema. If a table is part of the dbo schema its not used in the results, since that is the normal default:\n<!-- more -->\n \n\nSET nocount on;\n\nIF OBJECT\\_ID('tempdb.dbo.#NodeEdgeXRef', 'U') IS NOT NULL\n\nDROP TABLE ##NodeEdgeXRef;\n\nCREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\nDECLARE @NodeID as INT;\nDECLARE @NodeName as NVARCHAR(128);\nDECLARE @EdgeID as INT;\nDECLARE @SchemaName as NVARCHAR(128);\nDECLARE @sSQL as nvarchar(MAX);\nDECLARE @NodeCursor as CURSOR;\n\n/\\* Revised to include schema here \\*/\n\nSET @NodeCursor = CURSOR FOR\nSELECT  tb.name, tb.object\\_id, tb.object\\_id, sc.name\nFROM    sys.tables tb\nJOIN      sys.schemas sc ON tb.schema\\_id = sc.schema\\_id\nWHERE  is\\_edge = 1;\n\nOPEN @NodeCursor;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n/\\* Revised to include SCHEMA here \\*/\n\nWHILE @@FETCH\\_STATUS = 0\nBEGIN\n\nset @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\nobject\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n+ CAST(@EdgeID as NVARCHAR(MAX))+ ' as EdgeID from ' + @SchemaName + '.' +@NodeName;\n\nINSERT INTO ##NodeEdgeXRef\n\nEXEC sp\\_executesql @sSQL;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\nEND\n\nCLOSE @NodeCursor;\n\nDEALLOCATE @NodeCursor;\n\n\n/\\* Revised query to add schema \\*/\n\nSELECT distinct \n        CASE WHEN frs.name = 'dbo' THEN fr.name\n            ELSE frs.name +'.'+ fr.name \n        END as FromName, \n        FromNode, \n        CASE WHEN es.name = 'dbo' THEN en.name\n            ELSE es.name +'.'+ en.name \n        END as EdgeName, \n        EdgeID,\n        CASE WHEN ts.name = 'dbo' THEN tn.name\n             ELSE ts.name + '.' + tn.name \n        END as ToName, \n        ToNode\nFROM ##NodeEdgeXRef xr\nJOIN sys.tables fr ON xr.FromNode = fr.object\\_id \nJOIN sys.tables tn ON xr.ToNode = tn.object\\_id\nJOIN sys.tables en ON xr.EdgeID = en.object\\_id\nJOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\nJOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\nJOIN sys.schemas es ON en.schema\\_id = es.schema\\_id;\n\nDROP TABLE ##NodeEdgeXRef;","source":"_posts/diagram-sql-server-graph-databases-in-r-part-2-5.md","raw":"---\ntitle: Diagram SQL Server Graph Databases in R - Part 2.5\ntags:\n  - graph database\n  - howto\n  - metadata\n  - SQL Server\n  - SysAdmin\n  - technical\nid: '4403'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - R\ncomments: false\ndate: 2019-02-06 18:40:26\n---\n\n![](http://edpflager.com/wp-content/uploads/2019/02/cursorwithschema-300x55.png)Quick post tonight. While working on some examples to test out the cursor I posted in Part 2, I realized that I had only coded the cursor to retrieve information for the dbo schema. So here I present the revised cursor which pulls from any schema. If a table is part of the dbo schema its not used in the results, since that is the normal default:\n<!-- more -->\n \n\nSET nocount on;\n\nIF OBJECT\\_ID('tempdb.dbo.#NodeEdgeXRef', 'U') IS NOT NULL\n\nDROP TABLE ##NodeEdgeXRef;\n\nCREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\nDECLARE @NodeID as INT;\nDECLARE @NodeName as NVARCHAR(128);\nDECLARE @EdgeID as INT;\nDECLARE @SchemaName as NVARCHAR(128);\nDECLARE @sSQL as nvarchar(MAX);\nDECLARE @NodeCursor as CURSOR;\n\n/\\* Revised to include schema here \\*/\n\nSET @NodeCursor = CURSOR FOR\nSELECT  tb.name, tb.object\\_id, tb.object\\_id, sc.name\nFROM    sys.tables tb\nJOIN      sys.schemas sc ON tb.schema\\_id = sc.schema\\_id\nWHERE  is\\_edge = 1;\n\nOPEN @NodeCursor;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n/\\* Revised to include SCHEMA here \\*/\n\nWHILE @@FETCH\\_STATUS = 0\nBEGIN\n\nset @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\nobject\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n+ CAST(@EdgeID as NVARCHAR(MAX))+ ' as EdgeID from ' + @SchemaName + '.' +@NodeName;\n\nINSERT INTO ##NodeEdgeXRef\n\nEXEC sp\\_executesql @sSQL;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\nEND\n\nCLOSE @NodeCursor;\n\nDEALLOCATE @NodeCursor;\n\n\n/\\* Revised query to add schema \\*/\n\nSELECT distinct \n        CASE WHEN frs.name = 'dbo' THEN fr.name\n            ELSE frs.name +'.'+ fr.name \n        END as FromName, \n        FromNode, \n        CASE WHEN es.name = 'dbo' THEN en.name\n            ELSE es.name +'.'+ en.name \n        END as EdgeName, \n        EdgeID,\n        CASE WHEN ts.name = 'dbo' THEN tn.name\n             ELSE ts.name + '.' + tn.name \n        END as ToName, \n        ToNode\nFROM ##NodeEdgeXRef xr\nJOIN sys.tables fr ON xr.FromNode = fr.object\\_id \nJOIN sys.tables tn ON xr.ToNode = tn.object\\_id\nJOIN sys.tables en ON xr.EdgeID = en.object\\_id\nJOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\nJOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\nJOIN sys.schemas es ON en.schema\\_id = es.schema\\_id;\n\nDROP TABLE ##NodeEdgeXRef;","slug":"diagram-sql-server-graph-databases-in-r-part-2-5","published":1,"updated":"2020-08-23T20:54:35.222Z","layout":"post","photos":[],"link":"","_id":"ckeaq99uy001ksdjx0kddbabw","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2019/02/cursorwithschema-300x55.png\">Quick post tonight. While working on some examples to test out the cursor I posted in Part 2, I realized that I had only coded the cursor to retrieve information for the dbo schema. So here I present the revised cursor which pulls from any schema. If a table is part of the dbo schema its not used in the results, since that is the normal default:</p>\n<a id=\"more\"></a>\n<p> </p>\n<p>SET nocount on;</p>\n<p>IF OBJECT_ID(‘tempdb.dbo.#NodeEdgeXRef’, ‘U’) IS NOT NULL</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>\n<p>CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);</p>\n<p>DECLARE @NodeID as INT;<br>DECLARE @NodeName as NVARCHAR(128);<br>DECLARE @EdgeID as INT;<br>DECLARE @SchemaName as NVARCHAR(128);<br>DECLARE @sSQL as nvarchar(MAX);<br>DECLARE @NodeCursor as CURSOR;</p>\n<p>/* Revised to include schema here */</p>\n<p>SET @NodeCursor = CURSOR FOR<br>SELECT  tb.name, tb.object_id, tb.object_id, sc.name<br>FROM    sys.tables tb<br>JOIN      sys.schemas sc ON tb.schema_id = sc.schema_id<br>WHERE  is_edge = 1;</p>\n<p>OPEN @NodeCursor;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;</p>\n<p>/* Revised to include SCHEMA here */</p>\n<p>WHILE @@FETCH_STATUS = 0<br>BEGIN</p>\n<p>set @sSQL=N’select distinct object_id_from_node_id($from_id) as FromNode,<br>object_id_from_node_id($to_id) as ToNode,’</p>\n<ul>\n<li>CAST(@EdgeID as NVARCHAR(MAX))+ ‘ as EdgeID from ‘ + @SchemaName + ‘.’ +@NodeName;</li>\n</ul>\n<p>INSERT INTO ##NodeEdgeXRef</p>\n<p>EXEC sp_executesql @sSQL;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;</p>\n<p>END</p>\n<p>CLOSE @NodeCursor;</p>\n<p>DEALLOCATE @NodeCursor;</p>\n<p>/* Revised query to add schema */</p>\n<p>SELECT distinct<br>        CASE WHEN frs.name = ‘dbo’ THEN fr.name<br>            ELSE frs.name +’.’+ fr.name<br>        END as FromName,<br>        FromNode,<br>        CASE WHEN es.name = ‘dbo’ THEN en.name<br>            ELSE es.name +’.’+ en.name<br>        END as EdgeName,<br>        EdgeID,<br>        CASE WHEN ts.name = ‘dbo’ THEN tn.name<br>             ELSE ts.name + ‘.’ + tn.name<br>        END as ToName,<br>        ToNode<br>FROM ##NodeEdgeXRef xr<br>JOIN sys.tables fr ON xr.FromNode = fr.object_id<br>JOIN sys.tables tn ON xr.ToNode = tn.object_id<br>JOIN sys.tables en ON xr.EdgeID = en.object_id<br>JOIN sys.schemas frs ON fr.schema_id = frs.schema_id<br>JOIN sys.schemas ts ON tn.schema_id = ts.schema_id<br>JOIN sys.schemas es ON en.schema_id = es.schema_id;</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2019/02/cursorwithschema-300x55.png\">Quick post tonight. While working on some examples to test out the cursor I posted in Part 2, I realized that I had only coded the cursor to retrieve information for the dbo schema. So here I present the revised cursor which pulls from any schema. If a table is part of the dbo schema its not used in the results, since that is the normal default:</p>","more":"<p> </p>\n<p>SET nocount on;</p>\n<p>IF OBJECT_ID(‘tempdb.dbo.#NodeEdgeXRef’, ‘U’) IS NOT NULL</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>\n<p>CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);</p>\n<p>DECLARE @NodeID as INT;<br>DECLARE @NodeName as NVARCHAR(128);<br>DECLARE @EdgeID as INT;<br>DECLARE @SchemaName as NVARCHAR(128);<br>DECLARE @sSQL as nvarchar(MAX);<br>DECLARE @NodeCursor as CURSOR;</p>\n<p>/* Revised to include schema here */</p>\n<p>SET @NodeCursor = CURSOR FOR<br>SELECT  tb.name, tb.object_id, tb.object_id, sc.name<br>FROM    sys.tables tb<br>JOIN      sys.schemas sc ON tb.schema_id = sc.schema_id<br>WHERE  is_edge = 1;</p>\n<p>OPEN @NodeCursor;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;</p>\n<p>/* Revised to include SCHEMA here */</p>\n<p>WHILE @@FETCH_STATUS = 0<br>BEGIN</p>\n<p>set @sSQL=N’select distinct object_id_from_node_id($from_id) as FromNode,<br>object_id_from_node_id($to_id) as ToNode,’</p>\n<ul>\n<li>CAST(@EdgeID as NVARCHAR(MAX))+ ‘ as EdgeID from ‘ + @SchemaName + ‘.’ +@NodeName;</li>\n</ul>\n<p>INSERT INTO ##NodeEdgeXRef</p>\n<p>EXEC sp_executesql @sSQL;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;</p>\n<p>END</p>\n<p>CLOSE @NodeCursor;</p>\n<p>DEALLOCATE @NodeCursor;</p>\n<p>/* Revised query to add schema */</p>\n<p>SELECT distinct<br>        CASE WHEN frs.name = ‘dbo’ THEN fr.name<br>            ELSE frs.name +’.’+ fr.name<br>        END as FromName,<br>        FromNode,<br>        CASE WHEN es.name = ‘dbo’ THEN en.name<br>            ELSE es.name +’.’+ en.name<br>        END as EdgeName,<br>        EdgeID,<br>        CASE WHEN ts.name = ‘dbo’ THEN tn.name<br>             ELSE ts.name + ‘.’ + tn.name<br>        END as ToName,<br>        ToNode<br>FROM ##NodeEdgeXRef xr<br>JOIN sys.tables fr ON xr.FromNode = fr.object_id<br>JOIN sys.tables tn ON xr.ToNode = tn.object_id<br>JOIN sys.tables en ON xr.EdgeID = en.object_id<br>JOIN sys.schemas frs ON fr.schema_id = frs.schema_id<br>JOIN sys.schemas ts ON tn.schema_id = ts.schema_id<br>JOIN sys.schemas es ON en.schema_id = es.schema_id;</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>"},{"title":"Diagram SQL Server Graph Databases in R - Part 2","id":"4379","comments":0,"date":"2019-02-02T18:44:57.000Z","_content":"\nThis is part two of a series about diagramming SQL Server graph databases in R. [In the first part](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), I covered setting up a simple graph database in SQL Server, with just a handful of tables to use as a test bed for this series. The database had one edge (enrolledIn). I have revised the code to include a second edge that goes in the opposite direction, called enrolledStudent, and  that code has been appended to the end of the first part and is reproduced here as well:\n\nCREATE TABLE enrolledStudent as Edge;\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\n<!-- more -->\n##### GRAPH DATABASE ADDITIONS TO SQL SERVER\n\nWhen creating graph database tables, SQL Server generates a unique node\\_id and edge\\_id for the relevant tables. To make querying easier, generic functions **$node\\_id** and **$edge\\_id** have been added to T-SQL allowing for reusable code without having to determine that unique ID. You can see the **$node\\_id** being used in the code for the DML for the sample database. Here is a screenshot of the **enrolledIn** edge table on my system. Note that the edge\\_ID field is really just a substitute for a PK and the data for the from\\_ID and to\\_ID fields are essentially FKs to help in locating the relevant records in the node tables. The $node\\_id and $edge\\_id functions will be used later to identify the relationships between the graph tables. [![](http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png)](http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png) In SQL Server 2017, Microsoft modified some of the system views and functions that provide information on the various components in a database to support the new graph functionality. This [Microsoft webpage](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-architecture?view=sql-server-2017#metadata) provides a considerable amount of information for those interested in learning more. For the purpose of this article, I am working only with a subset of the new information provided. Specifically additional information added to **sys.tables** where tables now have two new bit datatype columns: **is\\_node** and **is\\_edge,** and the **node\\_id$** and **edge\\_id$** information added to the edge tables. When querying the sys.tables view for a table, if **is\\_node** = 1, the table is a node. If **is\\_edge** = 1, the table is an edge. If both values are zero, then the table is neither an edge or a node. The values cannot be 1 for both fields. After creating the database StudentClass and the tables from the provided code, I ran the query below to see the **sys.tables** information, which identifies which tables are nodes and edges. For databases with non graph tables, a WHERE clause excluding tables where is\\_edge and is\\_node are both zero would replicate the same functionality:\n\nUse StudentClass;\ngo\nselect name, object\\_id, is\\_edge, is\\_node from sys.tables\nwhere type = 'U';\n![](http://edpflager.com/wp-content/uploads/2019/02/systable.png)\n\nSix new system functions were also added to SQL Server to provide information on graph database components.  Of these, I used two  to determine the information needed to map out the structure of the graph database. **OBJECT\\_ID\\_FROM\\_NODE\\_ID** will take a $node\\_id as input and will return the object\\_id of the table that the node belongs to. **OBJECT\\_ID\\_FROM\\_EDGE\\_ID** is similar, returning the object\\_id of a table that and edge belongs to.\n\n##### CURSOR TO ACQUIRE THE GRAPH DATA\n\n[![](http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png)](http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png)\n\nUsing the information from the **sys.table** view and the two system functions discussed above, I was able to create a cursor in TSQL to generate the above output. As you can see, it shows that there are two paths in this graph database:\n\n**Student -> (enrolledIn) -> Class and Class -> (enrolledStudent) -> Student.** \n\nBecause the edge table acts like a cross reference between the node tables, the cursor only needs to get information from the edge table for the initial compilation of information. All of the data is stored in a temporary table so I can return one data set rather than a separate data set for each node table once everything has been pulled. The completed code for the cursor is shown below:\n\nSET nocount on;\n\nIF OBJECT\\_ID('tempdb.dbo.#NodeEdgeXRef', 'U') IS NOT NULL\nDROP TABLE ##NodeEdgeXRef;\n\nCREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\nDECLARE @NodeID asINT;\nDECLARE @NodeName asNVARCHAR(128);\nDECLARE @EdgeID asINT;\nDECLARE @sSQL asnvarchar(MAX);\nDECLARE @NodeCursor as CURSOR;\n\nSET @NodeCursor = CURSOR FOR\n   select name, object\\_id, object\\_id\n   from sys.tables\n   where is\\_edge =1;\n\nOPEN @NodeCursor;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;\n\nWHILE@@FETCH\\_STATUS=0\n  BEGIN\n     set @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                 object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n                 +CAST(@EdgeID as NVARCHAR(MAX))+' as EdgeID from '+@NodeName;\n\n      INSERT INTO ##NodeEdgeXRef\n\n      EXEC sp\\_executesql @sSQL;\n\n      FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;\n\nEND\n\nCLOSE @NodeCursor;\n\nDEALLOCATE @NodeCursor;\n\nSELECT fr.name as FromName, FromNode,en.name as EdgeName, EdgeID, tn.name as ToName, ToNode\nFROM ##NodeEdgeXRef xr\nJOIN sys.tables fr ON xr.FromNode = fr.object\\_id\nJOIN sys.tables tn ON xr.ToNode = tn.object\\_id\nJOIN sys.tables en ON xr.EdgeID = en.object\\_id;\n\nDROP TABLE ##NodeEdgeXRef;\n\nNext time, we'll look at dropping this cursor into an R script and then using the R package DiagrammeR to generate a visualization of the database structure!","source":"_posts/diagram-sql-server-graph-databases-in-r-part-2.md","raw":"---\ntitle: Diagram SQL Server Graph Databases in R - Part 2\ntags:\n  - graph database\n  - guides\n  - howto\n  - metadata\n  - SQL Server\n  - SysAdmin\nid: '4379'\ncategories:\n  - - Business Intelligence\n  - - Linux\n  - - R\ncomments: false\ndate: 2019-02-02 13:44:57\n---\n\nThis is part two of a series about diagramming SQL Server graph databases in R. [In the first part](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), I covered setting up a simple graph database in SQL Server, with just a handful of tables to use as a test bed for this series. The database had one edge (enrolledIn). I have revised the code to include a second edge that goes in the opposite direction, called enrolledStudent, and  that code has been appended to the end of the first part and is reproduced here as well:\n\nCREATE TABLE enrolledStudent as Edge;\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 1),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 2),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 1));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 3),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 2));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 4),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 3));\nINSERT INTO enrolledStudent VALUES ((Select $node\\_id from Class WHERE ClassID = 5),\n (SELECT $node\\_id FROM Student WHERE StudentID = 4));\n<!-- more -->\n##### GRAPH DATABASE ADDITIONS TO SQL SERVER\n\nWhen creating graph database tables, SQL Server generates a unique node\\_id and edge\\_id for the relevant tables. To make querying easier, generic functions **$node\\_id** and **$edge\\_id** have been added to T-SQL allowing for reusable code without having to determine that unique ID. You can see the **$node\\_id** being used in the code for the DML for the sample database. Here is a screenshot of the **enrolledIn** edge table on my system. Note that the edge\\_ID field is really just a substitute for a PK and the data for the from\\_ID and to\\_ID fields are essentially FKs to help in locating the relevant records in the node tables. The $node\\_id and $edge\\_id functions will be used later to identify the relationships between the graph tables. [![](http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png)](http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png) In SQL Server 2017, Microsoft modified some of the system views and functions that provide information on the various components in a database to support the new graph functionality. This [Microsoft webpage](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-architecture?view=sql-server-2017#metadata) provides a considerable amount of information for those interested in learning more. For the purpose of this article, I am working only with a subset of the new information provided. Specifically additional information added to **sys.tables** where tables now have two new bit datatype columns: **is\\_node** and **is\\_edge,** and the **node\\_id$** and **edge\\_id$** information added to the edge tables. When querying the sys.tables view for a table, if **is\\_node** = 1, the table is a node. If **is\\_edge** = 1, the table is an edge. If both values are zero, then the table is neither an edge or a node. The values cannot be 1 for both fields. After creating the database StudentClass and the tables from the provided code, I ran the query below to see the **sys.tables** information, which identifies which tables are nodes and edges. For databases with non graph tables, a WHERE clause excluding tables where is\\_edge and is\\_node are both zero would replicate the same functionality:\n\nUse StudentClass;\ngo\nselect name, object\\_id, is\\_edge, is\\_node from sys.tables\nwhere type = 'U';\n![](http://edpflager.com/wp-content/uploads/2019/02/systable.png)\n\nSix new system functions were also added to SQL Server to provide information on graph database components.  Of these, I used two  to determine the information needed to map out the structure of the graph database. **OBJECT\\_ID\\_FROM\\_NODE\\_ID** will take a $node\\_id as input and will return the object\\_id of the table that the node belongs to. **OBJECT\\_ID\\_FROM\\_EDGE\\_ID** is similar, returning the object\\_id of a table that and edge belongs to.\n\n##### CURSOR TO ACQUIRE THE GRAPH DATA\n\n[![](http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png)](http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png)\n\nUsing the information from the **sys.table** view and the two system functions discussed above, I was able to create a cursor in TSQL to generate the above output. As you can see, it shows that there are two paths in this graph database:\n\n**Student -> (enrolledIn) -> Class and Class -> (enrolledStudent) -> Student.** \n\nBecause the edge table acts like a cross reference between the node tables, the cursor only needs to get information from the edge table for the initial compilation of information. All of the data is stored in a temporary table so I can return one data set rather than a separate data set for each node table once everything has been pulled. The completed code for the cursor is shown below:\n\nSET nocount on;\n\nIF OBJECT\\_ID('tempdb.dbo.#NodeEdgeXRef', 'U') IS NOT NULL\nDROP TABLE ##NodeEdgeXRef;\n\nCREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\nDECLARE @NodeID asINT;\nDECLARE @NodeName asNVARCHAR(128);\nDECLARE @EdgeID asINT;\nDECLARE @sSQL asnvarchar(MAX);\nDECLARE @NodeCursor as CURSOR;\n\nSET @NodeCursor = CURSOR FOR\n   select name, object\\_id, object\\_id\n   from sys.tables\n   where is\\_edge =1;\n\nOPEN @NodeCursor;\n\nFETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;\n\nWHILE@@FETCH\\_STATUS=0\n  BEGIN\n     set @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                 object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n                 +CAST(@EdgeID as NVARCHAR(MAX))+' as EdgeID from '+@NodeName;\n\n      INSERT INTO ##NodeEdgeXRef\n\n      EXEC sp\\_executesql @sSQL;\n\n      FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;\n\nEND\n\nCLOSE @NodeCursor;\n\nDEALLOCATE @NodeCursor;\n\nSELECT fr.name as FromName, FromNode,en.name as EdgeName, EdgeID, tn.name as ToName, ToNode\nFROM ##NodeEdgeXRef xr\nJOIN sys.tables fr ON xr.FromNode = fr.object\\_id\nJOIN sys.tables tn ON xr.ToNode = tn.object\\_id\nJOIN sys.tables en ON xr.EdgeID = en.object\\_id;\n\nDROP TABLE ##NodeEdgeXRef;\n\nNext time, we'll look at dropping this cursor into an R script and then using the R package DiagrammeR to generate a visualization of the database structure!","slug":"diagram-sql-server-graph-databases-in-r-part-2","published":1,"updated":"2020-08-23T20:54:35.218Z","layout":"post","photos":[],"link":"","_id":"ckeaq99w9001nsdjxekcseqh8","content":"<p>This is part two of a series about diagramming SQL Server graph databases in R. <a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">In the first part</a>, I covered setting up a simple graph database in SQL Server, with just a handful of tables to use as a test bed for this series. The database had one edge (enrolledIn). I have revised the code to include a second edge that goes in the opposite direction, called enrolledStudent, and  that code has been appended to the end of the first part and is reproduced here as well:</p>\n<p>CREATE TABLE enrolledStudent as Edge;<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));</p>\n<a id=\"more\"></a>\n<h5 id=\"GRAPH-DATABASE-ADDITIONS-TO-SQL-SERVER\"><a href=\"#GRAPH-DATABASE-ADDITIONS-TO-SQL-SERVER\" class=\"headerlink\" title=\"GRAPH DATABASE ADDITIONS TO SQL SERVER\"></a>GRAPH DATABASE ADDITIONS TO SQL SERVER</h5><p>When creating graph database tables, SQL Server generates a unique node_id and edge_id for the relevant tables. To make querying easier, generic functions <strong>$node_id</strong> and <strong>$edge_id</strong> have been added to T-SQL allowing for reusable code without having to determine that unique ID. You can see the <strong>$node_id</strong> being used in the code for the DML for the sample database. Here is a screenshot of the <strong>enrolledIn</strong> edge table on my system. Note that the edge_ID field is really just a substitute for a PK and the data for the from_ID and to_ID fields are essentially FKs to help in locating the relevant records in the node tables. The $node_id and $edge_id functions will be used later to identify the relationships between the graph tables. <a href=\"http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png\"></a> In SQL Server 2017, Microsoft modified some of the system views and functions that provide information on the various components in a database to support the new graph functionality. This <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-architecture?view=sql-server-2017#metadata\">Microsoft webpage</a> provides a considerable amount of information for those interested in learning more. For the purpose of this article, I am working only with a subset of the new information provided. Specifically additional information added to <strong>sys.tables</strong> where tables now have two new bit datatype columns: <strong>is_node</strong> and <strong>is_edge,</strong> and the <strong>node_id$</strong> and <strong>edge_id$</strong> information added to the edge tables. When querying the sys.tables view for a table, if <strong>is_node</strong> = 1, the table is a node. If <strong>is_edge</strong> = 1, the table is an edge. If both values are zero, then the table is neither an edge or a node. The values cannot be 1 for both fields. After creating the database StudentClass and the tables from the provided code, I ran the query below to see the <strong>sys.tables</strong> information, which identifies which tables are nodes and edges. For databases with non graph tables, a WHERE clause excluding tables where is_edge and is_node are both zero would replicate the same functionality:</p>\n<p>Use StudentClass;<br>go<br>select name, object_id, is_edge, is_node from sys.tables<br>where type = ‘U’;<br><img src=\"http://edpflager.com/wp-content/uploads/2019/02/systable.png\"></p>\n<p>Six new system functions were also added to SQL Server to provide information on graph database components.  Of these, I used two  to determine the information needed to map out the structure of the graph database. <strong>OBJECT_ID_FROM_NODE_ID</strong> will take a $node_id as input and will return the object_id of the table that the node belongs to. <strong>OBJECT_ID_FROM_EDGE_ID</strong> is similar, returning the object_id of a table that and edge belongs to.</p>\n<h5 id=\"CURSOR-TO-ACQUIRE-THE-GRAPH-DATA\"><a href=\"#CURSOR-TO-ACQUIRE-THE-GRAPH-DATA\" class=\"headerlink\" title=\"CURSOR TO ACQUIRE THE GRAPH DATA\"></a>CURSOR TO ACQUIRE THE GRAPH DATA</h5><p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png\"></a></p>\n<p>Using the information from the <strong>sys.table</strong> view and the two system functions discussed above, I was able to create a cursor in TSQL to generate the above output. As you can see, it shows that there are two paths in this graph database:</p>\n<p><strong>Student -&gt; (enrolledIn) -&gt; Class and Class -&gt; (enrolledStudent) -&gt; Student.</strong> </p>\n<p>Because the edge table acts like a cross reference between the node tables, the cursor only needs to get information from the edge table for the initial compilation of information. All of the data is stored in a temporary table so I can return one data set rather than a separate data set for each node table once everything has been pulled. The completed code for the cursor is shown below:</p>\n<p>SET nocount on;</p>\n<p>IF OBJECT_ID(‘tempdb.dbo.#NodeEdgeXRef’, ‘U’) IS NOT NULL<br>DROP TABLE ##NodeEdgeXRef;</p>\n<p>CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);</p>\n<p>DECLARE @NodeID asINT;<br>DECLARE @NodeName asNVARCHAR(128);<br>DECLARE @EdgeID asINT;<br>DECLARE @sSQL asnvarchar(MAX);<br>DECLARE @NodeCursor as CURSOR;</p>\n<p>SET @NodeCursor = CURSOR FOR<br>   select name, object_id, object_id<br>   from sys.tables<br>   where is_edge =1;</p>\n<p>OPEN @NodeCursor;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;</p>\n<p>WHILE@@FETCH_STATUS=0<br>  BEGIN<br>     set @sSQL=N’select distinct object_id_from_node_id($from_id) as FromNode,<br>                 object_id_from_node_id($to_id) as ToNode,’<br>                 +CAST(@EdgeID as NVARCHAR(MAX))+’ as EdgeID from ‘+@NodeName;</p>\n<pre><code>  INSERT INTO ##NodeEdgeXRef\n\n  EXEC sp\\_executesql @sSQL;\n\n  FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;</code></pre>\n<p>END</p>\n<p>CLOSE @NodeCursor;</p>\n<p>DEALLOCATE @NodeCursor;</p>\n<p>SELECT fr.name as FromName, FromNode,en.name as EdgeName, EdgeID, tn.name as ToName, ToNode<br>FROM ##NodeEdgeXRef xr<br>JOIN sys.tables fr ON xr.FromNode = fr.object_id<br>JOIN sys.tables tn ON xr.ToNode = tn.object_id<br>JOIN sys.tables en ON xr.EdgeID = en.object_id;</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>\n<p>Next time, we’ll look at dropping this cursor into an R script and then using the R package DiagrammeR to generate a visualization of the database structure!</p>\n","site":{"data":{}},"excerpt":"<p>This is part two of a series about diagramming SQL Server graph databases in R. <a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">In the first part</a>, I covered setting up a simple graph database in SQL Server, with just a handful of tables to use as a test bed for this series. The database had one edge (enrolledIn). I have revised the code to include a second edge that goes in the opposite direction, called enrolledStudent, and  that code has been appended to the end of the first part and is reproduced here as well:</p>\n<p>CREATE TABLE enrolledStudent as Edge;<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 1),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 2),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 1));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 3),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 2));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 4),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 3));<br>INSERT INTO enrolledStudent VALUES ((Select $node_id from Class WHERE ClassID = 5),<br> (SELECT $node_id FROM Student WHERE StudentID = 4));</p>","more":"<h5 id=\"GRAPH-DATABASE-ADDITIONS-TO-SQL-SERVER\"><a href=\"#GRAPH-DATABASE-ADDITIONS-TO-SQL-SERVER\" class=\"headerlink\" title=\"GRAPH DATABASE ADDITIONS TO SQL SERVER\"></a>GRAPH DATABASE ADDITIONS TO SQL SERVER</h5><p>When creating graph database tables, SQL Server generates a unique node_id and edge_id for the relevant tables. To make querying easier, generic functions <strong>$node_id</strong> and <strong>$edge_id</strong> have been added to T-SQL allowing for reusable code without having to determine that unique ID. You can see the <strong>$node_id</strong> being used in the code for the DML for the sample database. Here is a screenshot of the <strong>enrolledIn</strong> edge table on my system. Note that the edge_ID field is really just a substitute for a PK and the data for the from_ID and to_ID fields are essentially FKs to help in locating the relevant records in the node tables. The $node_id and $edge_id functions will be used later to identify the relationships between the graph tables. <a href=\"http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/enrolledIn.png\"></a> In SQL Server 2017, Microsoft modified some of the system views and functions that provide information on the various components in a database to support the new graph functionality. This <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-architecture?view=sql-server-2017#metadata\">Microsoft webpage</a> provides a considerable amount of information for those interested in learning more. For the purpose of this article, I am working only with a subset of the new information provided. Specifically additional information added to <strong>sys.tables</strong> where tables now have two new bit datatype columns: <strong>is_node</strong> and <strong>is_edge,</strong> and the <strong>node_id$</strong> and <strong>edge_id$</strong> information added to the edge tables. When querying the sys.tables view for a table, if <strong>is_node</strong> = 1, the table is a node. If <strong>is_edge</strong> = 1, the table is an edge. If both values are zero, then the table is neither an edge or a node. The values cannot be 1 for both fields. After creating the database StudentClass and the tables from the provided code, I ran the query below to see the <strong>sys.tables</strong> information, which identifies which tables are nodes and edges. For databases with non graph tables, a WHERE clause excluding tables where is_edge and is_node are both zero would replicate the same functionality:</p>\n<p>Use StudentClass;<br>go<br>select name, object_id, is_edge, is_node from sys.tables<br>where type = ‘U’;<br><img src=\"http://edpflager.com/wp-content/uploads/2019/02/systable.png\"></p>\n<p>Six new system functions were also added to SQL Server to provide information on graph database components.  Of these, I used two  to determine the information needed to map out the structure of the graph database. <strong>OBJECT_ID_FROM_NODE_ID</strong> will take a $node_id as input and will return the object_id of the table that the node belongs to. <strong>OBJECT_ID_FROM_EDGE_ID</strong> is similar, returning the object_id of a table that and edge belongs to.</p>\n<h5 id=\"CURSOR-TO-ACQUIRE-THE-GRAPH-DATA\"><a href=\"#CURSOR-TO-ACQUIRE-THE-GRAPH-DATA\" class=\"headerlink\" title=\"CURSOR TO ACQUIRE THE GRAPH DATA\"></a>CURSOR TO ACQUIRE THE GRAPH DATA</h5><p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/NodeEdgeStudent.png\"></a></p>\n<p>Using the information from the <strong>sys.table</strong> view and the two system functions discussed above, I was able to create a cursor in TSQL to generate the above output. As you can see, it shows that there are two paths in this graph database:</p>\n<p><strong>Student -&gt; (enrolledIn) -&gt; Class and Class -&gt; (enrolledStudent) -&gt; Student.</strong> </p>\n<p>Because the edge table acts like a cross reference between the node tables, the cursor only needs to get information from the edge table for the initial compilation of information. All of the data is stored in a temporary table so I can return one data set rather than a separate data set for each node table once everything has been pulled. The completed code for the cursor is shown below:</p>\n<p>SET nocount on;</p>\n<p>IF OBJECT_ID(‘tempdb.dbo.#NodeEdgeXRef’, ‘U’) IS NOT NULL<br>DROP TABLE ##NodeEdgeXRef;</p>\n<p>CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);</p>\n<p>DECLARE @NodeID asINT;<br>DECLARE @NodeName asNVARCHAR(128);<br>DECLARE @EdgeID asINT;<br>DECLARE @sSQL asnvarchar(MAX);<br>DECLARE @NodeCursor as CURSOR;</p>\n<p>SET @NodeCursor = CURSOR FOR<br>   select name, object_id, object_id<br>   from sys.tables<br>   where is_edge =1;</p>\n<p>OPEN @NodeCursor;</p>\n<p>FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;</p>\n<p>WHILE@@FETCH_STATUS=0<br>  BEGIN<br>     set @sSQL=N’select distinct object_id_from_node_id($from_id) as FromNode,<br>                 object_id_from_node_id($to_id) as ToNode,’<br>                 +CAST(@EdgeID as NVARCHAR(MAX))+’ as EdgeID from ‘+@NodeName;</p>\n<pre><code>  INSERT INTO ##NodeEdgeXRef\n\n  EXEC sp\\_executesql @sSQL;\n\n  FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID;</code></pre>\n<p>END</p>\n<p>CLOSE @NodeCursor;</p>\n<p>DEALLOCATE @NodeCursor;</p>\n<p>SELECT fr.name as FromName, FromNode,en.name as EdgeName, EdgeID, tn.name as ToName, ToNode<br>FROM ##NodeEdgeXRef xr<br>JOIN sys.tables fr ON xr.FromNode = fr.object_id<br>JOIN sys.tables tn ON xr.ToNode = tn.object_id<br>JOIN sys.tables en ON xr.EdgeID = en.object_id;</p>\n<p>DROP TABLE ##NodeEdgeXRef;</p>\n<p>Next time, we’ll look at dropping this cursor into an R script and then using the R package DiagrammeR to generate a visualization of the database structure!</p>"},{"title":"Diagram SQL Server Graph Databases in R - Part 3","id":"4415","comments":0,"date":"2019-02-10T17:17:40.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2019/02/odbc2R-300x108.png)](http://edpflager.com/wp-content/uploads/2019/02/odbc2R.png)The first 2.5 parts of this series ([part 1](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), [part 2](http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/), [part 2.5](http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/))  focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In this part, we shift over to R and imbed the SQL Server cursor script to retrieve data for manipulation. As a prerequisite, on whatever operating system you are using, you need to install a Microsoft SQL Server ODBC driver. The Microsoft website has easy to follow tutorials for Mac OS X and Linux [here](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017) and [here](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/microsoft-odbc-driver-for-sql-server-on-windows?view=sql-server-2017) for Windows. You will also need to install the [ODBC](https://cran.r-project.org/web/packages/odbc/index.html) package in R for this code to work as well, so follow normal R package instructions for that.\n<!-- more -->\nOnce you have your R environment setup, start a new R script/project for the Graph Database diagram. Copy the code below into your script. Its basically the same script that was used in the cursor with a few changes to allow it to work with R. The biggest change involves how R parses strings. Do include a quote character in your string, you must proceed it with the   (slash) character. I have added comments in the script to document what each section is doing:\n\n\\## Load ODBC library and create connection to SQL Server. Replace the Uid and PWD fields with your UserID and Password info. \nlibrary(odbc)\n\ncon <- dbConnect(odbc::odbc(), \n       .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=StudentClass;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Define a data frame to hold the query and pass it the query.\n## Set Nocount On is necessary for R ODBC to return results from a temporary table.\n\nnodes\\_edges <- dbGetQuery(con, 'SET nocount on;\n\n        ## Check to see if the temporary table is present. If it is, drop it before starting\n        IF OBJECT\\_ID('tempdb.dbo.##NodeEdgeXRef','U') IS NOT NULL\n        DROP TABLE ##NodeEdgeXRef;\n\n        ## Create a temporary table to hold the various table ID values.\n        CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\n        ## Declare the Cursor variables.\n        DECLARE @NodeID as INT;\n        DECLARE @NodeName as NVARCHAR(128); \n        DECLARE @EdgeID as INT;\n        DECLARE @SchemaName as NVARCHAR(128);\n        DECLARE @sSQL as nvarchar(MAX);\n        DECLARE @NodeCursor as CURSOR;\n\n        ## Define the query used in the Cursor.\n        SET @NodeCursor = CURSOR FOR\n           select tb.name, tb.object\\_id, tb.object\\_id, sc.name\n           from sys.tables tb\n           JOIN sys.schemas sc ON tb.schema\\_id = sc.schema\\_id \n           where is\\_edge = 1; \n\n        ## Open the Cursor and fetch the first resultset.\n        OPEN @NodeCursor;\n        FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n        ## While the fetch of the last resultset was successful, here is the code to execute.\n        WHILE @@FETCH\\_STATUS = 0\n        BEGIN\n           set @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                        object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n                        +CAST(@EdgeID as NVARCHAR(MAX))+' as EdgeID from ' +@SchemaName + '.' + @NodeName;\n\n        ## Store the results in the temporary table\n        INSERT INTO ##NodeEdgeXRef \n\n        ## Execute the code\n        EXEC sp\\_executesql @sSQL;\n\n        ## Repeat\n        FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n        END\n\n        ## Close the cursor and deallocate it.\n        CLOSE @NodeCursor;\n        DEALLOCATE @NodeCursor; \n\n        ## Get the data from the temporary table, dropping the dbo schema if present, and returning it to R.\n        SELECT distinct \n               CASE WHEN frs.name = 'dbo' THEN fr.name\n                    ELSE frs.name + '.' + fr.name \n                 END as FromName, \n               FromNode,\n               CASE WHEN es.name = 'dbo' THEN en.name\n                    ELSE es.name + '.' + en.name \n                 END as EdgeName, \n               EdgeID, \n               CASE WHEN ts.name = 'dbo' THEN tn.name\n                    ELSE ts.name +'.' + tn.name \n                 END as ToName, \n               ToNode\n         FROM ##NodeEdgeXRef xr\n         JOIN sys.tables fr ON xr.FromNode = fr.object\\_id \n         JOIN sys.tables tn ON xr.ToNode = tn.object\\_id\n         JOIN sys.tables en ON xr.EdgeID = en.object\\_id\n         JOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\n         JOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\n         JOIN sys.schemas es ON en.schema\\_id = es.schema\\_id; \n\n         ## Drop the temporary table\n         DROP TABLE ##NodeEdgeXRef;\n        ')\n\n        View(nodes\\_edges);\n\nExecuting this code, should return a data.frame holding the names and tables IDs of the FROM and TO nodes, and the EDGE table name and ID. The View statement at the end will produce something similar to this, if you are using R Studio. (Your table IDs will vary). [![](http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png)](http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png) Almost complete now, next time I will share the R code that generates a Graph database diagram like this to illustrate the interrelatedness of our sample graph database: [![](http://edpflager.com/wp-content/uploads/2019/02/StudentClass-300x172.png)](http://edpflager.com/wp-content/uploads/2019/02/StudentClass.png)","source":"_posts/diagram-sql-server-graph-databases-in-r-part-3.md","raw":"---\ntitle: Diagram SQL Server Graph Databases in R - Part 3\ntags:\n  - graph database\n  - howto\n  - metadata\n  - SQL Server\n  - technical\nid: '4415'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - R\ncomments: false\ndate: 2019-02-10 12:17:40\n---\n\n[![](http://edpflager.com/wp-content/uploads/2019/02/odbc2R-300x108.png)](http://edpflager.com/wp-content/uploads/2019/02/odbc2R.png)The first 2.5 parts of this series ([part 1](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), [part 2](http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/), [part 2.5](http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/))  focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In this part, we shift over to R and imbed the SQL Server cursor script to retrieve data for manipulation. As a prerequisite, on whatever operating system you are using, you need to install a Microsoft SQL Server ODBC driver. The Microsoft website has easy to follow tutorials for Mac OS X and Linux [here](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017) and [here](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/microsoft-odbc-driver-for-sql-server-on-windows?view=sql-server-2017) for Windows. You will also need to install the [ODBC](https://cran.r-project.org/web/packages/odbc/index.html) package in R for this code to work as well, so follow normal R package instructions for that.\n<!-- more -->\nOnce you have your R environment setup, start a new R script/project for the Graph Database diagram. Copy the code below into your script. Its basically the same script that was used in the cursor with a few changes to allow it to work with R. The biggest change involves how R parses strings. Do include a quote character in your string, you must proceed it with the   (slash) character. I have added comments in the script to document what each section is doing:\n\n\\## Load ODBC library and create connection to SQL Server. Replace the Uid and PWD fields with your UserID and Password info. \nlibrary(odbc)\n\ncon <- dbConnect(odbc::odbc(), \n       .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=StudentClass;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Define a data frame to hold the query and pass it the query.\n## Set Nocount On is necessary for R ODBC to return results from a temporary table.\n\nnodes\\_edges <- dbGetQuery(con, 'SET nocount on;\n\n        ## Check to see if the temporary table is present. If it is, drop it before starting\n        IF OBJECT\\_ID('tempdb.dbo.##NodeEdgeXRef','U') IS NOT NULL\n        DROP TABLE ##NodeEdgeXRef;\n\n        ## Create a temporary table to hold the various table ID values.\n        CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\n        ## Declare the Cursor variables.\n        DECLARE @NodeID as INT;\n        DECLARE @NodeName as NVARCHAR(128); \n        DECLARE @EdgeID as INT;\n        DECLARE @SchemaName as NVARCHAR(128);\n        DECLARE @sSQL as nvarchar(MAX);\n        DECLARE @NodeCursor as CURSOR;\n\n        ## Define the query used in the Cursor.\n        SET @NodeCursor = CURSOR FOR\n           select tb.name, tb.object\\_id, tb.object\\_id, sc.name\n           from sys.tables tb\n           JOIN sys.schemas sc ON tb.schema\\_id = sc.schema\\_id \n           where is\\_edge = 1; \n\n        ## Open the Cursor and fetch the first resultset.\n        OPEN @NodeCursor;\n        FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n        ## While the fetch of the last resultset was successful, here is the code to execute.\n        WHILE @@FETCH\\_STATUS = 0\n        BEGIN\n           set @sSQL=N'select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                        object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,'\n                        +CAST(@EdgeID as NVARCHAR(MAX))+' as EdgeID from ' +@SchemaName + '.' + @NodeName;\n\n        ## Store the results in the temporary table\n        INSERT INTO ##NodeEdgeXRef \n\n        ## Execute the code\n        EXEC sp\\_executesql @sSQL;\n\n        ## Repeat\n        FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n        END\n\n        ## Close the cursor and deallocate it.\n        CLOSE @NodeCursor;\n        DEALLOCATE @NodeCursor; \n\n        ## Get the data from the temporary table, dropping the dbo schema if present, and returning it to R.\n        SELECT distinct \n               CASE WHEN frs.name = 'dbo' THEN fr.name\n                    ELSE frs.name + '.' + fr.name \n                 END as FromName, \n               FromNode,\n               CASE WHEN es.name = 'dbo' THEN en.name\n                    ELSE es.name + '.' + en.name \n                 END as EdgeName, \n               EdgeID, \n               CASE WHEN ts.name = 'dbo' THEN tn.name\n                    ELSE ts.name +'.' + tn.name \n                 END as ToName, \n               ToNode\n         FROM ##NodeEdgeXRef xr\n         JOIN sys.tables fr ON xr.FromNode = fr.object\\_id \n         JOIN sys.tables tn ON xr.ToNode = tn.object\\_id\n         JOIN sys.tables en ON xr.EdgeID = en.object\\_id\n         JOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\n         JOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\n         JOIN sys.schemas es ON en.schema\\_id = es.schema\\_id; \n\n         ## Drop the temporary table\n         DROP TABLE ##NodeEdgeXRef;\n        ')\n\n        View(nodes\\_edges);\n\nExecuting this code, should return a data.frame holding the names and tables IDs of the FROM and TO nodes, and the EDGE table name and ID. The View statement at the end will produce something similar to this, if you are using R Studio. (Your table IDs will vary). [![](http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png)](http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png) Almost complete now, next time I will share the R code that generates a Graph database diagram like this to illustrate the interrelatedness of our sample graph database: [![](http://edpflager.com/wp-content/uploads/2019/02/StudentClass-300x172.png)](http://edpflager.com/wp-content/uploads/2019/02/StudentClass.png)","slug":"diagram-sql-server-graph-databases-in-r-part-3","published":1,"updated":"2020-08-23T20:54:35.226Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wd001qsdjx8y705xah","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/odbc2R.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/odbc2R-300x108.png\"></a>The first 2.5 parts of this series (<a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">part 1</a>, <a href=\"http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/\">part 2</a>, <a href=\"http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/\">part 2.5</a>)  focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In this part, we shift over to R and imbed the SQL Server cursor script to retrieve data for manipulation. As a prerequisite, on whatever operating system you are using, you need to install a Microsoft SQL Server ODBC driver. The Microsoft website has easy to follow tutorials for Mac OS X and Linux <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017\">here</a> and <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/windows/microsoft-odbc-driver-for-sql-server-on-windows?view=sql-server-2017\">here</a> for Windows. You will also need to install the <a href=\"https://cran.r-project.org/web/packages/odbc/index.html\">ODBC</a> package in R for this code to work as well, so follow normal R package instructions for that.</p>\n<a id=\"more\"></a>\n<p>Once you have your R environment setup, start a new R script/project for the Graph Database diagram. Copy the code below into your script. Its basically the same script that was used in the cursor with a few changes to allow it to work with R. The biggest change involves how R parses strings. Do include a quote character in your string, you must proceed it with the   (slash) character. I have added comments in the script to document what each section is doing:</p>\n<p>## Load ODBC library and create connection to SQL Server. Replace the Uid and PWD fields with your UserID and Password info.<br>library(odbc)</p>\n<p>con &lt;- dbConnect(odbc::odbc(),<br>       .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=StudentClass;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Define-a-data-frame-to-hold-the-query-and-pass-it-the-query\"><a href=\"#Define-a-data-frame-to-hold-the-query-and-pass-it-the-query\" class=\"headerlink\" title=\"Define a data frame to hold the query and pass it the query.\"></a>Define a data frame to hold the query and pass it the query.</h2><h2 id=\"Set-Nocount-On-is-necessary-for-R-ODBC-to-return-results-from-a-temporary-table\"><a href=\"#Set-Nocount-On-is-necessary-for-R-ODBC-to-return-results-from-a-temporary-table\" class=\"headerlink\" title=\"Set Nocount On is necessary for R ODBC to return results from a temporary table.\"></a>Set Nocount On is necessary for R ODBC to return results from a temporary table.</h2><p>nodes_edges &lt;- dbGetQuery(con, ‘SET nocount on;</p>\n<pre><code>    ## Check to see if the temporary table is present. If it is, drop it before starting\n    IF OBJECT\\_ID(&#39;tempdb.dbo.##NodeEdgeXRef&#39;,&#39;U&#39;) IS NOT NULL\n    DROP TABLE ##NodeEdgeXRef;\n\n    ## Create a temporary table to hold the various table ID values.\n    CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\n    ## Declare the Cursor variables.\n    DECLARE @NodeID as INT;\n    DECLARE @NodeName as NVARCHAR(128); \n    DECLARE @EdgeID as INT;\n    DECLARE @SchemaName as NVARCHAR(128);\n    DECLARE @sSQL as nvarchar(MAX);\n    DECLARE @NodeCursor as CURSOR;\n\n    ## Define the query used in the Cursor.\n    SET @NodeCursor = CURSOR FOR\n       select tb.name, tb.object\\_id, tb.object\\_id, sc.name\n       from sys.tables tb\n       JOIN sys.schemas sc ON tb.schema\\_id = sc.schema\\_id \n       where is\\_edge = 1; \n\n    ## Open the Cursor and fetch the first resultset.\n    OPEN @NodeCursor;\n    FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n    ## While the fetch of the last resultset was successful, here is the code to execute.\n    WHILE @@FETCH\\_STATUS = 0\n    BEGIN\n       set @sSQL=N&#39;select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                    object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,&#39;\n                    +CAST(@EdgeID as NVARCHAR(MAX))+&#39; as EdgeID from &#39; +@SchemaName + &#39;.&#39; + @NodeName;\n\n    ## Store the results in the temporary table\n    INSERT INTO ##NodeEdgeXRef \n\n    ## Execute the code\n    EXEC sp\\_executesql @sSQL;\n\n    ## Repeat\n    FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n    END\n\n    ## Close the cursor and deallocate it.\n    CLOSE @NodeCursor;\n    DEALLOCATE @NodeCursor; \n\n    ## Get the data from the temporary table, dropping the dbo schema if present, and returning it to R.\n    SELECT distinct \n           CASE WHEN frs.name = &#39;dbo&#39; THEN fr.name\n                ELSE frs.name + &#39;.&#39; + fr.name \n             END as FromName, \n           FromNode,\n           CASE WHEN es.name = &#39;dbo&#39; THEN en.name\n                ELSE es.name + &#39;.&#39; + en.name \n             END as EdgeName, \n           EdgeID, \n           CASE WHEN ts.name = &#39;dbo&#39; THEN tn.name\n                ELSE ts.name +&#39;.&#39; + tn.name \n             END as ToName, \n           ToNode\n     FROM ##NodeEdgeXRef xr\n     JOIN sys.tables fr ON xr.FromNode = fr.object\\_id \n     JOIN sys.tables tn ON xr.ToNode = tn.object\\_id\n     JOIN sys.tables en ON xr.EdgeID = en.object\\_id\n     JOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\n     JOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\n     JOIN sys.schemas es ON en.schema\\_id = es.schema\\_id; \n\n     ## Drop the temporary table\n     DROP TABLE ##NodeEdgeXRef;\n    &#39;)\n\n    View(nodes\\_edges);</code></pre>\n<p>Executing this code, should return a data.frame holding the names and tables IDs of the FROM and TO nodes, and the EDGE table name and ID. The View statement at the end will produce something similar to this, if you are using R Studio. (Your table IDs will vary). <a href=\"http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png\"></a> Almost complete now, next time I will share the R code that generates a Graph database diagram like this to illustrate the interrelatedness of our sample graph database: <a href=\"http://edpflager.com/wp-content/uploads/2019/02/StudentClass.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/StudentClass-300x172.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/odbc2R.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/odbc2R-300x108.png\"></a>The first 2.5 parts of this series (<a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">part 1</a>, <a href=\"http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/\">part 2</a>, <a href=\"http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/\">part 2.5</a>)  focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In this part, we shift over to R and imbed the SQL Server cursor script to retrieve data for manipulation. As a prerequisite, on whatever operating system you are using, you need to install a Microsoft SQL Server ODBC driver. The Microsoft website has easy to follow tutorials for Mac OS X and Linux <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017\">here</a> and <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/windows/microsoft-odbc-driver-for-sql-server-on-windows?view=sql-server-2017\">here</a> for Windows. You will also need to install the <a href=\"https://cran.r-project.org/web/packages/odbc/index.html\">ODBC</a> package in R for this code to work as well, so follow normal R package instructions for that.</p>","more":"<p>Once you have your R environment setup, start a new R script/project for the Graph Database diagram. Copy the code below into your script. Its basically the same script that was used in the cursor with a few changes to allow it to work with R. The biggest change involves how R parses strings. Do include a quote character in your string, you must proceed it with the   (slash) character. I have added comments in the script to document what each section is doing:</p>\n<p>## Load ODBC library and create connection to SQL Server. Replace the Uid and PWD fields with your UserID and Password info.<br>library(odbc)</p>\n<p>con &lt;- dbConnect(odbc::odbc(),<br>       .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=StudentClass;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Define-a-data-frame-to-hold-the-query-and-pass-it-the-query\"><a href=\"#Define-a-data-frame-to-hold-the-query-and-pass-it-the-query\" class=\"headerlink\" title=\"Define a data frame to hold the query and pass it the query.\"></a>Define a data frame to hold the query and pass it the query.</h2><h2 id=\"Set-Nocount-On-is-necessary-for-R-ODBC-to-return-results-from-a-temporary-table\"><a href=\"#Set-Nocount-On-is-necessary-for-R-ODBC-to-return-results-from-a-temporary-table\" class=\"headerlink\" title=\"Set Nocount On is necessary for R ODBC to return results from a temporary table.\"></a>Set Nocount On is necessary for R ODBC to return results from a temporary table.</h2><p>nodes_edges &lt;- dbGetQuery(con, ‘SET nocount on;</p>\n<pre><code>    ## Check to see if the temporary table is present. If it is, drop it before starting\n    IF OBJECT\\_ID(&#39;tempdb.dbo.##NodeEdgeXRef&#39;,&#39;U&#39;) IS NOT NULL\n    DROP TABLE ##NodeEdgeXRef;\n\n    ## Create a temporary table to hold the various table ID values.\n    CREATE TABLE ##NodeEdgeXRef (FromNode INT, ToNode INT, EdgeID Int);\n\n    ## Declare the Cursor variables.\n    DECLARE @NodeID as INT;\n    DECLARE @NodeName as NVARCHAR(128); \n    DECLARE @EdgeID as INT;\n    DECLARE @SchemaName as NVARCHAR(128);\n    DECLARE @sSQL as nvarchar(MAX);\n    DECLARE @NodeCursor as CURSOR;\n\n    ## Define the query used in the Cursor.\n    SET @NodeCursor = CURSOR FOR\n       select tb.name, tb.object\\_id, tb.object\\_id, sc.name\n       from sys.tables tb\n       JOIN sys.schemas sc ON tb.schema\\_id = sc.schema\\_id \n       where is\\_edge = 1; \n\n    ## Open the Cursor and fetch the first resultset.\n    OPEN @NodeCursor;\n    FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n    ## While the fetch of the last resultset was successful, here is the code to execute.\n    WHILE @@FETCH\\_STATUS = 0\n    BEGIN\n       set @sSQL=N&#39;select distinct object\\_id\\_from\\_node\\_id($from\\_id) as FromNode,\n                    object\\_id\\_from\\_node\\_id($to\\_id) as ToNode,&#39;\n                    +CAST(@EdgeID as NVARCHAR(MAX))+&#39; as EdgeID from &#39; +@SchemaName + &#39;.&#39; + @NodeName;\n\n    ## Store the results in the temporary table\n    INSERT INTO ##NodeEdgeXRef \n\n    ## Execute the code\n    EXEC sp\\_executesql @sSQL;\n\n    ## Repeat\n    FETCH NEXT FROM @NodeCursor INTO @NodeName, @NodeID, @EdgeID, @SchemaName;\n\n    END\n\n    ## Close the cursor and deallocate it.\n    CLOSE @NodeCursor;\n    DEALLOCATE @NodeCursor; \n\n    ## Get the data from the temporary table, dropping the dbo schema if present, and returning it to R.\n    SELECT distinct \n           CASE WHEN frs.name = &#39;dbo&#39; THEN fr.name\n                ELSE frs.name + &#39;.&#39; + fr.name \n             END as FromName, \n           FromNode,\n           CASE WHEN es.name = &#39;dbo&#39; THEN en.name\n                ELSE es.name + &#39;.&#39; + en.name \n             END as EdgeName, \n           EdgeID, \n           CASE WHEN ts.name = &#39;dbo&#39; THEN tn.name\n                ELSE ts.name +&#39;.&#39; + tn.name \n             END as ToName, \n           ToNode\n     FROM ##NodeEdgeXRef xr\n     JOIN sys.tables fr ON xr.FromNode = fr.object\\_id \n     JOIN sys.tables tn ON xr.ToNode = tn.object\\_id\n     JOIN sys.tables en ON xr.EdgeID = en.object\\_id\n     JOIN sys.schemas frs ON fr.schema\\_id = frs.schema\\_id\n     JOIN sys.schemas ts ON tn.schema\\_id = ts.schema\\_id\n     JOIN sys.schemas es ON en.schema\\_id = es.schema\\_id; \n\n     ## Drop the temporary table\n     DROP TABLE ##NodeEdgeXRef;\n    &#39;)\n\n    View(nodes\\_edges);</code></pre>\n<p>Executing this code, should return a data.frame holding the names and tables IDs of the FROM and TO nodes, and the EDGE table name and ID. The View statement at the end will produce something similar to this, if you are using R Studio. (Your table IDs will vary). <a href=\"http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/NodesEdges.png\"></a> Almost complete now, next time I will share the R code that generates a Graph database diagram like this to illustrate the interrelatedness of our sample graph database: <a href=\"http://edpflager.com/wp-content/uploads/2019/02/StudentClass.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/StudentClass-300x172.png\"></a></p>"},{"title":"Diagram SQL Server Graph Databases in R - Part 4","id":"4432","comments":0,"date":"2019-02-13T19:44:27.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png)](http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png)Here we are at the last part for this series. To recap, in the first 2.5 parts of this series ([part 1](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), [part 2](http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/), [part 2.5](http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/)) I focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In the [third part](http://edpflager.com/2019/02/10/diagram-sql-server-graph-databases-in-r-part-3/), I shifted over to R and embedded the SQL Server cursor script in an R file to retrieve data for manipulation. Now with this final section, I'll show you how to use Rich Iannone's [DiagrammeR](http://rich-iannone.github.io/DiagrammeR/) package to generate a graph visualization. At the left is a visualization I created of a Microsoft sample graph database that I used along with the one I created in the first parts of this article to test this out. You can get the code from Microsoft's website at this [link](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-sample?view=sql-server-2017).\n<!-- more -->\nBelow is the additional R code required to generate the visualization. A couple of Tidyverse packages (tibble amd magrittr) and the DiagrammeR package and needed and all are available on the CRAN repository. Copy and paste the code below right after the last line of the part 3 code that ends with this:\n\nDROP TABLE ##NodeEdgeXRef;\n')\n\n## New code\n\n\\# Read in distinct node names and IDs into a tibble and call the column nodes, and \nnodelist <- tibble(nodeName = union(nodes\\_edges$FromName, nodes\\_edges$ToName), \n                   nodeId = union(nodes\\_edges$FromNode, nodes\\_edges$ToNode))\n\n# Create the graph object\ni\\_graph <-\n  create\\_graph()  %>%\n    ## Get the nodelist tibble and define it as a table for the create\\_graph function\n     add\\_nodes\\_from\\_table(\n        table = nodelist,\n        label\\_col = nodeName) %>%\n\n    ## Define the To and From edge names\n            add\\_edges\\_from\\_table(\n            table = nodes\\_edges,\n            from\\_col = FromNode,\n            to\\_col = ToNode,\n              from\\_to\\_map = nodeId)  %>%\n\n                    ## set the node fontsize to 5. Adjust larger or smaller depending on table name length   \n                    set\\_node\\_attrs(\n                       node\\_attr = fontsize,\n                       values = 5\n                    ) %>% \n\n                    ## Set the node name text so it will display on the nodes  \n                    set\\_edge\\_attrs(\n                       edge\\_attr = value,\n                       values = nodes\\_edges$EdgeName) %>%\n                    \n                    ## Set the edge fontsize to 5. Adjust larger or smaller depending on table name length\n                     set\\_edge\\_attrs(\n                        edge\\_attr = fontsize,\n                        values = 5) %>%\n                    ## Set the edge name values between nodes. \n                     set\\_edge\\_attr\\_to\\_display(\n                        edges = 1:(nrow(nodes\\_edges)),\n                        attr = value,\n                        default = NA);\n\n## Display the graph\nrender\\_graph(i\\_graph, layout = \"nicely\")\n\nThat's it! Save the script and run it, and you should see the graph I showed last time for the StudentClass demo database, or if you use the Microsoft sample, the visualization at the top of this page. I am continue to explore Graph Databases and will post more information as I develop it. I'm also happy to say that Microsoft has already announced additional features for graph databases in SQL Server 2019.","source":"_posts/diagram-sql-server-graph-databases-in-r-part-4.md","raw":"---\ntitle: Diagram SQL Server Graph Databases in R - Part 4\ntags:\n  - graph database\n  - howto\n  - SQL Server\n  - SysAdmin\n  - technical\nid: '4432'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - R\ncomments: false\ndate: 2019-02-13 14:44:27\n---\n\n[![](http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png)](http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png)Here we are at the last part for this series. To recap, in the first 2.5 parts of this series ([part 1](http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/), [part 2](http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/), [part 2.5](http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/)) I focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In the [third part](http://edpflager.com/2019/02/10/diagram-sql-server-graph-databases-in-r-part-3/), I shifted over to R and embedded the SQL Server cursor script in an R file to retrieve data for manipulation. Now with this final section, I'll show you how to use Rich Iannone's [DiagrammeR](http://rich-iannone.github.io/DiagrammeR/) package to generate a graph visualization. At the left is a visualization I created of a Microsoft sample graph database that I used along with the one I created in the first parts of this article to test this out. You can get the code from Microsoft's website at this [link](https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-sample?view=sql-server-2017).\n<!-- more -->\nBelow is the additional R code required to generate the visualization. A couple of Tidyverse packages (tibble amd magrittr) and the DiagrammeR package and needed and all are available on the CRAN repository. Copy and paste the code below right after the last line of the part 3 code that ends with this:\n\nDROP TABLE ##NodeEdgeXRef;\n')\n\n## New code\n\n\\# Read in distinct node names and IDs into a tibble and call the column nodes, and \nnodelist <- tibble(nodeName = union(nodes\\_edges$FromName, nodes\\_edges$ToName), \n                   nodeId = union(nodes\\_edges$FromNode, nodes\\_edges$ToNode))\n\n# Create the graph object\ni\\_graph <-\n  create\\_graph()  %>%\n    ## Get the nodelist tibble and define it as a table for the create\\_graph function\n     add\\_nodes\\_from\\_table(\n        table = nodelist,\n        label\\_col = nodeName) %>%\n\n    ## Define the To and From edge names\n            add\\_edges\\_from\\_table(\n            table = nodes\\_edges,\n            from\\_col = FromNode,\n            to\\_col = ToNode,\n              from\\_to\\_map = nodeId)  %>%\n\n                    ## set the node fontsize to 5. Adjust larger or smaller depending on table name length   \n                    set\\_node\\_attrs(\n                       node\\_attr = fontsize,\n                       values = 5\n                    ) %>% \n\n                    ## Set the node name text so it will display on the nodes  \n                    set\\_edge\\_attrs(\n                       edge\\_attr = value,\n                       values = nodes\\_edges$EdgeName) %>%\n                    \n                    ## Set the edge fontsize to 5. Adjust larger or smaller depending on table name length\n                     set\\_edge\\_attrs(\n                        edge\\_attr = fontsize,\n                        values = 5) %>%\n                    ## Set the edge name values between nodes. \n                     set\\_edge\\_attr\\_to\\_display(\n                        edges = 1:(nrow(nodes\\_edges)),\n                        attr = value,\n                        default = NA);\n\n## Display the graph\nrender\\_graph(i\\_graph, layout = \"nicely\")\n\nThat's it! Save the script and run it, and you should see the graph I showed last time for the StudentClass demo database, or if you use the Microsoft sample, the visualization at the top of this page. I am continue to explore Graph Databases and will post more information as I develop it. I'm also happy to say that Microsoft has already announced additional features for graph databases in SQL Server 2019.","slug":"diagram-sql-server-graph-databases-in-r-part-4","published":1,"updated":"2020-08-23T20:54:35.226Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wi001tsdjx92527s3n","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png\"></a>Here we are at the last part for this series. To recap, in the first 2.5 parts of this series (<a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">part 1</a>, <a href=\"http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/\">part 2</a>, <a href=\"http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/\">part 2.5</a>) I focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In the <a href=\"http://edpflager.com/2019/02/10/diagram-sql-server-graph-databases-in-r-part-3/\">third part</a>, I shifted over to R and embedded the SQL Server cursor script in an R file to retrieve data for manipulation. Now with this final section, I’ll show you how to use Rich Iannone’s <a href=\"http://rich-iannone.github.io/DiagrammeR/\">DiagrammeR</a> package to generate a graph visualization. At the left is a visualization I created of a Microsoft sample graph database that I used along with the one I created in the first parts of this article to test this out. You can get the code from Microsoft’s website at this <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-sample?view=sql-server-2017\">link</a>.</p>\n<a id=\"more\"></a>\n<p>Below is the additional R code required to generate the visualization. A couple of Tidyverse packages (tibble amd magrittr) and the DiagrammeR package and needed and all are available on the CRAN repository. Copy and paste the code below right after the last line of the part 3 code that ends with this:</p>\n<p>DROP TABLE ##NodeEdgeXRef;<br>‘)</p>\n<h2 id=\"New-code\"><a href=\"#New-code\" class=\"headerlink\" title=\"New code\"></a>New code</h2><p># Read in distinct node names and IDs into a tibble and call the column nodes, and<br>nodelist &lt;- tibble(nodeName = union(nodes_edges$FromName, nodes_edges$ToName),<br>                   nodeId = union(nodes_edges$FromNode, nodes_edges$ToNode))</p>\n<h1 id=\"Create-the-graph-object\"><a href=\"#Create-the-graph-object\" class=\"headerlink\" title=\"Create the graph object\"></a>Create the graph object</h1><p>i_graph &lt;-<br>  create_graph()  %&gt;%<br>    ## Get the nodelist tibble and define it as a table for the create_graph function<br>     add_nodes_from_table(<br>        table = nodelist,<br>        label_col = nodeName) %&gt;%</p>\n<pre><code>## Define the To and From edge names\n        add\\_edges\\_from\\_table(\n        table = nodes\\_edges,\n        from\\_col = FromNode,\n        to\\_col = ToNode,\n          from\\_to\\_map = nodeId)  %&gt;%\n\n                ## set the node fontsize to 5. Adjust larger or smaller depending on table name length   \n                set\\_node\\_attrs(\n                   node\\_attr = fontsize,\n                   values = 5\n                ) %&gt;% \n\n                ## Set the node name text so it will display on the nodes  \n                set\\_edge\\_attrs(\n                   edge\\_attr = value,\n                   values = nodes\\_edges$EdgeName) %&gt;%\n\n                ## Set the edge fontsize to 5. Adjust larger or smaller depending on table name length\n                 set\\_edge\\_attrs(\n                    edge\\_attr = fontsize,\n                    values = 5) %&gt;%\n                ## Set the edge name values between nodes. \n                 set\\_edge\\_attr\\_to\\_display(\n                    edges = 1:(nrow(nodes\\_edges)),\n                    attr = value,\n                    default = NA);</code></pre>\n<h2 id=\"Display-the-graph\"><a href=\"#Display-the-graph\" class=\"headerlink\" title=\"Display the graph\"></a>Display the graph</h2><p>render_graph(i_graph, layout = “nicely”)</p>\n<p>That’s it! Save the script and run it, and you should see the graph I showed last time for the StudentClass demo database, or if you use the Microsoft sample, the visualization at the top of this page. I am continue to explore Graph Databases and will post more information as I develop it. I’m also happy to say that Microsoft has already announced additional features for graph databases in SQL Server 2019.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/graphdemo.png\"></a>Here we are at the last part for this series. To recap, in the first 2.5 parts of this series (<a href=\"http://edpflager.com/2019/01/29/diagram-sql-server-graph-databases-in-r-part-1/\">part 1</a>, <a href=\"http://edpflager.com/2019/02/02/diagram-sql-server-graph-databases-in-r-part-2/\">part 2</a>, <a href=\"http://edpflager.com/2019/02/07/diagram-sql-server-graph-databases-in-r-part-2-5/\">part 2.5</a>) I focused on setting up a graph database in SQL Server, and querying the metadata to discover the nodes and edges that define the structure of the graph. In the <a href=\"http://edpflager.com/2019/02/10/diagram-sql-server-graph-databases-in-r-part-3/\">third part</a>, I shifted over to R and embedded the SQL Server cursor script in an R file to retrieve data for manipulation. Now with this final section, I’ll show you how to use Rich Iannone’s <a href=\"http://rich-iannone.github.io/DiagrammeR/\">DiagrammeR</a> package to generate a graph visualization. At the left is a visualization I created of a Microsoft sample graph database that I used along with the one I created in the first parts of this article to test this out. You can get the code from Microsoft’s website at this <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/graphs/sql-graph-sample?view=sql-server-2017\">link</a>.</p>","more":"<p>Below is the additional R code required to generate the visualization. A couple of Tidyverse packages (tibble amd magrittr) and the DiagrammeR package and needed and all are available on the CRAN repository. Copy and paste the code below right after the last line of the part 3 code that ends with this:</p>\n<p>DROP TABLE ##NodeEdgeXRef;<br>‘)</p>\n<h2 id=\"New-code\"><a href=\"#New-code\" class=\"headerlink\" title=\"New code\"></a>New code</h2><p># Read in distinct node names and IDs into a tibble and call the column nodes, and<br>nodelist &lt;- tibble(nodeName = union(nodes_edges$FromName, nodes_edges$ToName),<br>                   nodeId = union(nodes_edges$FromNode, nodes_edges$ToNode))</p>\n<h1 id=\"Create-the-graph-object\"><a href=\"#Create-the-graph-object\" class=\"headerlink\" title=\"Create the graph object\"></a>Create the graph object</h1><p>i_graph &lt;-<br>  create_graph()  %&gt;%<br>    ## Get the nodelist tibble and define it as a table for the create_graph function<br>     add_nodes_from_table(<br>        table = nodelist,<br>        label_col = nodeName) %&gt;%</p>\n<pre><code>## Define the To and From edge names\n        add\\_edges\\_from\\_table(\n        table = nodes\\_edges,\n        from\\_col = FromNode,\n        to\\_col = ToNode,\n          from\\_to\\_map = nodeId)  %&gt;%\n\n                ## set the node fontsize to 5. Adjust larger or smaller depending on table name length   \n                set\\_node\\_attrs(\n                   node\\_attr = fontsize,\n                   values = 5\n                ) %&gt;% \n\n                ## Set the node name text so it will display on the nodes  \n                set\\_edge\\_attrs(\n                   edge\\_attr = value,\n                   values = nodes\\_edges$EdgeName) %&gt;%\n\n                ## Set the edge fontsize to 5. Adjust larger or smaller depending on table name length\n                 set\\_edge\\_attrs(\n                    edge\\_attr = fontsize,\n                    values = 5) %&gt;%\n                ## Set the edge name values between nodes. \n                 set\\_edge\\_attr\\_to\\_display(\n                    edges = 1:(nrow(nodes\\_edges)),\n                    attr = value,\n                    default = NA);</code></pre>\n<h2 id=\"Display-the-graph\"><a href=\"#Display-the-graph\" class=\"headerlink\" title=\"Display the graph\"></a>Display the graph</h2><p>render_graph(i_graph, layout = “nicely”)</p>\n<p>That’s it! Save the script and run it, and you should see the graph I showed last time for the StudentClass demo database, or if you use the Microsoft sample, the visualization at the top of this page. I am continue to explore Graph Databases and will post more information as I develop it. I’m also happy to say that Microsoft has already announced additional features for graph databases in SQL Server 2019.</p>"},{"title":"Disable SELinux to install Cloudera","id":"1866","comments":0,"date":"2014-03-09T21:05:48.000Z","_content":"\n[![SELinux](http://edpflager.com/wp-content/uploads/2014/03/SELinux-300x186.png)](http://edpflager.com/wp-content/uploads/2014/03/SELinux.png)One of the gotcha's I've encountered when installing Cloudera on CentOS Linux (and I'm assuming it holds true for RHEL), is the presence of SELinux. SELinux is a kernel module that supports some very strong access control security measures. While disabling this module won't make your system completely vulnerable to outside hacking of your systems, it does make it easier. If you are planning on using your Hadoop system exposed to the Internet in any way, you probably do not want to do what I am going to show you here. Or if you do, reenable SELinux once you are finished. Use at your own risk. To disable SELinux, open a terminal prompt and switch to Super User mode. Then in your favorite editor, open the file /etc/sysconfig/selinux. The file itself is very small, and you only need to make one change. About halfway down you will see a line like this: SELINUX=enforcing Change the word \"enforcing\" to \"disabled\", and save the file. Restart your box, and SELINUX will be disabled, and you can install Cloudera.","source":"_posts/disable-selinux-to-install-cloudera.md","raw":"---\ntitle: Disable SELinux to install Cloudera\ntags:\n  - CDH\n  - Hadoop\nid: '1866'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2014-03-09 17:05:48\n---\n\n[![SELinux](http://edpflager.com/wp-content/uploads/2014/03/SELinux-300x186.png)](http://edpflager.com/wp-content/uploads/2014/03/SELinux.png)One of the gotcha's I've encountered when installing Cloudera on CentOS Linux (and I'm assuming it holds true for RHEL), is the presence of SELinux. SELinux is a kernel module that supports some very strong access control security measures. While disabling this module won't make your system completely vulnerable to outside hacking of your systems, it does make it easier. If you are planning on using your Hadoop system exposed to the Internet in any way, you probably do not want to do what I am going to show you here. Or if you do, reenable SELinux once you are finished. Use at your own risk. To disable SELinux, open a terminal prompt and switch to Super User mode. Then in your favorite editor, open the file /etc/sysconfig/selinux. The file itself is very small, and you only need to make one change. About halfway down you will see a line like this: SELINUX=enforcing Change the word \"enforcing\" to \"disabled\", and save the file. Restart your box, and SELINUX will be disabled, and you can install Cloudera.","slug":"disable-selinux-to-install-cloudera","published":1,"updated":"2020-08-23T20:54:34.794Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wp001xsdjxdp61c04i","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/SELinux.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/SELinux-300x186.png\" alt=\"SELinux\"></a>One of the gotcha’s I’ve encountered when installing Cloudera on CentOS Linux (and I’m assuming it holds true for RHEL), is the presence of SELinux. SELinux is a kernel module that supports some very strong access control security measures. While disabling this module won’t make your system completely vulnerable to outside hacking of your systems, it does make it easier. If you are planning on using your Hadoop system exposed to the Internet in any way, you probably do not want to do what I am going to show you here. Or if you do, reenable SELinux once you are finished. Use at your own risk. To disable SELinux, open a terminal prompt and switch to Super User mode. Then in your favorite editor, open the file /etc/sysconfig/selinux. The file itself is very small, and you only need to make one change. About halfway down you will see a line like this: SELINUX=enforcing Change the word “enforcing” to “disabled”, and save the file. Restart your box, and SELINUX will be disabled, and you can install Cloudera.</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/SELinux.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/SELinux-300x186.png\" alt=\"SELinux\"></a>One of the gotcha’s I’ve encountered when installing Cloudera on CentOS Linux (and I’m assuming it holds true for RHEL), is the presence of SELinux. SELinux is a kernel module that supports some very strong access control security measures. While disabling this module won’t make your system completely vulnerable to outside hacking of your systems, it does make it easier. If you are planning on using your Hadoop system exposed to the Internet in any way, you probably do not want to do what I am going to show you here. Or if you do, reenable SELinux once you are finished. Use at your own risk. To disable SELinux, open a terminal prompt and switch to Super User mode. Then in your favorite editor, open the file /etc/sysconfig/selinux. The file itself is very small, and you only need to make one change. About halfway down you will see a line like this: SELINUX=enforcing Change the word “enforcing” to “disabled”, and save the file. Restart your box, and SELINUX will be disabled, and you can install Cloudera.</p>\n"},{"title":"Docker Admin Cheat Sheet part 1","id":"3273","comments":0,"date":"2016-04-25T23:23:51.000Z","_content":"\n##### [![container-lock](http://edpflager.com/wp-content/uploads/2016/04/container-lock-300x201.jpg)](http://edpflager.com/?attachment_id=3275#main)Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.\n\n# Docker administration cheat sheet\n\n_Docker images are a source file (like an .ISO) that is used to start a container (an installed system). Periodic maintenance is necessary because a Docker container remains on your system even after it exits._\n\n## What’s running?\n\nShow all running local containers with container id, image it’s based on, any open ports, commands that it runs on startup, and when it was created.\n\n*   Docker ps\n    \n\nShow all local containers (running or not) with short container id, image it’s based on, any open ports, commands that run on startup, and when it was created\n\n*   Docker ps –a or -l\n    \n\nShow only the container id of all local containers (running or not running).\n\n*   Docker ps -aq or -lq\n    \n\nShow the same info as –a or –l but with the full container ID rather than the shortened one.\n\n*   Docker ps –a –no-trunc\n    \n\nPause the container with the specified ID (which you can get with the ps command)\n\n*   Docker pause <container id>\n    \n\nRestart a previously paused container with the specified ID (which you can get with the ps command)\n\n*   Docker start <container id>\n    \n\n## Local Images\n\nShow all locally stored image files, with a common name (referred to as a repository), a 12 character ID, the size of the image file and how long ago the image was created. There may also be a tag indicating the image file version\n\n*   Docker images\n    \n\n## Shutdown containers\n\nGracefully shut down an active container with the id or short name (which you can get with the ps command) – this is preferable to kill.\n\n*   Docker stop <container ID> or <name>\n    \n\nIf stop doesn't work you can forcefully shut down an active container with the id or short name (which you can get with the ps command)\n\n*   Docker kill <container ID> or <name>\n    \n\nTo force a shut down of a running container and delete it\n\n*   Docker rm –f <containerID>\n    \n\n## Cleanup\n\nDelete the container identified with the specified ID (which you can get with the ps command)\n\n*   Docker rm <container id>\n    \n\nDelete a docker image file (a source file for containers) identified with the specified id (which you can get with the ps command)\n\n*   Docker rmi <image name>\n    \n\nTo delete all of the containers on your local system (be careful!), use this command\n\n*   Docker rm $(docker ps –a –q)\n    \n\nThat's all for now! Coming up I have posts on running containers, and how to attach the host file system as a volume in a container.","source":"_posts/docker-admin-cheat-sheet-part-1.md","raw":"---\ntitle: Docker Admin Cheat Sheet part 1\ntags: []\nid: '3273'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-04-25 19:23:51\n---\n\n##### [![container-lock](http://edpflager.com/wp-content/uploads/2016/04/container-lock-300x201.jpg)](http://edpflager.com/?attachment_id=3275#main)Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.\n\n# Docker administration cheat sheet\n\n_Docker images are a source file (like an .ISO) that is used to start a container (an installed system). Periodic maintenance is necessary because a Docker container remains on your system even after it exits._\n\n## What’s running?\n\nShow all running local containers with container id, image it’s based on, any open ports, commands that it runs on startup, and when it was created.\n\n*   Docker ps\n    \n\nShow all local containers (running or not) with short container id, image it’s based on, any open ports, commands that run on startup, and when it was created\n\n*   Docker ps –a or -l\n    \n\nShow only the container id of all local containers (running or not running).\n\n*   Docker ps -aq or -lq\n    \n\nShow the same info as –a or –l but with the full container ID rather than the shortened one.\n\n*   Docker ps –a –no-trunc\n    \n\nPause the container with the specified ID (which you can get with the ps command)\n\n*   Docker pause <container id>\n    \n\nRestart a previously paused container with the specified ID (which you can get with the ps command)\n\n*   Docker start <container id>\n    \n\n## Local Images\n\nShow all locally stored image files, with a common name (referred to as a repository), a 12 character ID, the size of the image file and how long ago the image was created. There may also be a tag indicating the image file version\n\n*   Docker images\n    \n\n## Shutdown containers\n\nGracefully shut down an active container with the id or short name (which you can get with the ps command) – this is preferable to kill.\n\n*   Docker stop <container ID> or <name>\n    \n\nIf stop doesn't work you can forcefully shut down an active container with the id or short name (which you can get with the ps command)\n\n*   Docker kill <container ID> or <name>\n    \n\nTo force a shut down of a running container and delete it\n\n*   Docker rm –f <containerID>\n    \n\n## Cleanup\n\nDelete the container identified with the specified ID (which you can get with the ps command)\n\n*   Docker rm <container id>\n    \n\nDelete a docker image file (a source file for containers) identified with the specified id (which you can get with the ps command)\n\n*   Docker rmi <image name>\n    \n\nTo delete all of the containers on your local system (be careful!), use this command\n\n*   Docker rm $(docker ps –a –q)\n    \n\nThat's all for now! Coming up I have posts on running containers, and how to attach the host file system as a volume in a container.","slug":"docker-admin-cheat-sheet-part-1","published":1,"updated":"2020-08-23T20:54:35.030Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wt0021sdjx5lv9hmx0","content":"<h5 id=\"Here-is-part-one-of-a-personal-docker-administration-cheat-sheet-I-have-been-putting-together-I-know-there-are-a-number-of-sites-that-provide-similar-tools-but-for-my-own-purposes-its-easier-to-remember-different-commands-when-I-organize-them-myself\"><a href=\"#Here-is-part-one-of-a-personal-docker-administration-cheat-sheet-I-have-been-putting-together-I-know-there-are-a-number-of-sites-that-provide-similar-tools-but-for-my-own-purposes-its-easier-to-remember-different-commands-when-I-organize-them-myself\" class=\"headerlink\" title=\"Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.\"></a><a href=\"http://edpflager.com/?attachment_id=3275#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/04/container-lock-300x201.jpg\" alt=\"container-lock\"></a>Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.</h5><h1 id=\"Docker-administration-cheat-sheet\"><a href=\"#Docker-administration-cheat-sheet\" class=\"headerlink\" title=\"Docker administration cheat sheet\"></a>Docker administration cheat sheet</h1><p><em>Docker images are a source file (like an .ISO) that is used to start a container (an installed system). Periodic maintenance is necessary because a Docker container remains on your system even after it exits.</em></p>\n<h2 id=\"What’s-running\"><a href=\"#What’s-running\" class=\"headerlink\" title=\"What’s running?\"></a>What’s running?</h2><p>Show all running local containers with container id, image it’s based on, any open ports, commands that it runs on startup, and when it was created.</p>\n<ul>\n<li>Docker ps</li>\n</ul>\n<p>Show all local containers (running or not) with short container id, image it’s based on, any open ports, commands that run on startup, and when it was created</p>\n<ul>\n<li>Docker ps –a or -l</li>\n</ul>\n<p>Show only the container id of all local containers (running or not running).</p>\n<ul>\n<li>Docker ps -aq or -lq</li>\n</ul>\n<p>Show the same info as –a or –l but with the full container ID rather than the shortened one.</p>\n<ul>\n<li>Docker ps –a –no-trunc</li>\n</ul>\n<p>Pause the container with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker pause <container id></li>\n</ul>\n<p>Restart a previously paused container with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker start <container id></li>\n</ul>\n<h2 id=\"Local-Images\"><a href=\"#Local-Images\" class=\"headerlink\" title=\"Local Images\"></a>Local Images</h2><p>Show all locally stored image files, with a common name (referred to as a repository), a 12 character ID, the size of the image file and how long ago the image was created. There may also be a tag indicating the image file version</p>\n<ul>\n<li>Docker images</li>\n</ul>\n<h2 id=\"Shutdown-containers\"><a href=\"#Shutdown-containers\" class=\"headerlink\" title=\"Shutdown containers\"></a>Shutdown containers</h2><p>Gracefully shut down an active container with the id or short name (which you can get with the ps command) – this is preferable to kill.</p>\n<ul>\n<li>Docker stop <container ID> or <name></li>\n</ul>\n<p>If stop doesn’t work you can forcefully shut down an active container with the id or short name (which you can get with the ps command)</p>\n<ul>\n<li>Docker kill <container ID> or <name></li>\n</ul>\n<p>To force a shut down of a running container and delete it</p>\n<ul>\n<li>Docker rm –f <containerID></li>\n</ul>\n<h2 id=\"Cleanup\"><a href=\"#Cleanup\" class=\"headerlink\" title=\"Cleanup\"></a>Cleanup</h2><p>Delete the container identified with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker rm <container id></li>\n</ul>\n<p>Delete a docker image file (a source file for containers) identified with the specified id (which you can get with the ps command)</p>\n<ul>\n<li>Docker rmi <image name></li>\n</ul>\n<p>To delete all of the containers on your local system (be careful!), use this command</p>\n<ul>\n<li>Docker rm $(docker ps –a –q)</li>\n</ul>\n<p>That’s all for now! Coming up I have posts on running containers, and how to attach the host file system as a volume in a container.</p>\n","site":{"data":{}},"excerpt":"","more":"<h5 id=\"Here-is-part-one-of-a-personal-docker-administration-cheat-sheet-I-have-been-putting-together-I-know-there-are-a-number-of-sites-that-provide-similar-tools-but-for-my-own-purposes-its-easier-to-remember-different-commands-when-I-organize-them-myself\"><a href=\"#Here-is-part-one-of-a-personal-docker-administration-cheat-sheet-I-have-been-putting-together-I-know-there-are-a-number-of-sites-that-provide-similar-tools-but-for-my-own-purposes-its-easier-to-remember-different-commands-when-I-organize-them-myself\" class=\"headerlink\" title=\"Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.\"></a><a href=\"http://edpflager.com/?attachment_id=3275#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/04/container-lock-300x201.jpg\" alt=\"container-lock\"></a>Here is part one of a personal docker administration cheat sheet I have been putting together. I know there are a number of sites that provide similar tools, but for my own purposes, its easier to remember different commands when I organize them myself.</h5><h1 id=\"Docker-administration-cheat-sheet\"><a href=\"#Docker-administration-cheat-sheet\" class=\"headerlink\" title=\"Docker administration cheat sheet\"></a>Docker administration cheat sheet</h1><p><em>Docker images are a source file (like an .ISO) that is used to start a container (an installed system). Periodic maintenance is necessary because a Docker container remains on your system even after it exits.</em></p>\n<h2 id=\"What’s-running\"><a href=\"#What’s-running\" class=\"headerlink\" title=\"What’s running?\"></a>What’s running?</h2><p>Show all running local containers with container id, image it’s based on, any open ports, commands that it runs on startup, and when it was created.</p>\n<ul>\n<li>Docker ps</li>\n</ul>\n<p>Show all local containers (running or not) with short container id, image it’s based on, any open ports, commands that run on startup, and when it was created</p>\n<ul>\n<li>Docker ps –a or -l</li>\n</ul>\n<p>Show only the container id of all local containers (running or not running).</p>\n<ul>\n<li>Docker ps -aq or -lq</li>\n</ul>\n<p>Show the same info as –a or –l but with the full container ID rather than the shortened one.</p>\n<ul>\n<li>Docker ps –a –no-trunc</li>\n</ul>\n<p>Pause the container with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker pause <container id></li>\n</ul>\n<p>Restart a previously paused container with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker start <container id></li>\n</ul>\n<h2 id=\"Local-Images\"><a href=\"#Local-Images\" class=\"headerlink\" title=\"Local Images\"></a>Local Images</h2><p>Show all locally stored image files, with a common name (referred to as a repository), a 12 character ID, the size of the image file and how long ago the image was created. There may also be a tag indicating the image file version</p>\n<ul>\n<li>Docker images</li>\n</ul>\n<h2 id=\"Shutdown-containers\"><a href=\"#Shutdown-containers\" class=\"headerlink\" title=\"Shutdown containers\"></a>Shutdown containers</h2><p>Gracefully shut down an active container with the id or short name (which you can get with the ps command) – this is preferable to kill.</p>\n<ul>\n<li>Docker stop <container ID> or <name></li>\n</ul>\n<p>If stop doesn’t work you can forcefully shut down an active container with the id or short name (which you can get with the ps command)</p>\n<ul>\n<li>Docker kill <container ID> or <name></li>\n</ul>\n<p>To force a shut down of a running container and delete it</p>\n<ul>\n<li>Docker rm –f <containerID></li>\n</ul>\n<h2 id=\"Cleanup\"><a href=\"#Cleanup\" class=\"headerlink\" title=\"Cleanup\"></a>Cleanup</h2><p>Delete the container identified with the specified ID (which you can get with the ps command)</p>\n<ul>\n<li>Docker rm <container id></li>\n</ul>\n<p>Delete a docker image file (a source file for containers) identified with the specified id (which you can get with the ps command)</p>\n<ul>\n<li>Docker rmi <image name></li>\n</ul>\n<p>To delete all of the containers on your local system (be careful!), use this command</p>\n<ul>\n<li>Docker rm $(docker ps –a –q)</li>\n</ul>\n<p>That’s all for now! Coming up I have posts on running containers, and how to attach the host file system as a volume in a container.</p>\n"},{"title":"Docker Fun - Play Text Adventures","id":"3412","comments":0,"date":"2016-10-15T19:57:50.000Z","_content":"\n[![zork](http://edpflager.com/wp-content/uploads/2016/10/zork-300x165.png)](http://edpflager.com/?attachment_id=3414#main)This time around in the Docker Fun series, we are getting a little more ambitious. Zork was one of the first popular text adventures, building on the work of Will Crowther and Dan Wells who created the mainframe game Colossal Cave (aka Adventure). Zork was originally written to run on a DEC PDP-10 system, and was later ported to just about every personal computer that was available. While text adventures at that time were struggling with two word commands like \"Open Door\", Zork understood much more elaborate command like \"Hit the grue with the Elvish sword\". The developers of Zork founded Infocom and eventually released a number of sequels and prequels to Zork as well as several dozen other text adventures in a number of different genres. Eventually the company was sold to Activision and the Infocom games have since lapsed into a sort of purgatory. Technically the download file for Zork is in violation of copyright laws, so feel free to substitute one of the many free Z5 files available on the Internet.\n<!-- more -->\n1.  On your Docker host machine, open a terminal prompt and create a new folder called \"xyzzy\" (for interactive fiction).\n2.  Within that folder create a new Dockerfile text file. Enter the following text in the file:\n    \n    FROM ubuntu:14.04\n    MAINTAINER noone\n    RUN apt-get update && apt-get -y install frotz && useradd player \n    WORKDIR /data\n    \n3.  Save the file.\n4.  In the xyzzy folder, enter the following command to build your container (make sure to include that final period):\n    \n    docker build -t xyzzy .\n    \n5.  Several dozen lines will scroll by as the container is built, and eventually you should see a Successfully built message.\n6.  Because FROTZ won't run as ROOT, we include code to add a FROTZ user (player) in the Dockerfile. Then when the container is run you need to specify to use the FROTZ account rather than ROOT.\n7.  Finally we want to have our game file and any save files be persistent once the container ends, so we will map a folder on the host to the container. Create a folder in your user folder called data1 and save the z5 file there.\n8.  Run your new container with this command: docker run -it -u player -v /home/<username>/data1:/data xyzzy /usr/games/frotz /data/zork\\_1.z5\n9.  Lets dissect that command for a second:\n    *   \\-it is for an interactive container - one we can work with.\n    *   \\-u player is so we can run the container as user player.\n    *   \\-v /home/<username>/data1:data maps the host system user's data1 folder to the data folder in the container.\n    *   xyzzy is the name of the container\n    *   /usr/games/frotz is the application we want to run in the container\n    *   /data/zork\\_1.z5 is the data file we want the Frotz application to use.\n10.  The container will open and display a screen similar to the one above. To exit just hold CTRL-C on your keyboard. Save and Restore work because the working directory is changed to the data1 folder on the host.","source":"_posts/docker-fun-play-text-adventures.md","raw":"---\ntitle: Docker Fun - Play Text Adventures\ntags:\n  - guides\n  - How-to\n  - howto\n  - humor\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3412'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-10-15 15:57:50\n---\n\n[![zork](http://edpflager.com/wp-content/uploads/2016/10/zork-300x165.png)](http://edpflager.com/?attachment_id=3414#main)This time around in the Docker Fun series, we are getting a little more ambitious. Zork was one of the first popular text adventures, building on the work of Will Crowther and Dan Wells who created the mainframe game Colossal Cave (aka Adventure). Zork was originally written to run on a DEC PDP-10 system, and was later ported to just about every personal computer that was available. While text adventures at that time were struggling with two word commands like \"Open Door\", Zork understood much more elaborate command like \"Hit the grue with the Elvish sword\". The developers of Zork founded Infocom and eventually released a number of sequels and prequels to Zork as well as several dozen other text adventures in a number of different genres. Eventually the company was sold to Activision and the Infocom games have since lapsed into a sort of purgatory. Technically the download file for Zork is in violation of copyright laws, so feel free to substitute one of the many free Z5 files available on the Internet.\n<!-- more -->\n1.  On your Docker host machine, open a terminal prompt and create a new folder called \"xyzzy\" (for interactive fiction).\n2.  Within that folder create a new Dockerfile text file. Enter the following text in the file:\n    \n    FROM ubuntu:14.04\n    MAINTAINER noone\n    RUN apt-get update && apt-get -y install frotz && useradd player \n    WORKDIR /data\n    \n3.  Save the file.\n4.  In the xyzzy folder, enter the following command to build your container (make sure to include that final period):\n    \n    docker build -t xyzzy .\n    \n5.  Several dozen lines will scroll by as the container is built, and eventually you should see a Successfully built message.\n6.  Because FROTZ won't run as ROOT, we include code to add a FROTZ user (player) in the Dockerfile. Then when the container is run you need to specify to use the FROTZ account rather than ROOT.\n7.  Finally we want to have our game file and any save files be persistent once the container ends, so we will map a folder on the host to the container. Create a folder in your user folder called data1 and save the z5 file there.\n8.  Run your new container with this command: docker run -it -u player -v /home/<username>/data1:/data xyzzy /usr/games/frotz /data/zork\\_1.z5\n9.  Lets dissect that command for a second:\n    *   \\-it is for an interactive container - one we can work with.\n    *   \\-u player is so we can run the container as user player.\n    *   \\-v /home/<username>/data1:data maps the host system user's data1 folder to the data folder in the container.\n    *   xyzzy is the name of the container\n    *   /usr/games/frotz is the application we want to run in the container\n    *   /data/zork\\_1.z5 is the data file we want the Frotz application to use.\n10.  The container will open and display a screen similar to the one above. To exit just hold CTRL-C on your keyboard. Save and Restore work because the working directory is changed to the data1 folder on the host.","slug":"docker-fun-play-text-adventures","published":1,"updated":"2020-08-23T20:54:35.074Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wx0025sdjx0s288ryh","content":"<p><a href=\"http://edpflager.com/?attachment_id=3414#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/10/zork-300x165.png\" alt=\"zork\"></a>This time around in the Docker Fun series, we are getting a little more ambitious. Zork was one of the first popular text adventures, building on the work of Will Crowther and Dan Wells who created the mainframe game Colossal Cave (aka Adventure). Zork was originally written to run on a DEC PDP-10 system, and was later ported to just about every personal computer that was available. While text adventures at that time were struggling with two word commands like “Open Door”, Zork understood much more elaborate command like “Hit the grue with the Elvish sword”. The developers of Zork founded Infocom and eventually released a number of sequels and prequels to Zork as well as several dozen other text adventures in a number of different genres. Eventually the company was sold to Activision and the Infocom games have since lapsed into a sort of purgatory. Technically the download file for Zork is in violation of copyright laws, so feel free to substitute one of the many free Z5 files available on the Internet.</p>\n<a id=\"more\"></a>\n<ol>\n<li><p>On your Docker host machine, open a terminal prompt and create a new folder called “xyzzy” (for interactive fiction).</p>\n</li>\n<li><p>Within that folder create a new Dockerfile text file. Enter the following text in the file:</p>\n<p>FROM ubuntu:14.04<br>MAINTAINER noone<br>RUN apt-get update &amp;&amp; apt-get -y install frotz &amp;&amp; useradd player<br>WORKDIR /data</p>\n</li>\n<li><p>Save the file.</p>\n</li>\n<li><p>In the xyzzy folder, enter the following command to build your container (make sure to include that final period):</p>\n<p>docker build -t xyzzy .</p>\n</li>\n<li><p>Several dozen lines will scroll by as the container is built, and eventually you should see a Successfully built message.</p>\n</li>\n<li><p>Because FROTZ won’t run as ROOT, we include code to add a FROTZ user (player) in the Dockerfile. Then when the container is run you need to specify to use the FROTZ account rather than ROOT.</p>\n</li>\n<li><p>Finally we want to have our game file and any save files be persistent once the container ends, so we will map a folder on the host to the container. Create a folder in your user folder called data1 and save the z5 file there.</p>\n</li>\n<li><p>Run your new container with this command: docker run -it -u player -v /home/<username>/data1:/data xyzzy /usr/games/frotz /data/zork_1.z5</p>\n</li>\n<li><p>Lets dissect that command for a second:</p>\n<ul>\n<li>-it is for an interactive container - one we can work with.</li>\n<li>-u player is so we can run the container as user player.</li>\n<li>-v /home/<username>/data1:data maps the host system user’s data1 folder to the data folder in the container.</li>\n<li>xyzzy is the name of the container</li>\n<li>/usr/games/frotz is the application we want to run in the container</li>\n<li>/data/zork_1.z5 is the data file we want the Frotz application to use.</li>\n</ul>\n</li>\n<li><p>The container will open and display a screen similar to the one above. To exit just hold CTRL-C on your keyboard. Save and Restore work because the working directory is changed to the data1 folder on the host.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3414#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/10/zork-300x165.png\" alt=\"zork\"></a>This time around in the Docker Fun series, we are getting a little more ambitious. Zork was one of the first popular text adventures, building on the work of Will Crowther and Dan Wells who created the mainframe game Colossal Cave (aka Adventure). Zork was originally written to run on a DEC PDP-10 system, and was later ported to just about every personal computer that was available. While text adventures at that time were struggling with two word commands like “Open Door”, Zork understood much more elaborate command like “Hit the grue with the Elvish sword”. The developers of Zork founded Infocom and eventually released a number of sequels and prequels to Zork as well as several dozen other text adventures in a number of different genres. Eventually the company was sold to Activision and the Infocom games have since lapsed into a sort of purgatory. Technically the download file for Zork is in violation of copyright laws, so feel free to substitute one of the many free Z5 files available on the Internet.</p>","more":"<ol>\n<li><p>On your Docker host machine, open a terminal prompt and create a new folder called “xyzzy” (for interactive fiction).</p>\n</li>\n<li><p>Within that folder create a new Dockerfile text file. Enter the following text in the file:</p>\n<p>FROM ubuntu:14.04<br>MAINTAINER noone<br>RUN apt-get update &amp;&amp; apt-get -y install frotz &amp;&amp; useradd player<br>WORKDIR /data</p>\n</li>\n<li><p>Save the file.</p>\n</li>\n<li><p>In the xyzzy folder, enter the following command to build your container (make sure to include that final period):</p>\n<p>docker build -t xyzzy .</p>\n</li>\n<li><p>Several dozen lines will scroll by as the container is built, and eventually you should see a Successfully built message.</p>\n</li>\n<li><p>Because FROTZ won’t run as ROOT, we include code to add a FROTZ user (player) in the Dockerfile. Then when the container is run you need to specify to use the FROTZ account rather than ROOT.</p>\n</li>\n<li><p>Finally we want to have our game file and any save files be persistent once the container ends, so we will map a folder on the host to the container. Create a folder in your user folder called data1 and save the z5 file there.</p>\n</li>\n<li><p>Run your new container with this command: docker run -it -u player -v /home/<username>/data1:/data xyzzy /usr/games/frotz /data/zork_1.z5</p>\n</li>\n<li><p>Lets dissect that command for a second:</p>\n<ul>\n<li>-it is for an interactive container - one we can work with.</li>\n<li>-u player is so we can run the container as user player.</li>\n<li>-v /home/<username>/data1:data maps the host system user’s data1 folder to the data folder in the container.</li>\n<li>xyzzy is the name of the container</li>\n<li>/usr/games/frotz is the application we want to run in the container</li>\n<li>/data/zork_1.z5 is the data file we want the Frotz application to use.</li>\n</ul>\n</li>\n<li><p>The container will open and display a screen similar to the one above. To exit just hold CTRL-C on your keyboard. Save and Restore work because the working directory is changed to the data1 folder on the host.</p>\n</li>\n</ol>"},{"title":"Docker - Handy Tip for Frequent Command","id":"3371","comments":0,"date":"2016-07-09T23:42:50.000Z","_content":"\n[![containers-small](http://edpflager.com/wp-content/uploads/2016/07/containers-small-300x225.jpg)](http://edpflager.com/?attachment_id=3372#main)One command I use a lot when working with Docker, is **docker ps -a** to see what containers I have and which ones are active. To save a little time when entering that command, create an alias in your BASH profile for that command.\n\n1.  From a command prompt, change to your home folder: cd ~\n2.  Edit the .bashrc file with a text editor. In my case, I use NANO: nano ./.bashrc (Note: Earlier versions of Mint supported editing the .profile file. This no longer works).\n3.  Add this line to the end of the file:  **alias dpa=\"docker ps -a\"**\n4.  Save the file with CTRL-O and exit back to a command prompt with CTRL-X.\n5.  Logout and log back in.\n6.  At a command prompt enter: **dpa**. You'll see the results of **docker ps -a** but without all the typing!","source":"_posts/docker-handy-tip-for-frequent-command.md","raw":"---\ntitle: Docker - Handy Tip for Frequent Command\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - SysAdmin\n  - technical\nid: '3371'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-07-09 19:42:50\n---\n\n[![containers-small](http://edpflager.com/wp-content/uploads/2016/07/containers-small-300x225.jpg)](http://edpflager.com/?attachment_id=3372#main)One command I use a lot when working with Docker, is **docker ps -a** to see what containers I have and which ones are active. To save a little time when entering that command, create an alias in your BASH profile for that command.\n\n1.  From a command prompt, change to your home folder: cd ~\n2.  Edit the .bashrc file with a text editor. In my case, I use NANO: nano ./.bashrc (Note: Earlier versions of Mint supported editing the .profile file. This no longer works).\n3.  Add this line to the end of the file:  **alias dpa=\"docker ps -a\"**\n4.  Save the file with CTRL-O and exit back to a command prompt with CTRL-X.\n5.  Logout and log back in.\n6.  At a command prompt enter: **dpa**. You'll see the results of **docker ps -a** but without all the typing!","slug":"docker-handy-tip-for-frequent-command","published":1,"updated":"2020-08-23T20:54:35.066Z","layout":"post","photos":[],"link":"","_id":"ckeaq99wz0029sdjx8orv9syd","content":"<p><a href=\"http://edpflager.com/?attachment_id=3372#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/containers-small-300x225.jpg\" alt=\"containers-small\"></a>One command I use a lot when working with Docker, is <strong>docker ps -a</strong> to see what containers I have and which ones are active. To save a little time when entering that command, create an alias in your BASH profile for that command.</p>\n<ol>\n<li>From a command prompt, change to your home folder: cd ~</li>\n<li>Edit the .bashrc file with a text editor. In my case, I use NANO: nano ./.bashrc (Note: Earlier versions of Mint supported editing the .profile file. This no longer works).</li>\n<li>Add this line to the end of the file:  <strong>alias dpa=”docker ps -a”</strong></li>\n<li>Save the file with CTRL-O and exit back to a command prompt with CTRL-X.</li>\n<li>Logout and log back in.</li>\n<li>At a command prompt enter: <strong>dpa</strong>. You’ll see the results of <strong>docker ps -a</strong> but without all the typing!</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/?attachment_id=3372#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/containers-small-300x225.jpg\" alt=\"containers-small\"></a>One command I use a lot when working with Docker, is <strong>docker ps -a</strong> to see what containers I have and which ones are active. To save a little time when entering that command, create an alias in your BASH profile for that command.</p>\n<ol>\n<li>From a command prompt, change to your home folder: cd ~</li>\n<li>Edit the .bashrc file with a text editor. In my case, I use NANO: nano ./.bashrc (Note: Earlier versions of Mint supported editing the .profile file. This no longer works).</li>\n<li>Add this line to the end of the file:  <strong>alias dpa=”docker ps -a”</strong></li>\n<li>Save the file with CTRL-O and exit back to a command prompt with CTRL-X.</li>\n<li>Logout and log back in.</li>\n<li>At a command prompt enter: <strong>dpa</strong>. You’ll see the results of <strong>docker ps -a</strong> but without all the typing!</li>\n</ol>\n"},{"title":"edpflager.com is now using HTTPS","id":"4703","comments":0,"date":"2019-08-07T13:47:21.000Z","_content":"\n![Locked](https://edpflager.com/wp-content/uploads/2019/08/https-294x300.png) Just an FYI - this site is now running with the HTTPS protocol rather than HTTP. If you have links to the old address, please update them. Thanks! Why some of you may ask? There are a variety of reasons, but they all revolve around security. If you use Chrome and visit an HTTP site, the browser marks it as not secure. HTTP sites are easier to hack which I found out last year.\n<!-- more -->\nAnd while traveling this year, I've been forced to become more security conscious. So why shouldn't my website be more security conscious too?","source":"_posts/edpflager-com-is-now-using-https.md","raw":"---\ntitle: edpflager.com is now using HTTPS\ntags: []\nid: '4703'\ncategories:\n  - - Blog\ncomments: false\ndate: 2019-08-07 09:47:21\n---\n\n![Locked](https://edpflager.com/wp-content/uploads/2019/08/https-294x300.png) Just an FYI - this site is now running with the HTTPS protocol rather than HTTP. If you have links to the old address, please update them. Thanks! Why some of you may ask? There are a variety of reasons, but they all revolve around security. If you use Chrome and visit an HTTP site, the browser marks it as not secure. HTTP sites are easier to hack which I found out last year.\n<!-- more -->\nAnd while traveling this year, I've been forced to become more security conscious. So why shouldn't my website be more security conscious too?","slug":"edpflager-com-is-now-using-https","published":1,"updated":"2020-08-23T20:54:35.254Z","layout":"post","photos":[],"link":"","_id":"ckeaq99x5002dsdjxgfechhbu","content":"<p><img src=\"https://edpflager.com/wp-content/uploads/2019/08/https-294x300.png\" alt=\"Locked\"> Just an FYI - this site is now running with the HTTPS protocol rather than HTTP. If you have links to the old address, please update them. Thanks! Why some of you may ask? There are a variety of reasons, but they all revolve around security. If you use Chrome and visit an HTTP site, the browser marks it as not secure. HTTP sites are easier to hack which I found out last year.</p>\n<a id=\"more\"></a>\n<p>And while traveling this year, I’ve been forced to become more security conscious. So why shouldn’t my website be more security conscious too?</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"https://edpflager.com/wp-content/uploads/2019/08/https-294x300.png\" alt=\"Locked\"> Just an FYI - this site is now running with the HTTPS protocol rather than HTTP. If you have links to the old address, please update them. Thanks! Why some of you may ask? There are a variety of reasons, but they all revolve around security. If you use Chrome and visit an HTTP site, the browser marks it as not secure. HTTP sites are easier to hack which I found out last year.</p>","more":"<p>And while traveling this year, I’ve been forced to become more security conscious. So why shouldn’t my website be more security conscious too?</p>"},{"title":"Emails files from Kettle","id":"2357","comments":0,"date":"2014-08-22T23:00:11.000Z","_content":"\n[![mailenvelope](http://edpflager.com/wp-content/uploads/2014/08/mailenvelope-298x300.jpg)](http://edpflager.com/wp-content/uploads/2014/08/mailenvelope.jpg)A common task I encounter when working with ETL tools is to send output files somewhere. I often have to FTP files, but just as often, I need to email output files. I'll cover how to FTP in a future post, but this time I'll walk through how to set up a job in Pentaho Kettle (aka Pentaho Data Integration or PDI) to email data files. Unlike the \"Put FTP\" step in PDI, where you can specify the file or files you want to upload as part of the job component, when sending files via email, you have to create  a transformation step to define the files you want to send, and then pipe that information into the Email step. This is similar to how variables work in Pentaho, where you define the variables in a step before you can use them. If this is something you need to do, and you want to know how to do it, read on! At its most basic level, this kind of task in PDI is very simple building on the task of creating files in PDI, whether they are text , Excel, or whatever. Once the output files are created, sending them via email involved only a couple of steps.\n<!-- more -->\n##### DEFINE THE FILES TO SEND\n\n[![definefiles](http://edpflager.com/wp-content/uploads/2014/08/definefiles.png)](http://edpflager.com/wp-content/uploads/2014/08/definefiles.png)\n\n1.  Create a transformation step to define the file(s) you want to send. Drag a \"Get File Names\" component from the Input section of the Design panel in Spoon on to your work space, and then a \"Set files in result\" from the Job section. Create a hop between the two components.\n2.  Double click the Get File Names component to configure it. On the Files tab of the window that opens, click on the BROWSE button on the lower right side. Navigate to where the files are located that you want to send. You can also enter a UNC path if the files are on another computer in the format of: \\\\computernamesharefoldersubfolderfilename.\n3.  Click on the first file, and click Open to return to the \"Get File Names\" window. If you are only sending one file, and the name is not likely to change, go ahead and click the ADD button on the lower right side. The file and path will be moved down to the Selected Files grid. If you have other files you want to send repeat this process. ![GetFileNamesWildcard](http://edpflager.com/wp-content/uploads/2014/08/GetFileNamesWildcard.png)\n4.  If you are adding a number of files that follow a common naming convention, or if the filename is likely to change you can Regular Expressions to allow Kettle to figure out the file names for you. As an example, if I was going to send a number of .png (graphics) files that reside in the same folder, in the Selected Files grid, I could remove the file name from the first cell, leaving only the path to  the folder where the files are. Then in the second cell, I can add the regular expression: ^.\\*png to signify send all PNG files in that folder.\n5.  Once you have added all of the files to the Get Files window, click the Preview Rows button to get a dialog box showing you the names of the files PDI thinks you want to send. If all looks good, then click Close on the preview window, and the OK on the Get Files window. (There are a lot of other options here that I won't be covering, but feel free to experiment!) ![SetFileNames](http://edpflager.com/wp-content/uploads/2014/08/SetFileNames.png)\n6.  Open the Set Files component and a window will appear labeled Copy filesnames to result (confusing - I think that may be a bug). You can name the step whatever you like as long as its meaningful in the scope of your job. From the \"Filename field\" drop down list, select filename. You'll see there are other options you can use here to define the file(s) but for our purposes, we are defining the particular file name(s) to use. In the \"Type of file to\" list, select GENERAL. Click OK to return to your work space.\n7.  Save your transformation.\n\n#####  CREATE THE TRANSMIT JOB\n\n1.  After the transformation is setup that defines the files to transmit, create a new job (or add the following to an existing job) and drag a transformation step on to your workspace from the design panel. When creating a new job, be sure to add a START step first, and define a hop from that to the transformation. [![transforminfo](http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png)](http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png)\n2.  Open the transformation step and define the parameters to use the transformation created above. (If a repository is not used to store  transformations and jobs, the only option will be to navigate to where the KTR file was saved). Once the transformation info is entered, click OK to return to the job workspace.\n3.  From the Mail node in the Design panel, drag a Mail component onto your workspace. Create a hop from the GetFIles component to the Mail component. [![mailaddresses](http://edpflager.com/wp-content/uploads/2014/08/mailaddresses-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailaddresses.png)\n4.  Double click the Main component to open it. On the first panel, define the destination and sender email information. [![mailservertab](http://edpflager.com/wp-content/uploads/2014/08/mailservertab-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailservertab.png)\n5.  On the Server tab, define the information for the SMTP server. Check with the email server administrator for this information. At the very least, an SMTP server name and port must be defined. [![mailmessage](http://edpflager.com/wp-content/uploads/2014/08/mailmessage-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailmessage.png)\n6.  Click to the Mail Message tab, and enter the appropriate information. Generally its a good idea to enter a subject and a referencing message to make sure the recipient knows what the files are. [![mailattachfiles](http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles-300x276.png)](http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles.png)\n7.  Lastly, switch to the Attached Files tab. Check the box labeled \"Attach file(s) to message?\". In the Select File Type pane, click on General to highlight it. This will let Pentaho know to attach the files that were defined as General in the previous step of the job. Click OK to return to the Job works space.\n8.  Drag a \"Success\" job step on to your workspace, and add a hop from the \"Mail\" component to it. Save the job. [![emailfilesjob](http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png)](http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png)\n9.  If everything has been setup correctly the job will look like the image above and when run, the job will email the files that were defined in the Get Files step to the intended recipients.\n\nNote: Pentaho and Pentaho Data Integration are trademarks of Pentaho Inc.","source":"_posts/emails-files-from-kettle.md","raw":"---\ntitle: Emails files from Kettle\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2357'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-08-22 19:00:11\n---\n\n[![mailenvelope](http://edpflager.com/wp-content/uploads/2014/08/mailenvelope-298x300.jpg)](http://edpflager.com/wp-content/uploads/2014/08/mailenvelope.jpg)A common task I encounter when working with ETL tools is to send output files somewhere. I often have to FTP files, but just as often, I need to email output files. I'll cover how to FTP in a future post, but this time I'll walk through how to set up a job in Pentaho Kettle (aka Pentaho Data Integration or PDI) to email data files. Unlike the \"Put FTP\" step in PDI, where you can specify the file or files you want to upload as part of the job component, when sending files via email, you have to create  a transformation step to define the files you want to send, and then pipe that information into the Email step. This is similar to how variables work in Pentaho, where you define the variables in a step before you can use them. If this is something you need to do, and you want to know how to do it, read on! At its most basic level, this kind of task in PDI is very simple building on the task of creating files in PDI, whether they are text , Excel, or whatever. Once the output files are created, sending them via email involved only a couple of steps.\n<!-- more -->\n##### DEFINE THE FILES TO SEND\n\n[![definefiles](http://edpflager.com/wp-content/uploads/2014/08/definefiles.png)](http://edpflager.com/wp-content/uploads/2014/08/definefiles.png)\n\n1.  Create a transformation step to define the file(s) you want to send. Drag a \"Get File Names\" component from the Input section of the Design panel in Spoon on to your work space, and then a \"Set files in result\" from the Job section. Create a hop between the two components.\n2.  Double click the Get File Names component to configure it. On the Files tab of the window that opens, click on the BROWSE button on the lower right side. Navigate to where the files are located that you want to send. You can also enter a UNC path if the files are on another computer in the format of: \\\\computernamesharefoldersubfolderfilename.\n3.  Click on the first file, and click Open to return to the \"Get File Names\" window. If you are only sending one file, and the name is not likely to change, go ahead and click the ADD button on the lower right side. The file and path will be moved down to the Selected Files grid. If you have other files you want to send repeat this process. ![GetFileNamesWildcard](http://edpflager.com/wp-content/uploads/2014/08/GetFileNamesWildcard.png)\n4.  If you are adding a number of files that follow a common naming convention, or if the filename is likely to change you can Regular Expressions to allow Kettle to figure out the file names for you. As an example, if I was going to send a number of .png (graphics) files that reside in the same folder, in the Selected Files grid, I could remove the file name from the first cell, leaving only the path to  the folder where the files are. Then in the second cell, I can add the regular expression: ^.\\*png to signify send all PNG files in that folder.\n5.  Once you have added all of the files to the Get Files window, click the Preview Rows button to get a dialog box showing you the names of the files PDI thinks you want to send. If all looks good, then click Close on the preview window, and the OK on the Get Files window. (There are a lot of other options here that I won't be covering, but feel free to experiment!) ![SetFileNames](http://edpflager.com/wp-content/uploads/2014/08/SetFileNames.png)\n6.  Open the Set Files component and a window will appear labeled Copy filesnames to result (confusing - I think that may be a bug). You can name the step whatever you like as long as its meaningful in the scope of your job. From the \"Filename field\" drop down list, select filename. You'll see there are other options you can use here to define the file(s) but for our purposes, we are defining the particular file name(s) to use. In the \"Type of file to\" list, select GENERAL. Click OK to return to your work space.\n7.  Save your transformation.\n\n#####  CREATE THE TRANSMIT JOB\n\n1.  After the transformation is setup that defines the files to transmit, create a new job (or add the following to an existing job) and drag a transformation step on to your workspace from the design panel. When creating a new job, be sure to add a START step first, and define a hop from that to the transformation. [![transforminfo](http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png)](http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png)\n2.  Open the transformation step and define the parameters to use the transformation created above. (If a repository is not used to store  transformations and jobs, the only option will be to navigate to where the KTR file was saved). Once the transformation info is entered, click OK to return to the job workspace.\n3.  From the Mail node in the Design panel, drag a Mail component onto your workspace. Create a hop from the GetFIles component to the Mail component. [![mailaddresses](http://edpflager.com/wp-content/uploads/2014/08/mailaddresses-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailaddresses.png)\n4.  Double click the Main component to open it. On the first panel, define the destination and sender email information. [![mailservertab](http://edpflager.com/wp-content/uploads/2014/08/mailservertab-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailservertab.png)\n5.  On the Server tab, define the information for the SMTP server. Check with the email server administrator for this information. At the very least, an SMTP server name and port must be defined. [![mailmessage](http://edpflager.com/wp-content/uploads/2014/08/mailmessage-300x277.png)](http://edpflager.com/wp-content/uploads/2014/08/mailmessage.png)\n6.  Click to the Mail Message tab, and enter the appropriate information. Generally its a good idea to enter a subject and a referencing message to make sure the recipient knows what the files are. [![mailattachfiles](http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles-300x276.png)](http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles.png)\n7.  Lastly, switch to the Attached Files tab. Check the box labeled \"Attach file(s) to message?\". In the Select File Type pane, click on General to highlight it. This will let Pentaho know to attach the files that were defined as General in the previous step of the job. Click OK to return to the Job works space.\n8.  Drag a \"Success\" job step on to your workspace, and add a hop from the \"Mail\" component to it. Save the job. [![emailfilesjob](http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png)](http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png)\n9.  If everything has been setup correctly the job will look like the image above and when run, the job will email the files that were defined in the Get Files step to the intended recipients.\n\nNote: Pentaho and Pentaho Data Integration are trademarks of Pentaho Inc.","slug":"emails-files-from-kettle","published":1,"updated":"2020-08-23T20:54:34.890Z","layout":"post","photos":[],"link":"","_id":"ckeaq99xb002hsdjx5kj26tbv","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailenvelope.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailenvelope-298x300.jpg\" alt=\"mailenvelope\"></a>A common task I encounter when working with ETL tools is to send output files somewhere. I often have to FTP files, but just as often, I need to email output files. I’ll cover how to FTP in a future post, but this time I’ll walk through how to set up a job in Pentaho Kettle (aka Pentaho Data Integration or PDI) to email data files. Unlike the “Put FTP” step in PDI, where you can specify the file or files you want to upload as part of the job component, when sending files via email, you have to create  a transformation step to define the files you want to send, and then pipe that information into the Email step. This is similar to how variables work in Pentaho, where you define the variables in a step before you can use them. If this is something you need to do, and you want to know how to do it, read on! At its most basic level, this kind of task in PDI is very simple building on the task of creating files in PDI, whether they are text , Excel, or whatever. Once the output files are created, sending them via email involved only a couple of steps.</p>\n<a id=\"more\"></a>\n<h5 id=\"DEFINE-THE-FILES-TO-SEND\"><a href=\"#DEFINE-THE-FILES-TO-SEND\" class=\"headerlink\" title=\"DEFINE THE FILES TO SEND\"></a>DEFINE THE FILES TO SEND</h5><p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/definefiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/definefiles.png\" alt=\"definefiles\"></a></p>\n<ol>\n<li>Create a transformation step to define the file(s) you want to send. Drag a “Get File Names” component from the Input section of the Design panel in Spoon on to your work space, and then a “Set files in result” from the Job section. Create a hop between the two components.</li>\n<li>Double click the Get File Names component to configure it. On the Files tab of the window that opens, click on the BROWSE button on the lower right side. Navigate to where the files are located that you want to send. You can also enter a UNC path if the files are on another computer in the format of: \\computernamesharefoldersubfolderfilename.</li>\n<li>Click on the first file, and click Open to return to the “Get File Names” window. If you are only sending one file, and the name is not likely to change, go ahead and click the ADD button on the lower right side. The file and path will be moved down to the Selected Files grid. If you have other files you want to send repeat this process. <img src=\"http://edpflager.com/wp-content/uploads/2014/08/GetFileNamesWildcard.png\" alt=\"GetFileNamesWildcard\"></li>\n<li>If you are adding a number of files that follow a common naming convention, or if the filename is likely to change you can Regular Expressions to allow Kettle to figure out the file names for you. As an example, if I was going to send a number of .png (graphics) files that reside in the same folder, in the Selected Files grid, I could remove the file name from the first cell, leaving only the path to  the folder where the files are. Then in the second cell, I can add the regular expression: ^.*png to signify send all PNG files in that folder.</li>\n<li>Once you have added all of the files to the Get Files window, click the Preview Rows button to get a dialog box showing you the names of the files PDI thinks you want to send. If all looks good, then click Close on the preview window, and the OK on the Get Files window. (There are a lot of other options here that I won’t be covering, but feel free to experiment!) <img src=\"http://edpflager.com/wp-content/uploads/2014/08/SetFileNames.png\" alt=\"SetFileNames\"></li>\n<li>Open the Set Files component and a window will appear labeled Copy filesnames to result (confusing - I think that may be a bug). You can name the step whatever you like as long as its meaningful in the scope of your job. From the “Filename field” drop down list, select filename. You’ll see there are other options you can use here to define the file(s) but for our purposes, we are defining the particular file name(s) to use. In the “Type of file to” list, select GENERAL. Click OK to return to your work space.</li>\n<li>Save your transformation.</li>\n</ol>\n<h5 id=\"CREATE-THE-TRANSMIT-JOB\"><a href=\"#CREATE-THE-TRANSMIT-JOB\" class=\"headerlink\" title=\" CREATE THE TRANSMIT JOB\"></a> CREATE THE TRANSMIT JOB</h5><ol>\n<li>After the transformation is setup that defines the files to transmit, create a new job (or add the following to an existing job) and drag a transformation step on to your workspace from the design panel. When creating a new job, be sure to add a START step first, and define a hop from that to the transformation. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png\" alt=\"transforminfo\"></a></li>\n<li>Open the transformation step and define the parameters to use the transformation created above. (If a repository is not used to store  transformations and jobs, the only option will be to navigate to where the KTR file was saved). Once the transformation info is entered, click OK to return to the job workspace.</li>\n<li>From the Mail node in the Design panel, drag a Mail component onto your workspace. Create a hop from the GetFIles component to the Mail component. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailaddresses.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailaddresses-300x277.png\" alt=\"mailaddresses\"></a></li>\n<li>Double click the Main component to open it. On the first panel, define the destination and sender email information. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailservertab.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailservertab-300x277.png\" alt=\"mailservertab\"></a></li>\n<li>On the Server tab, define the information for the SMTP server. Check with the email server administrator for this information. At the very least, an SMTP server name and port must be defined. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailmessage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailmessage-300x277.png\" alt=\"mailmessage\"></a></li>\n<li>Click to the Mail Message tab, and enter the appropriate information. Generally its a good idea to enter a subject and a referencing message to make sure the recipient knows what the files are. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles-300x276.png\" alt=\"mailattachfiles\"></a></li>\n<li>Lastly, switch to the Attached Files tab. Check the box labeled “Attach file(s) to message?”. In the Select File Type pane, click on General to highlight it. This will let Pentaho know to attach the files that were defined as General in the previous step of the job. Click OK to return to the Job works space.</li>\n<li>Drag a “Success” job step on to your workspace, and add a hop from the “Mail” component to it. Save the job. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png\" alt=\"emailfilesjob\"></a></li>\n<li>If everything has been setup correctly the job will look like the image above and when run, the job will email the files that were defined in the Get Files step to the intended recipients.</li>\n</ol>\n<p>Note: Pentaho and Pentaho Data Integration are trademarks of Pentaho Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailenvelope.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailenvelope-298x300.jpg\" alt=\"mailenvelope\"></a>A common task I encounter when working with ETL tools is to send output files somewhere. I often have to FTP files, but just as often, I need to email output files. I’ll cover how to FTP in a future post, but this time I’ll walk through how to set up a job in Pentaho Kettle (aka Pentaho Data Integration or PDI) to email data files. Unlike the “Put FTP” step in PDI, where you can specify the file or files you want to upload as part of the job component, when sending files via email, you have to create  a transformation step to define the files you want to send, and then pipe that information into the Email step. This is similar to how variables work in Pentaho, where you define the variables in a step before you can use them. If this is something you need to do, and you want to know how to do it, read on! At its most basic level, this kind of task in PDI is very simple building on the task of creating files in PDI, whether they are text , Excel, or whatever. Once the output files are created, sending them via email involved only a couple of steps.</p>","more":"<h5 id=\"DEFINE-THE-FILES-TO-SEND\"><a href=\"#DEFINE-THE-FILES-TO-SEND\" class=\"headerlink\" title=\"DEFINE THE FILES TO SEND\"></a>DEFINE THE FILES TO SEND</h5><p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/definefiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/definefiles.png\" alt=\"definefiles\"></a></p>\n<ol>\n<li>Create a transformation step to define the file(s) you want to send. Drag a “Get File Names” component from the Input section of the Design panel in Spoon on to your work space, and then a “Set files in result” from the Job section. Create a hop between the two components.</li>\n<li>Double click the Get File Names component to configure it. On the Files tab of the window that opens, click on the BROWSE button on the lower right side. Navigate to where the files are located that you want to send. You can also enter a UNC path if the files are on another computer in the format of: \\computernamesharefoldersubfolderfilename.</li>\n<li>Click on the first file, and click Open to return to the “Get File Names” window. If you are only sending one file, and the name is not likely to change, go ahead and click the ADD button on the lower right side. The file and path will be moved down to the Selected Files grid. If you have other files you want to send repeat this process. <img src=\"http://edpflager.com/wp-content/uploads/2014/08/GetFileNamesWildcard.png\" alt=\"GetFileNamesWildcard\"></li>\n<li>If you are adding a number of files that follow a common naming convention, or if the filename is likely to change you can Regular Expressions to allow Kettle to figure out the file names for you. As an example, if I was going to send a number of .png (graphics) files that reside in the same folder, in the Selected Files grid, I could remove the file name from the first cell, leaving only the path to  the folder where the files are. Then in the second cell, I can add the regular expression: ^.*png to signify send all PNG files in that folder.</li>\n<li>Once you have added all of the files to the Get Files window, click the Preview Rows button to get a dialog box showing you the names of the files PDI thinks you want to send. If all looks good, then click Close on the preview window, and the OK on the Get Files window. (There are a lot of other options here that I won’t be covering, but feel free to experiment!) <img src=\"http://edpflager.com/wp-content/uploads/2014/08/SetFileNames.png\" alt=\"SetFileNames\"></li>\n<li>Open the Set Files component and a window will appear labeled Copy filesnames to result (confusing - I think that may be a bug). You can name the step whatever you like as long as its meaningful in the scope of your job. From the “Filename field” drop down list, select filename. You’ll see there are other options you can use here to define the file(s) but for our purposes, we are defining the particular file name(s) to use. In the “Type of file to” list, select GENERAL. Click OK to return to your work space.</li>\n<li>Save your transformation.</li>\n</ol>\n<h5 id=\"CREATE-THE-TRANSMIT-JOB\"><a href=\"#CREATE-THE-TRANSMIT-JOB\" class=\"headerlink\" title=\" CREATE THE TRANSMIT JOB\"></a> CREATE THE TRANSMIT JOB</h5><ol>\n<li>After the transformation is setup that defines the files to transmit, create a new job (or add the following to an existing job) and drag a transformation step on to your workspace from the design panel. When creating a new job, be sure to add a START step first, and define a hop from that to the transformation. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/transforminfo.png\" alt=\"transforminfo\"></a></li>\n<li>Open the transformation step and define the parameters to use the transformation created above. (If a repository is not used to store  transformations and jobs, the only option will be to navigate to where the KTR file was saved). Once the transformation info is entered, click OK to return to the job workspace.</li>\n<li>From the Mail node in the Design panel, drag a Mail component onto your workspace. Create a hop from the GetFIles component to the Mail component. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailaddresses.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailaddresses-300x277.png\" alt=\"mailaddresses\"></a></li>\n<li>Double click the Main component to open it. On the first panel, define the destination and sender email information. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailservertab.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailservertab-300x277.png\" alt=\"mailservertab\"></a></li>\n<li>On the Server tab, define the information for the SMTP server. Check with the email server administrator for this information. At the very least, an SMTP server name and port must be defined. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailmessage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailmessage-300x277.png\" alt=\"mailmessage\"></a></li>\n<li>Click to the Mail Message tab, and enter the appropriate information. Generally its a good idea to enter a subject and a referencing message to make sure the recipient knows what the files are. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/mailattachfiles-300x276.png\" alt=\"mailattachfiles\"></a></li>\n<li>Lastly, switch to the Attached Files tab. Check the box labeled “Attach file(s) to message?”. In the Select File Type pane, click on General to highlight it. This will let Pentaho know to attach the files that were defined as General in the previous step of the job. Click OK to return to the Job works space.</li>\n<li>Drag a “Success” job step on to your workspace, and add a hop from the “Mail” component to it. Save the job. <a href=\"http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/emailfilesjob.png\" alt=\"emailfilesjob\"></a></li>\n<li>If everything has been setup correctly the job will look like the image above and when run, the job will email the files that were defined in the Get Files step to the intended recipients.</li>\n</ol>\n<p>Note: Pentaho and Pentaho Data Integration are trademarks of Pentaho Inc.</p>"},{"title":"Embedding Graphics","id":"4044","comments":0,"date":"2018-10-09T19:40:02.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)The English idiom or cliche, that \"a picture is worth a thousand words\" does convey some truth. People by and large tend to be visual. We internalize concepts and ideas more easily that can be represented by visuals than be text alone.  So when working with R Markdown documents, you may want to embed graphical elements into your document. In this post, I will cover some of the basics of embedding graphics when generating PDF documents using R Markdown. I have several more posts queued up on this topic that will cover some more in-depth concepts, but for now let's look at some simple methods.\n<!-- more -->\nTo insert an image in an R Markdown file that produces a PDF, Word document or an HTML file, you can use the PANDOC syntax (be sure to include the exclamation point):\n\n!\\[\\](/path/to/image.png)\n\nThe brackets can be used to include a caption for your image if you want, or you can leave them empty for no caption. This command also works inline, but keep in mind that it will produce a lot of whitespace around your graphic. In these screen shots I am using the [GIMP](http://www.gimp.org) logo created by using the logo brush in that application. [![](http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics-e1539110259290.png)](http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics.png) The pandoc command does give you some options to control the size of your image. Just include the option in braces after the image file location:\n\n!\\[\\](/path/to/image.png){width=25%}\n\nThis will scale your image to 1/4 of its original size, which is what I did in the screen shot above. You can also use absolute values for your width or height, by specifying them. When using R Markdown, however, I have found that the smaller of the two values will determine the size of the graphic in the final document, with the aspect ration being maintained. For example, embedding your image like this:\n\n!\\[\\](gimp.png){width=300px height 50px}\n\nwon't produce a skewed image in a PDF or HTML, but instead will produce a very small image. Including the slash at the end of the line will help anchor the image so it does not end up floating near the bottom of the page. The output document in Word format will be skewed however. A comparison of the two is below: [![](http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png)](http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png)[![](http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png)](http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png) Coming up I'll cover more embedding graphics in your R Markdown documents with control over centering your images, and wrapping text around them.","source":"_posts/embedding-graphics.md","raw":"---\ntitle: Embedding Graphics\ntags:\n  - cookbook\n  - How-to\n  - R Markdown\nid: '4044'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-10-09 15:40:02\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)The English idiom or cliche, that \"a picture is worth a thousand words\" does convey some truth. People by and large tend to be visual. We internalize concepts and ideas more easily that can be represented by visuals than be text alone.  So when working with R Markdown documents, you may want to embed graphical elements into your document. In this post, I will cover some of the basics of embedding graphics when generating PDF documents using R Markdown. I have several more posts queued up on this topic that will cover some more in-depth concepts, but for now let's look at some simple methods.\n<!-- more -->\nTo insert an image in an R Markdown file that produces a PDF, Word document or an HTML file, you can use the PANDOC syntax (be sure to include the exclamation point):\n\n!\\[\\](/path/to/image.png)\n\nThe brackets can be used to include a caption for your image if you want, or you can leave them empty for no caption. This command also works inline, but keep in mind that it will produce a lot of whitespace around your graphic. In these screen shots I am using the [GIMP](http://www.gimp.org) logo created by using the logo brush in that application. [![](http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics-e1539110259290.png)](http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics.png) The pandoc command does give you some options to control the size of your image. Just include the option in braces after the image file location:\n\n!\\[\\](/path/to/image.png){width=25%}\n\nThis will scale your image to 1/4 of its original size, which is what I did in the screen shot above. You can also use absolute values for your width or height, by specifying them. When using R Markdown, however, I have found that the smaller of the two values will determine the size of the graphic in the final document, with the aspect ration being maintained. For example, embedding your image like this:\n\n!\\[\\](gimp.png){width=300px height 50px}\n\nwon't produce a skewed image in a PDF or HTML, but instead will produce a very small image. Including the slash at the end of the line will help anchor the image so it does not end up floating near the bottom of the page. The output document in Word format will be skewed however. A comparison of the two is below: [![](http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png)](http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png)[![](http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png)](http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png) Coming up I'll cover more embedding graphics in your R Markdown documents with control over centering your images, and wrapping text around them.","slug":"embedding-graphics","published":1,"updated":"2020-08-23T20:54:35.154Z","layout":"post","photos":[],"link":"","_id":"ckeaq99xh002lsdjx7ryn73g6","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>The English idiom or cliche, that “a picture is worth a thousand words” does convey some truth. People by and large tend to be visual. We internalize concepts and ideas more easily that can be represented by visuals than be text alone.  So when working with R Markdown documents, you may want to embed graphical elements into your document. In this post, I will cover some of the basics of embedding graphics when generating PDF documents using R Markdown. I have several more posts queued up on this topic that will cover some more in-depth concepts, but for now let’s look at some simple methods.</p>\n<a id=\"more\"></a>\n<p>To insert an image in an R Markdown file that produces a PDF, Word document or an HTML file, you can use the PANDOC syntax (be sure to include the exclamation point):</p>\n<p>![](/path/to/image.png)</p>\n<p>The brackets can be used to include a caption for your image if you want, or you can leave them empty for no caption. This command also works inline, but keep in mind that it will produce a lot of whitespace around your graphic. In these screen shots I am using the <a href=\"http://www.gimp.org/\">GIMP</a> logo created by using the logo brush in that application. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics-e1539110259290.png\"></a> The pandoc command does give you some options to control the size of your image. Just include the option in braces after the image file location:</p>\n<p>![](/path/to/image.png){width=25%}</p>\n<p>This will scale your image to 1/4 of its original size, which is what I did in the screen shot above. You can also use absolute values for your width or height, by specifying them. When using R Markdown, however, I have found that the smaller of the two values will determine the size of the graphic in the final document, with the aspect ration being maintained. For example, embedding your image like this:</p>\n<p>![](gimp.png){width=300px height 50px}</p>\n<p>won’t produce a skewed image in a PDF or HTML, but instead will produce a very small image. Including the slash at the end of the line will help anchor the image so it does not end up floating near the bottom of the page. The output document in Word format will be skewed however. A comparison of the two is below: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png\"></a><a href=\"http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png\"></a> Coming up I’ll cover more embedding graphics in your R Markdown documents with control over centering your images, and wrapping text around them.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>The English idiom or cliche, that “a picture is worth a thousand words” does convey some truth. People by and large tend to be visual. We internalize concepts and ideas more easily that can be represented by visuals than be text alone.  So when working with R Markdown documents, you may want to embed graphical elements into your document. In this post, I will cover some of the basics of embedding graphics when generating PDF documents using R Markdown. I have several more posts queued up on this topic that will cover some more in-depth concepts, but for now let’s look at some simple methods.</p>","more":"<p>To insert an image in an R Markdown file that produces a PDF, Word document or an HTML file, you can use the PANDOC syntax (be sure to include the exclamation point):</p>\n<p>![](/path/to/image.png)</p>\n<p>The brackets can be used to include a caption for your image if you want, or you can leave them empty for no caption. This command also works inline, but keep in mind that it will produce a lot of whitespace around your graphic. In these screen shots I am using the <a href=\"http://www.gimp.org/\">GIMP</a> logo created by using the logo brush in that application. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc-graphics-e1539110259290.png\"></a> The pandoc command does give you some options to control the size of your image. Just include the option in braces after the image file location:</p>\n<p>![](/path/to/image.png){width=25%}</p>\n<p>This will scale your image to 1/4 of its original size, which is what I did in the screen shot above. You can also use absolute values for your width or height, by specifying them. When using R Markdown, however, I have found that the smaller of the two values will determine the size of the graphic in the final document, with the aspect ration being maintained. For example, embedding your image like this:</p>\n<p>![](gimp.png){width=300px height 50px}</p>\n<p>won’t produce a skewed image in a PDF or HTML, but instead will produce a very small image. Including the slash at the end of the line will help anchor the image so it does not end up floating near the bottom of the page. The output document in Word format will be skewed however. A comparison of the two is below: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/pandoc2.png\"></a><a href=\"http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/wordskewed.png\"></a> Coming up I’ll cover more embedding graphics in your R Markdown documents with control over centering your images, and wrapping text around them.</p>"},{"title":"Enable Linux SQL Server Agent","id":"3567","comments":0,"date":"2018-03-05T23:16:38.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/02/sql-server-icon.png)](http://edpflager.com/2017/09/23/photo-break-ambassador-bridge-windsor/3623-revision-v1/)Short tip this time around. If you are running SQL Server on Linux and connect to it from a Windows system with Management Studio (SSMS), the SQL Agent will  be off. If you right click the Agent in SSMS try to get the properties of it, you will receive a lengthy  error with this included:\n\nSQL Server blocked access to procedure 'dbo.sp\\_get\\_sqlagent\\_properties' of component 'Agent XPs' because this component is turned off as part of the security configuration for this server.\n\nAll good and normal. So how do you turn SQL Agent on?\n<!-- more -->\nOpen a query window and execute this script:\n\n```\nsp_configure \n```\n\nFinally, right click on SQL Agent in SSMS and choose Refresh from the menu. You should now be able to access it.\n\n_**SQL Server is a product of Microsoft Corporation.**_","source":"_posts/enable-linux-sql-server-agent.md","raw":"---\ntitle: Enable Linux SQL Server Agent\ntags:\n  - How-to\n  - howto\n  - SQL Server\n  - SysAdmin\n  - technical\nid: '3567'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2018-03-05 18:16:38\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/02/sql-server-icon.png)](http://edpflager.com/2017/09/23/photo-break-ambassador-bridge-windsor/3623-revision-v1/)Short tip this time around. If you are running SQL Server on Linux and connect to it from a Windows system with Management Studio (SSMS), the SQL Agent will  be off. If you right click the Agent in SSMS try to get the properties of it, you will receive a lengthy  error with this included:\n\nSQL Server blocked access to procedure 'dbo.sp\\_get\\_sqlagent\\_properties' of component 'Agent XPs' because this component is turned off as part of the security configuration for this server.\n\nAll good and normal. So how do you turn SQL Agent on?\n<!-- more -->\nOpen a query window and execute this script:\n\n```\nsp_configure \n```\n\nFinally, right click on SQL Agent in SSMS and choose Refresh from the menu. You should now be able to access it.\n\n_**SQL Server is a product of Microsoft Corporation.**_","slug":"enable-linux-sql-server-agent","published":1,"updated":"2020-08-23T20:54:35.122Z","layout":"post","photos":[],"link":"","_id":"ckeaq99xm002psdjx5n5n4asm","content":"<p><a href=\"http://edpflager.com/2017/09/23/photo-break-ambassador-bridge-windsor/3623-revision-v1/\"><img src=\"http://edpflager.com/wp-content/uploads/2018/02/sql-server-icon.png\"></a>Short tip this time around. If you are running SQL Server on Linux and connect to it from a Windows system with Management Studio (SSMS), the SQL Agent will  be off. If you right click the Agent in SSMS try to get the properties of it, you will receive a lengthy  error with this included:</p>\n<p>SQL Server blocked access to procedure ‘dbo.sp_get_sqlagent_properties’ of component ‘Agent XPs’ because this component is turned off as part of the security configuration for this server.</p>\n<p>All good and normal. So how do you turn SQL Agent on?</p>\n<a id=\"more\"></a>\n<p>Open a query window and execute this script:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sp_configure </span><br></pre></td></tr></table></figure>\n\n<p>Finally, right click on SQL Agent in SSMS and choose Refresh from the menu. You should now be able to access it.</p>\n<p><em><strong>SQL Server is a product of Microsoft Corporation.</strong></em></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/2017/09/23/photo-break-ambassador-bridge-windsor/3623-revision-v1/\"><img src=\"http://edpflager.com/wp-content/uploads/2018/02/sql-server-icon.png\"></a>Short tip this time around. If you are running SQL Server on Linux and connect to it from a Windows system with Management Studio (SSMS), the SQL Agent will  be off. If you right click the Agent in SSMS try to get the properties of it, you will receive a lengthy  error with this included:</p>\n<p>SQL Server blocked access to procedure ‘dbo.sp_get_sqlagent_properties’ of component ‘Agent XPs’ because this component is turned off as part of the security configuration for this server.</p>\n<p>All good and normal. So how do you turn SQL Agent on?</p>","more":"<p>Open a query window and execute this script:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sp_configure </span><br></pre></td></tr></table></figure>\n\n<p>Finally, right click on SQL Agent in SSMS and choose Refresh from the menu. You should now be able to access it.</p>\n<p><em><strong>SQL Server is a product of Microsoft Corporation.</strong></em></p>"},{"title":"Extract PDF data with Tabula","id":"2967","comments":0,"date":"2015-09-28T23:34:28.000Z","_content":"\n[![tabula](http://edpflager.com/wp-content/uploads/2015/09/tabula-300x176.png)](http://edpflager.com/wp-content/uploads/2015/09/tabula.png)Adobe's PDF file format is a wonderful tool, allowing users on disparate operating systems to share documents easily. Because Adobe made the file format an open-standard in 2008, applications to create and read PDF files readers can be found on pretty much every operating system you can think of - Linux distros, Windows, Mac OS X and BSD,  just to name a few, from no cost to several hundred dollars. And in most cases, the original document, if not identical to the PDF, is close enough to identical to make the differences irrelevant. In my work as a BI developer, occasionally I have to extract data from PDF documents and get it into a database. While reading the file is no problem, getting the data out in a usable format where I am not having to retype or reformat the output excessively is often times not so easy. Luckily I have come across an open-source tool, called [Tabula](http://tabula.technology/),  that makes extracting data from a PDF much easier. It doesn't work for every PDF, only on text-based PDFs. That means reports and data sets that were exported to a PDF file, rather than documents that were scanned into a computer and saved as a PDF file. (The latter tends to be image type files rather than text based documents.)\n<!-- more -->\nTabula is available for Windows, Mac OS X and Linux and other operating systems. Generally it appears that if the OS has a JAVA 6 or 7 runtime engine,  then you should be able to use Tabula on it. The implementation is interesting in that it launches a localhost web server as an interface to the application, but I haven't seen any security problems with using it.\n\n1.  For illustration purposes, download this one page PDF from DATA.GOV: [Los Angeles Debt Ratings](https://catalog.data.gov/dataset/city-of-los-angeles-debt-ratings-df4b6) and then download the version of Tabula appropriate for your platform.(Scroll through the Tabula notes window on the right to find the link for the Linux/Other version).\n2.  Extract the file to a location that's easily accessible. Windows and Mac versions will have a dedicated application in the output folder and the Linux/Other version will have a JAR file.\n3.  To execute the JAR, open a terminal prompt and change directory to where Tabula was extracted. Enter this command to start the Tabula webserver: java -Dfile.encoding=utf-8 -Xms256M -Xmx1024M -jar tabula.jar\n4.  Several lines of text will indicate the web server is starting and once its ready, your default web browser will open, pointing to http://127.0.0.1:8080/ and prompting you to import a PDF.![TabulaImport](http://edpflager.com/wp-content/uploads/2015/09/Screenshot-from-2015-09-28-212858-300x99.png)\n5.  Click the Browse button and navigate to where you saved the Debt Ratings file, and then click the IMPORT button. The file will be imported, and a visual representation will be displayed once its complete. Click and drag around the data you want to import.![Selection2](http://edpflager.com/wp-content/uploads/2015/09/Selection2-300x169.png)\n6.  Once you have the data selected, click the Preview and Export Extracted Data button. Tabula will process your selection and then display a grid of the data you selected.![Output](http://edpflager.com/wp-content/uploads/2015/09/Output-300x163.png)\n7.  Select the file format you would like to produce from the drop down list at the top of the screen, and then click the Export button. Tabula will generate a file in our browser's download location after a few seconds of processing.\n8.  If you'd prefer to copy the information to your system's clipboard, click the Copy to Clipboard button instead, and then paste the data into whatever application you would like. Below is a screen shot from LibreOffice Calc where I have pasted the data from the sample PDF.[![LibreOfficePaste](http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487-300x188.png)](http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487.png)\n9.  To shut down Tabula, switch back to the Terminal window and enter Ctrl-C to exit the JAVA application, and then Exit the terminal window.Adobe logo clipart from http://www.wcpga.com/","source":"_posts/extract-pdf-data-with-tabula.md","raw":"---\ntitle: Extract PDF data with Tabula\ntags:\n  - centos\n  - ETL\n  - How-to\n  - howto\n  - install\n  - Mac\n  - technical\nid: '2967'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-09-28 19:34:28\n---\n\n[![tabula](http://edpflager.com/wp-content/uploads/2015/09/tabula-300x176.png)](http://edpflager.com/wp-content/uploads/2015/09/tabula.png)Adobe's PDF file format is a wonderful tool, allowing users on disparate operating systems to share documents easily. Because Adobe made the file format an open-standard in 2008, applications to create and read PDF files readers can be found on pretty much every operating system you can think of - Linux distros, Windows, Mac OS X and BSD,  just to name a few, from no cost to several hundred dollars. And in most cases, the original document, if not identical to the PDF, is close enough to identical to make the differences irrelevant. In my work as a BI developer, occasionally I have to extract data from PDF documents and get it into a database. While reading the file is no problem, getting the data out in a usable format where I am not having to retype or reformat the output excessively is often times not so easy. Luckily I have come across an open-source tool, called [Tabula](http://tabula.technology/),  that makes extracting data from a PDF much easier. It doesn't work for every PDF, only on text-based PDFs. That means reports and data sets that were exported to a PDF file, rather than documents that were scanned into a computer and saved as a PDF file. (The latter tends to be image type files rather than text based documents.)\n<!-- more -->\nTabula is available for Windows, Mac OS X and Linux and other operating systems. Generally it appears that if the OS has a JAVA 6 or 7 runtime engine,  then you should be able to use Tabula on it. The implementation is interesting in that it launches a localhost web server as an interface to the application, but I haven't seen any security problems with using it.\n\n1.  For illustration purposes, download this one page PDF from DATA.GOV: [Los Angeles Debt Ratings](https://catalog.data.gov/dataset/city-of-los-angeles-debt-ratings-df4b6) and then download the version of Tabula appropriate for your platform.(Scroll through the Tabula notes window on the right to find the link for the Linux/Other version).\n2.  Extract the file to a location that's easily accessible. Windows and Mac versions will have a dedicated application in the output folder and the Linux/Other version will have a JAR file.\n3.  To execute the JAR, open a terminal prompt and change directory to where Tabula was extracted. Enter this command to start the Tabula webserver: java -Dfile.encoding=utf-8 -Xms256M -Xmx1024M -jar tabula.jar\n4.  Several lines of text will indicate the web server is starting and once its ready, your default web browser will open, pointing to http://127.0.0.1:8080/ and prompting you to import a PDF.![TabulaImport](http://edpflager.com/wp-content/uploads/2015/09/Screenshot-from-2015-09-28-212858-300x99.png)\n5.  Click the Browse button and navigate to where you saved the Debt Ratings file, and then click the IMPORT button. The file will be imported, and a visual representation will be displayed once its complete. Click and drag around the data you want to import.![Selection2](http://edpflager.com/wp-content/uploads/2015/09/Selection2-300x169.png)\n6.  Once you have the data selected, click the Preview and Export Extracted Data button. Tabula will process your selection and then display a grid of the data you selected.![Output](http://edpflager.com/wp-content/uploads/2015/09/Output-300x163.png)\n7.  Select the file format you would like to produce from the drop down list at the top of the screen, and then click the Export button. Tabula will generate a file in our browser's download location after a few seconds of processing.\n8.  If you'd prefer to copy the information to your system's clipboard, click the Copy to Clipboard button instead, and then paste the data into whatever application you would like. Below is a screen shot from LibreOffice Calc where I have pasted the data from the sample PDF.[![LibreOfficePaste](http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487-300x188.png)](http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487.png)\n9.  To shut down Tabula, switch back to the Terminal window and enter Ctrl-C to exit the JAVA application, and then Exit the terminal window.Adobe logo clipart from http://www.wcpga.com/","slug":"extract-pdf-data-with-tabula","published":1,"updated":"2020-08-23T20:54:34.978Z","layout":"post","photos":[],"link":"","_id":"ckeaq99xq002tsdjx1bw52tik","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/09/tabula.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/tabula-300x176.png\" alt=\"tabula\"></a>Adobe’s PDF file format is a wonderful tool, allowing users on disparate operating systems to share documents easily. Because Adobe made the file format an open-standard in 2008, applications to create and read PDF files readers can be found on pretty much every operating system you can think of - Linux distros, Windows, Mac OS X and BSD,  just to name a few, from no cost to several hundred dollars. And in most cases, the original document, if not identical to the PDF, is close enough to identical to make the differences irrelevant. In my work as a BI developer, occasionally I have to extract data from PDF documents and get it into a database. While reading the file is no problem, getting the data out in a usable format where I am not having to retype or reformat the output excessively is often times not so easy. Luckily I have come across an open-source tool, called <a href=\"http://tabula.technology/\">Tabula</a>,  that makes extracting data from a PDF much easier. It doesn’t work for every PDF, only on text-based PDFs. That means reports and data sets that were exported to a PDF file, rather than documents that were scanned into a computer and saved as a PDF file. (The latter tends to be image type files rather than text based documents.)</p>\n<a id=\"more\"></a>\n<p>Tabula is available for Windows, Mac OS X and Linux and other operating systems. Generally it appears that if the OS has a JAVA 6 or 7 runtime engine,  then you should be able to use Tabula on it. The implementation is interesting in that it launches a localhost web server as an interface to the application, but I haven’t seen any security problems with using it.</p>\n<ol>\n<li>For illustration purposes, download this one page PDF from DATA.GOV: <a href=\"https://catalog.data.gov/dataset/city-of-los-angeles-debt-ratings-df4b6\">Los Angeles Debt Ratings</a> and then download the version of Tabula appropriate for your platform.(Scroll through the Tabula notes window on the right to find the link for the Linux/Other version).</li>\n<li>Extract the file to a location that’s easily accessible. Windows and Mac versions will have a dedicated application in the output folder and the Linux/Other version will have a JAR file.</li>\n<li>To execute the JAR, open a terminal prompt and change directory to where Tabula was extracted. Enter this command to start the Tabula webserver: java -Dfile.encoding=utf-8 -Xms256M -Xmx1024M -jar tabula.jar</li>\n<li>Several lines of text will indicate the web server is starting and once its ready, your default web browser will open, pointing to <a href=\"http://127.0.0.1:8080/\">http://127.0.0.1:8080/</a> and prompting you to import a PDF.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Screenshot-from-2015-09-28-212858-300x99.png\" alt=\"TabulaImport\"></li>\n<li>Click the Browse button and navigate to where you saved the Debt Ratings file, and then click the IMPORT button. The file will be imported, and a visual representation will be displayed once its complete. Click and drag around the data you want to import.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Selection2-300x169.png\" alt=\"Selection2\"></li>\n<li>Once you have the data selected, click the Preview and Export Extracted Data button. Tabula will process your selection and then display a grid of the data you selected.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Output-300x163.png\" alt=\"Output\"></li>\n<li>Select the file format you would like to produce from the drop down list at the top of the screen, and then click the Export button. Tabula will generate a file in our browser’s download location after a few seconds of processing.</li>\n<li>If you’d prefer to copy the information to your system’s clipboard, click the Copy to Clipboard button instead, and then paste the data into whatever application you would like. Below is a screen shot from LibreOffice Calc where I have pasted the data from the sample PDF.<a href=\"http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487-300x188.png\" alt=\"LibreOfficePaste\"></a></li>\n<li>To shut down Tabula, switch back to the Terminal window and enter Ctrl-C to exit the JAVA application, and then Exit the terminal window.Adobe logo clipart from <a href=\"http://www.wcpga.com/\">http://www.wcpga.com/</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/09/tabula.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/tabula-300x176.png\" alt=\"tabula\"></a>Adobe’s PDF file format is a wonderful tool, allowing users on disparate operating systems to share documents easily. Because Adobe made the file format an open-standard in 2008, applications to create and read PDF files readers can be found on pretty much every operating system you can think of - Linux distros, Windows, Mac OS X and BSD,  just to name a few, from no cost to several hundred dollars. And in most cases, the original document, if not identical to the PDF, is close enough to identical to make the differences irrelevant. In my work as a BI developer, occasionally I have to extract data from PDF documents and get it into a database. While reading the file is no problem, getting the data out in a usable format where I am not having to retype or reformat the output excessively is often times not so easy. Luckily I have come across an open-source tool, called <a href=\"http://tabula.technology/\">Tabula</a>,  that makes extracting data from a PDF much easier. It doesn’t work for every PDF, only on text-based PDFs. That means reports and data sets that were exported to a PDF file, rather than documents that were scanned into a computer and saved as a PDF file. (The latter tends to be image type files rather than text based documents.)</p>","more":"<p>Tabula is available for Windows, Mac OS X and Linux and other operating systems. Generally it appears that if the OS has a JAVA 6 or 7 runtime engine,  then you should be able to use Tabula on it. The implementation is interesting in that it launches a localhost web server as an interface to the application, but I haven’t seen any security problems with using it.</p>\n<ol>\n<li>For illustration purposes, download this one page PDF from DATA.GOV: <a href=\"https://catalog.data.gov/dataset/city-of-los-angeles-debt-ratings-df4b6\">Los Angeles Debt Ratings</a> and then download the version of Tabula appropriate for your platform.(Scroll through the Tabula notes window on the right to find the link for the Linux/Other version).</li>\n<li>Extract the file to a location that’s easily accessible. Windows and Mac versions will have a dedicated application in the output folder and the Linux/Other version will have a JAR file.</li>\n<li>To execute the JAR, open a terminal prompt and change directory to where Tabula was extracted. Enter this command to start the Tabula webserver: java -Dfile.encoding=utf-8 -Xms256M -Xmx1024M -jar tabula.jar</li>\n<li>Several lines of text will indicate the web server is starting and once its ready, your default web browser will open, pointing to <a href=\"http://127.0.0.1:8080/\">http://127.0.0.1:8080/</a> and prompting you to import a PDF.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Screenshot-from-2015-09-28-212858-300x99.png\" alt=\"TabulaImport\"></li>\n<li>Click the Browse button and navigate to where you saved the Debt Ratings file, and then click the IMPORT button. The file will be imported, and a visual representation will be displayed once its complete. Click and drag around the data you want to import.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Selection2-300x169.png\" alt=\"Selection2\"></li>\n<li>Once you have the data selected, click the Preview and Export Extracted Data button. Tabula will process your selection and then display a grid of the data you selected.<img src=\"http://edpflager.com/wp-content/uploads/2015/09/Output-300x163.png\" alt=\"Output\"></li>\n<li>Select the file format you would like to produce from the drop down list at the top of the screen, and then click the Export button. Tabula will generate a file in our browser’s download location after a few seconds of processing.</li>\n<li>If you’d prefer to copy the information to your system’s clipboard, click the Copy to Clipboard button instead, and then paste the data into whatever application you would like. Below is a screen shot from LibreOffice Calc where I have pasted the data from the sample PDF.<a href=\"http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/LibreOfficePaste-e1443491875487-300x188.png\" alt=\"LibreOfficePaste\"></a></li>\n<li>To shut down Tabula, switch back to the Terminal window and enter Ctrl-C to exit the JAVA application, and then Exit the terminal window.Adobe logo clipart from <a href=\"http://www.wcpga.com/\">http://www.wcpga.com/</a></li>\n</ol>"},{"title":"Floating and Rotating Embedded Graphics","id":"4046","comments":0,"date":"2018-10-12T16:29:09.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) The combination of R Markdown and Knitr provides a very versatile environment for generating output. You can mix using Pandoc and LaTeX packages in your document to achieve a remarkable level of control over the output you produce. Previously I covered some simple methods to embed graphics in your R Markdown file output using just Pandoc syntax. The results were passable. We could include a caption, resize the image in a couple of ways, and include our image inline so that it it appeared within a row of text. What we couldn't do very well was position the image in the center of our PDF document output. This post will cover using LaTeX packages to achieve that along with a couple of other ways we can manipulate our images.\n<!-- more -->\nTo include LaTeX functions in our R Markdown document, we need to specify them in the YAML header of our file. For this example, I am using the graphix and float packages:\n\nheader-includes: usepackage{graphicx} \n                 usepackage{float} \n\nTo embed a full size copy of the image, we just call it like below, and it generates an image as we would expect. Here is the code and a resulting screen shot:\n\nincludegraphics{pathtoimage}\n\nThere is one thing to take into account here. If there is text proceeding the image, include a blank line between the text and the image code to ensure the text stays above the image. [![](http://edpflager.com/wp-content/uploads/2018/10/includegraphix-300x262.png)](http://edpflager.com/wp-content/uploads/2018/10/includegraphix.png) You can also manage the size of the image here in a different way then I posted about previously with the pandoc options. The **includegraphics** function uses **textwidth** to reference the length of a line of text in your document. If you want to have your graphic image be dynamic depending on the length of the line, you can include code to calculate its width as a percentage of the **textwidth**. For example, to make it a quarter of the width of a line of text, you would specify the image embedding this way, being sure to use brackets around the size calculation and braces around the image name:\n\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\n\nYou can also specify absolute dimensions for your image using cm, mm or in values, like this:\n\nincludegraphics\\[height=3in\\]{gimp.png}\n\nJust as with the Pandoc syntax, when generating a PDF, whichever value you set as the smallest is the one that **includegraphics** uses with the aspect ratio of the image maintained.\n\n##### ROTATE AN IMAGE\n\nRotating an image using the LaTeX function **includegraphics** involves two settings. The first is an angle to determine how much you wish to rotate the image. Values range from 0 to 360, the same as normal circle coordinates. Second, define an origin value to set the point round which the image is rotated. Think of it as a pin in your image that keeps that part of the image stationary, while the rest of the image rotates around it. Options for the origin value are not granular, but define general areas. C would denote the center, t is top, and b is bottom. L and R define left and right. You can combine them into logical pairings as well, such as lc (left center), rb (right bottom) or tc (top center). Depending on the dimensions of your original image, using different values for your origin point may not result in much appreciable different.  In this example, using this code, produces the resulting image that follows it:\n\nincludegraphics\\[height=1in,angle=155,origin=lc\\]{gimp.png}\n\n[![](http://edpflager.com/wp-content/uploads/2018/10/rotate155-2-e1539371479559.png)](http://edpflager.com/wp-content/uploads/2018/10/rotate155-2.png)\n\n##### GRAPHICS SEQUENCE\n\nFinally, if there are multiple commands you would like to include for your image, define an image sequence, followed by the commands. The sequence starts with a begin statement with figure in braces. Follow that with brackets around the option \\[htbp!\\]. This tells your LaTeX interpreter the preferred placement order of your image: h=here, t=top of current page, b=bottom of the current page, and p=top of the next page. All of these are dependent on how much space is still available on the current page, and how the size of the image. The final gives LaTeX some leaway in how exact it in in following the placement options. Leave the ! out to have it be strict in its interpretation. After the opening line of the sequence, include the various processing commands. Here is an example, with an added centering to tell LaTeX to center the image, and a caption{GIMP logo} command to add a caption, which will include a sequence indicator:\n\nbegin{figure}\\[htbp!\\]\ncentering\nincludegraphics\\[width=0.25textwidth, angle=75, origin=lc\\]{gimp.png}\ncaption{GIMP logo}\nend{figure}\n\nFinally close the sequence with the end{figure}  command. The resulting image looks like this in my sample document: [![](http://edpflager.com/wp-content/uploads/2018/10/rotatecaption-e1539372334593.png)](http://edpflager.com/wp-content/uploads/2018/10/rotatecaption.png) And here is the full text of the R Markdown document:\n\n\\---\ntitle: \"GraphicsFloat\"\noutput: pdf\\_document\nheader-includes: usepackage{graphicx}\n                 usepackage{float}\n                 usepackage{blindtext}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nblindtext\n\nbegin{figure}\\[htbp!\\]\ncentering\nincludegraphics\\[width=0.25textwidth, angle=75, origin=lc\\]{gimp.png}\ncaption{GIMP logo}\nend{figure}\n\nblindtext","source":"_posts/floating-and-rotating-embedded-graphics.md","raw":"---\ntitle: Floating and Rotating Embedded Graphics\ntags:\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '4046'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-10-12 12:29:09\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) The combination of R Markdown and Knitr provides a very versatile environment for generating output. You can mix using Pandoc and LaTeX packages in your document to achieve a remarkable level of control over the output you produce. Previously I covered some simple methods to embed graphics in your R Markdown file output using just Pandoc syntax. The results were passable. We could include a caption, resize the image in a couple of ways, and include our image inline so that it it appeared within a row of text. What we couldn't do very well was position the image in the center of our PDF document output. This post will cover using LaTeX packages to achieve that along with a couple of other ways we can manipulate our images.\n<!-- more -->\nTo include LaTeX functions in our R Markdown document, we need to specify them in the YAML header of our file. For this example, I am using the graphix and float packages:\n\nheader-includes: usepackage{graphicx} \n                 usepackage{float} \n\nTo embed a full size copy of the image, we just call it like below, and it generates an image as we would expect. Here is the code and a resulting screen shot:\n\nincludegraphics{pathtoimage}\n\nThere is one thing to take into account here. If there is text proceeding the image, include a blank line between the text and the image code to ensure the text stays above the image. [![](http://edpflager.com/wp-content/uploads/2018/10/includegraphix-300x262.png)](http://edpflager.com/wp-content/uploads/2018/10/includegraphix.png) You can also manage the size of the image here in a different way then I posted about previously with the pandoc options. The **includegraphics** function uses **textwidth** to reference the length of a line of text in your document. If you want to have your graphic image be dynamic depending on the length of the line, you can include code to calculate its width as a percentage of the **textwidth**. For example, to make it a quarter of the width of a line of text, you would specify the image embedding this way, being sure to use brackets around the size calculation and braces around the image name:\n\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\n\nYou can also specify absolute dimensions for your image using cm, mm or in values, like this:\n\nincludegraphics\\[height=3in\\]{gimp.png}\n\nJust as with the Pandoc syntax, when generating a PDF, whichever value you set as the smallest is the one that **includegraphics** uses with the aspect ratio of the image maintained.\n\n##### ROTATE AN IMAGE\n\nRotating an image using the LaTeX function **includegraphics** involves two settings. The first is an angle to determine how much you wish to rotate the image. Values range from 0 to 360, the same as normal circle coordinates. Second, define an origin value to set the point round which the image is rotated. Think of it as a pin in your image that keeps that part of the image stationary, while the rest of the image rotates around it. Options for the origin value are not granular, but define general areas. C would denote the center, t is top, and b is bottom. L and R define left and right. You can combine them into logical pairings as well, such as lc (left center), rb (right bottom) or tc (top center). Depending on the dimensions of your original image, using different values for your origin point may not result in much appreciable different.  In this example, using this code, produces the resulting image that follows it:\n\nincludegraphics\\[height=1in,angle=155,origin=lc\\]{gimp.png}\n\n[![](http://edpflager.com/wp-content/uploads/2018/10/rotate155-2-e1539371479559.png)](http://edpflager.com/wp-content/uploads/2018/10/rotate155-2.png)\n\n##### GRAPHICS SEQUENCE\n\nFinally, if there are multiple commands you would like to include for your image, define an image sequence, followed by the commands. The sequence starts with a begin statement with figure in braces. Follow that with brackets around the option \\[htbp!\\]. This tells your LaTeX interpreter the preferred placement order of your image: h=here, t=top of current page, b=bottom of the current page, and p=top of the next page. All of these are dependent on how much space is still available on the current page, and how the size of the image. The final gives LaTeX some leaway in how exact it in in following the placement options. Leave the ! out to have it be strict in its interpretation. After the opening line of the sequence, include the various processing commands. Here is an example, with an added centering to tell LaTeX to center the image, and a caption{GIMP logo} command to add a caption, which will include a sequence indicator:\n\nbegin{figure}\\[htbp!\\]\ncentering\nincludegraphics\\[width=0.25textwidth, angle=75, origin=lc\\]{gimp.png}\ncaption{GIMP logo}\nend{figure}\n\nFinally close the sequence with the end{figure}  command. The resulting image looks like this in my sample document: [![](http://edpflager.com/wp-content/uploads/2018/10/rotatecaption-e1539372334593.png)](http://edpflager.com/wp-content/uploads/2018/10/rotatecaption.png) And here is the full text of the R Markdown document:\n\n\\---\ntitle: \"GraphicsFloat\"\noutput: pdf\\_document\nheader-includes: usepackage{graphicx}\n                 usepackage{float}\n                 usepackage{blindtext}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nblindtext\n\nbegin{figure}\\[htbp!\\]\ncentering\nincludegraphics\\[width=0.25textwidth, angle=75, origin=lc\\]{gimp.png}\ncaption{GIMP logo}\nend{figure}\n\nblindtext","slug":"floating-and-rotating-embedded-graphics","published":1,"updated":"2020-08-23T20:54:35.162Z","layout":"post","photos":[],"link":"","_id":"ckeaq99xz002wsdjx9voi5s68","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> The combination of R Markdown and Knitr provides a very versatile environment for generating output. You can mix using Pandoc and LaTeX packages in your document to achieve a remarkable level of control over the output you produce. Previously I covered some simple methods to embed graphics in your R Markdown file output using just Pandoc syntax. The results were passable. We could include a caption, resize the image in a couple of ways, and include our image inline so that it it appeared within a row of text. What we couldn’t do very well was position the image in the center of our PDF document output. This post will cover using LaTeX packages to achieve that along with a couple of other ways we can manipulate our images.</p>\n<a id=\"more\"></a>\n<p>To include LaTeX functions in our R Markdown document, we need to specify them in the YAML header of our file. For this example, I am using the graphix and float packages:</p>\n<p>header-includes: usepackage{graphicx}<br>                 usepackage{float} </p>\n<p>To embed a full size copy of the image, we just call it like below, and it generates an image as we would expect. Here is the code and a resulting screen shot:</p>\n<p>includegraphics{pathtoimage}</p>\n<p>There is one thing to take into account here. If there is text proceeding the image, include a blank line between the text and the image code to ensure the text stays above the image. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/includegraphix.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/includegraphix-300x262.png\"></a> You can also manage the size of the image here in a different way then I posted about previously with the pandoc options. The <strong>includegraphics</strong> function uses <strong>textwidth</strong> to reference the length of a line of text in your document. If you want to have your graphic image be dynamic depending on the length of the line, you can include code to calculate its width as a percentage of the <strong>textwidth</strong>. For example, to make it a quarter of the width of a line of text, you would specify the image embedding this way, being sure to use brackets around the size calculation and braces around the image name:</p>\n<p>includegraphics[width=0.25textwidth]{gimp.png}</p>\n<p>You can also specify absolute dimensions for your image using cm, mm or in values, like this:</p>\n<p>includegraphics[height=3in]{gimp.png}</p>\n<p>Just as with the Pandoc syntax, when generating a PDF, whichever value you set as the smallest is the one that <strong>includegraphics</strong> uses with the aspect ratio of the image maintained.</p>\n<h5 id=\"ROTATE-AN-IMAGE\"><a href=\"#ROTATE-AN-IMAGE\" class=\"headerlink\" title=\"ROTATE AN IMAGE\"></a>ROTATE AN IMAGE</h5><p>Rotating an image using the LaTeX function <strong>includegraphics</strong> involves two settings. The first is an angle to determine how much you wish to rotate the image. Values range from 0 to 360, the same as normal circle coordinates. Second, define an origin value to set the point round which the image is rotated. Think of it as a pin in your image that keeps that part of the image stationary, while the rest of the image rotates around it. Options for the origin value are not granular, but define general areas. C would denote the center, t is top, and b is bottom. L and R define left and right. You can combine them into logical pairings as well, such as lc (left center), rb (right bottom) or tc (top center). Depending on the dimensions of your original image, using different values for your origin point may not result in much appreciable different.  In this example, using this code, produces the resulting image that follows it:</p>\n<p>includegraphics[height=1in,angle=155,origin=lc]{gimp.png}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/10/rotate155-2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/rotate155-2-e1539371479559.png\"></a></p>\n<h5 id=\"GRAPHICS-SEQUENCE\"><a href=\"#GRAPHICS-SEQUENCE\" class=\"headerlink\" title=\"GRAPHICS SEQUENCE\"></a>GRAPHICS SEQUENCE</h5><p>Finally, if there are multiple commands you would like to include for your image, define an image sequence, followed by the commands. The sequence starts with a begin statement with figure in braces. Follow that with brackets around the option [htbp!]. This tells your LaTeX interpreter the preferred placement order of your image: h=here, t=top of current page, b=bottom of the current page, and p=top of the next page. All of these are dependent on how much space is still available on the current page, and how the size of the image. The final gives LaTeX some leaway in how exact it in in following the placement options. Leave the ! out to have it be strict in its interpretation. After the opening line of the sequence, include the various processing commands. Here is an example, with an added centering to tell LaTeX to center the image, and a caption{GIMP logo} command to add a caption, which will include a sequence indicator:</p>\n<p>begin{figure}[htbp!]<br>centering<br>includegraphics[width=0.25textwidth, angle=75, origin=lc]{gimp.png}<br>caption{GIMP logo}<br>end{figure}</p>\n<p>Finally close the sequence with the end{figure}  command. The resulting image looks like this in my sample document: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/rotatecaption.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/rotatecaption-e1539372334593.png\"></a> And here is the full text of the R Markdown document:</p>\n<p>-–<br>title: “GraphicsFloat”<br>output: pdf_document<br>header-includes: usepackage{graphicx}<br>                 usepackage{float}<br>                 usepackage{blindtext}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>blindtext</p>\n<p>begin{figure}[htbp!]<br>centering<br>includegraphics[width=0.25textwidth, angle=75, origin=lc]{gimp.png}<br>caption{GIMP logo}<br>end{figure}</p>\n<p>blindtext</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> The combination of R Markdown and Knitr provides a very versatile environment for generating output. You can mix using Pandoc and LaTeX packages in your document to achieve a remarkable level of control over the output you produce. Previously I covered some simple methods to embed graphics in your R Markdown file output using just Pandoc syntax. The results were passable. We could include a caption, resize the image in a couple of ways, and include our image inline so that it it appeared within a row of text. What we couldn’t do very well was position the image in the center of our PDF document output. This post will cover using LaTeX packages to achieve that along with a couple of other ways we can manipulate our images.</p>","more":"<p>To include LaTeX functions in our R Markdown document, we need to specify them in the YAML header of our file. For this example, I am using the graphix and float packages:</p>\n<p>header-includes: usepackage{graphicx}<br>                 usepackage{float} </p>\n<p>To embed a full size copy of the image, we just call it like below, and it generates an image as we would expect. Here is the code and a resulting screen shot:</p>\n<p>includegraphics{pathtoimage}</p>\n<p>There is one thing to take into account here. If there is text proceeding the image, include a blank line between the text and the image code to ensure the text stays above the image. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/includegraphix.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/includegraphix-300x262.png\"></a> You can also manage the size of the image here in a different way then I posted about previously with the pandoc options. The <strong>includegraphics</strong> function uses <strong>textwidth</strong> to reference the length of a line of text in your document. If you want to have your graphic image be dynamic depending on the length of the line, you can include code to calculate its width as a percentage of the <strong>textwidth</strong>. For example, to make it a quarter of the width of a line of text, you would specify the image embedding this way, being sure to use brackets around the size calculation and braces around the image name:</p>\n<p>includegraphics[width=0.25textwidth]{gimp.png}</p>\n<p>You can also specify absolute dimensions for your image using cm, mm or in values, like this:</p>\n<p>includegraphics[height=3in]{gimp.png}</p>\n<p>Just as with the Pandoc syntax, when generating a PDF, whichever value you set as the smallest is the one that <strong>includegraphics</strong> uses with the aspect ratio of the image maintained.</p>\n<h5 id=\"ROTATE-AN-IMAGE\"><a href=\"#ROTATE-AN-IMAGE\" class=\"headerlink\" title=\"ROTATE AN IMAGE\"></a>ROTATE AN IMAGE</h5><p>Rotating an image using the LaTeX function <strong>includegraphics</strong> involves two settings. The first is an angle to determine how much you wish to rotate the image. Values range from 0 to 360, the same as normal circle coordinates. Second, define an origin value to set the point round which the image is rotated. Think of it as a pin in your image that keeps that part of the image stationary, while the rest of the image rotates around it. Options for the origin value are not granular, but define general areas. C would denote the center, t is top, and b is bottom. L and R define left and right. You can combine them into logical pairings as well, such as lc (left center), rb (right bottom) or tc (top center). Depending on the dimensions of your original image, using different values for your origin point may not result in much appreciable different.  In this example, using this code, produces the resulting image that follows it:</p>\n<p>includegraphics[height=1in,angle=155,origin=lc]{gimp.png}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/10/rotate155-2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/rotate155-2-e1539371479559.png\"></a></p>\n<h5 id=\"GRAPHICS-SEQUENCE\"><a href=\"#GRAPHICS-SEQUENCE\" class=\"headerlink\" title=\"GRAPHICS SEQUENCE\"></a>GRAPHICS SEQUENCE</h5><p>Finally, if there are multiple commands you would like to include for your image, define an image sequence, followed by the commands. The sequence starts with a begin statement with figure in braces. Follow that with brackets around the option [htbp!]. This tells your LaTeX interpreter the preferred placement order of your image: h=here, t=top of current page, b=bottom of the current page, and p=top of the next page. All of these are dependent on how much space is still available on the current page, and how the size of the image. The final gives LaTeX some leaway in how exact it in in following the placement options. Leave the ! out to have it be strict in its interpretation. After the opening line of the sequence, include the various processing commands. Here is an example, with an added centering to tell LaTeX to center the image, and a caption{GIMP logo} command to add a caption, which will include a sequence indicator:</p>\n<p>begin{figure}[htbp!]<br>centering<br>includegraphics[width=0.25textwidth, angle=75, origin=lc]{gimp.png}<br>caption{GIMP logo}<br>end{figure}</p>\n<p>Finally close the sequence with the end{figure}  command. The resulting image looks like this in my sample document: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/rotatecaption.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/rotatecaption-e1539372334593.png\"></a> And here is the full text of the R Markdown document:</p>\n<p>-–<br>title: “GraphicsFloat”<br>output: pdf_document<br>header-includes: usepackage{graphicx}<br>                 usepackage{float}<br>                 usepackage{blindtext}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>blindtext</p>\n<p>begin{figure}[htbp!]<br>centering<br>includegraphics[width=0.25textwidth, angle=75, origin=lc]{gimp.png}<br>caption{GIMP logo}<br>end{figure}</p>\n<p>blindtext</p>"},{"title":"Font Formatting - Coloring and Emphasis","id":"4002","comments":0,"date":"2018-09-28T15:51:08.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)When working on an R Markdown document that will be output to a PDF, you may want to add a bit of color or embellishment to emphasize important information. By using the LaTeX function **xcolor** in your R Markdown code, you apply a wide palette of colors to enhance your text. The documentation for Xcolor is [available online](http://textdoc.net/pkg/xcolor) and includes a considerable amount of information on using the function in a LaTeX environment. Its not all applicable to the R Markdown library but I have been able to get the following color options to work when generating PDF output. Unfortunately it does not work in HTML or Word output. As with all enhancements, you should use this option sparingly. Too much of a good thing is likely to turn your readers off, if you are sharing the output. Also keep in mind those with visual impairments who can't discern different colors as well.\n<!-- more -->\nFirst we need to define our YAML header in the R Markdown file to include the LaTeX package **xcolor**. The header-includes option handles this. And be sure to watch your indentations.\n\n\\---\ntitle: \"Font Colors and Emphasizing\"\noutput:\n    pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 12 pt\nheader-includes: usepackage{xcolor}\n---\n\n#### COLOR\n\nOnce you have that option in your file, to specify a color for your font, proceed your text with the inline textcolor function, and then enclose your text in braces {} like this:\n\ntextcolor{colorname}{This is printed in your colorname}\n\nThe options I have tested successfully include:\n\n*   lightgray\n*   gray\n*   darkgray\n*   lime\n*   green\n*   olive\n*   teal\n*   blue\n*   cyan\n*   brown\n*   magenta\n*   red\n*   violet\n*   orange\n*   purple\n*   pink\n*   yellow\n*   white\n\nBlack is the default color when you don't use the textcolor function. My sample code for testing this function looks like this:\n\n\\---\ntitle: \"Font Colors and Emphasizing\"\noutput:\n    pdf\\_document: \n       latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 12 pt\nheader-includes: usepackage{xcolor}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nBy using the LaTeX function textcolor you can add some color to your R Markdown PDF documents. \n\ntextcolor{lightgray}{This is lightgray.}\n\ntextcolor{gray}{This is gray.}\n\ntextcolor{darkgray}{This is darkgray.}\n\ntextcolor{lime}{This is lime.}\n\ntextcolor{green}{This is green.}\n\ntextcolor{olive}{This is olive.}\n\ntextcolor{teal}{This is teal.}\n\ntextcolor{blue}{This is blue.}\n\ntextcolor{cyan}{This is cyan.}\n\ntextcolor{brown}{This is brown.}\n\ntextcolor{magenta}{This is magenta.}\n\ntextcolor{red}{This is red.}\n\ntextcolor{violet}{This is violet.}\n\ntextcolor{orange}{This is orange.}\n\ntextcolor{purple}{This is purple.}\n\ntextcolor{pink}{This is pink.}\n\ntextcolor{yellow}{This is yellow.}\n\ntextcolor{white}{This is white.} The proceeding text is white. .\n\nThis results in a PDF document that looks like this: [![](http://edpflager.com/wp-content/uploads/2018/09/Font-Colors-1024x1011.png)](http://edpflager.com/wp-content/uploads/2018/09/Font-Colors.png)\n\n#### EMPHASIS\n\nIf you don't want to use color in your document, you can also emphasize important portions of your document using the traditional methods: bold, underline and italicizing your text. These are accomplished using three different inline functions as shown below:\n\ntextbf{This is bold} \n\ntextit{This is italicized} \n\nunderline{This is underlined} \n\nDropping those lines into our R Markdown document produces this output: [![](http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png)](http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png)\n\n#### COMBINING THE TWO\n\nFinally, if you'd like to combine the options, its just a matter of nesting the commands in the right order. From my experience, the color setting should be first, followed by the emphasis settings. As an example, to include red bold text, you would enter the command like this: textcolor{red}{textbf{And you can combine them by nesting}} Which produces this PDF: [![](http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png)](http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png)","source":"_posts/font-formatting-coloring-and-emphasis.md","raw":"---\ntitle: Font Formatting - Coloring and Emphasis\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '4002'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-28 11:51:08\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)When working on an R Markdown document that will be output to a PDF, you may want to add a bit of color or embellishment to emphasize important information. By using the LaTeX function **xcolor** in your R Markdown code, you apply a wide palette of colors to enhance your text. The documentation for Xcolor is [available online](http://textdoc.net/pkg/xcolor) and includes a considerable amount of information on using the function in a LaTeX environment. Its not all applicable to the R Markdown library but I have been able to get the following color options to work when generating PDF output. Unfortunately it does not work in HTML or Word output. As with all enhancements, you should use this option sparingly. Too much of a good thing is likely to turn your readers off, if you are sharing the output. Also keep in mind those with visual impairments who can't discern different colors as well.\n<!-- more -->\nFirst we need to define our YAML header in the R Markdown file to include the LaTeX package **xcolor**. The header-includes option handles this. And be sure to watch your indentations.\n\n\\---\ntitle: \"Font Colors and Emphasizing\"\noutput:\n    pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 12 pt\nheader-includes: usepackage{xcolor}\n---\n\n#### COLOR\n\nOnce you have that option in your file, to specify a color for your font, proceed your text with the inline textcolor function, and then enclose your text in braces {} like this:\n\ntextcolor{colorname}{This is printed in your colorname}\n\nThe options I have tested successfully include:\n\n*   lightgray\n*   gray\n*   darkgray\n*   lime\n*   green\n*   olive\n*   teal\n*   blue\n*   cyan\n*   brown\n*   magenta\n*   red\n*   violet\n*   orange\n*   purple\n*   pink\n*   yellow\n*   white\n\nBlack is the default color when you don't use the textcolor function. My sample code for testing this function looks like this:\n\n\\---\ntitle: \"Font Colors and Emphasizing\"\noutput:\n    pdf\\_document: \n       latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 12 pt\nheader-includes: usepackage{xcolor}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nBy using the LaTeX function textcolor you can add some color to your R Markdown PDF documents. \n\ntextcolor{lightgray}{This is lightgray.}\n\ntextcolor{gray}{This is gray.}\n\ntextcolor{darkgray}{This is darkgray.}\n\ntextcolor{lime}{This is lime.}\n\ntextcolor{green}{This is green.}\n\ntextcolor{olive}{This is olive.}\n\ntextcolor{teal}{This is teal.}\n\ntextcolor{blue}{This is blue.}\n\ntextcolor{cyan}{This is cyan.}\n\ntextcolor{brown}{This is brown.}\n\ntextcolor{magenta}{This is magenta.}\n\ntextcolor{red}{This is red.}\n\ntextcolor{violet}{This is violet.}\n\ntextcolor{orange}{This is orange.}\n\ntextcolor{purple}{This is purple.}\n\ntextcolor{pink}{This is pink.}\n\ntextcolor{yellow}{This is yellow.}\n\ntextcolor{white}{This is white.} The proceeding text is white. .\n\nThis results in a PDF document that looks like this: [![](http://edpflager.com/wp-content/uploads/2018/09/Font-Colors-1024x1011.png)](http://edpflager.com/wp-content/uploads/2018/09/Font-Colors.png)\n\n#### EMPHASIS\n\nIf you don't want to use color in your document, you can also emphasize important portions of your document using the traditional methods: bold, underline and italicizing your text. These are accomplished using three different inline functions as shown below:\n\ntextbf{This is bold} \n\ntextit{This is italicized} \n\nunderline{This is underlined} \n\nDropping those lines into our R Markdown document produces this output: [![](http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png)](http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png)\n\n#### COMBINING THE TWO\n\nFinally, if you'd like to combine the options, its just a matter of nesting the commands in the right order. From my experience, the color setting should be first, followed by the emphasis settings. As an example, to include red bold text, you would enter the command like this: textcolor{red}{textbf{And you can combine them by nesting}} Which produces this PDF: [![](http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png)](http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png)","slug":"font-formatting-coloring-and-emphasis","published":1,"updated":"2020-08-23T20:54:35.146Z","layout":"post","photos":[],"link":"","_id":"ckeaq99yb0031sdjx1k0y5gua","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"></a>When working on an R Markdown document that will be output to a PDF, you may want to add a bit of color or embellishment to emphasize important information. By using the LaTeX function <strong>xcolor</strong> in your R Markdown code, you apply a wide palette of colors to enhance your text. The documentation for Xcolor is <a href=\"http://textdoc.net/pkg/xcolor\">available online</a> and includes a considerable amount of information on using the function in a LaTeX environment. Its not all applicable to the R Markdown library but I have been able to get the following color options to work when generating PDF output. Unfortunately it does not work in HTML or Word output. As with all enhancements, you should use this option sparingly. Too much of a good thing is likely to turn your readers off, if you are sharing the output. Also keep in mind those with visual impairments who can’t discern different colors as well.</p>\n<a id=\"more\"></a>\n<p>First we need to define our YAML header in the R Markdown file to include the LaTeX package <strong>xcolor</strong>. The header-includes option handles this. And be sure to watch your indentations.</p>\n<p>-–<br>title: “Font Colors and Emphasizing”<br>output:<br>    pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 12 pt<br>header-includes: usepackage{xcolor}</p>\n<hr>\n<h4 id=\"COLOR\"><a href=\"#COLOR\" class=\"headerlink\" title=\"COLOR\"></a>COLOR</h4><p>Once you have that option in your file, to specify a color for your font, proceed your text with the inline textcolor function, and then enclose your text in braces {} like this:</p>\n<p>textcolor{colorname}{This is printed in your colorname}</p>\n<p>The options I have tested successfully include:</p>\n<ul>\n<li>lightgray</li>\n<li>gray</li>\n<li>darkgray</li>\n<li>lime</li>\n<li>green</li>\n<li>olive</li>\n<li>teal</li>\n<li>blue</li>\n<li>cyan</li>\n<li>brown</li>\n<li>magenta</li>\n<li>red</li>\n<li>violet</li>\n<li>orange</li>\n<li>purple</li>\n<li>pink</li>\n<li>yellow</li>\n<li>white</li>\n</ul>\n<p>Black is the default color when you don’t use the textcolor function. My sample code for testing this function looks like this:</p>\n<p>-–<br>title: “Font Colors and Emphasizing”<br>output:<br>    pdf_document:<br>       latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 12 pt<br>header-includes: usepackage{xcolor}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>By using the LaTeX function textcolor you can add some color to your R Markdown PDF documents. </p>\n<p>textcolor{lightgray}{This is lightgray.}</p>\n<p>textcolor{gray}{This is gray.}</p>\n<p>textcolor{darkgray}{This is darkgray.}</p>\n<p>textcolor{lime}{This is lime.}</p>\n<p>textcolor{green}{This is green.}</p>\n<p>textcolor{olive}{This is olive.}</p>\n<p>textcolor{teal}{This is teal.}</p>\n<p>textcolor{blue}{This is blue.}</p>\n<p>textcolor{cyan}{This is cyan.}</p>\n<p>textcolor{brown}{This is brown.}</p>\n<p>textcolor{magenta}{This is magenta.}</p>\n<p>textcolor{red}{This is red.}</p>\n<p>textcolor{violet}{This is violet.}</p>\n<p>textcolor{orange}{This is orange.}</p>\n<p>textcolor{purple}{This is purple.}</p>\n<p>textcolor{pink}{This is pink.}</p>\n<p>textcolor{yellow}{This is yellow.}</p>\n<p>textcolor{white}{This is white.} The proceeding text is white. .</p>\n<p>This results in a PDF document that looks like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Font-Colors.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Font-Colors-1024x1011.png\"></a></p>\n<h4 id=\"EMPHASIS\"><a href=\"#EMPHASIS\" class=\"headerlink\" title=\"EMPHASIS\"></a>EMPHASIS</h4><p>If you don’t want to use color in your document, you can also emphasize important portions of your document using the traditional methods: bold, underline and italicizing your text. These are accomplished using three different inline functions as shown below:</p>\n<p>textbf{This is bold} </p>\n<p>textit{This is italicized} </p>\n<p>underline{This is underlined} </p>\n<p>Dropping those lines into our R Markdown document produces this output: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png\"></a></p>\n<h4 id=\"COMBINING-THE-TWO\"><a href=\"#COMBINING-THE-TWO\" class=\"headerlink\" title=\"COMBINING THE TWO\"></a>COMBINING THE TWO</h4><p>Finally, if you’d like to combine the options, its just a matter of nesting the commands in the right order. From my experience, the color setting should be first, followed by the emphasis settings. As an example, to include red bold text, you would enter the command like this: textcolor{red}{textbf{And you can combine them by nesting}} Which produces this PDF: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"></a>When working on an R Markdown document that will be output to a PDF, you may want to add a bit of color or embellishment to emphasize important information. By using the LaTeX function <strong>xcolor</strong> in your R Markdown code, you apply a wide palette of colors to enhance your text. The documentation for Xcolor is <a href=\"http://textdoc.net/pkg/xcolor\">available online</a> and includes a considerable amount of information on using the function in a LaTeX environment. Its not all applicable to the R Markdown library but I have been able to get the following color options to work when generating PDF output. Unfortunately it does not work in HTML or Word output. As with all enhancements, you should use this option sparingly. Too much of a good thing is likely to turn your readers off, if you are sharing the output. Also keep in mind those with visual impairments who can’t discern different colors as well.</p>","more":"<p>First we need to define our YAML header in the R Markdown file to include the LaTeX package <strong>xcolor</strong>. The header-includes option handles this. And be sure to watch your indentations.</p>\n<p>-–<br>title: “Font Colors and Emphasizing”<br>output:<br>    pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 12 pt<br>header-includes: usepackage{xcolor}</p>\n<hr>\n<h4 id=\"COLOR\"><a href=\"#COLOR\" class=\"headerlink\" title=\"COLOR\"></a>COLOR</h4><p>Once you have that option in your file, to specify a color for your font, proceed your text with the inline textcolor function, and then enclose your text in braces {} like this:</p>\n<p>textcolor{colorname}{This is printed in your colorname}</p>\n<p>The options I have tested successfully include:</p>\n<ul>\n<li>lightgray</li>\n<li>gray</li>\n<li>darkgray</li>\n<li>lime</li>\n<li>green</li>\n<li>olive</li>\n<li>teal</li>\n<li>blue</li>\n<li>cyan</li>\n<li>brown</li>\n<li>magenta</li>\n<li>red</li>\n<li>violet</li>\n<li>orange</li>\n<li>purple</li>\n<li>pink</li>\n<li>yellow</li>\n<li>white</li>\n</ul>\n<p>Black is the default color when you don’t use the textcolor function. My sample code for testing this function looks like this:</p>\n<p>-–<br>title: “Font Colors and Emphasizing”<br>output:<br>    pdf_document:<br>       latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 12 pt<br>header-includes: usepackage{xcolor}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>By using the LaTeX function textcolor you can add some color to your R Markdown PDF documents. </p>\n<p>textcolor{lightgray}{This is lightgray.}</p>\n<p>textcolor{gray}{This is gray.}</p>\n<p>textcolor{darkgray}{This is darkgray.}</p>\n<p>textcolor{lime}{This is lime.}</p>\n<p>textcolor{green}{This is green.}</p>\n<p>textcolor{olive}{This is olive.}</p>\n<p>textcolor{teal}{This is teal.}</p>\n<p>textcolor{blue}{This is blue.}</p>\n<p>textcolor{cyan}{This is cyan.}</p>\n<p>textcolor{brown}{This is brown.}</p>\n<p>textcolor{magenta}{This is magenta.}</p>\n<p>textcolor{red}{This is red.}</p>\n<p>textcolor{violet}{This is violet.}</p>\n<p>textcolor{orange}{This is orange.}</p>\n<p>textcolor{purple}{This is purple.}</p>\n<p>textcolor{pink}{This is pink.}</p>\n<p>textcolor{yellow}{This is yellow.}</p>\n<p>textcolor{white}{This is white.} The proceeding text is white. .</p>\n<p>This results in a PDF document that looks like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Font-Colors.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Font-Colors-1024x1011.png\"></a></p>\n<h4 id=\"EMPHASIS\"><a href=\"#EMPHASIS\" class=\"headerlink\" title=\"EMPHASIS\"></a>EMPHASIS</h4><p>If you don’t want to use color in your document, you can also emphasize important portions of your document using the traditional methods: bold, underline and italicizing your text. These are accomplished using three different inline functions as shown below:</p>\n<p>textbf{This is bold} </p>\n<p>textit{This is italicized} </p>\n<p>underline{This is underlined} </p>\n<p>Dropping those lines into our R Markdown document produces this output: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Emphasis.png\"></a></p>\n<h4 id=\"COMBINING-THE-TWO\"><a href=\"#COMBINING-THE-TWO\" class=\"headerlink\" title=\"COMBINING THE TWO\"></a>COMBINING THE TWO</h4><p>Finally, if you’d like to combine the options, its just a matter of nesting the commands in the right order. From my experience, the color setting should be first, followed by the emphasis settings. As an example, to include red bold text, you would enter the command like this: textcolor{red}{textbf{And you can combine them by nesting}} Which produces this PDF: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/nested-color-and-emphasis.png\"></a></p>"},{"title":"Font Formatting - Type Size","id":"4013","comments":0,"date":"2018-10-04T19:57:18.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)When using a word processor/text editor, you will often change the size of your type, for various reasons by setting a numeric size option. You can achieve the same results when creating an R Markdown document using some inline LaTeX code. I've written before about a setting the font size in your document by using settings within your YAML header, but noted that you could only use one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To overcome this, you can use inline LaTeX code in your R Markdown document to specify relative sizes for your text. Instead of using 12pt or 14 pt, you use Huge or Large or other variations to produce different sizes of text. Here is the output from an example R Markdown document that shows the different sizes:\n<!-- more -->\n[![](http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes-300x210.png)](http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes.png) To achieve this results, use the various codes in the below R Markdown document, using and then the descriptor for the text size you want.\n\n\\---\ntitle: \"Font test\"\noutput:\n   pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\n---\n\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n## Font size information comes from https://en.wikibooks.org/wiki/LaTeX/Fonts\n\nHuge This is HUGE\n\nhuge This is not as huge\n\nLARGE This is really LARGE\n\nLarge This is somewhat Large\n\nlarge This is large\n\nnormalsize (default)\n\nsmall This is a small font\n\nfootnotesize This is footnotesize\n\nscriptsize This is scriptsize (sub or super)\n\ntiny This is just tiny!\n\nThat's it!","source":"_posts/font-formatting-type-size.md","raw":"---\ntitle: Font Formatting - Type Size\ntags: []\nid: '4013'\ncategories:\n  - - Misc\ncomments: false\ndate: 2018-10-04 15:57:18\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)When using a word processor/text editor, you will often change the size of your type, for various reasons by setting a numeric size option. You can achieve the same results when creating an R Markdown document using some inline LaTeX code. I've written before about a setting the font size in your document by using settings within your YAML header, but noted that you could only use one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To overcome this, you can use inline LaTeX code in your R Markdown document to specify relative sizes for your text. Instead of using 12pt or 14 pt, you use Huge or Large or other variations to produce different sizes of text. Here is the output from an example R Markdown document that shows the different sizes:\n<!-- more -->\n[![](http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes-300x210.png)](http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes.png) To achieve this results, use the various codes in the below R Markdown document, using and then the descriptor for the text size you want.\n\n\\---\ntitle: \"Font test\"\noutput:\n   pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\n---\n\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n## Font size information comes from https://en.wikibooks.org/wiki/LaTeX/Fonts\n\nHuge This is HUGE\n\nhuge This is not as huge\n\nLARGE This is really LARGE\n\nLarge This is somewhat Large\n\nlarge This is large\n\nnormalsize (default)\n\nsmall This is a small font\n\nfootnotesize This is footnotesize\n\nscriptsize This is scriptsize (sub or super)\n\ntiny This is just tiny!\n\nThat's it!","slug":"font-formatting-type-size","published":1,"updated":"2020-08-23T20:54:35.150Z","layout":"post","photos":[],"link":"","_id":"ckeaq99yj0034sdjx89gw45n3","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>When using a word processor/text editor, you will often change the size of your type, for various reasons by setting a numeric size option. You can achieve the same results when creating an R Markdown document using some inline LaTeX code. I’ve written before about a setting the font size in your document by using settings within your YAML header, but noted that you could only use one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To overcome this, you can use inline LaTeX code in your R Markdown document to specify relative sizes for your text. Instead of using 12pt or 14 pt, you use Huge or Large or other variations to produce different sizes of text. Here is the output from an example R Markdown document that shows the different sizes:</p>\n<a id=\"more\"></a>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes-300x210.png\"></a> To achieve this results, use the various codes in the below R Markdown document, using and then the descriptor for the text size you want.</p>\n<p>-–<br>title: “Font test”<br>output:<br>   pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<h2 id=\"Font-size-information-comes-from-https-en-wikibooks-org-wiki-LaTeX-Fonts\"><a href=\"#Font-size-information-comes-from-https-en-wikibooks-org-wiki-LaTeX-Fonts\" class=\"headerlink\" title=\"Font size information comes from https://en.wikibooks.org/wiki/LaTeX/Fonts\"></a>Font size information comes from <a href=\"https://en.wikibooks.org/wiki/LaTeX/Fonts\">https://en.wikibooks.org/wiki/LaTeX/Fonts</a></h2><p>Huge This is HUGE</p>\n<p>huge This is not as huge</p>\n<p>LARGE This is really LARGE</p>\n<p>Large This is somewhat Large</p>\n<p>large This is large</p>\n<p>normalsize (default)</p>\n<p>small This is a small font</p>\n<p>footnotesize This is footnotesize</p>\n<p>scriptsize This is scriptsize (sub or super)</p>\n<p>tiny This is just tiny!</p>\n<p>That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>When using a word processor/text editor, you will often change the size of your type, for various reasons by setting a numeric size option. You can achieve the same results when creating an R Markdown document using some inline LaTeX code. I’ve written before about a setting the font size in your document by using settings within your YAML header, but noted that you could only use one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To overcome this, you can use inline LaTeX code in your R Markdown document to specify relative sizes for your text. Instead of using 12pt or 14 pt, you use Huge or Large or other variations to produce different sizes of text. Here is the output from an example R Markdown document that shows the different sizes:</p>","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/Font-Sizes-300x210.png\"></a> To achieve this results, use the various codes in the below R Markdown document, using and then the descriptor for the text size you want.</p>\n<p>-–<br>title: “Font test”<br>output:<br>   pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<h2 id=\"Font-size-information-comes-from-https-en-wikibooks-org-wiki-LaTeX-Fonts\"><a href=\"#Font-size-information-comes-from-https-en-wikibooks-org-wiki-LaTeX-Fonts\" class=\"headerlink\" title=\"Font size information comes from https://en.wikibooks.org/wiki/LaTeX/Fonts\"></a>Font size information comes from <a href=\"https://en.wikibooks.org/wiki/LaTeX/Fonts\">https://en.wikibooks.org/wiki/LaTeX/Fonts</a></h2><p>Huge This is HUGE</p>\n<p>huge This is not as huge</p>\n<p>LARGE This is really LARGE</p>\n<p>Large This is somewhat Large</p>\n<p>large This is large</p>\n<p>normalsize (default)</p>\n<p>small This is a small font</p>\n<p>footnotesize This is footnotesize</p>\n<p>scriptsize This is scriptsize (sub or super)</p>\n<p>tiny This is just tiny!</p>\n<p>That’s it!</p>"},{"title":"Formulas in R Markdown","id":"4131","comments":0,"date":"2018-11-06T23:02:07.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)R Markdown is used to create documents to show the analysis that data was put through and the results of that analysis in a one document. But what if you need to include the formula notation for a calculation in your document? While the common format for a calculation may be well known and understood, reproducing it may be difficult because of mathematical symbols that aren't generally included in a computer font set. In the interest of thoroughness though, the notation should be included. I will preface the rest of this article with this note: some of this works with HTML, Word and PDF. However if you have nested formula notations like I do on the Variance section, some of it may not work properly when using Word output.\n\n### Pythagorean theorem\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/theorem-266x300.png)](http://edpflager.com/wp-content/uploads/2018/11/theorem.png)Let me illustrate first with a common formula used in geometry: the Pythagorean theorem. In its simplest form, it states that for a right triangle, summing the squares of the sides of the triangle adjacent to the right angle equals the square of the side opposite the right angle (hypotenuse). As a refresher, this is usually notated as I have shown here. For the most part this is a fairly easy formula to reproduce on a computer keyboard. The letters are no problem, the + and = symbols are readily accessible. However including the superscripts that represent the square of the values a, b and c may require a little digging. For Windows: Ctrl and Shift, then press + may work depending on the application your are using. Mac OS X it may work by pressing the Command, Option, Shift  and + keys. To recreate it with R Markdown, you type:\n\n$a^{2} + b^{2} = c^{2}$\n\nand when you knit your document, it produces the familiar notation. But additionally with R you can have R do the calculation for you, by setting the variables a and b and defining the calculation. Then if the variables change, the calculation will also change when the document is passed to KNIT again. The code can be entered like this in an R Markdown document:\n\n\\`\\`\\`{r echo=FALSE}\na<-2\nb<-4\n\\`\\`\\`\n\n$a =$ \\`r a\\`, $b =$ \\`r b\\`, $c =$ ? ## Display values\n\n$a^{2} + b^{2} = c^{2}$ ## Display formula to plug values into\n\n$a^{2} + b^{2} =$ \\`r a^2 + b^2\\` ## Show the result of first calculation\n\n\\`r a^2 + b^2\\` = $c^{2}$ ## Result is equal to $c^{2}$\n\n$sqrt(\\`r a^2 + b^2\\`)$ = \\`r sqrt(a^2 + b^2)\\` ## Get the square root\n\n$c =$ \\`r sqrt(a^2 + b^2)\\` ## Value of the hypothenuse\n\nThe first section defines your a and b values but does not echo that to the document.  The resulting Markdown document looks like this (without the comments): [![](http://edpflager.com/wp-content/uploads/2018/11/theorumResults-300x274.png)](http://edpflager.com/wp-content/uploads/2018/11/theorumResults.png)\n\n### Variance and Standard Deviation of a Sample\n\nLet's do one more example, calculating the variance and standard deviation of a sample data set. As a refresher, both measures tell us how spread out our data set is, but are expressed in different ways. Variance measures the average degree each data point differs from the mean of the elements in the  data set. Standard deviation is the square root of the variance and restores it to the same unit of measure as the original data set. There as slight differences in calculating variance and standard deviation depending on whether you are working with the entire population or just a sample of data. The mathematical notation for a variance can be shown in an R Markdown document using the formula below and is displayed in the second figure:\n\n$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/variance.png)](http://edpflager.com/wp-content/uploads/2018/11/variance.png) Showing the steps in our work, we need to define our sample data set and calculate the variance and standard deviation with built-in R functions to use as a check later in our document.\n\nsm <- c(4,5,6,7,8,9)\nvariance(sm) = 3.5 \nstandard deviation(sm) = 1.8708287\n\nWe go through several other steps in the R Markdown document to define the components that lead to the variance and standard deviation outcome. The full code for this section is at the end of this post, but here is a sample of the PDF output that incorporates text, inline R code, R code chunks and formulas: [![](http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed-1024x296.png)](http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed.png)\n\n* * *\n\n\\---\ntitle: \"MathFormulas\"\noutput:\n   pdf\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nWorks in PDF, Word or HTML (generally)\n\nAll equations have to be enclosed between dollar ($) signs. If you have matching ones, in R Studio, by default they will turn red unless you use them inline. You can also have formulas in line, like this $sqrt(2)$ , or on a separate line like this:\n\n$sqrt(2)$\n\nSome examples:\n$2^{2}$\n\n$sum\\_(z=c)^{a}$\n\n$a^{2} + b^{2} = c^{2}$\n\n$frac{5}{3}$\n\nIf you want the formula to be larger and stand out from the text, you can use presentation mode, by enclosing your formula with two dollar signs on either side. It will also center the text in the output:\n\n$$sum$$\n\n\n\\`\\`\\`{r echo=FALSE}\na<-2 ## define side a\nb<-4 ## define side b\n\\`\\`\\`\n\nPythagorean theorem example using R formula nominclature\n\n$a =$ \\`r a\\`, $b =$ \\`r b\\`, $c =$ ? \n\n$a^{2} + b^{2} = c^{2}$\n\n$a^{2} + b^{2} =$ \\`r a^2 + b^2\\`\n\n\\`r a^2 + b^2\\` = $c^{2}$\n\n$sqrt(\\`r a^2 + b^2\\`)$ = \\`r sqrt(a^2 + b^2)\\`\n\n$c =$ \\`r sqrt(a^2 + b^2)\\`\n\n* * *\n\n\\---\ntitle: \"MathFormulas2\"\noutput:\n   pdf\\_document\n---\n\n##Calculate variance for a sample, in mathematical notation or using R code:\n\n$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$ OR $sigma^{2}$ = (sum((sm-mean(sm))^2))/(length(sm)-1)\n\n## Taking the square root of the variance is the standard deviation for the sample\n\n$sigma$ = $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\n\n##Define a sample data set and calculate the variance and standard deviation with built-in R functions as a check on our calculations.\n\\`\\`\\`{r echo=FALSE} \nsm <- c(4,5,6,7,8,9)\n\nsm\\_mean <- mean(sm)\n\nsm\\_size <- length(sm) - 1\n\nsm\\_sum = sum((sm-mean(sm))^2)\n\n\\`\\`\\`\n\nsm <- c(4,5,6,7,8,9)\n\nvariance(sm) = \\`r var(sm)\\` and standard deviation(sm) = \\`r sd(sm)\\`\n\n## Now calculate the various parts, starting with the sample mean: $bar{x}$ \n(mean(sm)) = \\`r mean(sm)\\`\n\n## Get the total number of dataset elements minus one: n-1 \nsm\\_size = length(sm)- 1 = \\`r length(sm)-1\\`\n\n## Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$ \nsm\\_sum = sum((sm-mean(sm))^2) = \\`r sum((sm-mean(sm))^2)\\`\n\n## Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$ \n(sm\\_sum / sm\\_size) = \\`r sm\\_sum / sm\\_size\\`\n\n## Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\nsqrt((sm\\_sum / sm\\_size)) = \\`r sqrt((sm\\_sum / sm\\_size))\\`\n\n## Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.\n(sum((sm-mean(sm))^2))/(length(sm)-1) = \\`r (sum((sm-mean(sm))^2))/(length(sm)-1)\\`","source":"_posts/formulas-in-r-markdown.md","raw":"---\ntitle: Formulas in R Markdown\ntags:\n  - How-to\n  - R Markdown\n  - technical\nid: '4131'\ncategories:\n  - - Business Intelligence\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-11-06 18:02:07\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)R Markdown is used to create documents to show the analysis that data was put through and the results of that analysis in a one document. But what if you need to include the formula notation for a calculation in your document? While the common format for a calculation may be well known and understood, reproducing it may be difficult because of mathematical symbols that aren't generally included in a computer font set. In the interest of thoroughness though, the notation should be included. I will preface the rest of this article with this note: some of this works with HTML, Word and PDF. However if you have nested formula notations like I do on the Variance section, some of it may not work properly when using Word output.\n\n### Pythagorean theorem\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/theorem-266x300.png)](http://edpflager.com/wp-content/uploads/2018/11/theorem.png)Let me illustrate first with a common formula used in geometry: the Pythagorean theorem. In its simplest form, it states that for a right triangle, summing the squares of the sides of the triangle adjacent to the right angle equals the square of the side opposite the right angle (hypotenuse). As a refresher, this is usually notated as I have shown here. For the most part this is a fairly easy formula to reproduce on a computer keyboard. The letters are no problem, the + and = symbols are readily accessible. However including the superscripts that represent the square of the values a, b and c may require a little digging. For Windows: Ctrl and Shift, then press + may work depending on the application your are using. Mac OS X it may work by pressing the Command, Option, Shift  and + keys. To recreate it with R Markdown, you type:\n\n$a^{2} + b^{2} = c^{2}$\n\nand when you knit your document, it produces the familiar notation. But additionally with R you can have R do the calculation for you, by setting the variables a and b and defining the calculation. Then if the variables change, the calculation will also change when the document is passed to KNIT again. The code can be entered like this in an R Markdown document:\n\n\\`\\`\\`{r echo=FALSE}\na<-2\nb<-4\n\\`\\`\\`\n\n$a =$ \\`r a\\`, $b =$ \\`r b\\`, $c =$ ? ## Display values\n\n$a^{2} + b^{2} = c^{2}$ ## Display formula to plug values into\n\n$a^{2} + b^{2} =$ \\`r a^2 + b^2\\` ## Show the result of first calculation\n\n\\`r a^2 + b^2\\` = $c^{2}$ ## Result is equal to $c^{2}$\n\n$sqrt(\\`r a^2 + b^2\\`)$ = \\`r sqrt(a^2 + b^2)\\` ## Get the square root\n\n$c =$ \\`r sqrt(a^2 + b^2)\\` ## Value of the hypothenuse\n\nThe first section defines your a and b values but does not echo that to the document.  The resulting Markdown document looks like this (without the comments): [![](http://edpflager.com/wp-content/uploads/2018/11/theorumResults-300x274.png)](http://edpflager.com/wp-content/uploads/2018/11/theorumResults.png)\n\n### Variance and Standard Deviation of a Sample\n\nLet's do one more example, calculating the variance and standard deviation of a sample data set. As a refresher, both measures tell us how spread out our data set is, but are expressed in different ways. Variance measures the average degree each data point differs from the mean of the elements in the  data set. Standard deviation is the square root of the variance and restores it to the same unit of measure as the original data set. There as slight differences in calculating variance and standard deviation depending on whether you are working with the entire population or just a sample of data. The mathematical notation for a variance can be shown in an R Markdown document using the formula below and is displayed in the second figure:\n\n$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/variance.png)](http://edpflager.com/wp-content/uploads/2018/11/variance.png) Showing the steps in our work, we need to define our sample data set and calculate the variance and standard deviation with built-in R functions to use as a check later in our document.\n\nsm <- c(4,5,6,7,8,9)\nvariance(sm) = 3.5 \nstandard deviation(sm) = 1.8708287\n\nWe go through several other steps in the R Markdown document to define the components that lead to the variance and standard deviation outcome. The full code for this section is at the end of this post, but here is a sample of the PDF output that incorporates text, inline R code, R code chunks and formulas: [![](http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed-1024x296.png)](http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed.png)\n\n* * *\n\n\\---\ntitle: \"MathFormulas\"\noutput:\n   pdf\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\nWorks in PDF, Word or HTML (generally)\n\nAll equations have to be enclosed between dollar ($) signs. If you have matching ones, in R Studio, by default they will turn red unless you use them inline. You can also have formulas in line, like this $sqrt(2)$ , or on a separate line like this:\n\n$sqrt(2)$\n\nSome examples:\n$2^{2}$\n\n$sum\\_(z=c)^{a}$\n\n$a^{2} + b^{2} = c^{2}$\n\n$frac{5}{3}$\n\nIf you want the formula to be larger and stand out from the text, you can use presentation mode, by enclosing your formula with two dollar signs on either side. It will also center the text in the output:\n\n$$sum$$\n\n\n\\`\\`\\`{r echo=FALSE}\na<-2 ## define side a\nb<-4 ## define side b\n\\`\\`\\`\n\nPythagorean theorem example using R formula nominclature\n\n$a =$ \\`r a\\`, $b =$ \\`r b\\`, $c =$ ? \n\n$a^{2} + b^{2} = c^{2}$\n\n$a^{2} + b^{2} =$ \\`r a^2 + b^2\\`\n\n\\`r a^2 + b^2\\` = $c^{2}$\n\n$sqrt(\\`r a^2 + b^2\\`)$ = \\`r sqrt(a^2 + b^2)\\`\n\n$c =$ \\`r sqrt(a^2 + b^2)\\`\n\n* * *\n\n\\---\ntitle: \"MathFormulas2\"\noutput:\n   pdf\\_document\n---\n\n##Calculate variance for a sample, in mathematical notation or using R code:\n\n$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$ OR $sigma^{2}$ = (sum((sm-mean(sm))^2))/(length(sm)-1)\n\n## Taking the square root of the variance is the standard deviation for the sample\n\n$sigma$ = $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\n\n##Define a sample data set and calculate the variance and standard deviation with built-in R functions as a check on our calculations.\n\\`\\`\\`{r echo=FALSE} \nsm <- c(4,5,6,7,8,9)\n\nsm\\_mean <- mean(sm)\n\nsm\\_size <- length(sm) - 1\n\nsm\\_sum = sum((sm-mean(sm))^2)\n\n\\`\\`\\`\n\nsm <- c(4,5,6,7,8,9)\n\nvariance(sm) = \\`r var(sm)\\` and standard deviation(sm) = \\`r sd(sm)\\`\n\n## Now calculate the various parts, starting with the sample mean: $bar{x}$ \n(mean(sm)) = \\`r mean(sm)\\`\n\n## Get the total number of dataset elements minus one: n-1 \nsm\\_size = length(sm)- 1 = \\`r length(sm)-1\\`\n\n## Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$ \nsm\\_sum = sum((sm-mean(sm))^2) = \\`r sum((sm-mean(sm))^2)\\`\n\n## Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$ \n(sm\\_sum / sm\\_size) = \\`r sm\\_sum / sm\\_size\\`\n\n## Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\nsqrt((sm\\_sum / sm\\_size)) = \\`r sqrt((sm\\_sum / sm\\_size))\\`\n\n## Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.\n(sum((sm-mean(sm))^2))/(length(sm)-1) = \\`r (sum((sm-mean(sm))^2))/(length(sm)-1)\\`","slug":"formulas-in-r-markdown","published":1,"updated":"2020-08-23T20:54:35.182Z","layout":"post","photos":[],"link":"","_id":"ckeaq99yu0038sdjxhnid1pl7","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>R Markdown is used to create documents to show the analysis that data was put through and the results of that analysis in a one document. But what if you need to include the formula notation for a calculation in your document? While the common format for a calculation may be well known and understood, reproducing it may be difficult because of mathematical symbols that aren’t generally included in a computer font set. In the interest of thoroughness though, the notation should be included. I will preface the rest of this article with this note: some of this works with HTML, Word and PDF. However if you have nested formula notations like I do on the Variance section, some of it may not work properly when using Word output.</p>\n<h3 id=\"Pythagorean-theorem\"><a href=\"#Pythagorean-theorem\" class=\"headerlink\" title=\"Pythagorean theorem\"></a>Pythagorean theorem</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/theorem.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/theorem-266x300.png\"></a>Let me illustrate first with a common formula used in geometry: the Pythagorean theorem. In its simplest form, it states that for a right triangle, summing the squares of the sides of the triangle adjacent to the right angle equals the square of the side opposite the right angle (hypotenuse). As a refresher, this is usually notated as I have shown here. For the most part this is a fairly easy formula to reproduce on a computer keyboard. The letters are no problem, the + and = symbols are readily accessible. However including the superscripts that represent the square of the values a, b and c may require a little digging. For Windows: Ctrl and Shift, then press + may work depending on the application your are using. Mac OS X it may work by pressing the Command, Option, Shift  and + keys. To recreate it with R Markdown, you type:</p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>and when you knit your document, it produces the familiar notation. But additionally with R you can have R do the calculation for you, by setting the variables a and b and defining the calculation. Then if the variables change, the calculation will also change when the document is passed to KNIT again. The code can be entered like this in an R Markdown document:</p>\n<p>```{r echo=FALSE}<br>a&lt;-2<br>b&lt;-4<br>```</p>\n<p>$a =$ `r a`, $b =$ `r b`, $c =$ ? ## Display values</p>\n<p>$a^{2} + b^{2} = c^{2}$ ## Display formula to plug values into</p>\n<p>$a^{2} + b^{2} =$ `r a^2 + b^2` ## Show the result of first calculation</p>\n<p>`r a^2 + b^2` = $c^{2}$ ## Result is equal to $c^{2}$</p>\n<p>$sqrt(`r a^2 + b^2`)$ = `r sqrt(a^2 + b^2)` ## Get the square root</p>\n<p>$c =$ `r sqrt(a^2 + b^2)` ## Value of the hypothenuse</p>\n<p>The first section defines your a and b values but does not echo that to the document.  The resulting Markdown document looks like this (without the comments): <a href=\"http://edpflager.com/wp-content/uploads/2018/11/theorumResults.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/theorumResults-300x274.png\"></a></p>\n<h3 id=\"Variance-and-Standard-Deviation-of-a-Sample\"><a href=\"#Variance-and-Standard-Deviation-of-a-Sample\" class=\"headerlink\" title=\"Variance and Standard Deviation of a Sample\"></a>Variance and Standard Deviation of a Sample</h3><p>Let’s do one more example, calculating the variance and standard deviation of a sample data set. As a refresher, both measures tell us how spread out our data set is, but are expressed in different ways. Variance measures the average degree each data point differs from the mean of the elements in the  data set. Standard deviation is the square root of the variance and restores it to the same unit of measure as the original data set. There as slight differences in calculating variance and standard deviation depending on whether you are working with the entire population or just a sample of data. The mathematical notation for a variance can be shown in an R Markdown document using the formula below and is displayed in the second figure:</p>\n<p>$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/variance.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/variance.png\"></a> Showing the steps in our work, we need to define our sample data set and calculate the variance and standard deviation with built-in R functions to use as a check later in our document.</p>\n<p>sm &lt;- c(4,5,6,7,8,9)<br>variance(sm) = 3.5<br>standard deviation(sm) = 1.8708287</p>\n<p>We go through several other steps in the R Markdown document to define the components that lead to the variance and standard deviation outcome. The full code for this section is at the end of this post, but here is a sample of the PDF output that incorporates text, inline R code, R code chunks and formulas: <a href=\"http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed-1024x296.png\"></a></p>\n<hr>\n<p>-–<br>title: “MathFormulas”<br>output:<br>   pdf_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>Works in PDF, Word or HTML (generally)</p>\n<p>All equations have to be enclosed between dollar ($) signs. If you have matching ones, in R Studio, by default they will turn red unless you use them inline. You can also have formulas in line, like this $sqrt(2)$ , or on a separate line like this:</p>\n<p>$sqrt(2)$</p>\n<p>Some examples:<br>$2^{2}$</p>\n<p>$sum_(z=c)^{a}$</p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>$frac{5}{3}$</p>\n<p>If you want the formula to be larger and stand out from the text, you can use presentation mode, by enclosing your formula with two dollar signs on either side. It will also center the text in the output:</p>\n<p>$$sum$$</p>\n<p>```{r echo=FALSE}<br>a&lt;-2 ## define side a<br>b&lt;-4 ## define side b<br>```</p>\n<p>Pythagorean theorem example using R formula nominclature</p>\n<p>$a =$ `r a`, $b =$ `r b`, $c =$ ? </p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>$a^{2} + b^{2} =$ `r a^2 + b^2`</p>\n<p>`r a^2 + b^2` = $c^{2}$</p>\n<p>$sqrt(`r a^2 + b^2`)$ = `r sqrt(a^2 + b^2)`</p>\n<p>$c =$ `r sqrt(a^2 + b^2)`</p>\n<hr>\n<p>-–<br>title: “MathFormulas2”<br>output:<br>   pdf_document</p>\n<hr>\n<p>##Calculate variance for a sample, in mathematical notation or using R code:</p>\n<p>$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$ OR $sigma^{2}$ = (sum((sm-mean(sm))^2))/(length(sm)-1)</p>\n<h2 id=\"Taking-the-square-root-of-the-variance-is-the-standard-deviation-for-the-sample\"><a href=\"#Taking-the-square-root-of-the-variance-is-the-standard-deviation-for-the-sample\" class=\"headerlink\" title=\"Taking the square root of the variance is the standard deviation for the sample\"></a>Taking the square root of the variance is the standard deviation for the sample</h2><p>$sigma$ = $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$</p>\n<p>##Define a sample data set and calculate the variance and standard deviation with built-in R functions as a check on our calculations.<br>```{r echo=FALSE}<br>sm &lt;- c(4,5,6,7,8,9)</p>\n<p>sm_mean &lt;- mean(sm)</p>\n<p>sm_size &lt;- length(sm) - 1</p>\n<p>sm_sum = sum((sm-mean(sm))^2)</p>\n<p>```</p>\n<p>sm &lt;- c(4,5,6,7,8,9)</p>\n<p>variance(sm) = `r var(sm)` and standard deviation(sm) = `r sd(sm)`</p>\n<h2 id=\"Now-calculate-the-various-parts-starting-with-the-sample-mean-bar-x\"><a href=\"#Now-calculate-the-various-parts-starting-with-the-sample-mean-bar-x\" class=\"headerlink\" title=\"Now calculate the various parts, starting with the sample mean: $bar{x}$\"></a>Now calculate the various parts, starting with the sample mean: $bar{x}$</h2><p>(mean(sm)) = `r mean(sm)`</p>\n<h2 id=\"Get-the-total-number-of-dataset-elements-minus-one-n-1\"><a href=\"#Get-the-total-number-of-dataset-elements-minus-one-n-1\" class=\"headerlink\" title=\"Get the total number of dataset elements minus one: n-1\"></a>Get the total number of dataset elements minus one: n-1</h2><p>sm_size = length(sm)- 1 = `r length(sm)-1`</p>\n<h2 id=\"Find-the-difference-between-the-data-set-mean-and-each-element-square-it-and-sum-them-sum-x-bar-x-2\"><a href=\"#Find-the-difference-between-the-data-set-mean-and-each-element-square-it-and-sum-them-sum-x-bar-x-2\" class=\"headerlink\" title=\"Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$\"></a>Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$</h2><p>sm_sum = sum((sm-mean(sm))^2) = `r sum((sm-mean(sm))^2)`</p>\n<h2 id=\"Calculate-the-variance-using-the-values-frac-sum-x-bar-x-2-n-1\"><a href=\"#Calculate-the-variance-using-the-values-frac-sum-x-bar-x-2-n-1\" class=\"headerlink\" title=\"Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$\"></a>Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$</h2><p>(sm_sum / sm_size) = `r sm_sum / sm_size`</p>\n<h2 id=\"Standard-Deviation-equals-the-square-root-of-the-variance-sqrtfrac-sum-x-bar-x-2-n-1\"><a href=\"#Standard-Deviation-equals-the-square-root-of-the-variance-sqrtfrac-sum-x-bar-x-2-n-1\" class=\"headerlink\" title=\"Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\"></a>Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$</h2><p>sqrt((sm_sum / sm_size)) = `r sqrt((sm_sum / sm_size))`</p>\n<h2 id=\"Putting-it-all-together-into-one-line-of-R-code-we-see-that-the-variance-is-the-same-using-the-built-in-function-or-by-calculating-the-various-components\"><a href=\"#Putting-it-all-together-into-one-line-of-R-code-we-see-that-the-variance-is-the-same-using-the-built-in-function-or-by-calculating-the-various-components\" class=\"headerlink\" title=\"Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.\"></a>Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.</h2><p>(sum((sm-mean(sm))^2))/(length(sm)-1) = `r (sum((sm-mean(sm))^2))/(length(sm)-1)`</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>R Markdown is used to create documents to show the analysis that data was put through and the results of that analysis in a one document. But what if you need to include the formula notation for a calculation in your document? While the common format for a calculation may be well known and understood, reproducing it may be difficult because of mathematical symbols that aren’t generally included in a computer font set. In the interest of thoroughness though, the notation should be included. I will preface the rest of this article with this note: some of this works with HTML, Word and PDF. However if you have nested formula notations like I do on the Variance section, some of it may not work properly when using Word output.</p>\n<h3 id=\"Pythagorean-theorem\"><a href=\"#Pythagorean-theorem\" class=\"headerlink\" title=\"Pythagorean theorem\"></a>Pythagorean theorem</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/theorem.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/theorem-266x300.png\"></a>Let me illustrate first with a common formula used in geometry: the Pythagorean theorem. In its simplest form, it states that for a right triangle, summing the squares of the sides of the triangle adjacent to the right angle equals the square of the side opposite the right angle (hypotenuse). As a refresher, this is usually notated as I have shown here. For the most part this is a fairly easy formula to reproduce on a computer keyboard. The letters are no problem, the + and = symbols are readily accessible. However including the superscripts that represent the square of the values a, b and c may require a little digging. For Windows: Ctrl and Shift, then press + may work depending on the application your are using. Mac OS X it may work by pressing the Command, Option, Shift  and + keys. To recreate it with R Markdown, you type:</p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>and when you knit your document, it produces the familiar notation. But additionally with R you can have R do the calculation for you, by setting the variables a and b and defining the calculation. Then if the variables change, the calculation will also change when the document is passed to KNIT again. The code can be entered like this in an R Markdown document:</p>\n<p>```{r echo=FALSE}<br>a&lt;-2<br>b&lt;-4<br>```</p>\n<p>$a =$ `r a`, $b =$ `r b`, $c =$ ? ## Display values</p>\n<p>$a^{2} + b^{2} = c^{2}$ ## Display formula to plug values into</p>\n<p>$a^{2} + b^{2} =$ `r a^2 + b^2` ## Show the result of first calculation</p>\n<p>`r a^2 + b^2` = $c^{2}$ ## Result is equal to $c^{2}$</p>\n<p>$sqrt(`r a^2 + b^2`)$ = `r sqrt(a^2 + b^2)` ## Get the square root</p>\n<p>$c =$ `r sqrt(a^2 + b^2)` ## Value of the hypothenuse</p>\n<p>The first section defines your a and b values but does not echo that to the document.  The resulting Markdown document looks like this (without the comments): <a href=\"http://edpflager.com/wp-content/uploads/2018/11/theorumResults.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/theorumResults-300x274.png\"></a></p>\n<h3 id=\"Variance-and-Standard-Deviation-of-a-Sample\"><a href=\"#Variance-and-Standard-Deviation-of-a-Sample\" class=\"headerlink\" title=\"Variance and Standard Deviation of a Sample\"></a>Variance and Standard Deviation of a Sample</h3><p>Let’s do one more example, calculating the variance and standard deviation of a sample data set. As a refresher, both measures tell us how spread out our data set is, but are expressed in different ways. Variance measures the average degree each data point differs from the mean of the elements in the  data set. Standard deviation is the square root of the variance and restores it to the same unit of measure as the original data set. There as slight differences in calculating variance and standard deviation depending on whether you are working with the entire population or just a sample of data. The mathematical notation for a variance can be shown in an R Markdown document using the formula below and is displayed in the second figure:</p>\n<p>$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/variance.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/variance.png\"></a> Showing the steps in our work, we need to define our sample data set and calculate the variance and standard deviation with built-in R functions to use as a check later in our document.</p>\n<p>sm &lt;- c(4,5,6,7,8,9)<br>variance(sm) = 3.5<br>standard deviation(sm) = 1.8708287</p>\n<p>We go through several other steps in the R Markdown document to define the components that lead to the variance and standard deviation outcome. The full code for this section is at the end of this post, but here is a sample of the PDF output that incorporates text, inline R code, R code chunks and formulas: <a href=\"http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/VarianceMixed-1024x296.png\"></a></p>\n<hr>\n<p>-–<br>title: “MathFormulas”<br>output:<br>   pdf_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br>Works in PDF, Word or HTML (generally)</p>\n<p>All equations have to be enclosed between dollar ($) signs. If you have matching ones, in R Studio, by default they will turn red unless you use them inline. You can also have formulas in line, like this $sqrt(2)$ , or on a separate line like this:</p>\n<p>$sqrt(2)$</p>\n<p>Some examples:<br>$2^{2}$</p>\n<p>$sum_(z=c)^{a}$</p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>$frac{5}{3}$</p>\n<p>If you want the formula to be larger and stand out from the text, you can use presentation mode, by enclosing your formula with two dollar signs on either side. It will also center the text in the output:</p>\n<p>$$sum$$</p>\n<p>```{r echo=FALSE}<br>a&lt;-2 ## define side a<br>b&lt;-4 ## define side b<br>```</p>\n<p>Pythagorean theorem example using R formula nominclature</p>\n<p>$a =$ `r a`, $b =$ `r b`, $c =$ ? </p>\n<p>$a^{2} + b^{2} = c^{2}$</p>\n<p>$a^{2} + b^{2} =$ `r a^2 + b^2`</p>\n<p>`r a^2 + b^2` = $c^{2}$</p>\n<p>$sqrt(`r a^2 + b^2`)$ = `r sqrt(a^2 + b^2)`</p>\n<p>$c =$ `r sqrt(a^2 + b^2)`</p>\n<hr>\n<p>-–<br>title: “MathFormulas2”<br>output:<br>   pdf_document</p>\n<hr>\n<p>##Calculate variance for a sample, in mathematical notation or using R code:</p>\n<p>$sigma^{2}$ = $frac{sum((x - bar{x})^{2})}{n-1}$ OR $sigma^{2}$ = (sum((sm-mean(sm))^2))/(length(sm)-1)</p>\n<h2 id=\"Taking-the-square-root-of-the-variance-is-the-standard-deviation-for-the-sample\"><a href=\"#Taking-the-square-root-of-the-variance-is-the-standard-deviation-for-the-sample\" class=\"headerlink\" title=\"Taking the square root of the variance is the standard deviation for the sample\"></a>Taking the square root of the variance is the standard deviation for the sample</h2><p>$sigma$ = $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$</p>\n<p>##Define a sample data set and calculate the variance and standard deviation with built-in R functions as a check on our calculations.<br>```{r echo=FALSE}<br>sm &lt;- c(4,5,6,7,8,9)</p>\n<p>sm_mean &lt;- mean(sm)</p>\n<p>sm_size &lt;- length(sm) - 1</p>\n<p>sm_sum = sum((sm-mean(sm))^2)</p>\n<p>```</p>\n<p>sm &lt;- c(4,5,6,7,8,9)</p>\n<p>variance(sm) = `r var(sm)` and standard deviation(sm) = `r sd(sm)`</p>\n<h2 id=\"Now-calculate-the-various-parts-starting-with-the-sample-mean-bar-x\"><a href=\"#Now-calculate-the-various-parts-starting-with-the-sample-mean-bar-x\" class=\"headerlink\" title=\"Now calculate the various parts, starting with the sample mean: $bar{x}$\"></a>Now calculate the various parts, starting with the sample mean: $bar{x}$</h2><p>(mean(sm)) = `r mean(sm)`</p>\n<h2 id=\"Get-the-total-number-of-dataset-elements-minus-one-n-1\"><a href=\"#Get-the-total-number-of-dataset-elements-minus-one-n-1\" class=\"headerlink\" title=\"Get the total number of dataset elements minus one: n-1\"></a>Get the total number of dataset elements minus one: n-1</h2><p>sm_size = length(sm)- 1 = `r length(sm)-1`</p>\n<h2 id=\"Find-the-difference-between-the-data-set-mean-and-each-element-square-it-and-sum-them-sum-x-bar-x-2\"><a href=\"#Find-the-difference-between-the-data-set-mean-and-each-element-square-it-and-sum-them-sum-x-bar-x-2\" class=\"headerlink\" title=\"Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$\"></a>Find the difference between the data set mean and each element, square it, and sum them: $sum((x - bar{x})^{2})$</h2><p>sm_sum = sum((sm-mean(sm))^2) = `r sum((sm-mean(sm))^2)`</p>\n<h2 id=\"Calculate-the-variance-using-the-values-frac-sum-x-bar-x-2-n-1\"><a href=\"#Calculate-the-variance-using-the-values-frac-sum-x-bar-x-2-n-1\" class=\"headerlink\" title=\"Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$\"></a>Calculate the variance using the values: $frac{sum((x - bar{x})^{2})}{n-1}$</h2><p>(sm_sum / sm_size) = `r sm_sum / sm_size`</p>\n<h2 id=\"Standard-Deviation-equals-the-square-root-of-the-variance-sqrtfrac-sum-x-bar-x-2-n-1\"><a href=\"#Standard-Deviation-equals-the-square-root-of-the-variance-sqrtfrac-sum-x-bar-x-2-n-1\" class=\"headerlink\" title=\"Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$\"></a>Standard Deviation equals the square root of the variance: $sqrtfrac{sum((x - bar{x})^{2})}{n-1}$</h2><p>sqrt((sm_sum / sm_size)) = `r sqrt((sm_sum / sm_size))`</p>\n<h2 id=\"Putting-it-all-together-into-one-line-of-R-code-we-see-that-the-variance-is-the-same-using-the-built-in-function-or-by-calculating-the-various-components\"><a href=\"#Putting-it-all-together-into-one-line-of-R-code-we-see-that-the-variance-is-the-same-using-the-built-in-function-or-by-calculating-the-various-components\" class=\"headerlink\" title=\"Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.\"></a>Putting it all together into one line of R code, we see that the variance is the same using the built in function or by calculating the various components.</h2><p>(sum((sm-mean(sm))^2))/(length(sm)-1) = `r (sum((sm-mean(sm))^2))/(length(sm)-1)`</p>\n"},{"title":"Getting started with Kettle's Insert/Update function","id":"1818","comments":0,"date":"2014-02-27T21:36:15.000Z","_content":"\n[![MySQL](http://edpflager.com/wp-content/uploads/2014/02/MySQL-300x133.jpg)](http://edpflager.com/wp-content/uploads/2014/02/MySQL.jpg)When using an ETL tool, you often create a workflow that will run multiple times, picking up new and changed records. Getting just those changes is pretty easy using Kettle's Insert/Update tool. (BTW - Kettle is one component in the Pentaho Data Integration application - PDI for short).\n\n### **Assumptions and requirements**\n\nFor this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We'll be creating a a sample database based on one originally created by [Fusheng Wang and Carlo Zaniolo](https://launchpad.net/test-db/) at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly).\n<!-- more -->\nDROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept\\_no CHAR(4) NOT NULL, dept\\_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept\\_no), UNIQUE KEY (dept\\_name) );\n\nINSERT INTO \\`departments\\` VALUES ('d001','Marketing'), ('d002','Finance'), ('d003','Human Resources'), ('d004','Produktion'), ('d005','Development');\n\nOnce you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department.\n\ndept\\_no;dept\\_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service\n\n**Getting Started**\n\n1.  Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.\n2.  Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.\n3.  From under the Output node, drag an Insert/Update step onto the canvas.\n4.  Connect the two, by holding down your Shift key and click on the Input step. Drag over to the output step and release your mouse. The results should look like this:![insert-update-transform](http://edpflager.com/wp-content/uploads/2014/02/insert-update-transform-300x97.jpg)\n\n5\\. Double click the Text file in put step, and on the first tab - File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table, like this.\n\n[![inputfilelocation](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg)](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg)\n\n 6. At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:\n\n![showfile](http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg)\n\n7\\. Close the Preview data window, and double click on the Insert/Update icon. We need to populate the information here to tell Pentaho how to handle our incoming data. We need to specify:\n\n*   the database connection: \"Sample\"\n*   the target table: \"departments\"\n*   whether to perform updates or not. Check the box to only add new records to your database.\n*   the key(s) to look up the value(s): dept\\_no = dept\\_no. Because we are using the same field names in the input and output streams we can have the same values here. You can use multiple input fields to define your table field, and you can choose different comparison operators as well. Click the Get fields button if you would like to have the fields populated for you. Warning - this will get all of the fields not just primary key ones.\n*   In the bottom table, you specify where data coming in from your source gets written to in your destination. If you have a lot of fields in your tables, use the Get update fields button to populate the table. You can always delete items. If you want fields to be updated when changed data comes in, make sure the Update column in this table is set to Y. Change it to N for those fields you don't want to change.\n*   The results should look like this:[![insert-updatewindow](http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow-259x300.jpg)](http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow.jpg)\n\n 8. Click OK to return to the canvas. Click the run button to run your transformation, and  if you did everything correctly, you should see these results: [![transform_results](http://edpflager.com/wp-content/uploads/2014/02/transform_results-300x24.jpg)](http://edpflager.com/wp-content/uploads/2014/02/transform_results.jpg) Notice that the Input was five rows, but the Output only says 4. The difference is in the Updated field, where there is one record. Pentaho added four new records to your table, and updated one. Verify the results by switching back to MySQL and doing a Select on the table. You should see the four new departments, and the updated Production department with the correct spelling. If your source database has a timestamp field, cast it as a date or a time field to be able to update it in your destination table","source":"_posts/getting-started-with-kettles-insertupdate-function.md","raw":"---\ntitle: Getting started with Kettle's Insert/Update function\ntags:\n  - ETL\n  - kettle\n  - PDI\nid: '1818'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-02-27 16:36:15\n---\n\n[![MySQL](http://edpflager.com/wp-content/uploads/2014/02/MySQL-300x133.jpg)](http://edpflager.com/wp-content/uploads/2014/02/MySQL.jpg)When using an ETL tool, you often create a workflow that will run multiple times, picking up new and changed records. Getting just those changes is pretty easy using Kettle's Insert/Update tool. (BTW - Kettle is one component in the Pentaho Data Integration application - PDI for short).\n\n### **Assumptions and requirements**\n\nFor this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We'll be creating a a sample database based on one originally created by [Fusheng Wang and Carlo Zaniolo](https://launchpad.net/test-db/) at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly).\n<!-- more -->\nDROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept\\_no CHAR(4) NOT NULL, dept\\_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept\\_no), UNIQUE KEY (dept\\_name) );\n\nINSERT INTO \\`departments\\` VALUES ('d001','Marketing'), ('d002','Finance'), ('d003','Human Resources'), ('d004','Produktion'), ('d005','Development');\n\nOnce you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department.\n\ndept\\_no;dept\\_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service\n\n**Getting Started**\n\n1.  Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.\n2.  Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.\n3.  From under the Output node, drag an Insert/Update step onto the canvas.\n4.  Connect the two, by holding down your Shift key and click on the Input step. Drag over to the output step and release your mouse. The results should look like this:![insert-update-transform](http://edpflager.com/wp-content/uploads/2014/02/insert-update-transform-300x97.jpg)\n\n5\\. Double click the Text file in put step, and on the first tab - File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table, like this.\n\n[![inputfilelocation](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg)](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg)\n\n 6. At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:\n\n![showfile](http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg)\n\n7\\. Close the Preview data window, and double click on the Insert/Update icon. We need to populate the information here to tell Pentaho how to handle our incoming data. We need to specify:\n\n*   the database connection: \"Sample\"\n*   the target table: \"departments\"\n*   whether to perform updates or not. Check the box to only add new records to your database.\n*   the key(s) to look up the value(s): dept\\_no = dept\\_no. Because we are using the same field names in the input and output streams we can have the same values here. You can use multiple input fields to define your table field, and you can choose different comparison operators as well. Click the Get fields button if you would like to have the fields populated for you. Warning - this will get all of the fields not just primary key ones.\n*   In the bottom table, you specify where data coming in from your source gets written to in your destination. If you have a lot of fields in your tables, use the Get update fields button to populate the table. You can always delete items. If you want fields to be updated when changed data comes in, make sure the Update column in this table is set to Y. Change it to N for those fields you don't want to change.\n*   The results should look like this:[![insert-updatewindow](http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow-259x300.jpg)](http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow.jpg)\n\n 8. Click OK to return to the canvas. Click the run button to run your transformation, and  if you did everything correctly, you should see these results: [![transform_results](http://edpflager.com/wp-content/uploads/2014/02/transform_results-300x24.jpg)](http://edpflager.com/wp-content/uploads/2014/02/transform_results.jpg) Notice that the Input was five rows, but the Output only says 4. The difference is in the Updated field, where there is one record. Pentaho added four new records to your table, and updated one. Verify the results by switching back to MySQL and doing a Select on the table. You should see the four new departments, and the updated Production department with the correct spelling. If your source database has a timestamp field, cast it as a date or a time field to be able to update it in your destination table","slug":"getting-started-with-kettles-insertupdate-function","published":1,"updated":"2020-08-23T20:54:34.790Z","layout":"post","photos":[],"link":"","_id":"ckeaq99yz003csdjx70udhs2n","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/MySQL.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/MySQL-300x133.jpg\" alt=\"MySQL\"></a>When using an ETL tool, you often create a workflow that will run multiple times, picking up new and changed records. Getting just those changes is pretty easy using Kettle’s Insert/Update tool. (BTW - Kettle is one component in the Pentaho Data Integration application - PDI for short).</p>\n<h3 id=\"Assumptions-and-requirements\"><a href=\"#Assumptions-and-requirements\" class=\"headerlink\" title=\"Assumptions and requirements\"></a><strong>Assumptions and requirements</strong></h3><p>For this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by <a href=\"https://launchpad.net/test-db/\">Fusheng Wang and Carlo Zaniolo</a> at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly).</p>\n<a id=\"more\"></a>\n<p>DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept_no CHAR(4) NOT NULL, dept_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept_no), UNIQUE KEY (dept_name) );</p>\n<p>INSERT INTO `departments` VALUES (‘d001’,’Marketing’), (‘d002’,’Finance’), (‘d003’,’Human Resources’), (‘d004’,’Produktion’), (‘d005’,’Development’);</p>\n<p>Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department.</p>\n<p>dept_no;dept_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service</p>\n<p><strong>Getting Started</strong></p>\n<ol>\n<li>Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.</li>\n<li>Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.</li>\n<li>From under the Output node, drag an Insert/Update step onto the canvas.</li>\n<li>Connect the two, by holding down your Shift key and click on the Input step. Drag over to the output step and release your mouse. The results should look like this:<img src=\"http://edpflager.com/wp-content/uploads/2014/02/insert-update-transform-300x97.jpg\" alt=\"insert-update-transform\"></li>\n</ol>\n<p>5. Double click the Text file in put step, and on the first tab - File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table, like this.</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg\" alt=\"inputfilelocation\"></a></p>\n<p> 6. At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:</p>\n<p><img src=\"http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg\" alt=\"showfile\"></p>\n<p>7. Close the Preview data window, and double click on the Insert/Update icon. We need to populate the information here to tell Pentaho how to handle our incoming data. We need to specify:</p>\n<ul>\n<li><p>the database connection: “Sample”</p>\n</li>\n<li><p>the target table: “departments”</p>\n</li>\n<li><p>whether to perform updates or not. Check the box to only add new records to your database.</p>\n</li>\n<li><p>the key(s) to look up the value(s): dept_no = dept_no. Because we are using the same field names in the input and output streams we can have the same values here. You can use multiple input fields to define your table field, and you can choose different comparison operators as well. Click the Get fields button if you would like to have the fields populated for you. Warning - this will get all of the fields not just primary key ones.</p>\n</li>\n<li><p>In the bottom table, you specify where data coming in from your source gets written to in your destination. If you have a lot of fields in your tables, use the Get update fields button to populate the table. You can always delete items. If you want fields to be updated when changed data comes in, make sure the Update column in this table is set to Y. Change it to N for those fields you don’t want to change.</p>\n</li>\n<li><p>The results should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow-259x300.jpg\" alt=\"insert-updatewindow\"></a></p>\n<ol start=\"8\">\n<li>Click OK to return to the canvas. Click the run button to run your transformation, and  if you did everything correctly, you should see these results: <a href=\"http://edpflager.com/wp-content/uploads/2014/02/transform_results.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/transform_results-300x24.jpg\" alt=\"transform_results\"></a> Notice that the Input was five rows, but the Output only says 4. The difference is in the Updated field, where there is one record. Pentaho added four new records to your table, and updated one. Verify the results by switching back to MySQL and doing a Select on the table. You should see the four new departments, and the updated Production department with the correct spelling. If your source database has a timestamp field, cast it as a date or a time field to be able to update it in your destination table</li>\n</ol>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/MySQL.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/MySQL-300x133.jpg\" alt=\"MySQL\"></a>When using an ETL tool, you often create a workflow that will run multiple times, picking up new and changed records. Getting just those changes is pretty easy using Kettle’s Insert/Update tool. (BTW - Kettle is one component in the Pentaho Data Integration application - PDI for short).</p>\n<h3 id=\"Assumptions-and-requirements\"><a href=\"#Assumptions-and-requirements\" class=\"headerlink\" title=\"Assumptions and requirements\"></a><strong>Assumptions and requirements</strong></h3><p>For this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by <a href=\"https://launchpad.net/test-db/\">Fusheng Wang and Carlo Zaniolo</a> at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly).</p>","more":"<p>DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept_no CHAR(4) NOT NULL, dept_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept_no), UNIQUE KEY (dept_name) );</p>\n<p>INSERT INTO `departments` VALUES (‘d001’,’Marketing’), (‘d002’,’Finance’), (‘d003’,’Human Resources’), (‘d004’,’Produktion’), (‘d005’,’Development’);</p>\n<p>Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department.</p>\n<p>dept_no;dept_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service</p>\n<p><strong>Getting Started</strong></p>\n<ol>\n<li>Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.</li>\n<li>Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.</li>\n<li>From under the Output node, drag an Insert/Update step onto the canvas.</li>\n<li>Connect the two, by holding down your Shift key and click on the Input step. Drag over to the output step and release your mouse. The results should look like this:<img src=\"http://edpflager.com/wp-content/uploads/2014/02/insert-update-transform-300x97.jpg\" alt=\"insert-update-transform\"></li>\n</ol>\n<p>5. Double click the Text file in put step, and on the first tab - File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table, like this.</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg\" alt=\"inputfilelocation\"></a></p>\n<p> 6. At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:</p>\n<p><img src=\"http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg\" alt=\"showfile\"></p>\n<p>7. Close the Preview data window, and double click on the Insert/Update icon. We need to populate the information here to tell Pentaho how to handle our incoming data. We need to specify:</p>\n<ul>\n<li><p>the database connection: “Sample”</p>\n</li>\n<li><p>the target table: “departments”</p>\n</li>\n<li><p>whether to perform updates or not. Check the box to only add new records to your database.</p>\n</li>\n<li><p>the key(s) to look up the value(s): dept_no = dept_no. Because we are using the same field names in the input and output streams we can have the same values here. You can use multiple input fields to define your table field, and you can choose different comparison operators as well. Click the Get fields button if you would like to have the fields populated for you. Warning - this will get all of the fields not just primary key ones.</p>\n</li>\n<li><p>In the bottom table, you specify where data coming in from your source gets written to in your destination. If you have a lot of fields in your tables, use the Get update fields button to populate the table. You can always delete items. If you want fields to be updated when changed data comes in, make sure the Update column in this table is set to Y. Change it to N for those fields you don’t want to change.</p>\n</li>\n<li><p>The results should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/insert-updatewindow-259x300.jpg\" alt=\"insert-updatewindow\"></a></p>\n<ol start=\"8\">\n<li>Click OK to return to the canvas. Click the run button to run your transformation, and  if you did everything correctly, you should see these results: <a href=\"http://edpflager.com/wp-content/uploads/2014/02/transform_results.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/transform_results-300x24.jpg\" alt=\"transform_results\"></a> Notice that the Input was five rows, but the Output only says 4. The difference is in the Updated field, where there is one record. Pentaho added four new records to your table, and updated one. Verify the results by switching back to MySQL and doing a Select on the table. You should see the four new departments, and the updated Production department with the correct spelling. If your source database has a timestamp field, cast it as a date or a time field to be able to update it in your destination table</li>\n</ol>\n</li>\n</ul>"},{"title":"Graph Database Visualizations in R","id":"4446","comments":0,"date":"2019-03-01T08:52:39.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png)](http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png)Its been a couple of weeks since my last post, and I've been experimenting more with graph databases. One of the more common graph database examples explores the relationship between air traffic carriers, and their routes. The nodes are the various cities that carriers fly between, routes are the edges connecting the nodes, and the number of carriers or the flights per day servicing those routes can be a value for the edge. For my data exploration, I started with the latest data from the [Bureau of Transportation Statistics website](https://www.bts.gov/), to get a data set of US airports, and the [latest breakdown of US air carriers](https://www.transtats.bts.gov/Tables.asp?DB_ID=110&DB_Name=Air%20Carrier%20Statistics%20%28Form%2041%20Traffic%29-%20%20U.S.%20Carriers&DB_Short_Name=Air%20Carriers). I uploaded the raw data to a SQL Server to create tables - a set of Graph database tables and a set of normal tables. Working with an assumption that a carrier who had a route out of city has a corresponding route into a city, I created queries to narrow down the returned data to only routes from Cleveland-Hopkins International and Detroit Wayne County airports. (As an aside, population for the Cleveland Metro area is about 2.0 million people, and for Detroit 4.3 million which looks to play into the results.\n<!-- more -->\nMy first stab at visualizing this data was similar to what I did previously with DiagrammeR.  I loaded  the data from SQL Server into two data frames and added edges and nodes to a graph object. I did encounter some issues, since I am running SQL Server 2017 for Linux and R Studio on a laptop with only 4GB or RAM, where if I attempted to graph more that 30 nodes, my code would error out. Limiting the output to routes with more than 2 carriers allowed me to generate this visualization: [![](http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline-1024x619.png)](http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline.png) The center node is the Cleveland airport with the US Department of Transportation's three letter abbreviation. Surrounding it is a dandelion effect showing the various nodes that have a direct connection. Those nodes are also labeled with the destinations three letter abbreviation.\n\n### ISSUES WITH THIS VISUALIZATION\n\nI wasn't happy with visualization because it has a number of issues. First the abbreviations are difficult to decipher unless you are familiar with them. JFK and LGA are John Kennedy and LaGuardia airports in New York respectively, DTW is Detroit, but after that I have to look them up. Second, the placement of the airport nodes is somewhat random. If you run the code multiple times, it generates different placements. Third, there is no designation for routes with more traffic. Looking at the chart at  the top, you can see that there are eight carriers running between Cleveland and Chicago (ORD), but the edge line is no different  that the one to Raleigh/Durham which has three carriers servicing it. While I could have added a label or tool tip to the edge line to indicate the route count, and the same to the destination nodes, I wanted more geographic information as well. Unfortunately, DiagrammeR does not supply that  type of graph visualization. In the interest of completeness, here is the code to generate the dandelion.\n\nlibrary c(odbc)\nlibrary(DiagrammeR)\nlibrary(dplyr)\nlibrary(maps)\n\n## Connect to the Database server\ncon <- dbConnect(odbc::odbc(), \n                 .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Union of Cleveland airport information so its included in Airport list.\n###### ADD in the ID code from the dataset and join on that. \nairports <- \n    dbGetQuery(con, 'SET nocount on;\n                     SELECT ait.AIRPORT\\_ID, \n                            LEFT(ait.AIRPORT,3) as AIRPORT\\_CODE, \n                            ait.AIRPORT\\_ID as DESTINATION\\_ID,\n                            ait.DISPLAY\\_AIRPORT\\_NAME, \n                            ait.DISPLAY\\_AIRPORT\\_CITY\\_NAME\\_FULL,\n                            ait.LATITUDE,\n                            ait.LONGITUDE,\n                            ait.AIRPORT\\_STATE\\_NAME\n                     FROM Routes rt\n                     JOIN Airports ai ON rt.$from\\_id = ai.$node\\_id\n                     JOIN Airports ait ON rt.$to\\_id = ait.$node\\_id\n                     WHERE ai.AIRPORT\\_ID = 11042 and rt.CarrierCount > 2\n                     UNION ALL \n                     SELECT top 1 ai2.AIRPORT\\_ID,\n                            LEFT(ai2.AIRPORT,3) as AIRPORT\\_CODE ,\n                            ai2.AIRPORT\\_ID as DESTINATION\\_ID,\n                            ai2.DISPLAY\\_AIRPORT\\_NAME, \n                            ai2.DISPLAY\\_AIRPORT\\_CITY\\_NAME\\_FULL,\n                            ai2.LATITUDE,\n                            ai2.LONGITUDE,\n                            ai2.AIRPORT\\_STATE\\_NAME\n                       FROM Routes rt2\n                       JOIN Airports ai2 ON rt2.$from\\_id = ai2.$node\\_id\n                      WHERE ai2.AIRPORT\\_ID = 11042;')\n\n###### ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.\n## Get Routes edge data \nroutes <-\n     dbGetQuery(con, 'SET nocount on;\n                      SELECT TOP 30 ait.AIRPORT\\_ID as TO\\_COL,\n                             ai.AIRPORT\\_ID as FROM\\_COL,\n                             CarrierCount\n                      FROM Routes rt\n                      JOIN Airports ai ON rt.$from\\_id = ai.$node\\_id\n                      JOIN Airports ait ON rt.$to\\_id = ait.$node\\_id\n                      WHERE ai.AIRPORT\\_ID = 11042 and rt.CarrierCount > 2;')\n\ngraph <-\n     create\\_graph() %>%\n      add\\_nodes\\_from\\_table( table = airports, \n               label\\_col = AIRPORT\\_CODE) %>%\n                    add\\_edges\\_from\\_table(\n                       table = routes, \n                       from\\_col = FROM\\_COL, \n                       to\\_col= TO\\_COL,\n                       from\\_to\\_map = AIRPORT\\_ID)\n\nrender\\_graph(graph, layout = \"nicely\")   \n\n### MAPS visualization\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/CLERoutes-1024x620.png)](http://edpflager.com/wp-content/uploads/2019/03/CLERoutes.png)   I like this visual much better than the dandelion. It shows the location of the origin node relative to the destination nodes, and by coloring only the states where there are destinations, you can see that there are clusters where you can't get a direct flight from Cleveland. Unlike the first visual, this one remains static so whenever you run it with the same dataset, you will get the same visual. There are still some things I don't like with this visual, though. The destination nodes don't indicate the city, and the lines don't indicate the number of carriers servicing those routes. I did attempt to incorporate the plotly package into this graph to provide tooltips, but it does not work with the geom\\_curve aesthetic in ggplot2. I'd also like to use parameters to make the code reusable. You would be prompted to select a city and then the visual would update from that the location. Although this is not complete, the code as it currently looks is below. I expect to be working longer hours on a work project so I wanted to get this out before too much time has passed between posts.\n\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(odbc)\nlibrary(dplyr)\n\n## Connect to the Database server \ncon <- dbConnect(odbc::odbc(), \n           .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.\n## Union of Cleveland airport information so its included in Airport list.\n\n###### This is the destination data set. \nairports <- \n     dbGetQuery(con, 'SET nocount on;\n                      SELECT DISTANCE, \n                             DEST\\_CITY\\_NAME, \n                             aid.DISPLAY\\_AIRPORT\\_NAME as DEST\\_AIRPORT, \n                             aid.LATITUDE as DEST\\_LATITUDE, \n                             aid.LONGITUDE as DEST\\_LONGITUDE,\n                             aid.\\[Age (in years)\\] as DEST\\_AIRPORT\\_AGE,\n                             aid.AIRPORT\\_STATE\\_NAME,\n                             aio.LATITUDE as ORIG\\_LATITUDE, \n                             aio.LONGITUDE as ORIG\\_LONGITUDE, \n                             aio.\\[Age (in years)\\] as ORIG\\_AIRPORT\\_AGE, \n                             sum(CarrierCount) as TotalRoute\n                        FROM Routes2018 rte \n                        JOIN USAirports aid ON rte.DEST\\_AIRPORT\\_ID = aid.AIRPORT\\_ID \n                        JOIN USAirports aio ON rte.ORIGIN\\_AIRPORT\\_ID = aio.AIRPORT\\_ID\n                       WHERE ORIGIN\\_AIRPORT\\_ID = 11042\n                         AND aid.AIRPORT\\_IS\\_LATEST = 1\n                         AND aio.AIRPORT\\_IS\\_LATEST = 1\n                         AND class = 'F'\n                         AND aid.AIRPORT\\_STATE\\_NAME NOT IN ('Hawaii', 'Puerto Rico', 'Alaska')\n                    GROUP BY DISTANCE, DEST\\_CITY\\_NAME, aid.DISPLAY\\_AIRPORT\\_NAME, aid.LATITUDE, \n                         aid.LONGITUDE, aid.\\[Age (in years)\\], aid.AIRPORT\\_STATE\\_NAME, \n                         aio.LATITUDE, aio.LONGITUDE, aio.\\[Age (in years)\\];')\n\n###### This is the origin data set. Use a parameter to generate this?\norig\\_airport <- \n     dbGetQuery(con, 'SET nocount on;\n                      SELECT LATITUDE as ORIG\\_LATITUDE, \n                             LONGITUDE as ORIG\\_LONGITUDE, \n                             \\[Age (in years)\\] as ORIG\\_AIRPORT\\_AGE\n                        FROM USAirports \n                       WHERE AIRPORT\\_ID = 11042\n                         AND AIRPORT\\_IS\\_LATEST = 1;')\n## Call the US States map\nusa <- map\\_data(\"state\")\n\n## Convert State names into a character vector. Have to convert names to lower case.\nstate\\_names <- unique(airports$AIRPORT\\_STATE\\_NAME) %>% \n     lapply(tolower)\n\n## Create a seperate data frame of destination states. This allows ggplot2 and maps to color them\ndestinations <- subset(usa, region %in% state\\_names)\n\n## Generate the visualization\nggplot(data = destinations) + \n       geom\\_polygon(aes(x = long, y = lat, group = group), fill = \"lightsteelblue\", color = \"black\") + \n       coord\\_fixed(1.3) + geom\\_point(data = airports, aes(x= DEST\\_LONGITUDE, y=DEST\\_LATITUDE), shape = 10, color = \"midnightblue\", size = 2) +\n       geom\\_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = \"black\") + \n       geom\\_curve(data=airports, aes(x=ORIG\\_LONGITUDE, y=ORIG\\_LATITUDE, xend=DEST\\_LONGITUDE, yend=DEST\\_LATITUDE), curvature = 0.2, color=\"red\") + \n       geom\\_point(data=orig\\_airport, aes(x=ORIG\\_LONGITUDE, y=ORIG\\_LATITUDE), shape = 8, color=\"red\", size = 5)\n\n  That's all I have for now. As I spend some more time on this, I will post a followup.","source":"_posts/graph-database-visualizations-in-r.md","raw":"---\ntitle: Graph Database Visualizations in R\ntags:\n  - ggplot2\n  - graph database\n  - howto\n  - SQL Server\nid: '4446'\ncategories:\n  - - Big Data\n  - - Business Intelligence\n  - - Linux\n  - - Misc\n  - - R\ncomments: false\ndate: 2019-03-01 03:52:39\n---\n\n[![](http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png)](http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png)Its been a couple of weeks since my last post, and I've been experimenting more with graph databases. One of the more common graph database examples explores the relationship between air traffic carriers, and their routes. The nodes are the various cities that carriers fly between, routes are the edges connecting the nodes, and the number of carriers or the flights per day servicing those routes can be a value for the edge. For my data exploration, I started with the latest data from the [Bureau of Transportation Statistics website](https://www.bts.gov/), to get a data set of US airports, and the [latest breakdown of US air carriers](https://www.transtats.bts.gov/Tables.asp?DB_ID=110&DB_Name=Air%20Carrier%20Statistics%20%28Form%2041%20Traffic%29-%20%20U.S.%20Carriers&DB_Short_Name=Air%20Carriers). I uploaded the raw data to a SQL Server to create tables - a set of Graph database tables and a set of normal tables. Working with an assumption that a carrier who had a route out of city has a corresponding route into a city, I created queries to narrow down the returned data to only routes from Cleveland-Hopkins International and Detroit Wayne County airports. (As an aside, population for the Cleveland Metro area is about 2.0 million people, and for Detroit 4.3 million which looks to play into the results.\n<!-- more -->\nMy first stab at visualizing this data was similar to what I did previously with DiagrammeR.  I loaded  the data from SQL Server into two data frames and added edges and nodes to a graph object. I did encounter some issues, since I am running SQL Server 2017 for Linux and R Studio on a laptop with only 4GB or RAM, where if I attempted to graph more that 30 nodes, my code would error out. Limiting the output to routes with more than 2 carriers allowed me to generate this visualization: [![](http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline-1024x619.png)](http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline.png) The center node is the Cleveland airport with the US Department of Transportation's three letter abbreviation. Surrounding it is a dandelion effect showing the various nodes that have a direct connection. Those nodes are also labeled with the destinations three letter abbreviation.\n\n### ISSUES WITH THIS VISUALIZATION\n\nI wasn't happy with visualization because it has a number of issues. First the abbreviations are difficult to decipher unless you are familiar with them. JFK and LGA are John Kennedy and LaGuardia airports in New York respectively, DTW is Detroit, but after that I have to look them up. Second, the placement of the airport nodes is somewhat random. If you run the code multiple times, it generates different placements. Third, there is no designation for routes with more traffic. Looking at the chart at  the top, you can see that there are eight carriers running between Cleveland and Chicago (ORD), but the edge line is no different  that the one to Raleigh/Durham which has three carriers servicing it. While I could have added a label or tool tip to the edge line to indicate the route count, and the same to the destination nodes, I wanted more geographic information as well. Unfortunately, DiagrammeR does not supply that  type of graph visualization. In the interest of completeness, here is the code to generate the dandelion.\n\nlibrary c(odbc)\nlibrary(DiagrammeR)\nlibrary(dplyr)\nlibrary(maps)\n\n## Connect to the Database server\ncon <- dbConnect(odbc::odbc(), \n                 .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Union of Cleveland airport information so its included in Airport list.\n###### ADD in the ID code from the dataset and join on that. \nairports <- \n    dbGetQuery(con, 'SET nocount on;\n                     SELECT ait.AIRPORT\\_ID, \n                            LEFT(ait.AIRPORT,3) as AIRPORT\\_CODE, \n                            ait.AIRPORT\\_ID as DESTINATION\\_ID,\n                            ait.DISPLAY\\_AIRPORT\\_NAME, \n                            ait.DISPLAY\\_AIRPORT\\_CITY\\_NAME\\_FULL,\n                            ait.LATITUDE,\n                            ait.LONGITUDE,\n                            ait.AIRPORT\\_STATE\\_NAME\n                     FROM Routes rt\n                     JOIN Airports ai ON rt.$from\\_id = ai.$node\\_id\n                     JOIN Airports ait ON rt.$to\\_id = ait.$node\\_id\n                     WHERE ai.AIRPORT\\_ID = 11042 and rt.CarrierCount > 2\n                     UNION ALL \n                     SELECT top 1 ai2.AIRPORT\\_ID,\n                            LEFT(ai2.AIRPORT,3) as AIRPORT\\_CODE ,\n                            ai2.AIRPORT\\_ID as DESTINATION\\_ID,\n                            ai2.DISPLAY\\_AIRPORT\\_NAME, \n                            ai2.DISPLAY\\_AIRPORT\\_CITY\\_NAME\\_FULL,\n                            ai2.LATITUDE,\n                            ai2.LONGITUDE,\n                            ai2.AIRPORT\\_STATE\\_NAME\n                       FROM Routes rt2\n                       JOIN Airports ai2 ON rt2.$from\\_id = ai2.$node\\_id\n                      WHERE ai2.AIRPORT\\_ID = 11042;')\n\n###### ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.\n## Get Routes edge data \nroutes <-\n     dbGetQuery(con, 'SET nocount on;\n                      SELECT TOP 30 ait.AIRPORT\\_ID as TO\\_COL,\n                             ai.AIRPORT\\_ID as FROM\\_COL,\n                             CarrierCount\n                      FROM Routes rt\n                      JOIN Airports ai ON rt.$from\\_id = ai.$node\\_id\n                      JOIN Airports ait ON rt.$to\\_id = ait.$node\\_id\n                      WHERE ai.AIRPORT\\_ID = 11042 and rt.CarrierCount > 2;')\n\ngraph <-\n     create\\_graph() %>%\n      add\\_nodes\\_from\\_table( table = airports, \n               label\\_col = AIRPORT\\_CODE) %>%\n                    add\\_edges\\_from\\_table(\n                       table = routes, \n                       from\\_col = FROM\\_COL, \n                       to\\_col= TO\\_COL,\n                       from\\_to\\_map = AIRPORT\\_ID)\n\nrender\\_graph(graph, layout = \"nicely\")   \n\n### MAPS visualization\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/CLERoutes-1024x620.png)](http://edpflager.com/wp-content/uploads/2019/03/CLERoutes.png)   I like this visual much better than the dandelion. It shows the location of the origin node relative to the destination nodes, and by coloring only the states where there are destinations, you can see that there are clusters where you can't get a direct flight from Cleveland. Unlike the first visual, this one remains static so whenever you run it with the same dataset, you will get the same visual. There are still some things I don't like with this visual, though. The destination nodes don't indicate the city, and the lines don't indicate the number of carriers servicing those routes. I did attempt to incorporate the plotly package into this graph to provide tooltips, but it does not work with the geom\\_curve aesthetic in ggplot2. I'd also like to use parameters to make the code reusable. You would be prompted to select a city and then the visual would update from that the location. Although this is not complete, the code as it currently looks is below. I expect to be working longer hours on a work project so I wanted to get this out before too much time has passed between posts.\n\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(odbc)\nlibrary(dplyr)\n\n## Connect to the Database server \ncon <- dbConnect(odbc::odbc(), \n           .connection\\_string = \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=\\*\\*\\*\\*\\*\\*\\*\\*;\")\n\n## Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.\n## Union of Cleveland airport information so its included in Airport list.\n\n###### This is the destination data set. \nairports <- \n     dbGetQuery(con, 'SET nocount on;\n                      SELECT DISTANCE, \n                             DEST\\_CITY\\_NAME, \n                             aid.DISPLAY\\_AIRPORT\\_NAME as DEST\\_AIRPORT, \n                             aid.LATITUDE as DEST\\_LATITUDE, \n                             aid.LONGITUDE as DEST\\_LONGITUDE,\n                             aid.\\[Age (in years)\\] as DEST\\_AIRPORT\\_AGE,\n                             aid.AIRPORT\\_STATE\\_NAME,\n                             aio.LATITUDE as ORIG\\_LATITUDE, \n                             aio.LONGITUDE as ORIG\\_LONGITUDE, \n                             aio.\\[Age (in years)\\] as ORIG\\_AIRPORT\\_AGE, \n                             sum(CarrierCount) as TotalRoute\n                        FROM Routes2018 rte \n                        JOIN USAirports aid ON rte.DEST\\_AIRPORT\\_ID = aid.AIRPORT\\_ID \n                        JOIN USAirports aio ON rte.ORIGIN\\_AIRPORT\\_ID = aio.AIRPORT\\_ID\n                       WHERE ORIGIN\\_AIRPORT\\_ID = 11042\n                         AND aid.AIRPORT\\_IS\\_LATEST = 1\n                         AND aio.AIRPORT\\_IS\\_LATEST = 1\n                         AND class = 'F'\n                         AND aid.AIRPORT\\_STATE\\_NAME NOT IN ('Hawaii', 'Puerto Rico', 'Alaska')\n                    GROUP BY DISTANCE, DEST\\_CITY\\_NAME, aid.DISPLAY\\_AIRPORT\\_NAME, aid.LATITUDE, \n                         aid.LONGITUDE, aid.\\[Age (in years)\\], aid.AIRPORT\\_STATE\\_NAME, \n                         aio.LATITUDE, aio.LONGITUDE, aio.\\[Age (in years)\\];')\n\n###### This is the origin data set. Use a parameter to generate this?\norig\\_airport <- \n     dbGetQuery(con, 'SET nocount on;\n                      SELECT LATITUDE as ORIG\\_LATITUDE, \n                             LONGITUDE as ORIG\\_LONGITUDE, \n                             \\[Age (in years)\\] as ORIG\\_AIRPORT\\_AGE\n                        FROM USAirports \n                       WHERE AIRPORT\\_ID = 11042\n                         AND AIRPORT\\_IS\\_LATEST = 1;')\n## Call the US States map\nusa <- map\\_data(\"state\")\n\n## Convert State names into a character vector. Have to convert names to lower case.\nstate\\_names <- unique(airports$AIRPORT\\_STATE\\_NAME) %>% \n     lapply(tolower)\n\n## Create a seperate data frame of destination states. This allows ggplot2 and maps to color them\ndestinations <- subset(usa, region %in% state\\_names)\n\n## Generate the visualization\nggplot(data = destinations) + \n       geom\\_polygon(aes(x = long, y = lat, group = group), fill = \"lightsteelblue\", color = \"black\") + \n       coord\\_fixed(1.3) + geom\\_point(data = airports, aes(x= DEST\\_LONGITUDE, y=DEST\\_LATITUDE), shape = 10, color = \"midnightblue\", size = 2) +\n       geom\\_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = \"black\") + \n       geom\\_curve(data=airports, aes(x=ORIG\\_LONGITUDE, y=ORIG\\_LATITUDE, xend=DEST\\_LONGITUDE, yend=DEST\\_LATITUDE), curvature = 0.2, color=\"red\") + \n       geom\\_point(data=orig\\_airport, aes(x=ORIG\\_LONGITUDE, y=ORIG\\_LATITUDE), shape = 8, color=\"red\", size = 5)\n\n  That's all I have for now. As I spend some more time on this, I will post a followup.","slug":"graph-database-visualizations-in-r","published":1,"updated":"2020-08-23T20:54:35.230Z","layout":"post","photos":[],"link":"","_id":"ckeaq99z4003gsdjx9hz71dd0","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png\"></a>Its been a couple of weeks since my last post, and I’ve been experimenting more with graph databases. One of the more common graph database examples explores the relationship between air traffic carriers, and their routes. The nodes are the various cities that carriers fly between, routes are the edges connecting the nodes, and the number of carriers or the flights per day servicing those routes can be a value for the edge. For my data exploration, I started with the latest data from the <a href=\"https://www.bts.gov/\">Bureau of Transportation Statistics website</a>, to get a data set of US airports, and the <a href=\"https://www.transtats.bts.gov/Tables.asp?DB_ID=110&DB_Name=Air%20Carrier%20Statistics%20(Form%2041%20Traffic)-%20%20U.S.%20Carriers&DB_Short_Name=Air%20Carriers\">latest breakdown of US air carriers</a>. I uploaded the raw data to a SQL Server to create tables - a set of Graph database tables and a set of normal tables. Working with an assumption that a carrier who had a route out of city has a corresponding route into a city, I created queries to narrow down the returned data to only routes from Cleveland-Hopkins International and Detroit Wayne County airports. (As an aside, population for the Cleveland Metro area is about 2.0 million people, and for Detroit 4.3 million which looks to play into the results.</p>\n<a id=\"more\"></a>\n<p>My first stab at visualizing this data was similar to what I did previously with DiagrammeR.  I loaded  the data from SQL Server into two data frames and added edges and nodes to a graph object. I did encounter some issues, since I am running SQL Server 2017 for Linux and R Studio on a laptop with only 4GB or RAM, where if I attempted to graph more that 30 nodes, my code would error out. Limiting the output to routes with more than 2 carriers allowed me to generate this visualization: <a href=\"http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline-1024x619.png\"></a> The center node is the Cleveland airport with the US Department of Transportation’s three letter abbreviation. Surrounding it is a dandelion effect showing the various nodes that have a direct connection. Those nodes are also labeled with the destinations three letter abbreviation.</p>\n<h3 id=\"ISSUES-WITH-THIS-VISUALIZATION\"><a href=\"#ISSUES-WITH-THIS-VISUALIZATION\" class=\"headerlink\" title=\"ISSUES WITH THIS VISUALIZATION\"></a>ISSUES WITH THIS VISUALIZATION</h3><p>I wasn’t happy with visualization because it has a number of issues. First the abbreviations are difficult to decipher unless you are familiar with them. JFK and LGA are John Kennedy and LaGuardia airports in New York respectively, DTW is Detroit, but after that I have to look them up. Second, the placement of the airport nodes is somewhat random. If you run the code multiple times, it generates different placements. Third, there is no designation for routes with more traffic. Looking at the chart at  the top, you can see that there are eight carriers running between Cleveland and Chicago (ORD), but the edge line is no different  that the one to Raleigh/Durham which has three carriers servicing it. While I could have added a label or tool tip to the edge line to indicate the route count, and the same to the destination nodes, I wanted more geographic information as well. Unfortunately, DiagrammeR does not supply that  type of graph visualization. In the interest of completeness, here is the code to generate the dandelion.</p>\n<p>library c(odbc)<br>library(DiagrammeR)<br>library(dplyr)<br>library(maps)</p>\n<h2 id=\"Connect-to-the-Database-server\"><a href=\"#Connect-to-the-Database-server\" class=\"headerlink\" title=\"Connect to the Database server\"></a>Connect to the Database server</h2><p>con &lt;- dbConnect(odbc::odbc(),<br>                 .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Union-of-Cleveland-airport-information-so-its-included-in-Airport-list\"><a href=\"#Union-of-Cleveland-airport-information-so-its-included-in-Airport-list\" class=\"headerlink\" title=\"Union of Cleveland airport information so its included in Airport list.\"></a>Union of Cleveland airport information so its included in Airport list.</h2><h6 id=\"ADD-in-the-ID-code-from-the-dataset-and-join-on-that\"><a href=\"#ADD-in-the-ID-code-from-the-dataset-and-join-on-that\" class=\"headerlink\" title=\"ADD in the ID code from the dataset and join on that.\"></a>ADD in the ID code from the dataset and join on that.</h6><p>airports &lt;-<br>    dbGetQuery(con, ‘SET nocount on;<br>                     SELECT ait.AIRPORT_ID,<br>                            LEFT(ait.AIRPORT,3) as AIRPORT_CODE,<br>                            ait.AIRPORT_ID as DESTINATION_ID,<br>                            ait.DISPLAY_AIRPORT_NAME,<br>                            ait.DISPLAY_AIRPORT_CITY_NAME_FULL,<br>                            ait.LATITUDE,<br>                            ait.LONGITUDE,<br>                            ait.AIRPORT_STATE_NAME<br>                     FROM Routes rt<br>                     JOIN Airports ai ON rt.$from_id = ai.$node_id<br>                     JOIN Airports ait ON rt.$to_id = ait.$node_id<br>                     WHERE ai.AIRPORT_ID = 11042 and rt.CarrierCount &gt; 2<br>                     UNION ALL<br>                     SELECT top 1 ai2.AIRPORT_ID,<br>                            LEFT(ai2.AIRPORT,3) as AIRPORT_CODE ,<br>                            ai2.AIRPORT_ID as DESTINATION_ID,<br>                            ai2.DISPLAY_AIRPORT_NAME,<br>                            ai2.DISPLAY_AIRPORT_CITY_NAME_FULL,<br>                            ai2.LATITUDE,<br>                            ai2.LONGITUDE,<br>                            ai2.AIRPORT_STATE_NAME<br>                       FROM Routes rt2<br>                       JOIN Airports ai2 ON rt2.$from_id = ai2.$node_id<br>                      WHERE ai2.AIRPORT_ID = 11042;’)</p>\n<h6 id=\"ADD-IN-THE-FROM-AND-TO-AIRPORT-ID-CODES-and-JOIN-ON-THOSE\"><a href=\"#ADD-IN-THE-FROM-AND-TO-AIRPORT-ID-CODES-and-JOIN-ON-THOSE\" class=\"headerlink\" title=\"ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.\"></a>ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.</h6><h2 id=\"Get-Routes-edge-data\"><a href=\"#Get-Routes-edge-data\" class=\"headerlink\" title=\"Get Routes edge data\"></a>Get Routes edge data</h2><p>routes &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT TOP 30 ait.AIRPORT_ID as TO_COL,<br>                             ai.AIRPORT_ID as FROM_COL,<br>                             CarrierCount<br>                      FROM Routes rt<br>                      JOIN Airports ai ON rt.$from_id = ai.$node_id<br>                      JOIN Airports ait ON rt.$to_id = ait.$node_id<br>                      WHERE ai.AIRPORT_ID = 11042 and rt.CarrierCount &gt; 2;’)</p>\n<p>graph &lt;-<br>     create_graph() %&gt;%<br>      add_nodes_from_table( table = airports,<br>               label_col = AIRPORT_CODE) %&gt;%<br>                    add_edges_from_table(<br>                       table = routes,<br>                       from_col = FROM_COL,<br>                       to_col= TO_COL,<br>                       from_to_map = AIRPORT_ID)</p>\n<p>render_graph(graph, layout = “nicely”)   </p>\n<h3 id=\"MAPS-visualization\"><a href=\"#MAPS-visualization\" class=\"headerlink\" title=\"MAPS visualization\"></a>MAPS visualization</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/CLERoutes.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/CLERoutes-1024x620.png\"></a>   I like this visual much better than the dandelion. It shows the location of the origin node relative to the destination nodes, and by coloring only the states where there are destinations, you can see that there are clusters where you can’t get a direct flight from Cleveland. Unlike the first visual, this one remains static so whenever you run it with the same dataset, you will get the same visual. There are still some things I don’t like with this visual, though. The destination nodes don’t indicate the city, and the lines don’t indicate the number of carriers servicing those routes. I did attempt to incorporate the plotly package into this graph to provide tooltips, but it does not work with the geom_curve aesthetic in ggplot2. I’d also like to use parameters to make the code reusable. You would be prompted to select a city and then the visual would update from that the location. Although this is not complete, the code as it currently looks is below. I expect to be working longer hours on a work project so I wanted to get this out before too much time has passed between posts.</p>\n<p>library(ggplot2)<br>library(maps)<br>library(odbc)<br>library(dplyr)</p>\n<h2 id=\"Connect-to-the-Database-server-1\"><a href=\"#Connect-to-the-Database-server-1\" class=\"headerlink\" title=\"Connect to the Database server\"></a>Connect to the Database server</h2><p>con &lt;- dbConnect(odbc::odbc(),<br>           .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Get-Airport-node-data-that-are-destinations-from-selected-airport-could-retrieve-more-data-as-necessary\"><a href=\"#Get-Airport-node-data-that-are-destinations-from-selected-airport-could-retrieve-more-data-as-necessary\" class=\"headerlink\" title=\"Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.\"></a>Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.</h2><h2 id=\"Union-of-Cleveland-airport-information-so-its-included-in-Airport-list-1\"><a href=\"#Union-of-Cleveland-airport-information-so-its-included-in-Airport-list-1\" class=\"headerlink\" title=\"Union of Cleveland airport information so its included in Airport list.\"></a>Union of Cleveland airport information so its included in Airport list.</h2><h6 id=\"This-is-the-destination-data-set\"><a href=\"#This-is-the-destination-data-set\" class=\"headerlink\" title=\"This is the destination data set.\"></a>This is the destination data set.</h6><p>airports &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT DISTANCE,<br>                             DEST_CITY_NAME,<br>                             aid.DISPLAY_AIRPORT_NAME as DEST_AIRPORT,<br>                             aid.LATITUDE as DEST_LATITUDE,<br>                             aid.LONGITUDE as DEST_LONGITUDE,<br>                             aid.[Age (in years)] as DEST_AIRPORT_AGE,<br>                             aid.AIRPORT_STATE_NAME,<br>                             aio.LATITUDE as ORIG_LATITUDE,<br>                             aio.LONGITUDE as ORIG_LONGITUDE,<br>                             aio.[Age (in years)] as ORIG_AIRPORT_AGE,<br>                             sum(CarrierCount) as TotalRoute<br>                        FROM Routes2018 rte<br>                        JOIN USAirports aid ON rte.DEST_AIRPORT_ID = aid.AIRPORT_ID<br>                        JOIN USAirports aio ON rte.ORIGIN_AIRPORT_ID = aio.AIRPORT_ID<br>                       WHERE ORIGIN_AIRPORT_ID = 11042<br>                         AND aid.AIRPORT_IS_LATEST = 1<br>                         AND aio.AIRPORT_IS_LATEST = 1<br>                         AND class = ‘F’<br>                         AND aid.AIRPORT_STATE_NAME NOT IN (‘Hawaii’, ‘Puerto Rico’, ‘Alaska’)<br>                    GROUP BY DISTANCE, DEST_CITY_NAME, aid.DISPLAY_AIRPORT_NAME, aid.LATITUDE,<br>                         aid.LONGITUDE, aid.[Age (in years)], aid.AIRPORT_STATE_NAME,<br>                         aio.LATITUDE, aio.LONGITUDE, aio.[Age (in years)];’)</p>\n<h6 id=\"This-is-the-origin-data-set-Use-a-parameter-to-generate-this\"><a href=\"#This-is-the-origin-data-set-Use-a-parameter-to-generate-this\" class=\"headerlink\" title=\"This is the origin data set. Use a parameter to generate this?\"></a>This is the origin data set. Use a parameter to generate this?</h6><p>orig_airport &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT LATITUDE as ORIG_LATITUDE,<br>                             LONGITUDE as ORIG_LONGITUDE,<br>                             [Age (in years)] as ORIG_AIRPORT_AGE<br>                        FROM USAirports<br>                       WHERE AIRPORT_ID = 11042<br>                         AND AIRPORT_IS_LATEST = 1;’)</p>\n<h2 id=\"Call-the-US-States-map\"><a href=\"#Call-the-US-States-map\" class=\"headerlink\" title=\"Call the US States map\"></a>Call the US States map</h2><p>usa &lt;- map_data(“state”)</p>\n<h2 id=\"Convert-State-names-into-a-character-vector-Have-to-convert-names-to-lower-case\"><a href=\"#Convert-State-names-into-a-character-vector-Have-to-convert-names-to-lower-case\" class=\"headerlink\" title=\"Convert State names into a character vector. Have to convert names to lower case.\"></a>Convert State names into a character vector. Have to convert names to lower case.</h2><p>state_names &lt;- unique(airports$AIRPORT_STATE_NAME) %&gt;%<br>     lapply(tolower)</p>\n<h2 id=\"Create-a-seperate-data-frame-of-destination-states-This-allows-ggplot2-and-maps-to-color-them\"><a href=\"#Create-a-seperate-data-frame-of-destination-states-This-allows-ggplot2-and-maps-to-color-them\" class=\"headerlink\" title=\"Create a seperate data frame of destination states. This allows ggplot2 and maps to color them\"></a>Create a seperate data frame of destination states. This allows ggplot2 and maps to color them</h2><p>destinations &lt;- subset(usa, region %in% state_names)</p>\n<h2 id=\"Generate-the-visualization\"><a href=\"#Generate-the-visualization\" class=\"headerlink\" title=\"Generate the visualization\"></a>Generate the visualization</h2><p>ggplot(data = destinations) +<br>       geom_polygon(aes(x = long, y = lat, group = group), fill = “lightsteelblue”, color = “black”) +<br>       coord_fixed(1.3) + geom_point(data = airports, aes(x= DEST_LONGITUDE, y=DEST_LATITUDE), shape = 10, color = “midnightblue”, size = 2) +<br>       geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = “black”) +<br>       geom_curve(data=airports, aes(x=ORIG_LONGITUDE, y=ORIG_LATITUDE, xend=DEST_LONGITUDE, yend=DEST_LATITUDE), curvature = 0.2, color=”red”) +<br>       geom_point(data=orig_airport, aes(x=ORIG_LONGITUDE, y=ORIG_LATITUDE), shape = 8, color=”red”, size = 5)</p>\n<p>  That’s all I have for now. As I spend some more time on this, I will post a followup.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/02/ClevelandCarriers.png\"></a>Its been a couple of weeks since my last post, and I’ve been experimenting more with graph databases. One of the more common graph database examples explores the relationship between air traffic carriers, and their routes. The nodes are the various cities that carriers fly between, routes are the edges connecting the nodes, and the number of carriers or the flights per day servicing those routes can be a value for the edge. For my data exploration, I started with the latest data from the <a href=\"https://www.bts.gov/\">Bureau of Transportation Statistics website</a>, to get a data set of US airports, and the <a href=\"https://www.transtats.bts.gov/Tables.asp?DB_ID=110&DB_Name=Air%20Carrier%20Statistics%20(Form%2041%20Traffic)-%20%20U.S.%20Carriers&DB_Short_Name=Air%20Carriers\">latest breakdown of US air carriers</a>. I uploaded the raw data to a SQL Server to create tables - a set of Graph database tables and a set of normal tables. Working with an assumption that a carrier who had a route out of city has a corresponding route into a city, I created queries to narrow down the returned data to only routes from Cleveland-Hopkins International and Detroit Wayne County airports. (As an aside, population for the Cleveland Metro area is about 2.0 million people, and for Detroit 4.3 million which looks to play into the results.</p>","more":"<p>My first stab at visualizing this data was similar to what I did previously with DiagrammeR.  I loaded  the data from SQL Server into two data frames and added edges and nodes to a graph object. I did encounter some issues, since I am running SQL Server 2017 for Linux and R Studio on a laptop with only 4GB or RAM, where if I attempted to graph more that 30 nodes, my code would error out. Limiting the output to routes with more than 2 carriers allowed me to generate this visualization: <a href=\"http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/DiagrammerAirline-1024x619.png\"></a> The center node is the Cleveland airport with the US Department of Transportation’s three letter abbreviation. Surrounding it is a dandelion effect showing the various nodes that have a direct connection. Those nodes are also labeled with the destinations three letter abbreviation.</p>\n<h3 id=\"ISSUES-WITH-THIS-VISUALIZATION\"><a href=\"#ISSUES-WITH-THIS-VISUALIZATION\" class=\"headerlink\" title=\"ISSUES WITH THIS VISUALIZATION\"></a>ISSUES WITH THIS VISUALIZATION</h3><p>I wasn’t happy with visualization because it has a number of issues. First the abbreviations are difficult to decipher unless you are familiar with them. JFK and LGA are John Kennedy and LaGuardia airports in New York respectively, DTW is Detroit, but after that I have to look them up. Second, the placement of the airport nodes is somewhat random. If you run the code multiple times, it generates different placements. Third, there is no designation for routes with more traffic. Looking at the chart at  the top, you can see that there are eight carriers running between Cleveland and Chicago (ORD), but the edge line is no different  that the one to Raleigh/Durham which has three carriers servicing it. While I could have added a label or tool tip to the edge line to indicate the route count, and the same to the destination nodes, I wanted more geographic information as well. Unfortunately, DiagrammeR does not supply that  type of graph visualization. In the interest of completeness, here is the code to generate the dandelion.</p>\n<p>library c(odbc)<br>library(DiagrammeR)<br>library(dplyr)<br>library(maps)</p>\n<h2 id=\"Connect-to-the-Database-server\"><a href=\"#Connect-to-the-Database-server\" class=\"headerlink\" title=\"Connect to the Database server\"></a>Connect to the Database server</h2><p>con &lt;- dbConnect(odbc::odbc(),<br>                 .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Union-of-Cleveland-airport-information-so-its-included-in-Airport-list\"><a href=\"#Union-of-Cleveland-airport-information-so-its-included-in-Airport-list\" class=\"headerlink\" title=\"Union of Cleveland airport information so its included in Airport list.\"></a>Union of Cleveland airport information so its included in Airport list.</h2><h6 id=\"ADD-in-the-ID-code-from-the-dataset-and-join-on-that\"><a href=\"#ADD-in-the-ID-code-from-the-dataset-and-join-on-that\" class=\"headerlink\" title=\"ADD in the ID code from the dataset and join on that.\"></a>ADD in the ID code from the dataset and join on that.</h6><p>airports &lt;-<br>    dbGetQuery(con, ‘SET nocount on;<br>                     SELECT ait.AIRPORT_ID,<br>                            LEFT(ait.AIRPORT,3) as AIRPORT_CODE,<br>                            ait.AIRPORT_ID as DESTINATION_ID,<br>                            ait.DISPLAY_AIRPORT_NAME,<br>                            ait.DISPLAY_AIRPORT_CITY_NAME_FULL,<br>                            ait.LATITUDE,<br>                            ait.LONGITUDE,<br>                            ait.AIRPORT_STATE_NAME<br>                     FROM Routes rt<br>                     JOIN Airports ai ON rt.$from_id = ai.$node_id<br>                     JOIN Airports ait ON rt.$to_id = ait.$node_id<br>                     WHERE ai.AIRPORT_ID = 11042 and rt.CarrierCount &gt; 2<br>                     UNION ALL<br>                     SELECT top 1 ai2.AIRPORT_ID,<br>                            LEFT(ai2.AIRPORT,3) as AIRPORT_CODE ,<br>                            ai2.AIRPORT_ID as DESTINATION_ID,<br>                            ai2.DISPLAY_AIRPORT_NAME,<br>                            ai2.DISPLAY_AIRPORT_CITY_NAME_FULL,<br>                            ai2.LATITUDE,<br>                            ai2.LONGITUDE,<br>                            ai2.AIRPORT_STATE_NAME<br>                       FROM Routes rt2<br>                       JOIN Airports ai2 ON rt2.$from_id = ai2.$node_id<br>                      WHERE ai2.AIRPORT_ID = 11042;’)</p>\n<h6 id=\"ADD-IN-THE-FROM-AND-TO-AIRPORT-ID-CODES-and-JOIN-ON-THOSE\"><a href=\"#ADD-IN-THE-FROM-AND-TO-AIRPORT-ID-CODES-and-JOIN-ON-THOSE\" class=\"headerlink\" title=\"ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.\"></a>ADD IN THE FROM AND TO AIRPORT ID CODES and JOIN ON THOSE.</h6><h2 id=\"Get-Routes-edge-data\"><a href=\"#Get-Routes-edge-data\" class=\"headerlink\" title=\"Get Routes edge data\"></a>Get Routes edge data</h2><p>routes &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT TOP 30 ait.AIRPORT_ID as TO_COL,<br>                             ai.AIRPORT_ID as FROM_COL,<br>                             CarrierCount<br>                      FROM Routes rt<br>                      JOIN Airports ai ON rt.$from_id = ai.$node_id<br>                      JOIN Airports ait ON rt.$to_id = ait.$node_id<br>                      WHERE ai.AIRPORT_ID = 11042 and rt.CarrierCount &gt; 2;’)</p>\n<p>graph &lt;-<br>     create_graph() %&gt;%<br>      add_nodes_from_table( table = airports,<br>               label_col = AIRPORT_CODE) %&gt;%<br>                    add_edges_from_table(<br>                       table = routes,<br>                       from_col = FROM_COL,<br>                       to_col= TO_COL,<br>                       from_to_map = AIRPORT_ID)</p>\n<p>render_graph(graph, layout = “nicely”)   </p>\n<h3 id=\"MAPS-visualization\"><a href=\"#MAPS-visualization\" class=\"headerlink\" title=\"MAPS visualization\"></a>MAPS visualization</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/CLERoutes.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/CLERoutes-1024x620.png\"></a>   I like this visual much better than the dandelion. It shows the location of the origin node relative to the destination nodes, and by coloring only the states where there are destinations, you can see that there are clusters where you can’t get a direct flight from Cleveland. Unlike the first visual, this one remains static so whenever you run it with the same dataset, you will get the same visual. There are still some things I don’t like with this visual, though. The destination nodes don’t indicate the city, and the lines don’t indicate the number of carriers servicing those routes. I did attempt to incorporate the plotly package into this graph to provide tooltips, but it does not work with the geom_curve aesthetic in ggplot2. I’d also like to use parameters to make the code reusable. You would be prompted to select a city and then the visual would update from that the location. Although this is not complete, the code as it currently looks is below. I expect to be working longer hours on a work project so I wanted to get this out before too much time has passed between posts.</p>\n<p>library(ggplot2)<br>library(maps)<br>library(odbc)<br>library(dplyr)</p>\n<h2 id=\"Connect-to-the-Database-server-1\"><a href=\"#Connect-to-the-Database-server-1\" class=\"headerlink\" title=\"Connect to the Database server\"></a>Connect to the Database server</h2><p>con &lt;- dbConnect(odbc::odbc(),<br>           .connection_string = “Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=AirTravel;Uid=sa;Pwd=********;”)</p>\n<h2 id=\"Get-Airport-node-data-that-are-destinations-from-selected-airport-could-retrieve-more-data-as-necessary\"><a href=\"#Get-Airport-node-data-that-are-destinations-from-selected-airport-could-retrieve-more-data-as-necessary\" class=\"headerlink\" title=\"Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.\"></a>Get Airport node data that are destinations from selected airport - could retrieve more data as necessary.</h2><h2 id=\"Union-of-Cleveland-airport-information-so-its-included-in-Airport-list-1\"><a href=\"#Union-of-Cleveland-airport-information-so-its-included-in-Airport-list-1\" class=\"headerlink\" title=\"Union of Cleveland airport information so its included in Airport list.\"></a>Union of Cleveland airport information so its included in Airport list.</h2><h6 id=\"This-is-the-destination-data-set\"><a href=\"#This-is-the-destination-data-set\" class=\"headerlink\" title=\"This is the destination data set.\"></a>This is the destination data set.</h6><p>airports &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT DISTANCE,<br>                             DEST_CITY_NAME,<br>                             aid.DISPLAY_AIRPORT_NAME as DEST_AIRPORT,<br>                             aid.LATITUDE as DEST_LATITUDE,<br>                             aid.LONGITUDE as DEST_LONGITUDE,<br>                             aid.[Age (in years)] as DEST_AIRPORT_AGE,<br>                             aid.AIRPORT_STATE_NAME,<br>                             aio.LATITUDE as ORIG_LATITUDE,<br>                             aio.LONGITUDE as ORIG_LONGITUDE,<br>                             aio.[Age (in years)] as ORIG_AIRPORT_AGE,<br>                             sum(CarrierCount) as TotalRoute<br>                        FROM Routes2018 rte<br>                        JOIN USAirports aid ON rte.DEST_AIRPORT_ID = aid.AIRPORT_ID<br>                        JOIN USAirports aio ON rte.ORIGIN_AIRPORT_ID = aio.AIRPORT_ID<br>                       WHERE ORIGIN_AIRPORT_ID = 11042<br>                         AND aid.AIRPORT_IS_LATEST = 1<br>                         AND aio.AIRPORT_IS_LATEST = 1<br>                         AND class = ‘F’<br>                         AND aid.AIRPORT_STATE_NAME NOT IN (‘Hawaii’, ‘Puerto Rico’, ‘Alaska’)<br>                    GROUP BY DISTANCE, DEST_CITY_NAME, aid.DISPLAY_AIRPORT_NAME, aid.LATITUDE,<br>                         aid.LONGITUDE, aid.[Age (in years)], aid.AIRPORT_STATE_NAME,<br>                         aio.LATITUDE, aio.LONGITUDE, aio.[Age (in years)];’)</p>\n<h6 id=\"This-is-the-origin-data-set-Use-a-parameter-to-generate-this\"><a href=\"#This-is-the-origin-data-set-Use-a-parameter-to-generate-this\" class=\"headerlink\" title=\"This is the origin data set. Use a parameter to generate this?\"></a>This is the origin data set. Use a parameter to generate this?</h6><p>orig_airport &lt;-<br>     dbGetQuery(con, ‘SET nocount on;<br>                      SELECT LATITUDE as ORIG_LATITUDE,<br>                             LONGITUDE as ORIG_LONGITUDE,<br>                             [Age (in years)] as ORIG_AIRPORT_AGE<br>                        FROM USAirports<br>                       WHERE AIRPORT_ID = 11042<br>                         AND AIRPORT_IS_LATEST = 1;’)</p>\n<h2 id=\"Call-the-US-States-map\"><a href=\"#Call-the-US-States-map\" class=\"headerlink\" title=\"Call the US States map\"></a>Call the US States map</h2><p>usa &lt;- map_data(“state”)</p>\n<h2 id=\"Convert-State-names-into-a-character-vector-Have-to-convert-names-to-lower-case\"><a href=\"#Convert-State-names-into-a-character-vector-Have-to-convert-names-to-lower-case\" class=\"headerlink\" title=\"Convert State names into a character vector. Have to convert names to lower case.\"></a>Convert State names into a character vector. Have to convert names to lower case.</h2><p>state_names &lt;- unique(airports$AIRPORT_STATE_NAME) %&gt;%<br>     lapply(tolower)</p>\n<h2 id=\"Create-a-seperate-data-frame-of-destination-states-This-allows-ggplot2-and-maps-to-color-them\"><a href=\"#Create-a-seperate-data-frame-of-destination-states-This-allows-ggplot2-and-maps-to-color-them\" class=\"headerlink\" title=\"Create a seperate data frame of destination states. This allows ggplot2 and maps to color them\"></a>Create a seperate data frame of destination states. This allows ggplot2 and maps to color them</h2><p>destinations &lt;- subset(usa, region %in% state_names)</p>\n<h2 id=\"Generate-the-visualization\"><a href=\"#Generate-the-visualization\" class=\"headerlink\" title=\"Generate the visualization\"></a>Generate the visualization</h2><p>ggplot(data = destinations) +<br>       geom_polygon(aes(x = long, y = lat, group = group), fill = “lightsteelblue”, color = “black”) +<br>       coord_fixed(1.3) + geom_point(data = airports, aes(x= DEST_LONGITUDE, y=DEST_LATITUDE), shape = 10, color = “midnightblue”, size = 2) +<br>       geom_polygon(data = usa, aes(x=long, y = lat, group = group), fill = NA, color = “black”) +<br>       geom_curve(data=airports, aes(x=ORIG_LONGITUDE, y=ORIG_LATITUDE, xend=DEST_LONGITUDE, yend=DEST_LATITUDE), curvature = 0.2, color=”red”) +<br>       geom_point(data=orig_airport, aes(x=ORIG_LONGITUDE, y=ORIG_LATITUDE), shape = 8, color=”red”, size = 5)</p>\n<p>  That’s all I have for now. As I spend some more time on this, I will post a followup.</p>"},{"title":"Great site for fonts","id":"238","comments":0,"date":"2009-03-29T02:19:08.000Z","_content":"\nIf you are looking for some great fonts, head over to www.dafont.com They are free for personal use!","source":"_posts/great-site-for-fonts.md","raw":"---\ntitle: Great site for fonts\ntags: []\nid: '238'\ncategories:\n  - - Blog\ncomments: false\ndate: 2009-03-28 22:19:08\n---\n\nIf you are looking for some great fonts, head over to www.dafont.com They are free for personal use!","slug":"great-site-for-fonts","published":1,"updated":"2020-08-23T20:54:34.686Z","layout":"post","photos":[],"link":"","_id":"ckeaq99za003ksdjxfr9hfk5i","content":"<p>If you are looking for some great fonts, head over to <a href=\"http://www.dafont.com/\">www.dafont.com</a> They are free for personal use!</p>\n","site":{"data":{}},"excerpt":"","more":"<p>If you are looking for some great fonts, head over to <a href=\"http://www.dafont.com/\">www.dafont.com</a> They are free for personal use!</p>\n"},{"title":"Hadoop and Hive","id":"1535","comments":0,"date":"2013-05-28T14:57:27.000Z","_content":"\n[![bee_hive_](http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png)](http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png)Hadoop is the Apache Foundation's open source MapReduce platform. If you don't know what that means, its OK. Basically, its a Big Data platform based on specifications that Google developed and published. You can run it on one computer, or 10 or 100 or 1000, all talking together.You feed the system a large volume of data, tell it how to organize it (map) and then aggregate it (reduce) to get the results. Its not a great platform for every type of data analysis, but the ones that it is good for work extremely well on it. The biggest problem that I can see people having with Hadoop is the need to know how to program in Java to get it to do any major processing. Hive is a another project in the Hadoop world that lets you use commands similar to those you would with SQL Server or Oracle to manipulate a Hadoop database. Having spent just a little time playing with it, I'm pretty impressed.\n<!-- more -->\nUsing Hive, my one node Hadoop system churned through a 6 million row data set of patent information in a couple of minutes, and provided a list of the most referenced patent IDs with a count of how often they are cited in other patents. Now, I'm not a big fan of the command line, but if you want to work with Hadoop and Hive you have to learn the Linux command line (you can install them on Windows or Mac OSX, but the bulk of development and use is on Linux systems).  There is a lot of development going on to provide GUI interfaces to the various components but the progress is slow. All of these tools are not even at a 1.0 release mark yet, so they are pretty raw, and documentation is very limited. The latest version of Hive comes with a web interface to allow you to access it via a browser, so I've been trying to get that to work. Here is a little tidbit for anyone trying to get the Hive web interface in Linux to work. Enter the command:\n\nhive --service hwi\n\nIf you get an error about the hive-site.xml file not being found, change over to the folder where you installed Hive, and go to the conf subfolder. Execute a command to copy the hive-default.xml.template file to hive-site.xml and then try to run the hive --service hwi command again. If you don't get any errors, you can now open a web browser on another machine, and go to: http://<<ipaddress>>:9999/hwi and you'll see the Hive Web Interface screen. (Don't forget the /hwi at the end!). The only problem I've found with this is, you can't access the command line while its running!","source":"_posts/hadoop-and-hive.md","raw":"---\ntitle: Hadoop and Hive\ntags: []\nid: '1535'\ncategories:\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2013-05-28 10:57:27\n---\n\n[![bee_hive_](http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png)](http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png)Hadoop is the Apache Foundation's open source MapReduce platform. If you don't know what that means, its OK. Basically, its a Big Data platform based on specifications that Google developed and published. You can run it on one computer, or 10 or 100 or 1000, all talking together.You feed the system a large volume of data, tell it how to organize it (map) and then aggregate it (reduce) to get the results. Its not a great platform for every type of data analysis, but the ones that it is good for work extremely well on it. The biggest problem that I can see people having with Hadoop is the need to know how to program in Java to get it to do any major processing. Hive is a another project in the Hadoop world that lets you use commands similar to those you would with SQL Server or Oracle to manipulate a Hadoop database. Having spent just a little time playing with it, I'm pretty impressed.\n<!-- more -->\nUsing Hive, my one node Hadoop system churned through a 6 million row data set of patent information in a couple of minutes, and provided a list of the most referenced patent IDs with a count of how often they are cited in other patents. Now, I'm not a big fan of the command line, but if you want to work with Hadoop and Hive you have to learn the Linux command line (you can install them on Windows or Mac OSX, but the bulk of development and use is on Linux systems).  There is a lot of development going on to provide GUI interfaces to the various components but the progress is slow. All of these tools are not even at a 1.0 release mark yet, so they are pretty raw, and documentation is very limited. The latest version of Hive comes with a web interface to allow you to access it via a browser, so I've been trying to get that to work. Here is a little tidbit for anyone trying to get the Hive web interface in Linux to work. Enter the command:\n\nhive --service hwi\n\nIf you get an error about the hive-site.xml file not being found, change over to the folder where you installed Hive, and go to the conf subfolder. Execute a command to copy the hive-default.xml.template file to hive-site.xml and then try to run the hive --service hwi command again. If you don't get any errors, you can now open a web browser on another machine, and go to: http://<<ipaddress>>:9999/hwi and you'll see the Hive Web Interface screen. (Don't forget the /hwi at the end!). The only problem I've found with this is, you can't access the command line while its running!","slug":"hadoop-and-hive","published":1,"updated":"2020-08-23T20:54:34.734Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zf003osdjx7h2j0kx2","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png\" alt=\"bee_hive_\"></a>Hadoop is the Apache Foundation’s open source MapReduce platform. If you don’t know what that means, its OK. Basically, its a Big Data platform based on specifications that Google developed and published. You can run it on one computer, or 10 or 100 or 1000, all talking together.You feed the system a large volume of data, tell it how to organize it (map) and then aggregate it (reduce) to get the results. Its not a great platform for every type of data analysis, but the ones that it is good for work extremely well on it. The biggest problem that I can see people having with Hadoop is the need to know how to program in Java to get it to do any major processing. Hive is a another project in the Hadoop world that lets you use commands similar to those you would with SQL Server or Oracle to manipulate a Hadoop database. Having spent just a little time playing with it, I’m pretty impressed.</p>\n<a id=\"more\"></a>\n<p>Using Hive, my one node Hadoop system churned through a 6 million row data set of patent information in a couple of minutes, and provided a list of the most referenced patent IDs with a count of how often they are cited in other patents. Now, I’m not a big fan of the command line, but if you want to work with Hadoop and Hive you have to learn the Linux command line (you can install them on Windows or Mac OSX, but the bulk of development and use is on Linux systems).  There is a lot of development going on to provide GUI interfaces to the various components but the progress is slow. All of these tools are not even at a 1.0 release mark yet, so they are pretty raw, and documentation is very limited. The latest version of Hive comes with a web interface to allow you to access it via a browser, so I’ve been trying to get that to work. Here is a little tidbit for anyone trying to get the Hive web interface in Linux to work. Enter the command:</p>\n<p>hive –service hwi</p>\n<p>If you get an error about the hive-site.xml file not being found, change over to the folder where you installed Hive, and go to the conf subfolder. Execute a command to copy the hive-default.xml.template file to hive-site.xml and then try to run the hive –service hwi command again. If you don’t get any errors, you can now open a web browser on another machine, and go to: http://&lt;<ipaddress>&gt;:9999/hwi and you’ll see the Hive Web Interface screen. (Don’t forget the /hwi at the end!). The only problem I’ve found with this is, you can’t access the command line while its running!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/05/bee_hive_.png\" alt=\"bee_hive_\"></a>Hadoop is the Apache Foundation’s open source MapReduce platform. If you don’t know what that means, its OK. Basically, its a Big Data platform based on specifications that Google developed and published. You can run it on one computer, or 10 or 100 or 1000, all talking together.You feed the system a large volume of data, tell it how to organize it (map) and then aggregate it (reduce) to get the results. Its not a great platform for every type of data analysis, but the ones that it is good for work extremely well on it. The biggest problem that I can see people having with Hadoop is the need to know how to program in Java to get it to do any major processing. Hive is a another project in the Hadoop world that lets you use commands similar to those you would with SQL Server or Oracle to manipulate a Hadoop database. Having spent just a little time playing with it, I’m pretty impressed.</p>","more":"<p>Using Hive, my one node Hadoop system churned through a 6 million row data set of patent information in a couple of minutes, and provided a list of the most referenced patent IDs with a count of how often they are cited in other patents. Now, I’m not a big fan of the command line, but if you want to work with Hadoop and Hive you have to learn the Linux command line (you can install them on Windows or Mac OSX, but the bulk of development and use is on Linux systems).  There is a lot of development going on to provide GUI interfaces to the various components but the progress is slow. All of these tools are not even at a 1.0 release mark yet, so they are pretty raw, and documentation is very limited. The latest version of Hive comes with a web interface to allow you to access it via a browser, so I’ve been trying to get that to work. Here is a little tidbit for anyone trying to get the Hive web interface in Linux to work. Enter the command:</p>\n<p>hive –service hwi</p>\n<p>If you get an error about the hive-site.xml file not being found, change over to the folder where you installed Hive, and go to the conf subfolder. Execute a command to copy the hive-default.xml.template file to hive-site.xml and then try to run the hive –service hwi command again. If you don’t get any errors, you can now open a web browser on another machine, and go to: http://&lt;<ipaddress>&gt;:9999/hwi and you’ll see the Hive Web Interface screen. (Don’t forget the /hwi at the end!). The only problem I’ve found with this is, you can’t access the command line while its running!</p>"},{"title":"Hadoop - What's the point?","id":"1785","comments":0,"date":"2014-02-17T14:14:11.000Z","_content":"\n[![Small vs Large](http://edpflager.com/wp-content/uploads/2014/02/planes-300x110.jpg)](http://edpflager.com/wp-content/uploads/2014/02/planes.jpg)Its often said when you work closely with something, you sometimes lose track of the big picture (you can't see the forest for the trees). But I also find that a lot of times, people who aren't familiar with something focus too much on the details and miss the big picture. In discussing my Hadoop cluster project with people, they don't understand the point of it. \"Why setup four PCs to work on something. Can't you just get one really powerful PC and do the work with that?\" they ask. **DIVIDE THE WORK** The innovation  that Hadoop provides is in HOW it does the work. For many years, in IT as well as other areas, the solution when faced with bigger and bigger workloads has been to get a bigger, faster, stronger tool. Here are some examples:\n<!-- more -->\n*   Have to cut your home's lawn? Use a lawnmower. Have to cut the lawn for a large campus? Get a riding mower.\n*   Want to make a pizza? Use your oven. Want to open a pizzeria? Better buy a large pizza oven.\n*   Have to process payroll for a small store? You use a personal computer, and maybe use Excel or Quicken to cut the checks. Have to process payroll for manufacturing plant? Use a server and a dedicated application to handle it.\n\nThe downside to all of these approaches is two-fold: the cost involved with scaling up to larger tools is larger than the spending on the smaller one, and you don't always need the capacity of the larger tool.  Something else in common with these examples is that you can also do the work by dividing it up.\n\n*   Cutting the lawn for a large campus could be done using several small lawnmowers with someone to push each of them.\n*   You could make a large number of pizzas using several small ovens, rather than one large one.\n*   Same thing with payroll for a large plant. Divide it up and spread it among several or more staff members to get it done.\n\n**CONQUER THE CHALLENGE** The challenge when dealing with BIG DATA processing is dividing it and then assembling the workflows back into one complete one. Hadoop was designed to meet these challenges. Rather than try to process incredibly large data sets on a very large powerful computer system, it uses a cluster of smaller commodity systems, divides the workflow across them, and then reassembles the output into a complete result. Because you are using less powerful PCs (commodity ones), the hardware costs are less. In my case, I spent about $400 for the four PCs that make up my cluster. In order to get a single more powerful PC, I would have had to spend at least that, and probably more. A lot more. Another benefit of this approach is maintenance. If you have one powerful PC, its not any less likely to break down than any other PC. But if it does, you are effectively at a stand still for doing any work. But in the case of my cluster, if one of the machines breaks down, I still can work with the remaining three. Sure it will take longer, but that trade off is small when weighed against the absolute downtime of a bigger machine being down. Finally, lets think about our examples from above in terms of resource allocation:\n\n*   Don't need all of the lawn movers on one job? Use some of them for another job.\n*   Don't need to make all the pizzas you would normally on a given day? Use the spare ovens for something else, or shut them down to save on costs.\n*   One shift of  your plant has been idled for a couple of weeks so you don't need all of your payroll people to cut checks? Assign them to do other tasks, like W2's.\n\nIn the same way, if you don't need to use all of your processing power in a Hadoop cluster for a specific job, use that excess capacity for something else or shut them down to save on costs. Its an innovative concept, although in hindsight its really pretty self-evident.","source":"_posts/hadoop-whats-the-point.md","raw":"---\ntitle: Hadoop - What's the point?\ntags:\n  - Hadoop\nid: '1785'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-02-17 09:14:11\n---\n\n[![Small vs Large](http://edpflager.com/wp-content/uploads/2014/02/planes-300x110.jpg)](http://edpflager.com/wp-content/uploads/2014/02/planes.jpg)Its often said when you work closely with something, you sometimes lose track of the big picture (you can't see the forest for the trees). But I also find that a lot of times, people who aren't familiar with something focus too much on the details and miss the big picture. In discussing my Hadoop cluster project with people, they don't understand the point of it. \"Why setup four PCs to work on something. Can't you just get one really powerful PC and do the work with that?\" they ask. **DIVIDE THE WORK** The innovation  that Hadoop provides is in HOW it does the work. For many years, in IT as well as other areas, the solution when faced with bigger and bigger workloads has been to get a bigger, faster, stronger tool. Here are some examples:\n<!-- more -->\n*   Have to cut your home's lawn? Use a lawnmower. Have to cut the lawn for a large campus? Get a riding mower.\n*   Want to make a pizza? Use your oven. Want to open a pizzeria? Better buy a large pizza oven.\n*   Have to process payroll for a small store? You use a personal computer, and maybe use Excel or Quicken to cut the checks. Have to process payroll for manufacturing plant? Use a server and a dedicated application to handle it.\n\nThe downside to all of these approaches is two-fold: the cost involved with scaling up to larger tools is larger than the spending on the smaller one, and you don't always need the capacity of the larger tool.  Something else in common with these examples is that you can also do the work by dividing it up.\n\n*   Cutting the lawn for a large campus could be done using several small lawnmowers with someone to push each of them.\n*   You could make a large number of pizzas using several small ovens, rather than one large one.\n*   Same thing with payroll for a large plant. Divide it up and spread it among several or more staff members to get it done.\n\n**CONQUER THE CHALLENGE** The challenge when dealing with BIG DATA processing is dividing it and then assembling the workflows back into one complete one. Hadoop was designed to meet these challenges. Rather than try to process incredibly large data sets on a very large powerful computer system, it uses a cluster of smaller commodity systems, divides the workflow across them, and then reassembles the output into a complete result. Because you are using less powerful PCs (commodity ones), the hardware costs are less. In my case, I spent about $400 for the four PCs that make up my cluster. In order to get a single more powerful PC, I would have had to spend at least that, and probably more. A lot more. Another benefit of this approach is maintenance. If you have one powerful PC, its not any less likely to break down than any other PC. But if it does, you are effectively at a stand still for doing any work. But in the case of my cluster, if one of the machines breaks down, I still can work with the remaining three. Sure it will take longer, but that trade off is small when weighed against the absolute downtime of a bigger machine being down. Finally, lets think about our examples from above in terms of resource allocation:\n\n*   Don't need all of the lawn movers on one job? Use some of them for another job.\n*   Don't need to make all the pizzas you would normally on a given day? Use the spare ovens for something else, or shut them down to save on costs.\n*   One shift of  your plant has been idled for a couple of weeks so you don't need all of your payroll people to cut checks? Assign them to do other tasks, like W2's.\n\nIn the same way, if you don't need to use all of your processing power in a Hadoop cluster for a specific job, use that excess capacity for something else or shut them down to save on costs. Its an innovative concept, although in hindsight its really pretty self-evident.","slug":"hadoop-whats-the-point","published":1,"updated":"2020-08-23T20:54:34.786Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zj003ssdjx0kalcfbi","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/planes.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/planes-300x110.jpg\" alt=\"Small vs Large\"></a>Its often said when you work closely with something, you sometimes lose track of the big picture (you can’t see the forest for the trees). But I also find that a lot of times, people who aren’t familiar with something focus too much on the details and miss the big picture. In discussing my Hadoop cluster project with people, they don’t understand the point of it. “Why setup four PCs to work on something. Can’t you just get one really powerful PC and do the work with that?” they ask. <strong>DIVIDE THE WORK</strong> The innovation  that Hadoop provides is in HOW it does the work. For many years, in IT as well as other areas, the solution when faced with bigger and bigger workloads has been to get a bigger, faster, stronger tool. Here are some examples:</p>\n<a id=\"more\"></a>\n<ul>\n<li>Have to cut your home’s lawn? Use a lawnmower. Have to cut the lawn for a large campus? Get a riding mower.</li>\n<li>Want to make a pizza? Use your oven. Want to open a pizzeria? Better buy a large pizza oven.</li>\n<li>Have to process payroll for a small store? You use a personal computer, and maybe use Excel or Quicken to cut the checks. Have to process payroll for manufacturing plant? Use a server and a dedicated application to handle it.</li>\n</ul>\n<p>The downside to all of these approaches is two-fold: the cost involved with scaling up to larger tools is larger than the spending on the smaller one, and you don’t always need the capacity of the larger tool.  Something else in common with these examples is that you can also do the work by dividing it up.</p>\n<ul>\n<li>Cutting the lawn for a large campus could be done using several small lawnmowers with someone to push each of them.</li>\n<li>You could make a large number of pizzas using several small ovens, rather than one large one.</li>\n<li>Same thing with payroll for a large plant. Divide it up and spread it among several or more staff members to get it done.</li>\n</ul>\n<p><strong>CONQUER THE CHALLENGE</strong> The challenge when dealing with BIG DATA processing is dividing it and then assembling the workflows back into one complete one. Hadoop was designed to meet these challenges. Rather than try to process incredibly large data sets on a very large powerful computer system, it uses a cluster of smaller commodity systems, divides the workflow across them, and then reassembles the output into a complete result. Because you are using less powerful PCs (commodity ones), the hardware costs are less. In my case, I spent about $400 for the four PCs that make up my cluster. In order to get a single more powerful PC, I would have had to spend at least that, and probably more. A lot more. Another benefit of this approach is maintenance. If you have one powerful PC, its not any less likely to break down than any other PC. But if it does, you are effectively at a stand still for doing any work. But in the case of my cluster, if one of the machines breaks down, I still can work with the remaining three. Sure it will take longer, but that trade off is small when weighed against the absolute downtime of a bigger machine being down. Finally, lets think about our examples from above in terms of resource allocation:</p>\n<ul>\n<li>Don’t need all of the lawn movers on one job? Use some of them for another job.</li>\n<li>Don’t need to make all the pizzas you would normally on a given day? Use the spare ovens for something else, or shut them down to save on costs.</li>\n<li>One shift of  your plant has been idled for a couple of weeks so you don’t need all of your payroll people to cut checks? Assign them to do other tasks, like W2’s.</li>\n</ul>\n<p>In the same way, if you don’t need to use all of your processing power in a Hadoop cluster for a specific job, use that excess capacity for something else or shut them down to save on costs. Its an innovative concept, although in hindsight its really pretty self-evident.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/planes.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/planes-300x110.jpg\" alt=\"Small vs Large\"></a>Its often said when you work closely with something, you sometimes lose track of the big picture (you can’t see the forest for the trees). But I also find that a lot of times, people who aren’t familiar with something focus too much on the details and miss the big picture. In discussing my Hadoop cluster project with people, they don’t understand the point of it. “Why setup four PCs to work on something. Can’t you just get one really powerful PC and do the work with that?” they ask. <strong>DIVIDE THE WORK</strong> The innovation  that Hadoop provides is in HOW it does the work. For many years, in IT as well as other areas, the solution when faced with bigger and bigger workloads has been to get a bigger, faster, stronger tool. Here are some examples:</p>","more":"<ul>\n<li>Have to cut your home’s lawn? Use a lawnmower. Have to cut the lawn for a large campus? Get a riding mower.</li>\n<li>Want to make a pizza? Use your oven. Want to open a pizzeria? Better buy a large pizza oven.</li>\n<li>Have to process payroll for a small store? You use a personal computer, and maybe use Excel or Quicken to cut the checks. Have to process payroll for manufacturing plant? Use a server and a dedicated application to handle it.</li>\n</ul>\n<p>The downside to all of these approaches is two-fold: the cost involved with scaling up to larger tools is larger than the spending on the smaller one, and you don’t always need the capacity of the larger tool.  Something else in common with these examples is that you can also do the work by dividing it up.</p>\n<ul>\n<li>Cutting the lawn for a large campus could be done using several small lawnmowers with someone to push each of them.</li>\n<li>You could make a large number of pizzas using several small ovens, rather than one large one.</li>\n<li>Same thing with payroll for a large plant. Divide it up and spread it among several or more staff members to get it done.</li>\n</ul>\n<p><strong>CONQUER THE CHALLENGE</strong> The challenge when dealing with BIG DATA processing is dividing it and then assembling the workflows back into one complete one. Hadoop was designed to meet these challenges. Rather than try to process incredibly large data sets on a very large powerful computer system, it uses a cluster of smaller commodity systems, divides the workflow across them, and then reassembles the output into a complete result. Because you are using less powerful PCs (commodity ones), the hardware costs are less. In my case, I spent about $400 for the four PCs that make up my cluster. In order to get a single more powerful PC, I would have had to spend at least that, and probably more. A lot more. Another benefit of this approach is maintenance. If you have one powerful PC, its not any less likely to break down than any other PC. But if it does, you are effectively at a stand still for doing any work. But in the case of my cluster, if one of the machines breaks down, I still can work with the remaining three. Sure it will take longer, but that trade off is small when weighed against the absolute downtime of a bigger machine being down. Finally, lets think about our examples from above in terms of resource allocation:</p>\n<ul>\n<li>Don’t need all of the lawn movers on one job? Use some of them for another job.</li>\n<li>Don’t need to make all the pizzas you would normally on a given day? Use the spare ovens for something else, or shut them down to save on costs.</li>\n<li>One shift of  your plant has been idled for a couple of weeks so you don’t need all of your payroll people to cut checks? Assign them to do other tasks, like W2’s.</li>\n</ul>\n<p>In the same way, if you don’t need to use all of your processing power in a Hadoop cluster for a specific job, use that excess capacity for something else or shut them down to save on costs. Its an innovative concept, although in hindsight its really pretty self-evident.</p>"},{"title":"Add Horizontal Lines or Rules","id":"4088","comments":0,"date":"2018-10-18T07:27:53.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) A very quick tip that works if you are creating PDF, Word or HTML output from your R Markdown code. If you want to insert a horizontal line (or rule) in your document, you can use three (or more) asterisks (\\*\\*\\*), three or more underscores (\\_\\_\\_) or three or more hyphens (---). Just remember to have a blank line before and after them so the R Markdown engine can parse the code correctly. Here is a code example:\n<!-- more -->\n\\---\ntitle: \"Horizontal Rules\"\noutput:\n  pdf\\_document: default\n  html\\_document:\n    df\\_print: paged\n  word\\_document: default\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n\\*\\*\\*\n\nThree (or more) asterisks inserts a horizontal line \n\n\\_\\_\\_\n\nand so does three (or more) underscores\n\n---\n\nas well as three (or more) hyphens. Just remember to have a blank line \nbefore and after them so the R Markdown engine can parse the code correctly.\n\nThe resulting document looks like this: [![](http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines-1024x382.png)](http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines.png)","source":"_posts/horizontal-lines-or-rules-in-your-documents.md","raw":"---\ntitle: Add Horizontal Lines or Rules\ntags:\n  - guides\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '4088'\ncategories:\n  - - Blog\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-10-18 03:27:53\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) A very quick tip that works if you are creating PDF, Word or HTML output from your R Markdown code. If you want to insert a horizontal line (or rule) in your document, you can use three (or more) asterisks (\\*\\*\\*), three or more underscores (\\_\\_\\_) or three or more hyphens (---). Just remember to have a blank line before and after them so the R Markdown engine can parse the code correctly. Here is a code example:\n<!-- more -->\n\\---\ntitle: \"Horizontal Rules\"\noutput:\n  pdf\\_document: default\n  html\\_document:\n    df\\_print: paged\n  word\\_document: default\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n\\*\\*\\*\n\nThree (or more) asterisks inserts a horizontal line \n\n\\_\\_\\_\n\nand so does three (or more) underscores\n\n---\n\nas well as three (or more) hyphens. Just remember to have a blank line \nbefore and after them so the R Markdown engine can parse the code correctly.\n\nThe resulting document looks like this: [![](http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines-1024x382.png)](http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines.png)","slug":"horizontal-lines-or-rules-in-your-documents","published":1,"updated":"2020-08-23T20:54:35.166Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zo003wsdjx37fsgeys","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> A very quick tip that works if you are creating PDF, Word or HTML output from your R Markdown code. If you want to insert a horizontal line (or rule) in your document, you can use three (or more) asterisks (***), three or more underscores (___) or three or more hyphens (—). Just remember to have a blank line before and after them so the R Markdown engine can parse the code correctly. Here is a code example:</p>\n<a id=\"more\"></a>\n<p>-–<br>title: “Horizontal Rules”<br>output:<br>  pdf_document: default<br>  html_document:<br>    df_print: paged<br>  word_document: default</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>***</p>\n<p>Three (or more) asterisks inserts a horizontal line </p>\n<p>___</p>\n<p>and so does three (or more) underscores</p>\n<hr>\n<p>as well as three (or more) hyphens. Just remember to have a blank line<br>before and after them so the R Markdown engine can parse the code correctly.</p>\n<p>The resulting document looks like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines-1024x382.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> A very quick tip that works if you are creating PDF, Word or HTML output from your R Markdown code. If you want to insert a horizontal line (or rule) in your document, you can use three (or more) asterisks (***), three or more underscores (___) or three or more hyphens (—). Just remember to have a blank line before and after them so the R Markdown engine can parse the code correctly. Here is a code example:</p>","more":"<p>-–<br>title: “Horizontal Rules”<br>output:<br>  pdf_document: default<br>  html_document:<br>    df_print: paged<br>  word_document: default</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>***</p>\n<p>Three (or more) asterisks inserts a horizontal line </p>\n<p>___</p>\n<p>and so does three (or more) underscores</p>\n<hr>\n<p>as well as three (or more) hyphens. Just remember to have a blank line<br>before and after them so the R Markdown engine can parse the code correctly.</p>\n<p>The resulting document looks like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/HorizontalLines-1024x382.png\"></a></p>"},{"title":"How not to do an update","id":"1701","comments":0,"date":"2013-12-18T17:36:51.000Z","_content":"\n[![epic-fail](http://edpflager.com/wp-content/uploads/2013/12/epic-fail-300x240.jpg)](http://edpflager.com/wp-content/uploads/2013/12/epic-fail.jpg)I killed off my Yahoo email account this week. I had used that address for close to ten years, and I was very reluctant to get rid of it. It was the address I had given out to a whole bunch of friends and acquaintances. I had used it to buy way more products that I really care to think about. And it was an account I checked too frequently throughout the day. So what changed? Yahoo did. Over the course of the last six months, I've had too many problems with that account. More than what should be expected. Granted it was a FREE account, so the old axiom that you get what you pay for definitely holds true. But for years it had been a rock solid platform. So what happened?\n\n*   First I went through a period of about a week where I didn't get any new email. Yahoo reported no outages on any social media or their home page, and I could still log into the account. I just wasn't getting anything. Finally, I started digging into it, and discovered that a setting from more than five years ago had been reactivated. Email was being forwarded to another account that I no longer have access to. WTF?!!!\n*   Then Yahoo redesigned the interface. I wasn't crazy about it, but meh - I could live with it.\n*   Then they redesigned the interface AGAIN. And it was worse!\n*   This was followed by SPAM filter malfunctions, and my inbox was flooded with crap that any rudimentary filter should have caught.\n*   Finally, the web interface became unusable on my Macbook. It appeared to lock up for several minutes at a time, until a little link would should up in the bottom of the window, asking Too slow? Switch to classic view. But I couldn't find anyplace to make classic view the default!\n\nSo I finally decided, me and Yahoo were going to go our separate ways. Sniff. Too bad, but nothing stays the same for ever. If you've emailed me at my Yahoo address and gotten a bounced email, sorry. Shoot me an email through the contact page on this website, and I will get back to you with my new address.","source":"_posts/how-not-to-do-an-update.md","raw":"---\ntitle: How not to do an update\ntags: []\nid: '1701'\ncategories:\n  - - Blog\ncomments: false\ndate: 2013-12-18 12:36:51\n---\n\n[![epic-fail](http://edpflager.com/wp-content/uploads/2013/12/epic-fail-300x240.jpg)](http://edpflager.com/wp-content/uploads/2013/12/epic-fail.jpg)I killed off my Yahoo email account this week. I had used that address for close to ten years, and I was very reluctant to get rid of it. It was the address I had given out to a whole bunch of friends and acquaintances. I had used it to buy way more products that I really care to think about. And it was an account I checked too frequently throughout the day. So what changed? Yahoo did. Over the course of the last six months, I've had too many problems with that account. More than what should be expected. Granted it was a FREE account, so the old axiom that you get what you pay for definitely holds true. But for years it had been a rock solid platform. So what happened?\n\n*   First I went through a period of about a week where I didn't get any new email. Yahoo reported no outages on any social media or their home page, and I could still log into the account. I just wasn't getting anything. Finally, I started digging into it, and discovered that a setting from more than five years ago had been reactivated. Email was being forwarded to another account that I no longer have access to. WTF?!!!\n*   Then Yahoo redesigned the interface. I wasn't crazy about it, but meh - I could live with it.\n*   Then they redesigned the interface AGAIN. And it was worse!\n*   This was followed by SPAM filter malfunctions, and my inbox was flooded with crap that any rudimentary filter should have caught.\n*   Finally, the web interface became unusable on my Macbook. It appeared to lock up for several minutes at a time, until a little link would should up in the bottom of the window, asking Too slow? Switch to classic view. But I couldn't find anyplace to make classic view the default!\n\nSo I finally decided, me and Yahoo were going to go our separate ways. Sniff. Too bad, but nothing stays the same for ever. If you've emailed me at my Yahoo address and gotten a bounced email, sorry. Shoot me an email through the contact page on this website, and I will get back to you with my new address.","slug":"how-not-to-do-an-update","published":1,"updated":"2020-08-23T20:54:34.754Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zu0040sdjxd58abysn","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/12/epic-fail.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/12/epic-fail-300x240.jpg\" alt=\"epic-fail\"></a>I killed off my Yahoo email account this week. I had used that address for close to ten years, and I was very reluctant to get rid of it. It was the address I had given out to a whole bunch of friends and acquaintances. I had used it to buy way more products that I really care to think about. And it was an account I checked too frequently throughout the day. So what changed? Yahoo did. Over the course of the last six months, I’ve had too many problems with that account. More than what should be expected. Granted it was a FREE account, so the old axiom that you get what you pay for definitely holds true. But for years it had been a rock solid platform. So what happened?</p>\n<ul>\n<li>First I went through a period of about a week where I didn’t get any new email. Yahoo reported no outages on any social media or their home page, and I could still log into the account. I just wasn’t getting anything. Finally, I started digging into it, and discovered that a setting from more than five years ago had been reactivated. Email was being forwarded to another account that I no longer have access to. WTF?!!!</li>\n<li>Then Yahoo redesigned the interface. I wasn’t crazy about it, but meh - I could live with it.</li>\n<li>Then they redesigned the interface AGAIN. And it was worse!</li>\n<li>This was followed by SPAM filter malfunctions, and my inbox was flooded with crap that any rudimentary filter should have caught.</li>\n<li>Finally, the web interface became unusable on my Macbook. It appeared to lock up for several minutes at a time, until a little link would should up in the bottom of the window, asking Too slow? Switch to classic view. But I couldn’t find anyplace to make classic view the default!</li>\n</ul>\n<p>So I finally decided, me and Yahoo were going to go our separate ways. Sniff. Too bad, but nothing stays the same for ever. If you’ve emailed me at my Yahoo address and gotten a bounced email, sorry. Shoot me an email through the contact page on this website, and I will get back to you with my new address.</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/12/epic-fail.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/12/epic-fail-300x240.jpg\" alt=\"epic-fail\"></a>I killed off my Yahoo email account this week. I had used that address for close to ten years, and I was very reluctant to get rid of it. It was the address I had given out to a whole bunch of friends and acquaintances. I had used it to buy way more products that I really care to think about. And it was an account I checked too frequently throughout the day. So what changed? Yahoo did. Over the course of the last six months, I’ve had too many problems with that account. More than what should be expected. Granted it was a FREE account, so the old axiom that you get what you pay for definitely holds true. But for years it had been a rock solid platform. So what happened?</p>\n<ul>\n<li>First I went through a period of about a week where I didn’t get any new email. Yahoo reported no outages on any social media or their home page, and I could still log into the account. I just wasn’t getting anything. Finally, I started digging into it, and discovered that a setting from more than five years ago had been reactivated. Email was being forwarded to another account that I no longer have access to. WTF?!!!</li>\n<li>Then Yahoo redesigned the interface. I wasn’t crazy about it, but meh - I could live with it.</li>\n<li>Then they redesigned the interface AGAIN. And it was worse!</li>\n<li>This was followed by SPAM filter malfunctions, and my inbox was flooded with crap that any rudimentary filter should have caught.</li>\n<li>Finally, the web interface became unusable on my Macbook. It appeared to lock up for several minutes at a time, until a little link would should up in the bottom of the window, asking Too slow? Switch to classic view. But I couldn’t find anyplace to make classic view the default!</li>\n</ul>\n<p>So I finally decided, me and Yahoo were going to go our separate ways. Sniff. Too bad, but nothing stays the same for ever. If you’ve emailed me at my Yahoo address and gotten a bounced email, sorry. Shoot me an email through the contact page on this website, and I will get back to you with my new address.</p>\n"},{"title":"Hyperlinks in R Markdown HTML output","id":"4161","comments":0,"date":"2018-11-15T22:22:41.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)R Markdown documents can be generated as PDF, HTML or Word documents. Often times you may want to include hyperlinks in your document to go to a specific webpage, to open an email program, or to even just jump to another location in the document. Lets look at how to add these types of hyperlinks to each document output type. In this post, I'll be covering HTML Hyperlinks and I'll cover [Word](http://edpflager.com/2018/11/21/hyperlinks-in-r-markdown-word-output/) and [PDF](http://edpflager.com/2018/11/23/hyperlinks-in-r-markdown-pdf-output/) in subsequent posts. By far, adding hyperlinks when generating an HTML document is easy in R Markdown. An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <http://edpflager.com>.\n<!-- more -->\nThe resulting HTML doc then resembles this:\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png)\n\nIf you want to hide the hyperlink there are a couple of ways to do it, enclose the text you want to substitute for your hyperlink in brackets and follow it with the hyperlink tag in parentheses right after it. The other method is a more traditional HTML approach, enclosing the link inside a set of greater-than and less-than tags, using an href call with the hyperlink, and nesting your substitute for the hyperlink before a closing /a tag. A simple example to illustrate both looks like this with the resulting generated HTML putput:\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this or\nlike <a href=\"http://edpflager.com\">this</a>[![](http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png)](http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png)\n\nTo include a hyperlink to open an email program (assuming the reader has one installed), we can use either of the methods described above. The code to put in your R Markdown document looks like this:\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks>) or \n<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a>[![](http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png)\n\nFinally, to include a hyperlink to a point further in the document, you can use the HTML anchor tag by placing your hyperlink at some point in your document and then defining the landing location for the tag in another part of the document. The initial hyperlink is similar to the ones discussed above, but instead of including a full URL, you use a pound sign (#) and a unique name for the anchor. Then later in the document where you want the hyperlink to jump to, you define the landing link with the id= tag. Let's look at an example:\n\n<a href=\"#land-here\">Jump to the landing spot.</a>\n\nOTHER TEXT GOES HERE\n\n<a id=\"land-here\">Landing spot</a>\n\nIn my sample R Markdown document, I pasted several paragraphs of Lorem Epsum text to separate the two tags. Also of note:\n\n*   You don't need to include actual text as part of your landing spot. If you have the opening and closing tags next to each other for the landing spot, it still works fine.\n*   Its generally a good idea to format your landing spot as a section header, but its not required.\n\nHere is the code for my sample document:\n\n\\---\ntitle: \"Hyperlinks-HTML\"\noutput: html\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n<a href=\"#land-here\">Jump to the landing spot.</a>\n\nAn inline hyperlink to a website in your document is enclosed in less-than\nand greater-than tags, like this: <http://edpflager.com>\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this or \nlike <a href=\"http://edpflager.com\">this</a>\n\nProvide an email hyperlink like this:\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks>) or \n<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a>\n\nLorem ipsum\n\n<a id=\"land-here\">Landing spot</a>","source":"_posts/html-hyperlinks-in-r-markdown-pdfs.md","raw":"---\ntitle: Hyperlinks in R Markdown HTML output\ntags:\n  - cookbook\n  - howto\n  - R Markdown\nid: '4161'\ncategories:\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-11-15 17:22:41\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)R Markdown documents can be generated as PDF, HTML or Word documents. Often times you may want to include hyperlinks in your document to go to a specific webpage, to open an email program, or to even just jump to another location in the document. Lets look at how to add these types of hyperlinks to each document output type. In this post, I'll be covering HTML Hyperlinks and I'll cover [Word](http://edpflager.com/2018/11/21/hyperlinks-in-r-markdown-word-output/) and [PDF](http://edpflager.com/2018/11/23/hyperlinks-in-r-markdown-pdf-output/) in subsequent posts. By far, adding hyperlinks when generating an HTML document is easy in R Markdown. An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <http://edpflager.com>.\n<!-- more -->\nThe resulting HTML doc then resembles this:\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png)\n\nIf you want to hide the hyperlink there are a couple of ways to do it, enclose the text you want to substitute for your hyperlink in brackets and follow it with the hyperlink tag in parentheses right after it. The other method is a more traditional HTML approach, enclosing the link inside a set of greater-than and less-than tags, using an href call with the hyperlink, and nesting your substitute for the hyperlink before a closing /a tag. A simple example to illustrate both looks like this with the resulting generated HTML putput:\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this or\nlike <a href=\"http://edpflager.com\">this</a>[![](http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png)](http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png)\n\nTo include a hyperlink to open an email program (assuming the reader has one installed), we can use either of the methods described above. The code to put in your R Markdown document looks like this:\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks>) or \n<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a>[![](http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png)\n\nFinally, to include a hyperlink to a point further in the document, you can use the HTML anchor tag by placing your hyperlink at some point in your document and then defining the landing location for the tag in another part of the document. The initial hyperlink is similar to the ones discussed above, but instead of including a full URL, you use a pound sign (#) and a unique name for the anchor. Then later in the document where you want the hyperlink to jump to, you define the landing link with the id= tag. Let's look at an example:\n\n<a href=\"#land-here\">Jump to the landing spot.</a>\n\nOTHER TEXT GOES HERE\n\n<a id=\"land-here\">Landing spot</a>\n\nIn my sample R Markdown document, I pasted several paragraphs of Lorem Epsum text to separate the two tags. Also of note:\n\n*   You don't need to include actual text as part of your landing spot. If you have the opening and closing tags next to each other for the landing spot, it still works fine.\n*   Its generally a good idea to format your landing spot as a section header, but its not required.\n\nHere is the code for my sample document:\n\n\\---\ntitle: \"Hyperlinks-HTML\"\noutput: html\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n<a href=\"#land-here\">Jump to the landing spot.</a>\n\nAn inline hyperlink to a website in your document is enclosed in less-than\nand greater-than tags, like this: <http://edpflager.com>\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this or \nlike <a href=\"http://edpflager.com\">this</a>\n\nProvide an email hyperlink like this:\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks>) or \n<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a>\n\nLorem ipsum\n\n<a id=\"land-here\">Landing spot</a>","slug":"html-hyperlinks-in-r-markdown-pdfs","published":1,"updated":"2020-08-23T20:54:35.194Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zw0044sdjx6devbh5o","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>R Markdown documents can be generated as PDF, HTML or Word documents. Often times you may want to include hyperlinks in your document to go to a specific webpage, to open an email program, or to even just jump to another location in the document. Lets look at how to add these types of hyperlinks to each document output type. In this post, I’ll be covering HTML Hyperlinks and I’ll cover <a href=\"http://edpflager.com/2018/11/21/hyperlinks-in-r-markdown-word-output/\">Word</a> and <a href=\"http://edpflager.com/2018/11/23/hyperlinks-in-r-markdown-pdf-output/\">PDF</a> in subsequent posts. By far, adding hyperlinks when generating an HTML document is easy in R Markdown. An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a>.</p>\n<a id=\"more\"></a>\n<p>The resulting HTML doc then resembles this:</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png\"></a></p>\n<p>If you want to hide the hyperlink there are a couple of ways to do it, enclose the text you want to substitute for your hyperlink in brackets and follow it with the hyperlink tag in parentheses right after it. The other method is a more traditional HTML approach, enclosing the link inside a set of greater-than and less-than tags, using an href call with the hyperlink, and nesting your substitute for the hyperlink before a closing /a tag. A simple example to illustrate both looks like this with the resulting generated HTML putput:</p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this or<br>like <a href=\"http://edpflager.com\">this</a><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png\"></a></p>\n<p>To include a hyperlink to open an email program (assuming the reader has one installed), we can use either of the methods described above. The code to put in your R Markdown document looks like this:</p>\n<p>[email me](<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">mailto:test@foobar.com?subject=Hyperlinks</a>) or<br><a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png\"></a></p>\n<p>Finally, to include a hyperlink to a point further in the document, you can use the HTML anchor tag by placing your hyperlink at some point in your document and then defining the landing location for the tag in another part of the document. The initial hyperlink is similar to the ones discussed above, but instead of including a full URL, you use a pound sign (#) and a unique name for the anchor. Then later in the document where you want the hyperlink to jump to, you define the landing link with the id= tag. Let’s look at an example:</p>\n<p><a href=\"#land-here\">Jump to the landing spot.</a></p>\n<p>OTHER TEXT GOES HERE</p>\n<p><a id=\"land-here\">Landing spot</a></p>\n<p>In my sample R Markdown document, I pasted several paragraphs of Lorem Epsum text to separate the two tags. Also of note:</p>\n<ul>\n<li>You don’t need to include actual text as part of your landing spot. If you have the opening and closing tags next to each other for the landing spot, it still works fine.</li>\n<li>Its generally a good idea to format your landing spot as a section header, but its not required.</li>\n</ul>\n<p>Here is the code for my sample document:</p>\n<p>-–<br>title: “Hyperlinks-HTML”<br>output: html_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br><a href=\"#land-here\">Jump to the landing spot.</a></p>\n<p>An inline hyperlink to a website in your document is enclosed in less-than<br>and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a></p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this or<br>like <a href=\"http://edpflager.com\">this</a></p>\n<p>Provide an email hyperlink like this:</p>\n<p>[email me](<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">mailto:test@foobar.com?subject=Hyperlinks</a>) or<br><a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a></p>\n<p>Lorem ipsum</p>\n<p><a id=\"land-here\">Landing spot</a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>R Markdown documents can be generated as PDF, HTML or Word documents. Often times you may want to include hyperlinks in your document to go to a specific webpage, to open an email program, or to even just jump to another location in the document. Lets look at how to add these types of hyperlinks to each document output type. In this post, I’ll be covering HTML Hyperlinks and I’ll cover <a href=\"http://edpflager.com/2018/11/21/hyperlinks-in-r-markdown-word-output/\">Word</a> and <a href=\"http://edpflager.com/2018/11/23/hyperlinks-in-r-markdown-pdf-output/\">PDF</a> in subsequent posts. By far, adding hyperlinks when generating an HTML document is easy in R Markdown. An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a>.</p>","more":"<p>The resulting HTML doc then resembles this:</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-hyperlink.png\"></a></p>\n<p>If you want to hide the hyperlink there are a couple of ways to do it, enclose the text you want to substitute for your hyperlink in brackets and follow it with the hyperlink tag in parentheses right after it. The other method is a more traditional HTML approach, enclosing the link inside a set of greater-than and less-than tags, using an href call with the hyperlink, and nesting your substitute for the hyperlink before a closing /a tag. A simple example to illustrate both looks like this with the resulting generated HTML putput:</p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this or<br>like <a href=\"http://edpflager.com\">this</a><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-hidehyperlink-1.png\"></a></p>\n<p>To include a hyperlink to open an email program (assuming the reader has one installed), we can use either of the methods described above. The code to put in your R Markdown document looks like this:</p>\n<p>[email me](<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">mailto:test@foobar.com?subject=Hyperlinks</a>) or<br><a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a><a href=\"http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/html-emailhyperlink.png\"></a></p>\n<p>Finally, to include a hyperlink to a point further in the document, you can use the HTML anchor tag by placing your hyperlink at some point in your document and then defining the landing location for the tag in another part of the document. The initial hyperlink is similar to the ones discussed above, but instead of including a full URL, you use a pound sign (#) and a unique name for the anchor. Then later in the document where you want the hyperlink to jump to, you define the landing link with the id= tag. Let’s look at an example:</p>\n<p><a href=\"#land-here\">Jump to the landing spot.</a></p>\n<p>OTHER TEXT GOES HERE</p>\n<p><a id=\"land-here\">Landing spot</a></p>\n<p>In my sample R Markdown document, I pasted several paragraphs of Lorem Epsum text to separate the two tags. Also of note:</p>\n<ul>\n<li>You don’t need to include actual text as part of your landing spot. If you have the opening and closing tags next to each other for the landing spot, it still works fine.</li>\n<li>Its generally a good idea to format your landing spot as a section header, but its not required.</li>\n</ul>\n<p>Here is the code for my sample document:</p>\n<p>-–<br>title: “Hyperlinks-HTML”<br>output: html_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```<br><a href=\"#land-here\">Jump to the landing spot.</a></p>\n<p>An inline hyperlink to a website in your document is enclosed in less-than<br>and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a></p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this or<br>like <a href=\"http://edpflager.com\">this</a></p>\n<p>Provide an email hyperlink like this:</p>\n<p>[email me](<a href=\"mailto:test@foobar.com?subject=Hyperlinks\">mailto:test@foobar.com?subject=Hyperlinks</a>) or<br><a href=\"mailto:test@foobar.com?subject=Hyperlinks\">email me</a></p>\n<p>Lorem ipsum</p>\n<p><a id=\"land-here\">Landing spot</a></p>"},{"title":"Hyperlinks in R Markdown Word output","id":"4178","comments":0,"date":"2018-11-21T17:14:13.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) In my [last post](http://edpflager.com/2018/11/16/html-hyperlinks-in-r-markdown-pdfs/), I covered adding hyperlinks to R Markdown files for HTML output. This time, I'll look at adding similar hyperlinks to Microsoft Word output from R Markdown documents. In my experience, many people are not aware that you can have hyperlinks in Word documents, but they do provide some useful functionality. Although there isn't as many available options, the main ones are available: link to a website, and link to sending an email. You can show the destination webpage URL or replace it with alternative text and have a tooltip that pops up with the actual link when you hover over the hyperlink. When setting up an email hyperlink, you can include a subject for the email, and also a pre-filled message body if you like! The only option I have not been able to replicate from the HTML options was the landing page option (but I'm working on it, and I will update this page if I find a solution)\n<!-- more -->\nAs with HTML output, in Word output, an inline hyperlink to a website in your document can be enclosed in less-than and greater-than tags, like this: <http://edpflager.com> or can simply be typed out: http://edpflager.com. The resulting Word doc then resembles this:\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink-300x64.png)](http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink.png)\n\nTo hide your URL and display plain text instead, enclose the alternative text in brackets, and follow it with your URL inside of parentheses. The code would then look like this:\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this and provide \na tooltip instead. Whatever is contained in the preceding brackets becomes\n the hyperlink.\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png) Finally, if you want to include a hyperlink in your Word document to  create an email, the syntax is identical to one I used in the HTML example.\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks \nwork in Word?body=This website is awesome>)\n\nUsing this syntax will result in the words \"email me\" being formatted as a hyperlink in your output Word document. Clicking that link will launch your email program if one is installed and start a new email message with the specified recipient, subject line and message body! And here is my code for this post:\n\n\\---\ntitle: \"Hyperlinks-Word\"\noutput:\n   word\\_document: default\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\nAn inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <http://edpflager.com>\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this and provide a tooltip instead. Whatever is contained in the preceding brackets becomes the hyperlink.\n\nThe HTML version doesn't work for Word output: like <a href=\"http://edpflager.com\">this</a>\n\nProvide an email hyperlink like this:\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks\nwork in Word?body=This website is awesome>)","source":"_posts/hyperlinks-in-r-markdown-word-output.md","raw":"---\ntitle: Hyperlinks in R Markdown Word output\ntags:\n  - howto\n  - Pandoc\n  - R Markdown\nid: '4178'\ncategories:\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-11-21 12:14:13\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) In my [last post](http://edpflager.com/2018/11/16/html-hyperlinks-in-r-markdown-pdfs/), I covered adding hyperlinks to R Markdown files for HTML output. This time, I'll look at adding similar hyperlinks to Microsoft Word output from R Markdown documents. In my experience, many people are not aware that you can have hyperlinks in Word documents, but they do provide some useful functionality. Although there isn't as many available options, the main ones are available: link to a website, and link to sending an email. You can show the destination webpage URL or replace it with alternative text and have a tooltip that pops up with the actual link when you hover over the hyperlink. When setting up an email hyperlink, you can include a subject for the email, and also a pre-filled message body if you like! The only option I have not been able to replicate from the HTML options was the landing page option (but I'm working on it, and I will update this page if I find a solution)\n<!-- more -->\nAs with HTML output, in Word output, an inline hyperlink to a website in your document can be enclosed in less-than and greater-than tags, like this: <http://edpflager.com> or can simply be typed out: http://edpflager.com. The resulting Word doc then resembles this:\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink-300x64.png)](http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink.png)\n\nTo hide your URL and display plain text instead, enclose the alternative text in brackets, and follow it with your URL inside of parentheses. The code would then look like this:\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this and provide \na tooltip instead. Whatever is contained in the preceding brackets becomes\n the hyperlink.\n\n[![](http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png)](http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png) Finally, if you want to include a hyperlink in your Word document to  create an email, the syntax is identical to one I used in the HTML example.\n\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks \nwork in Word?body=This website is awesome>)\n\nUsing this syntax will result in the words \"email me\" being formatted as a hyperlink in your output Word document. Clicking that link will launch your email program if one is installed and start a new email message with the specified recipient, subject line and message body! And here is my code for this post:\n\n\\---\ntitle: \"Hyperlinks-Word\"\noutput:\n   word\\_document: default\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\nAn inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <http://edpflager.com>\n\nHide your \\[hyperlink\\](<http://edpflager.com>) like this and provide a tooltip instead. Whatever is contained in the preceding brackets becomes the hyperlink.\n\nThe HTML version doesn't work for Word output: like <a href=\"http://edpflager.com\">this</a>\n\nProvide an email hyperlink like this:\n\\[email me\\](<mailto:test@foobar.com?subject=Hyperlinks\nwork in Word?body=This website is awesome>)","slug":"hyperlinks-in-r-markdown-word-output","published":1,"updated":"2020-08-23T20:54:35.194Z","layout":"post","photos":[],"link":"","_id":"ckeaq99zz0048sdjxd4lic37l","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> In my <a href=\"http://edpflager.com/2018/11/16/html-hyperlinks-in-r-markdown-pdfs/\">last post</a>, I covered adding hyperlinks to R Markdown files for HTML output. This time, I’ll look at adding similar hyperlinks to Microsoft Word output from R Markdown documents. In my experience, many people are not aware that you can have hyperlinks in Word documents, but they do provide some useful functionality. Although there isn’t as many available options, the main ones are available: link to a website, and link to sending an email. You can show the destination webpage URL or replace it with alternative text and have a tooltip that pops up with the actual link when you hover over the hyperlink. When setting up an email hyperlink, you can include a subject for the email, and also a pre-filled message body if you like! The only option I have not been able to replicate from the HTML options was the landing page option (but I’m working on it, and I will update this page if I find a solution)</p>\n<a id=\"more\"></a>\n<p>As with HTML output, in Word output, an inline hyperlink to a website in your document can be enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a> or can simply be typed out: <a href=\"http://edpflager.com/\">http://edpflager.com</a>. The resulting Word doc then resembles this:</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink-300x64.png\"></a></p>\n<p>To hide your URL and display plain text instead, enclose the alternative text in brackets, and follow it with your URL inside of parentheses. The code would then look like this:</p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this and provide<br>a tooltip instead. Whatever is contained in the preceding brackets becomes<br> the hyperlink.</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png\"></a> Finally, if you want to include a hyperlink in your Word document to  create an email, the syntax is identical to one I used in the HTML example.</p>\n<p>[email me](&lt;mailto:<a href=\"mailto:&#x74;&#x65;&#x73;&#x74;&#x40;&#x66;&#x6f;&#x6f;&#x62;&#x61;&#x72;&#46;&#x63;&#x6f;&#109;\">&#x74;&#x65;&#x73;&#x74;&#x40;&#x66;&#x6f;&#x6f;&#x62;&#x61;&#x72;&#46;&#x63;&#x6f;&#109;</a>?subject=Hyperlinks<br>work in Word?body=This website is awesome&gt;)</p>\n<p>Using this syntax will result in the words “email me” being formatted as a hyperlink in your output Word document. Clicking that link will launch your email program if one is installed and start a new email message with the specified recipient, subject line and message body! And here is my code for this post:</p>\n<p>-–<br>title: “Hyperlinks-Word”<br>output:<br>   word_document: default</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a></p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this and provide a tooltip instead. Whatever is contained in the preceding brackets becomes the hyperlink.</p>\n<p>The HTML version doesn’t work for Word output: like <a href=\"http://edpflager.com\">this</a></p>\n<p>Provide an email hyperlink like this:<br>[email me](&lt;mailto:<a href=\"mailto:&#x74;&#101;&#115;&#116;&#x40;&#102;&#111;&#x6f;&#x62;&#x61;&#114;&#x2e;&#x63;&#x6f;&#109;\">&#x74;&#101;&#115;&#116;&#x40;&#102;&#111;&#x6f;&#x62;&#x61;&#114;&#x2e;&#x63;&#x6f;&#109;</a>?subject=Hyperlinks<br>work in Word?body=This website is awesome&gt;)</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> In my <a href=\"http://edpflager.com/2018/11/16/html-hyperlinks-in-r-markdown-pdfs/\">last post</a>, I covered adding hyperlinks to R Markdown files for HTML output. This time, I’ll look at adding similar hyperlinks to Microsoft Word output from R Markdown documents. In my experience, many people are not aware that you can have hyperlinks in Word documents, but they do provide some useful functionality. Although there isn’t as many available options, the main ones are available: link to a website, and link to sending an email. You can show the destination webpage URL or replace it with alternative text and have a tooltip that pops up with the actual link when you hover over the hyperlink. When setting up an email hyperlink, you can include a subject for the email, and also a pre-filled message body if you like! The only option I have not been able to replicate from the HTML options was the landing page option (but I’m working on it, and I will update this page if I find a solution)</p>","more":"<p>As with HTML output, in Word output, an inline hyperlink to a website in your document can be enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a> or can simply be typed out: <a href=\"http://edpflager.com/\">http://edpflager.com</a>. The resulting Word doc then resembles this:</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hyperlink-300x64.png\"></a></p>\n<p>To hide your URL and display plain text instead, enclose the alternative text in brackets, and follow it with your URL inside of parentheses. The code would then look like this:</p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this and provide<br>a tooltip instead. Whatever is contained in the preceding brackets becomes<br> the hyperlink.</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/11/Word-hidehyperlink.png\"></a> Finally, if you want to include a hyperlink in your Word document to  create an email, the syntax is identical to one I used in the HTML example.</p>\n<p>[email me](&lt;mailto:<a href=\"mailto:&#x74;&#x65;&#x73;&#x74;&#x40;&#x66;&#x6f;&#x6f;&#x62;&#x61;&#x72;&#46;&#x63;&#x6f;&#109;\">&#x74;&#x65;&#x73;&#x74;&#x40;&#x66;&#x6f;&#x6f;&#x62;&#x61;&#x72;&#46;&#x63;&#x6f;&#109;</a>?subject=Hyperlinks<br>work in Word?body=This website is awesome&gt;)</p>\n<p>Using this syntax will result in the words “email me” being formatted as a hyperlink in your output Word document. Clicking that link will launch your email program if one is installed and start a new email message with the specified recipient, subject line and message body! And here is my code for this post:</p>\n<p>-–<br>title: “Hyperlinks-Word”<br>output:<br>   word_document: default</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>An inline hyperlink to a website in your document is enclosed in less-than and greater-than tags, like this: <a href=\"http://edpflager.com/\">http://edpflager.com</a></p>\n<p>Hide your [hyperlink](<a href=\"http://edpflager.com/\">http://edpflager.com</a>) like this and provide a tooltip instead. Whatever is contained in the preceding brackets becomes the hyperlink.</p>\n<p>The HTML version doesn’t work for Word output: like <a href=\"http://edpflager.com\">this</a></p>\n<p>Provide an email hyperlink like this:<br>[email me](&lt;mailto:<a href=\"mailto:&#x74;&#101;&#115;&#116;&#x40;&#102;&#111;&#x6f;&#x62;&#x61;&#114;&#x2e;&#x63;&#x6f;&#109;\">&#x74;&#101;&#115;&#116;&#x40;&#102;&#111;&#x6f;&#x62;&#x61;&#114;&#x2e;&#x63;&#x6f;&#109;</a>?subject=Hyperlinks<br>work in Word?body=This website is awesome&gt;)</p>"},{"title":"Illustration of ETL","id":"2102","comments":0,"date":"2014-05-19T14:45:20.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2014/05/ETL-300x161.png)](http://edpflager.com/wp-content/uploads/2014/05/ETL.png)Illustration of ETL This is an updated graphic that hung in my office space at work for a number of years. I was asked many times what I did, and had to explain what ETL meant. Eventually I came up with this idea, grabbed some stock images from iStockPhoto and put it together. Enjoy!","source":"_posts/illustration-of-etl.md","raw":"---\ntitle: Illustration of ETL\ntags:\n  - ETL\n  - goofy\n  - humor\nid: '2102'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-05-19 10:45:20\n---\n\n[![](http://edpflager.com/wp-content/uploads/2014/05/ETL-300x161.png)](http://edpflager.com/wp-content/uploads/2014/05/ETL.png)Illustration of ETL This is an updated graphic that hung in my office space at work for a number of years. I was asked many times what I did, and had to explain what ETL meant. Eventually I came up with this idea, grabbed some stock images from iStockPhoto and put it together. Enjoy!","slug":"illustration-of-etl","published":1,"updated":"2020-08-23T20:54:34.846Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a02004bsdjx98b80r6n","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/ETL.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/ETL-300x161.png\"></a>Illustration of ETL This is an updated graphic that hung in my office space at work for a number of years. I was asked many times what I did, and had to explain what ETL meant. Eventually I came up with this idea, grabbed some stock images from iStockPhoto and put it together. Enjoy!</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/ETL.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/ETL-300x161.png\"></a>Illustration of ETL This is an updated graphic that hung in my office space at work for a number of years. I was asked many times what I did, and had to explain what ETL meant. Eventually I came up with this idea, grabbed some stock images from iStockPhoto and put it together. Enjoy!</p>\n"},{"title":"ApacheDS (LDAP) - Part 1","id":"3029","comments":0,"date":"2015-11-19T20:45:15.000Z","_content":"\n[![directory](http://edpflager.com/wp-content/uploads/2015/11/directory-300x274.jpg)](http://edpflager.com/wp-content/uploads/2015/11/directory.jpg)While working on a recent project, I found the software I was researching had the capability to work with an LDAP server. My experience with LDAP has been pretty limited, mainly using Microsoft's Active Directory for work, so I decided to look into open source alternatives. For the uninitiated, LDAP (Lightweight Directory Access Protocol) and an LDAP server provide a centralized database where an individual's user account and password is stored and then shared between many services on a network. Many other entities on the network (groups, servers, printers, etc) can also be referenced in an LDAP catalog, making discovery and access much easier. When a user logs into a company's network, they are then able to access other network resources (email, other internal applications, file shares, printers), by using the same user name and password. The network administrator only needs to enter the user's information in one location, and all security settings are defined in that location as well. There are a number of Open Source implementations of LDAP, including OpenLDAP, OpenDS/DJ, and ApacheDS.  OpenLDAP is a well documented project, and has been around for a number of years. OpenDS is owned by Sun MIcroSystems and is no longer maintained and was forked to the OpenDJ project.  After researching, I decided for my purposes that the ApacheDS server would best meet my needs, so this article will walk through installing and getting ApacheDS started on a Linux Mint box. This first section should work OK on any Ubuntu/Debian based distribution, and most of this series will work fine on any distribution. YMMV\n<!-- more -->\nSET YOUR HOSTNAME\n\n1.  As a first step, make sure your Linux Mint box is up to date. From a command prompt, run the below command:\n    \n    sudo apt-get update\n    \n2.  Check your hostname, and rename your box as appropriate. To check your current hostname, enter:\n    \n    hostname\n    \n3.  To edit the host name, edit the **/etc/hostname** file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com. ** Open nano to edit the file with this command:\n    \n    sudo nano /etc/hostname\n    \n    Save the file and close it when you are complete.\n\nSET A STATIC IP ADDRESS\n\n1.   You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:\n    \n    **ifconfig**\n    \n2.  From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.[![NetworkConnection](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection-300x243.png)](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png)\n3.  A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.[![IPv4](http://edpflager.com/wp-content/uploads/2015/11/IPv4-238x300.png)](http://edpflager.com/wp-content/uploads/2015/11/IPv4.png)\n4.  Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the **ifconfig** command from step 1. **Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.**\n5.  Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google's public DNS servers, by adding in **8.8.8.8** and/or **8.8.4.4.** Separate multiple server addresses by using a comma.\n6.  Click over to the IPv6 tab. For our purposes, we don't need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.\n7.  At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the **\\-** setting to reconnect. (you should get another onscreen notification that you are connected).[![restartnetwork](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)\n8.  Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:\n    \n    ping www.linuxmint.com\n    \n    (or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.\n\nEDIT THE LOCAL HOST FILE\n\n1.  The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The **hosts file** is used by the operating system to map host names to IP addresses.\n2.  Open a terminal prompt again, and enter this command:\n    \n     sudo nano /etc/hosts\n    \n3.  The nano text editor will open and display the contents of your HOSTS file. It should look something like this:\n    \n    127.0.0.1    localhost\n    127.0.1.1    originalname\n    \n4.  Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:\n    \n    127.0.0.1         localhost\n    10.0.1.22         **ldap       ldap.test.com**\n    \n5.  Restart (just in case).\n\nCHECK YOUR JAVA VERSION\n\n1.  Open a terminal prompt and enter the command:\n    \n    java -version\n    \n    to see if JAVA is installed. By default,  OpenJDK is installed, or you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation [how to](http://community.linuxmint.com/tutorial/view/1091).\n\nFINALLY, INSTALLING APACHE DS\n\n1.  Download the latest version of ApacheDS from the [app website](http://directory.apache.org/apacheds/download/download-linux-deb.html) (either 64-bit or 32-bit depending on your system's architecture).\n2.  Open a Terminal and navigate to where you downloaded the DEB package  and run:\n    \n     sudo dpkg -i apacheds-2.0.0-M20-amd64.deb (substitute the version number you downloaded)\n    \n3.  After supplying your SUDO password, the software will install very quickly and you will see a message that the reconfigured ureadahead will take effect after the next reboot.\n4.  Before you reboot, set the LDAP service to start automatically when you boot the system. Open a terminal prompt, and execute the following commands:\n    \n    $ cd /etc/init.d\n    $ sudo update\\-rc.d apacheds\\-2.0.0\\-M20\\-default defaults\n    \n5.  You'll see several lines of output indicating that ApacheDS has been added to various runlevels on your system.  Go ahead and reboot now.\n6.  That's it! ApacheDS should now be up and running on your system.\n\nIn Part 2, I'll cover installing the ApacheDS Studio and configuring it to connect to your LDAP catalog, as well as entering your first entity in the catalog.","source":"_posts/install-apacheldap-on-linuxmint-part-1.md","raw":"---\ntitle: ApacheDS (LDAP) - Part 1\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - LDAP\n  - Mint\n  - SysAdmin\nid: '3029'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-11-19 15:45:15\n---\n\n[![directory](http://edpflager.com/wp-content/uploads/2015/11/directory-300x274.jpg)](http://edpflager.com/wp-content/uploads/2015/11/directory.jpg)While working on a recent project, I found the software I was researching had the capability to work with an LDAP server. My experience with LDAP has been pretty limited, mainly using Microsoft's Active Directory for work, so I decided to look into open source alternatives. For the uninitiated, LDAP (Lightweight Directory Access Protocol) and an LDAP server provide a centralized database where an individual's user account and password is stored and then shared between many services on a network. Many other entities on the network (groups, servers, printers, etc) can also be referenced in an LDAP catalog, making discovery and access much easier. When a user logs into a company's network, they are then able to access other network resources (email, other internal applications, file shares, printers), by using the same user name and password. The network administrator only needs to enter the user's information in one location, and all security settings are defined in that location as well. There are a number of Open Source implementations of LDAP, including OpenLDAP, OpenDS/DJ, and ApacheDS.  OpenLDAP is a well documented project, and has been around for a number of years. OpenDS is owned by Sun MIcroSystems and is no longer maintained and was forked to the OpenDJ project.  After researching, I decided for my purposes that the ApacheDS server would best meet my needs, so this article will walk through installing and getting ApacheDS started on a Linux Mint box. This first section should work OK on any Ubuntu/Debian based distribution, and most of this series will work fine on any distribution. YMMV\n<!-- more -->\nSET YOUR HOSTNAME\n\n1.  As a first step, make sure your Linux Mint box is up to date. From a command prompt, run the below command:\n    \n    sudo apt-get update\n    \n2.  Check your hostname, and rename your box as appropriate. To check your current hostname, enter:\n    \n    hostname\n    \n3.  To edit the host name, edit the **/etc/hostname** file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com. ** Open nano to edit the file with this command:\n    \n    sudo nano /etc/hostname\n    \n    Save the file and close it when you are complete.\n\nSET A STATIC IP ADDRESS\n\n1.   You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:\n    \n    **ifconfig**\n    \n2.  From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.[![NetworkConnection](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection-300x243.png)](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png)\n3.  A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.[![IPv4](http://edpflager.com/wp-content/uploads/2015/11/IPv4-238x300.png)](http://edpflager.com/wp-content/uploads/2015/11/IPv4.png)\n4.  Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the **ifconfig** command from step 1. **Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.**\n5.  Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google's public DNS servers, by adding in **8.8.8.8** and/or **8.8.4.4.** Separate multiple server addresses by using a comma.\n6.  Click over to the IPv6 tab. For our purposes, we don't need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.\n7.  At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the **\\-** setting to reconnect. (you should get another onscreen notification that you are connected).[![restartnetwork](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)\n8.  Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:\n    \n    ping www.linuxmint.com\n    \n    (or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.\n\nEDIT THE LOCAL HOST FILE\n\n1.  The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The **hosts file** is used by the operating system to map host names to IP addresses.\n2.  Open a terminal prompt again, and enter this command:\n    \n     sudo nano /etc/hosts\n    \n3.  The nano text editor will open and display the contents of your HOSTS file. It should look something like this:\n    \n    127.0.0.1    localhost\n    127.0.1.1    originalname\n    \n4.  Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:\n    \n    127.0.0.1         localhost\n    10.0.1.22         **ldap       ldap.test.com**\n    \n5.  Restart (just in case).\n\nCHECK YOUR JAVA VERSION\n\n1.  Open a terminal prompt and enter the command:\n    \n    java -version\n    \n    to see if JAVA is installed. By default,  OpenJDK is installed, or you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation [how to](http://community.linuxmint.com/tutorial/view/1091).\n\nFINALLY, INSTALLING APACHE DS\n\n1.  Download the latest version of ApacheDS from the [app website](http://directory.apache.org/apacheds/download/download-linux-deb.html) (either 64-bit or 32-bit depending on your system's architecture).\n2.  Open a Terminal and navigate to where you downloaded the DEB package  and run:\n    \n     sudo dpkg -i apacheds-2.0.0-M20-amd64.deb (substitute the version number you downloaded)\n    \n3.  After supplying your SUDO password, the software will install very quickly and you will see a message that the reconfigured ureadahead will take effect after the next reboot.\n4.  Before you reboot, set the LDAP service to start automatically when you boot the system. Open a terminal prompt, and execute the following commands:\n    \n    $ cd /etc/init.d\n    $ sudo update\\-rc.d apacheds\\-2.0.0\\-M20\\-default defaults\n    \n5.  You'll see several lines of output indicating that ApacheDS has been added to various runlevels on your system.  Go ahead and reboot now.\n6.  That's it! ApacheDS should now be up and running on your system.\n\nIn Part 2, I'll cover installing the ApacheDS Studio and configuring it to connect to your LDAP catalog, as well as entering your first entity in the catalog.","slug":"install-apacheldap-on-linuxmint-part-1","published":1,"updated":"2020-08-23T20:54:34.990Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a0e004gsdjxgbup1958","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/directory.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/directory-300x274.jpg\" alt=\"directory\"></a>While working on a recent project, I found the software I was researching had the capability to work with an LDAP server. My experience with LDAP has been pretty limited, mainly using Microsoft’s Active Directory for work, so I decided to look into open source alternatives. For the uninitiated, LDAP (Lightweight Directory Access Protocol) and an LDAP server provide a centralized database where an individual’s user account and password is stored and then shared between many services on a network. Many other entities on the network (groups, servers, printers, etc) can also be referenced in an LDAP catalog, making discovery and access much easier. When a user logs into a company’s network, they are then able to access other network resources (email, other internal applications, file shares, printers), by using the same user name and password. The network administrator only needs to enter the user’s information in one location, and all security settings are defined in that location as well. There are a number of Open Source implementations of LDAP, including OpenLDAP, OpenDS/DJ, and ApacheDS.  OpenLDAP is a well documented project, and has been around for a number of years. OpenDS is owned by Sun MIcroSystems and is no longer maintained and was forked to the OpenDJ project.  After researching, I decided for my purposes that the ApacheDS server would best meet my needs, so this article will walk through installing and getting ApacheDS started on a Linux Mint box. This first section should work OK on any Ubuntu/Debian based distribution, and most of this series will work fine on any distribution. YMMV</p>\n<a id=\"more\"></a>\n<p>SET YOUR HOSTNAME</p>\n<ol>\n<li><p>As a first step, make sure your Linux Mint box is up to date. From a command prompt, run the below command:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Check your hostname, and rename your box as appropriate. To check your current hostname, enter:</p>\n<p>hostname</p>\n</li>\n<li><p>To edit the host name, edit the <strong>/etc/hostname</strong> file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com. ** Open nano to edit the file with this command:</p>\n<p>sudo nano /etc/hostname</p>\n<p>Save the file and close it when you are complete.</p>\n</li>\n</ol>\n<p>SET A STATIC IP ADDRESS</p>\n<ol>\n<li><p> You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:</p>\n<p><strong>ifconfig</strong></p>\n</li>\n<li><p>From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection-300x243.png\" alt=\"NetworkConnection\"></a></p>\n</li>\n<li><p>A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4-238x300.png\" alt=\"IPv4\"></a></p>\n</li>\n<li><p>Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the <strong>ifconfig</strong> command from step 1. <strong>Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.</strong></p>\n</li>\n<li><p>Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in <strong>8.8.8.8</strong> and/or <strong>8.8.4.4.</strong> Separate multiple server addresses by using a comma.</p>\n</li>\n<li><p>Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.</p>\n</li>\n<li><p>At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the <strong>-</strong> setting to reconnect. (you should get another onscreen notification that you are connected).<a href=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\" alt=\"restartnetwork\"></a></p>\n</li>\n<li><p>Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:</p>\n<p>ping <a href=\"http://www.linuxmint.com/\">www.linuxmint.com</a></p>\n<p>(or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.</p>\n</li>\n</ol>\n<p>EDIT THE LOCAL HOST FILE</p>\n<ol>\n<li><p>The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The <strong>hosts file</strong> is used by the operating system to map host names to IP addresses.</p>\n</li>\n<li><p>Open a terminal prompt again, and enter this command:</p>\n<p> sudo nano /etc/hosts</p>\n</li>\n<li><p>The nano text editor will open and display the contents of your HOSTS file. It should look something like this:</p>\n<p>127.0.0.1    localhost<br>127.0.1.1    originalname</p>\n</li>\n<li><p>Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:</p>\n<p>127.0.0.1         localhost<br>10.0.1.22         <strong>ldap       ldap.test.com</strong></p>\n</li>\n<li><p>Restart (just in case).</p>\n</li>\n</ol>\n<p>CHECK YOUR JAVA VERSION</p>\n<ol>\n<li><p>Open a terminal prompt and enter the command:</p>\n<p>java -version</p>\n<p>to see if JAVA is installed. By default,  OpenJDK is installed, or you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation <a href=\"http://community.linuxmint.com/tutorial/view/1091\">how to</a>.</p>\n</li>\n</ol>\n<p>FINALLY, INSTALLING APACHE DS</p>\n<ol>\n<li><p>Download the latest version of ApacheDS from the <a href=\"http://directory.apache.org/apacheds/download/download-linux-deb.html\">app website</a> (either 64-bit or 32-bit depending on your system’s architecture).</p>\n</li>\n<li><p>Open a Terminal and navigate to where you downloaded the DEB package  and run:</p>\n<p> sudo dpkg -i apacheds-2.0.0-M20-amd64.deb (substitute the version number you downloaded)</p>\n</li>\n<li><p>After supplying your SUDO password, the software will install very quickly and you will see a message that the reconfigured ureadahead will take effect after the next reboot.</p>\n</li>\n<li><p>Before you reboot, set the LDAP service to start automatically when you boot the system. Open a terminal prompt, and execute the following commands:</p>\n<p>$ cd /etc/init.d<br>$ sudo update-rc.d apacheds-2.0.0-M20-default defaults</p>\n</li>\n<li><p>You’ll see several lines of output indicating that ApacheDS has been added to various runlevels on your system.  Go ahead and reboot now.</p>\n</li>\n<li><p>That’s it! ApacheDS should now be up and running on your system.</p>\n</li>\n</ol>\n<p>In Part 2, I’ll cover installing the ApacheDS Studio and configuring it to connect to your LDAP catalog, as well as entering your first entity in the catalog.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/directory.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/directory-300x274.jpg\" alt=\"directory\"></a>While working on a recent project, I found the software I was researching had the capability to work with an LDAP server. My experience with LDAP has been pretty limited, mainly using Microsoft’s Active Directory for work, so I decided to look into open source alternatives. For the uninitiated, LDAP (Lightweight Directory Access Protocol) and an LDAP server provide a centralized database where an individual’s user account and password is stored and then shared between many services on a network. Many other entities on the network (groups, servers, printers, etc) can also be referenced in an LDAP catalog, making discovery and access much easier. When a user logs into a company’s network, they are then able to access other network resources (email, other internal applications, file shares, printers), by using the same user name and password. The network administrator only needs to enter the user’s information in one location, and all security settings are defined in that location as well. There are a number of Open Source implementations of LDAP, including OpenLDAP, OpenDS/DJ, and ApacheDS.  OpenLDAP is a well documented project, and has been around for a number of years. OpenDS is owned by Sun MIcroSystems and is no longer maintained and was forked to the OpenDJ project.  After researching, I decided for my purposes that the ApacheDS server would best meet my needs, so this article will walk through installing and getting ApacheDS started on a Linux Mint box. This first section should work OK on any Ubuntu/Debian based distribution, and most of this series will work fine on any distribution. YMMV</p>","more":"<p>SET YOUR HOSTNAME</p>\n<ol>\n<li><p>As a first step, make sure your Linux Mint box is up to date. From a command prompt, run the below command:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Check your hostname, and rename your box as appropriate. To check your current hostname, enter:</p>\n<p>hostname</p>\n</li>\n<li><p>To edit the host name, edit the <strong>/etc/hostname</strong> file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com. ** Open nano to edit the file with this command:</p>\n<p>sudo nano /etc/hostname</p>\n<p>Save the file and close it when you are complete.</p>\n</li>\n</ol>\n<p>SET A STATIC IP ADDRESS</p>\n<ol>\n<li><p> You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:</p>\n<p><strong>ifconfig</strong></p>\n</li>\n<li><p>From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection-300x243.png\" alt=\"NetworkConnection\"></a></p>\n</li>\n<li><p>A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.<a href=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4-238x300.png\" alt=\"IPv4\"></a></p>\n</li>\n<li><p>Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the <strong>ifconfig</strong> command from step 1. <strong>Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.</strong></p>\n</li>\n<li><p>Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in <strong>8.8.8.8</strong> and/or <strong>8.8.4.4.</strong> Separate multiple server addresses by using a comma.</p>\n</li>\n<li><p>Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.</p>\n</li>\n<li><p>At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the <strong>-</strong> setting to reconnect. (you should get another onscreen notification that you are connected).<a href=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\" alt=\"restartnetwork\"></a></p>\n</li>\n<li><p>Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:</p>\n<p>ping <a href=\"http://www.linuxmint.com/\">www.linuxmint.com</a></p>\n<p>(or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.</p>\n</li>\n</ol>\n<p>EDIT THE LOCAL HOST FILE</p>\n<ol>\n<li><p>The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The <strong>hosts file</strong> is used by the operating system to map host names to IP addresses.</p>\n</li>\n<li><p>Open a terminal prompt again, and enter this command:</p>\n<p> sudo nano /etc/hosts</p>\n</li>\n<li><p>The nano text editor will open and display the contents of your HOSTS file. It should look something like this:</p>\n<p>127.0.0.1    localhost<br>127.0.1.1    originalname</p>\n</li>\n<li><p>Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:</p>\n<p>127.0.0.1         localhost<br>10.0.1.22         <strong>ldap       ldap.test.com</strong></p>\n</li>\n<li><p>Restart (just in case).</p>\n</li>\n</ol>\n<p>CHECK YOUR JAVA VERSION</p>\n<ol>\n<li><p>Open a terminal prompt and enter the command:</p>\n<p>java -version</p>\n<p>to see if JAVA is installed. By default,  OpenJDK is installed, or you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation <a href=\"http://community.linuxmint.com/tutorial/view/1091\">how to</a>.</p>\n</li>\n</ol>\n<p>FINALLY, INSTALLING APACHE DS</p>\n<ol>\n<li><p>Download the latest version of ApacheDS from the <a href=\"http://directory.apache.org/apacheds/download/download-linux-deb.html\">app website</a> (either 64-bit or 32-bit depending on your system’s architecture).</p>\n</li>\n<li><p>Open a Terminal and navigate to where you downloaded the DEB package  and run:</p>\n<p> sudo dpkg -i apacheds-2.0.0-M20-amd64.deb (substitute the version number you downloaded)</p>\n</li>\n<li><p>After supplying your SUDO password, the software will install very quickly and you will see a message that the reconfigured ureadahead will take effect after the next reboot.</p>\n</li>\n<li><p>Before you reboot, set the LDAP service to start automatically when you boot the system. Open a terminal prompt, and execute the following commands:</p>\n<p>$ cd /etc/init.d<br>$ sudo update-rc.d apacheds-2.0.0-M20-default defaults</p>\n</li>\n<li><p>You’ll see several lines of output indicating that ApacheDS has been added to various runlevels on your system.  Go ahead and reboot now.</p>\n</li>\n<li><p>That’s it! ApacheDS should now be up and running on your system.</p>\n</li>\n</ol>\n<p>In Part 2, I’ll cover installing the ApacheDS Studio and configuring it to connect to your LDAP catalog, as well as entering your first entity in the catalog.</p>"},{"title":"Install MongoDB as a Service in CentOS 6.x","id":"2220","comments":0,"date":"2014-07-15T23:17:32.000Z","_content":"\n[![servicebell](http://edpflager.com/wp-content/uploads/2014/07/servicebell-298x300.png)](http://edpflager.com/wp-content/uploads/2014/07/servicebell.png)MongoDB provides a package for people to download and install on their system to try out the software, but you may want to install the software using their dedicated repository instead. Using the repository has a few advantages: MongoDB is configured as a service, so you can enable it to run when your system starts up if you like, and when newer versions of the software are released, you can update your installation via the YUM update functionality.\n<!-- more -->\n1.  Open a terminal session, switch to the root user and change to the /etc/yum.repos.d folder.\n2.  Create a new file called mongodb.repo, add the following lines to it and save it: \\[mongodb\\] name=MongoDB Repository baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86\\_64/ gpgcheck=0 enabled=1\n3.  To install all of the various MongoDB components on your system as well as configure a service for running the Mongo background server, enter this command: **yum install mongodb-org** (installs all of the packages below)\n4.  If you don't want all of the components, you can elect to install the packages individually by running:\n\n*   **yum install mongodb-org-server** (installs the main mongo background processes,  configuration files  and init scripts.)\n*   **yum install mongodb-org-mongos** (installs the Mongos daemon that is used to locate sharded data in your MongoDB system and retrieve it.)\n*   **yum install mongodb-org-shell** (installs the command line interface to the mongo server.)\n*   **yum install mongodb-org-tools** (installs a number of tools for interacting with MongoDB.)\n\nExit the terminal prompt and you have installed MongoDB.\n\n##### Start the MongoDB daemon/configure the service\n\n1.  Click on System in the menu bar, and then Administration in the submenu. Click on the Service option to open the Service Configuration GUI.\n2.  Scroll through the list of services until you find **mongod**. Click on it to highlight it.\n3.  If you only want to run it during your current session, click on the Start icon in toolbar at the top. Otherwise, click on the Enable icon, and then the Start icon to start the service and allow it to start when your system is restarted.\n4.  You'll be prompted to provide your root password to make the change. Enter it and click Authenticate to continue.\n5.  Exit out of the Service Configuration window and open a terminal window.\n6.  At the terminal prompt enter the command: **mongo** and you should see the MongoDB shell version (2.6.3 on mine), and a message telling you which database you are connecting to (test initially). [![mongoshell](http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png)](http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png)\n7.  To exit the shell, enter the command: quit()\n\nCongratulations - You have installed MongoDB as a service on your CentOS server.","source":"_posts/install-mongodb-as-a-service-in-centos-6-x.md","raw":"---\ntitle: Install MongoDB as a Service in CentOS 6.x\ntags:\n  - How-to\n  - technical\nid: '2220'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-07-15 19:17:32\n---\n\n[![servicebell](http://edpflager.com/wp-content/uploads/2014/07/servicebell-298x300.png)](http://edpflager.com/wp-content/uploads/2014/07/servicebell.png)MongoDB provides a package for people to download and install on their system to try out the software, but you may want to install the software using their dedicated repository instead. Using the repository has a few advantages: MongoDB is configured as a service, so you can enable it to run when your system starts up if you like, and when newer versions of the software are released, you can update your installation via the YUM update functionality.\n<!-- more -->\n1.  Open a terminal session, switch to the root user and change to the /etc/yum.repos.d folder.\n2.  Create a new file called mongodb.repo, add the following lines to it and save it: \\[mongodb\\] name=MongoDB Repository baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86\\_64/ gpgcheck=0 enabled=1\n3.  To install all of the various MongoDB components on your system as well as configure a service for running the Mongo background server, enter this command: **yum install mongodb-org** (installs all of the packages below)\n4.  If you don't want all of the components, you can elect to install the packages individually by running:\n\n*   **yum install mongodb-org-server** (installs the main mongo background processes,  configuration files  and init scripts.)\n*   **yum install mongodb-org-mongos** (installs the Mongos daemon that is used to locate sharded data in your MongoDB system and retrieve it.)\n*   **yum install mongodb-org-shell** (installs the command line interface to the mongo server.)\n*   **yum install mongodb-org-tools** (installs a number of tools for interacting with MongoDB.)\n\nExit the terminal prompt and you have installed MongoDB.\n\n##### Start the MongoDB daemon/configure the service\n\n1.  Click on System in the menu bar, and then Administration in the submenu. Click on the Service option to open the Service Configuration GUI.\n2.  Scroll through the list of services until you find **mongod**. Click on it to highlight it.\n3.  If you only want to run it during your current session, click on the Start icon in toolbar at the top. Otherwise, click on the Enable icon, and then the Start icon to start the service and allow it to start when your system is restarted.\n4.  You'll be prompted to provide your root password to make the change. Enter it and click Authenticate to continue.\n5.  Exit out of the Service Configuration window and open a terminal window.\n6.  At the terminal prompt enter the command: **mongo** and you should see the MongoDB shell version (2.6.3 on mine), and a message telling you which database you are connecting to (test initially). [![mongoshell](http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png)](http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png)\n7.  To exit the shell, enter the command: quit()\n\nCongratulations - You have installed MongoDB as a service on your CentOS server.","slug":"install-mongodb-as-a-service-in-centos-6-x","published":1,"updated":"2020-08-23T20:54:34.886Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a0g004jsdjxb95q4met","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/servicebell.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/servicebell-298x300.png\" alt=\"servicebell\"></a>MongoDB provides a package for people to download and install on their system to try out the software, but you may want to install the software using their dedicated repository instead. Using the repository has a few advantages: MongoDB is configured as a service, so you can enable it to run when your system starts up if you like, and when newer versions of the software are released, you can update your installation via the YUM update functionality.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Open a terminal session, switch to the root user and change to the /etc/yum.repos.d folder.</li>\n<li>Create a new file called mongodb.repo, add the following lines to it and save it: [mongodb] name=MongoDB Repository baseurl=<a href=\"http://downloads-distro.mongodb.org/repo/redhat/os/x86/_64/\">http://downloads-distro.mongodb.org/repo/redhat/os/x86\\_64/</a> gpgcheck=0 enabled=1</li>\n<li>To install all of the various MongoDB components on your system as well as configure a service for running the Mongo background server, enter this command: <strong>yum install mongodb-org</strong> (installs all of the packages below)</li>\n<li>If you don’t want all of the components, you can elect to install the packages individually by running:</li>\n</ol>\n<ul>\n<li><strong>yum install mongodb-org-server</strong> (installs the main mongo background processes,  configuration files  and init scripts.)</li>\n<li><strong>yum install mongodb-org-mongos</strong> (installs the Mongos daemon that is used to locate sharded data in your MongoDB system and retrieve it.)</li>\n<li><strong>yum install mongodb-org-shell</strong> (installs the command line interface to the mongo server.)</li>\n<li><strong>yum install mongodb-org-tools</strong> (installs a number of tools for interacting with MongoDB.)</li>\n</ul>\n<p>Exit the terminal prompt and you have installed MongoDB.</p>\n<h5 id=\"Start-the-MongoDB-daemon-configure-the-service\"><a href=\"#Start-the-MongoDB-daemon-configure-the-service\" class=\"headerlink\" title=\"Start the MongoDB daemon/configure the service\"></a>Start the MongoDB daemon/configure the service</h5><ol>\n<li>Click on System in the menu bar, and then Administration in the submenu. Click on the Service option to open the Service Configuration GUI.</li>\n<li>Scroll through the list of services until you find <strong>mongod</strong>. Click on it to highlight it.</li>\n<li>If you only want to run it during your current session, click on the Start icon in toolbar at the top. Otherwise, click on the Enable icon, and then the Start icon to start the service and allow it to start when your system is restarted.</li>\n<li>You’ll be prompted to provide your root password to make the change. Enter it and click Authenticate to continue.</li>\n<li>Exit out of the Service Configuration window and open a terminal window.</li>\n<li>At the terminal prompt enter the command: <strong>mongo</strong> and you should see the MongoDB shell version (2.6.3 on mine), and a message telling you which database you are connecting to (test initially). <a href=\"http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png\" alt=\"mongoshell\"></a></li>\n<li>To exit the shell, enter the command: quit()</li>\n</ol>\n<p>Congratulations - You have installed MongoDB as a service on your CentOS server.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/servicebell.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/servicebell-298x300.png\" alt=\"servicebell\"></a>MongoDB provides a package for people to download and install on their system to try out the software, but you may want to install the software using their dedicated repository instead. Using the repository has a few advantages: MongoDB is configured as a service, so you can enable it to run when your system starts up if you like, and when newer versions of the software are released, you can update your installation via the YUM update functionality.</p>","more":"<ol>\n<li>Open a terminal session, switch to the root user and change to the /etc/yum.repos.d folder.</li>\n<li>Create a new file called mongodb.repo, add the following lines to it and save it: [mongodb] name=MongoDB Repository baseurl=<a href=\"http://downloads-distro.mongodb.org/repo/redhat/os/x86/_64/\">http://downloads-distro.mongodb.org/repo/redhat/os/x86\\_64/</a> gpgcheck=0 enabled=1</li>\n<li>To install all of the various MongoDB components on your system as well as configure a service for running the Mongo background server, enter this command: <strong>yum install mongodb-org</strong> (installs all of the packages below)</li>\n<li>If you don’t want all of the components, you can elect to install the packages individually by running:</li>\n</ol>\n<ul>\n<li><strong>yum install mongodb-org-server</strong> (installs the main mongo background processes,  configuration files  and init scripts.)</li>\n<li><strong>yum install mongodb-org-mongos</strong> (installs the Mongos daemon that is used to locate sharded data in your MongoDB system and retrieve it.)</li>\n<li><strong>yum install mongodb-org-shell</strong> (installs the command line interface to the mongo server.)</li>\n<li><strong>yum install mongodb-org-tools</strong> (installs a number of tools for interacting with MongoDB.)</li>\n</ul>\n<p>Exit the terminal prompt and you have installed MongoDB.</p>\n<h5 id=\"Start-the-MongoDB-daemon-configure-the-service\"><a href=\"#Start-the-MongoDB-daemon-configure-the-service\" class=\"headerlink\" title=\"Start the MongoDB daemon/configure the service\"></a>Start the MongoDB daemon/configure the service</h5><ol>\n<li>Click on System in the menu bar, and then Administration in the submenu. Click on the Service option to open the Service Configuration GUI.</li>\n<li>Scroll through the list of services until you find <strong>mongod</strong>. Click on it to highlight it.</li>\n<li>If you only want to run it during your current session, click on the Start icon in toolbar at the top. Otherwise, click on the Enable icon, and then the Start icon to start the service and allow it to start when your system is restarted.</li>\n<li>You’ll be prompted to provide your root password to make the change. Enter it and click Authenticate to continue.</li>\n<li>Exit out of the Service Configuration window and open a terminal window.</li>\n<li>At the terminal prompt enter the command: <strong>mongo</strong> and you should see the MongoDB shell version (2.6.3 on mine), and a message telling you which database you are connecting to (test initially). <a href=\"http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/mongoshell.png\" alt=\"mongoshell\"></a></li>\n<li>To exit the shell, enter the command: quit()</li>\n</ol>\n<p>Congratulations - You have installed MongoDB as a service on your CentOS server.</p>"},{"title":"Install MySQL Workbench 6.2 on Centos","id":"2578","comments":0,"date":"2014-11-29T00:12:11.000Z","_content":"\n[![workbench](http://edpflager.com/wp-content/uploads/2014/11/workbench-300x102.png)](http://edpflager.com/wp-content/uploads/2014/11/workbench.png)The world of computers is constantly evolving, and that means having to upgrade your software periodically if you want to stay current. The GA version of MySQL Workbench, the GUI tool for interacting with the MySQL database engine was recently updated. For information on changes, you can check out the official documentation at this [link](http://dev.mysql.com/downloads/workbench/), but a couple of the biggest changes revolve around Microsoft products:\n\n*   you can now migrate Microsoft Access databases, and\n*   64-bit Windows binaries are now provided to go along with the 32-bit ones.\n\nI use MySQL as a test bed for a lot of Pentaho development, so I like to keep the related tools up to date. Although this version does work with Centos 6.6 (the version I am using of the RHEL  distribution), its not as easy as it should be to install.\n<!-- more -->\nTo get MySQL Workbench working on Centos 6.6: 1. Install the EPEL repository if you haven't already done so. This is an add on repository that provides a number of packages that are not included in the normal RHEL/Centos distributions. From a terminal prompt, switch to SU mode and enter this command for 32-bit installations:\n\n###### rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\n\nor for 64-bit installations:\n\n###### rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm\n\nUpdate YUM by entering this command:\n\n###### yum update\n\n2\\. Next, a couple of dependencies for MySQL Workbench need to be installed. From the command line enter the following commands, and when prompted answer yes:\n\n###### yum install libzip-0.9-3.1.el6\n\n###### yum install tinyxml\n\n4\\. Add the MySQL software repository by visiting the [MySQL web site](http://dev.mysql.com/downloads/repo/yum/) and choosing the link appropriate for your distribution. You'll be prompted to login (skip it by scrolling down to  the bottom of the screen and clicking the No thanks link). In the window that appears, choose the Open with Package Installer option, and click OK.  When asked if you want to install the file, click the Install button. After a few minutes you'll be prompted for your administrator password and the repo and dependencies will be added to your system. 5. Back at the command prompt run the following to install the application:\n\n###### yum install mysql-workbench-community\n\nAnswer yes when prompted. After a few moments, workbench will be installed, and a shortcut will appear under the Applications menu - Programming submenu. Click on it to start the application. [![logo-mysql](http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png)](http://www.mysql.com/)","source":"_posts/install-mysql-workbench-6-2-on-centos.md","raw":"---\ntitle: Install MySQL Workbench 6.2 on Centos\ntags:\n  - centos\n  - ETL\n  - How-to\n  - howto\n  - install\n  - MySQL\n  - SysAdmin\n  - technical\nid: '2578'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-11-28 19:12:11\n---\n\n[![workbench](http://edpflager.com/wp-content/uploads/2014/11/workbench-300x102.png)](http://edpflager.com/wp-content/uploads/2014/11/workbench.png)The world of computers is constantly evolving, and that means having to upgrade your software periodically if you want to stay current. The GA version of MySQL Workbench, the GUI tool for interacting with the MySQL database engine was recently updated. For information on changes, you can check out the official documentation at this [link](http://dev.mysql.com/downloads/workbench/), but a couple of the biggest changes revolve around Microsoft products:\n\n*   you can now migrate Microsoft Access databases, and\n*   64-bit Windows binaries are now provided to go along with the 32-bit ones.\n\nI use MySQL as a test bed for a lot of Pentaho development, so I like to keep the related tools up to date. Although this version does work with Centos 6.6 (the version I am using of the RHEL  distribution), its not as easy as it should be to install.\n<!-- more -->\nTo get MySQL Workbench working on Centos 6.6: 1. Install the EPEL repository if you haven't already done so. This is an add on repository that provides a number of packages that are not included in the normal RHEL/Centos distributions. From a terminal prompt, switch to SU mode and enter this command for 32-bit installations:\n\n###### rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\n\nor for 64-bit installations:\n\n###### rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm\n\nUpdate YUM by entering this command:\n\n###### yum update\n\n2\\. Next, a couple of dependencies for MySQL Workbench need to be installed. From the command line enter the following commands, and when prompted answer yes:\n\n###### yum install libzip-0.9-3.1.el6\n\n###### yum install tinyxml\n\n4\\. Add the MySQL software repository by visiting the [MySQL web site](http://dev.mysql.com/downloads/repo/yum/) and choosing the link appropriate for your distribution. You'll be prompted to login (skip it by scrolling down to  the bottom of the screen and clicking the No thanks link). In the window that appears, choose the Open with Package Installer option, and click OK.  When asked if you want to install the file, click the Install button. After a few minutes you'll be prompted for your administrator password and the repo and dependencies will be added to your system. 5. Back at the command prompt run the following to install the application:\n\n###### yum install mysql-workbench-community\n\nAnswer yes when prompted. After a few moments, workbench will be installed, and a shortcut will appear under the Applications menu - Programming submenu. Click on it to start the application. [![logo-mysql](http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png)](http://www.mysql.com/)","slug":"install-mysql-workbench-6-2-on-centos","published":1,"updated":"2020-08-23T20:54:34.914Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a0r004osdjx22cqboc6","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/11/workbench.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/workbench-300x102.png\" alt=\"workbench\"></a>The world of computers is constantly evolving, and that means having to upgrade your software periodically if you want to stay current. The GA version of MySQL Workbench, the GUI tool for interacting with the MySQL database engine was recently updated. For information on changes, you can check out the official documentation at this <a href=\"http://dev.mysql.com/downloads/workbench/\">link</a>, but a couple of the biggest changes revolve around Microsoft products:</p>\n<ul>\n<li>you can now migrate Microsoft Access databases, and</li>\n<li>64-bit Windows binaries are now provided to go along with the 32-bit ones.</li>\n</ul>\n<p>I use MySQL as a test bed for a lot of Pentaho development, so I like to keep the related tools up to date. Although this version does work with Centos 6.6 (the version I am using of the RHEL  distribution), its not as easy as it should be to install.</p>\n<a id=\"more\"></a>\n<p>To get MySQL Workbench working on Centos 6.6: 1. Install the EPEL repository if you haven’t already done so. This is an add on repository that provides a number of packages that are not included in the normal RHEL/Centos distributions. From a terminal prompt, switch to SU mode and enter this command for 32-bit installations:</p>\n<h6 id=\"rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-i386-epel-release-6-8-noarch-rpm\"><a href=\"#rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-i386-epel-release-6-8-noarch-rpm\" class=\"headerlink\" title=\"rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\"></a>rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a></h6><p>or for 64-bit installations:</p>\n<h6 id=\"rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-x86-64-epel-release-6-8-noarch-rpm\"><a href=\"#rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-x86-64-epel-release-6-8-noarch-rpm\" class=\"headerlink\" title=\"rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm\"></a>rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/x86/_64/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm</a></h6><p>Update YUM by entering this command:</p>\n<h6 id=\"yum-update\"><a href=\"#yum-update\" class=\"headerlink\" title=\"yum update\"></a>yum update</h6><p>2. Next, a couple of dependencies for MySQL Workbench need to be installed. From the command line enter the following commands, and when prompted answer yes:</p>\n<h6 id=\"yum-install-libzip-0-9-3-1-el6\"><a href=\"#yum-install-libzip-0-9-3-1-el6\" class=\"headerlink\" title=\"yum install libzip-0.9-3.1.el6\"></a>yum install libzip-0.9-3.1.el6</h6><h6 id=\"yum-install-tinyxml\"><a href=\"#yum-install-tinyxml\" class=\"headerlink\" title=\"yum install tinyxml\"></a>yum install tinyxml</h6><p>4. Add the MySQL software repository by visiting the <a href=\"http://dev.mysql.com/downloads/repo/yum/\">MySQL web site</a> and choosing the link appropriate for your distribution. You’ll be prompted to login (skip it by scrolling down to  the bottom of the screen and clicking the No thanks link). In the window that appears, choose the Open with Package Installer option, and click OK.  When asked if you want to install the file, click the Install button. After a few minutes you’ll be prompted for your administrator password and the repo and dependencies will be added to your system. 5. Back at the command prompt run the following to install the application:</p>\n<h6 id=\"yum-install-mysql-workbench-community\"><a href=\"#yum-install-mysql-workbench-community\" class=\"headerlink\" title=\"yum install mysql-workbench-community\"></a>yum install mysql-workbench-community</h6><p>Answer yes when prompted. After a few moments, workbench will be installed, and a shortcut will appear under the Applications menu - Programming submenu. Click on it to start the application. <a href=\"http://www.mysql.com/\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png\" alt=\"logo-mysql\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/11/workbench.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/workbench-300x102.png\" alt=\"workbench\"></a>The world of computers is constantly evolving, and that means having to upgrade your software periodically if you want to stay current. The GA version of MySQL Workbench, the GUI tool for interacting with the MySQL database engine was recently updated. For information on changes, you can check out the official documentation at this <a href=\"http://dev.mysql.com/downloads/workbench/\">link</a>, but a couple of the biggest changes revolve around Microsoft products:</p>\n<ul>\n<li>you can now migrate Microsoft Access databases, and</li>\n<li>64-bit Windows binaries are now provided to go along with the 32-bit ones.</li>\n</ul>\n<p>I use MySQL as a test bed for a lot of Pentaho development, so I like to keep the related tools up to date. Although this version does work with Centos 6.6 (the version I am using of the RHEL  distribution), its not as easy as it should be to install.</p>","more":"<p>To get MySQL Workbench working on Centos 6.6: 1. Install the EPEL repository if you haven’t already done so. This is an add on repository that provides a number of packages that are not included in the normal RHEL/Centos distributions. From a terminal prompt, switch to SU mode and enter this command for 32-bit installations:</p>\n<h6 id=\"rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-i386-epel-release-6-8-noarch-rpm\"><a href=\"#rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-i386-epel-release-6-8-noarch-rpm\" class=\"headerlink\" title=\"rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\"></a>rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm</a></h6><p>or for 64-bit installations:</p>\n<h6 id=\"rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-x86-64-epel-release-6-8-noarch-rpm\"><a href=\"#rpm-Uvh-http-download-fedoraproject-org-pub-epel-6-x86-64-epel-release-6-8-noarch-rpm\" class=\"headerlink\" title=\"rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm\"></a>rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/x86/_64/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm</a></h6><p>Update YUM by entering this command:</p>\n<h6 id=\"yum-update\"><a href=\"#yum-update\" class=\"headerlink\" title=\"yum update\"></a>yum update</h6><p>2. Next, a couple of dependencies for MySQL Workbench need to be installed. From the command line enter the following commands, and when prompted answer yes:</p>\n<h6 id=\"yum-install-libzip-0-9-3-1-el6\"><a href=\"#yum-install-libzip-0-9-3-1-el6\" class=\"headerlink\" title=\"yum install libzip-0.9-3.1.el6\"></a>yum install libzip-0.9-3.1.el6</h6><h6 id=\"yum-install-tinyxml\"><a href=\"#yum-install-tinyxml\" class=\"headerlink\" title=\"yum install tinyxml\"></a>yum install tinyxml</h6><p>4. Add the MySQL software repository by visiting the <a href=\"http://dev.mysql.com/downloads/repo/yum/\">MySQL web site</a> and choosing the link appropriate for your distribution. You’ll be prompted to login (skip it by scrolling down to  the bottom of the screen and clicking the No thanks link). In the window that appears, choose the Open with Package Installer option, and click OK.  When asked if you want to install the file, click the Install button. After a few minutes you’ll be prompted for your administrator password and the repo and dependencies will be added to your system. 5. Back at the command prompt run the following to install the application:</p>\n<h6 id=\"yum-install-mysql-workbench-community\"><a href=\"#yum-install-mysql-workbench-community\" class=\"headerlink\" title=\"yum install mysql-workbench-community\"></a>yum install mysql-workbench-community</h6><p>Answer yes when prompted. After a few moments, workbench will be installed, and a shortcut will appear under the Applications menu - Programming submenu. Click on it to start the application. <a href=\"http://www.mysql.com/\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/logo-mysql.png\" alt=\"logo-mysql\"></a></p>"},{"title":"Install RStudio's Shiny Server on Linux Mint","id":"4465","comments":0,"date":"2019-03-27T19:26:49.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2019/03/shiny.png)](http://edpflager.com/wp-content/uploads/2019/03/shiny.png)I'm back, after my day job related hiatus! This time I'm looking at installing RStudio's open source version of Shiny Server on Linux Mint. If you aren't familiar with Shiny, its a product from the [RStudio team](https://shiny.rstudio.com/) that provides a web framework to host interactive data analysis apps using a variety of different technologies including but not limited to CSS, HTMLWidgets and Javascript actions. You built your apps directly from the R programming language for data analytics and visualizations to embed R Markdown documents to present your analysis with notes, put together dashboards and develop other interactive data apps.\n\n##### SYSTEM PREPARATION\n\nMy development environment is running Linux Mint 19.1 Cinnamon 64-bit with 4GB of RAM. When I started I removed most of the default applications to have just a basic GUI system and patched it to the most current versions of everything that remained. Then, I followed my previous post for [installing R and RStudio on Linux Mint](http://edpflager.com/2018/12/18/rstudio-on-linux-mint-19/) to get a basic R installation up and running.\n<!-- more -->\nI am assuming that if you are installing a server application, then you have root access via **sudo** to your machine. If you don't then this tutorial won't work for you. You are very likely to  get an error message like this:\n\nWarning in install.packages :\n  'lib = \"/usr/local/lib/R/site-library\"' is not writable\n\n##### INSTALLATION\n\nOpen a terminal prompt and run RStudio as root:\n\nsudo /usr/bin/rstudio\n\nShiny server runs under a user account called Shiny. When you install packages in R on Linux Mint by default, it installs those packages under **your** user account's R package library. But they need to be under the Shiny account's package library or preferably under the system-wide R package library so the Shiny account can access them. When RStudio comes up, in the console pane, install the Shiny package using the syntax below. The **repos** clause will force R to get the Shiny package from the RStudio Cran mirror and the **lib** clause will install it into the system library instead of your personal one so it is accessible to the Shiny account:\n\ninstall.packages(\"shiny\", repos = \"https://cran.us.r-project.org\", lib = \"/usr/local/lib/R/site-library\")\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png)](http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png)As part of the installation, a number of dependencies will be installed as well if they are not already present so it may take awhile to complete.\n\nAfter the shiny installation completes, repeat the process with RMarkdown. This package is required for the default startup page for Shiny, and you'll probably be using it a lot with Shiny:\n\ninstall.packages(\"rmarkdown\", repos = \"https://cran.us.r-project.org\", lib = \"/usr/local/lib/R/site-library\")\n\nOnce completed, if you switch to the Packages tab in RStudio, you'll see a System Library header with Shiny and RMarkdown included along with a number of other packages. If you deploy any code to your server that uses packages that are not listed in the System Library, be sure to install them using the method outlined here.\n\nBTW, if you'd prefer there is another way to do the package installation strictly via the command line. Because you are running this as **sudo** it will automatically push the packages in to the system library for R:\n\nsudo su - \n-c \"R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\"\"\n\nNow exit R Studio and from a browser check the [RStudio Shiny download site](https://www.rstudio.com/products/shiny/download-server/) for the latest version of the Shiny server. Because Linux Mint is a derivative of Ubuntu I used that. As of this writing, the most current version is for Ubuntu 14.04 and the Shiny version is 1.5.9.923. From a command line use **wget** to download the installation package.\n\nwget https://download3.rstudio.org/ubuntu-14.04/x86\\_64/shiny-server-1.5.9.923-amd64.deb\n\nTo install the downloaded package, gdebi is required which is installed by default on my version of Linux Mint. If its not on your system install it from the command line with:\n\nsudo apt-get install gdebi-core\n\nThen as root again, install the Shiny application.\n\nsudo gdebi shiny-server-1.5.9.923-amd64.deb\n\nIt will prompt you to accept the install and then take several minutes to complete. After that, if everything goes OK, you'll see a message indicating the server is up and running! Launch a browser and point it to the default URL to see the Welcome page for Shiny Server!\n\nhttp://localhost:3838\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/WelcomePage-248x300.png)](http://edpflager.com/wp-content/uploads/2019/03/WelcomePage.png)There are two small web apps on the right side of this page, and both should be active when you open the Welcome screen. If not, go back and check to make sure there weren't any problems. A final reminder here: When you install packages in R on Linux Mint by default, it installs those packages under **your** user account's R package library. But they need to be under the Shiny account's package library or preferably under the system-wide R package library so the Shiny account can access them. Be sure to use the syntax I have supplied above to make sure they go into the correct location.","source":"_posts/install-rstudios-shiny-server-on-linux-mint.md","raw":"---\ntitle: Install RStudio's Shiny Server on Linux Mint\ntags:\n  - cookbook\n  - How-to\n  - howto\n  - install\n  - Linux\n  - Mint\n  - technical\nid: '4465'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\ncomments: false\ndate: 2019-03-27 15:26:49\n---\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/shiny.png)](http://edpflager.com/wp-content/uploads/2019/03/shiny.png)I'm back, after my day job related hiatus! This time I'm looking at installing RStudio's open source version of Shiny Server on Linux Mint. If you aren't familiar with Shiny, its a product from the [RStudio team](https://shiny.rstudio.com/) that provides a web framework to host interactive data analysis apps using a variety of different technologies including but not limited to CSS, HTMLWidgets and Javascript actions. You built your apps directly from the R programming language for data analytics and visualizations to embed R Markdown documents to present your analysis with notes, put together dashboards and develop other interactive data apps.\n\n##### SYSTEM PREPARATION\n\nMy development environment is running Linux Mint 19.1 Cinnamon 64-bit with 4GB of RAM. When I started I removed most of the default applications to have just a basic GUI system and patched it to the most current versions of everything that remained. Then, I followed my previous post for [installing R and RStudio on Linux Mint](http://edpflager.com/2018/12/18/rstudio-on-linux-mint-19/) to get a basic R installation up and running.\n<!-- more -->\nI am assuming that if you are installing a server application, then you have root access via **sudo** to your machine. If you don't then this tutorial won't work for you. You are very likely to  get an error message like this:\n\nWarning in install.packages :\n  'lib = \"/usr/local/lib/R/site-library\"' is not writable\n\n##### INSTALLATION\n\nOpen a terminal prompt and run RStudio as root:\n\nsudo /usr/bin/rstudio\n\nShiny server runs under a user account called Shiny. When you install packages in R on Linux Mint by default, it installs those packages under **your** user account's R package library. But they need to be under the Shiny account's package library or preferably under the system-wide R package library so the Shiny account can access them. When RStudio comes up, in the console pane, install the Shiny package using the syntax below. The **repos** clause will force R to get the Shiny package from the RStudio Cran mirror and the **lib** clause will install it into the system library instead of your personal one so it is accessible to the Shiny account:\n\ninstall.packages(\"shiny\", repos = \"https://cran.us.r-project.org\", lib = \"/usr/local/lib/R/site-library\")\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png)](http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png)As part of the installation, a number of dependencies will be installed as well if they are not already present so it may take awhile to complete.\n\nAfter the shiny installation completes, repeat the process with RMarkdown. This package is required for the default startup page for Shiny, and you'll probably be using it a lot with Shiny:\n\ninstall.packages(\"rmarkdown\", repos = \"https://cran.us.r-project.org\", lib = \"/usr/local/lib/R/site-library\")\n\nOnce completed, if you switch to the Packages tab in RStudio, you'll see a System Library header with Shiny and RMarkdown included along with a number of other packages. If you deploy any code to your server that uses packages that are not listed in the System Library, be sure to install them using the method outlined here.\n\nBTW, if you'd prefer there is another way to do the package installation strictly via the command line. Because you are running this as **sudo** it will automatically push the packages in to the system library for R:\n\nsudo su - \n-c \"R -e \"install.packages('shiny', repos='https://cran.rstudio.com/')\"\"\n\nNow exit R Studio and from a browser check the [RStudio Shiny download site](https://www.rstudio.com/products/shiny/download-server/) for the latest version of the Shiny server. Because Linux Mint is a derivative of Ubuntu I used that. As of this writing, the most current version is for Ubuntu 14.04 and the Shiny version is 1.5.9.923. From a command line use **wget** to download the installation package.\n\nwget https://download3.rstudio.org/ubuntu-14.04/x86\\_64/shiny-server-1.5.9.923-amd64.deb\n\nTo install the downloaded package, gdebi is required which is installed by default on my version of Linux Mint. If its not on your system install it from the command line with:\n\nsudo apt-get install gdebi-core\n\nThen as root again, install the Shiny application.\n\nsudo gdebi shiny-server-1.5.9.923-amd64.deb\n\nIt will prompt you to accept the install and then take several minutes to complete. After that, if everything goes OK, you'll see a message indicating the server is up and running! Launch a browser and point it to the default URL to see the Welcome page for Shiny Server!\n\nhttp://localhost:3838\n\n[![](http://edpflager.com/wp-content/uploads/2019/03/WelcomePage-248x300.png)](http://edpflager.com/wp-content/uploads/2019/03/WelcomePage.png)There are two small web apps on the right side of this page, and both should be active when you open the Welcome screen. If not, go back and check to make sure there weren't any problems. A final reminder here: When you install packages in R on Linux Mint by default, it installs those packages under **your** user account's R package library. But they need to be under the Shiny account's package library or preferably under the system-wide R package library so the Shiny account can access them. Be sure to use the syntax I have supplied above to make sure they go into the correct location.","slug":"install-rstudios-shiny-server-on-linux-mint","published":1,"updated":"2020-08-23T20:54:35.230Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a0w004rsdjxeln8er3w","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/shiny.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/shiny.png\"></a>I’m back, after my day job related hiatus! This time I’m looking at installing RStudio’s open source version of Shiny Server on Linux Mint. If you aren’t familiar with Shiny, its a product from the <a href=\"https://shiny.rstudio.com/\">RStudio team</a> that provides a web framework to host interactive data analysis apps using a variety of different technologies including but not limited to CSS, HTMLWidgets and Javascript actions. You built your apps directly from the R programming language for data analytics and visualizations to embed R Markdown documents to present your analysis with notes, put together dashboards and develop other interactive data apps.</p>\n<h5 id=\"SYSTEM-PREPARATION\"><a href=\"#SYSTEM-PREPARATION\" class=\"headerlink\" title=\"SYSTEM PREPARATION\"></a>SYSTEM PREPARATION</h5><p>My development environment is running Linux Mint 19.1 Cinnamon 64-bit with 4GB of RAM. When I started I removed most of the default applications to have just a basic GUI system and patched it to the most current versions of everything that remained. Then, I followed my previous post for <a href=\"http://edpflager.com/2018/12/18/rstudio-on-linux-mint-19/\">installing R and RStudio on Linux Mint</a> to get a basic R installation up and running.</p>\n<a id=\"more\"></a>\n<p>I am assuming that if you are installing a server application, then you have root access via <strong>sudo</strong> to your machine. If you don’t then this tutorial won’t work for you. You are very likely to  get an error message like this:</p>\n<p>Warning in install.packages :<br>  ‘lib = “/usr/local/lib/R/site-library”‘ is not writable</p>\n<h5 id=\"INSTALLATION\"><a href=\"#INSTALLATION\" class=\"headerlink\" title=\"INSTALLATION\"></a>INSTALLATION</h5><p>Open a terminal prompt and run RStudio as root:</p>\n<p>sudo /usr/bin/rstudio</p>\n<p>Shiny server runs under a user account called Shiny. When you install packages in R on Linux Mint by default, it installs those packages under <strong>your</strong> user account’s R package library. But they need to be under the Shiny account’s package library or preferably under the system-wide R package library so the Shiny account can access them. When RStudio comes up, in the console pane, install the Shiny package using the syntax below. The <strong>repos</strong> clause will force R to get the Shiny package from the RStudio Cran mirror and the <strong>lib</strong> clause will install it into the system library instead of your personal one so it is accessible to the Shiny account:</p>\n<p>install.packages(“shiny”, repos = “<a href=\"https://cran.us.r-project.org&quot;/\">https://cran.us.r-project.org&quot;</a>, lib = “/usr/local/lib/R/site-library”)</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png\"></a>As part of the installation, a number of dependencies will be installed as well if they are not already present so it may take awhile to complete.</p>\n<p>After the shiny installation completes, repeat the process with RMarkdown. This package is required for the default startup page for Shiny, and you’ll probably be using it a lot with Shiny:</p>\n<p>install.packages(“rmarkdown”, repos = “<a href=\"https://cran.us.r-project.org&quot;/\">https://cran.us.r-project.org&quot;</a>, lib = “/usr/local/lib/R/site-library”)</p>\n<p>Once completed, if you switch to the Packages tab in RStudio, you’ll see a System Library header with Shiny and RMarkdown included along with a number of other packages. If you deploy any code to your server that uses packages that are not listed in the System Library, be sure to install them using the method outlined here.</p>\n<p>BTW, if you’d prefer there is another way to do the package installation strictly via the command line. Because you are running this as <strong>sudo</strong> it will automatically push the packages in to the system library for R:</p>\n<p>sudo su -<br>-c “R -e “install.packages(‘shiny’, repos=’<a href=\"https://cran.rstudio.com/&#39;)&quot;&quot;\">https://cran.rstudio.com/&#39;)&quot;&quot;</a></p>\n<p>Now exit R Studio and from a browser check the <a href=\"https://www.rstudio.com/products/shiny/download-server/\">RStudio Shiny download site</a> for the latest version of the Shiny server. Because Linux Mint is a derivative of Ubuntu I used that. As of this writing, the most current version is for Ubuntu 14.04 and the Shiny version is 1.5.9.923. From a command line use <strong>wget</strong> to download the installation package.</p>\n<p>wget <a href=\"https://download3.rstudio.org/ubuntu-14.04/x86/_64/shiny-server-1.5.9.923-amd64.deb\">https://download3.rstudio.org/ubuntu-14.04/x86\\_64/shiny-server-1.5.9.923-amd64.deb</a></p>\n<p>To install the downloaded package, gdebi is required which is installed by default on my version of Linux Mint. If its not on your system install it from the command line with:</p>\n<p>sudo apt-get install gdebi-core</p>\n<p>Then as root again, install the Shiny application.</p>\n<p>sudo gdebi shiny-server-1.5.9.923-amd64.deb</p>\n<p>It will prompt you to accept the install and then take several minutes to complete. After that, if everything goes OK, you’ll see a message indicating the server is up and running! Launch a browser and point it to the default URL to see the Welcome page for Shiny Server!</p>\n<p><a href=\"http://localhost:3838/\">http://localhost:3838</a></p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/WelcomePage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/WelcomePage-248x300.png\"></a>There are two small web apps on the right side of this page, and both should be active when you open the Welcome screen. If not, go back and check to make sure there weren’t any problems. A final reminder here: When you install packages in R on Linux Mint by default, it installs those packages under <strong>your</strong> user account’s R package library. But they need to be under the Shiny account’s package library or preferably under the system-wide R package library so the Shiny account can access them. Be sure to use the syntax I have supplied above to make sure they go into the correct location.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/shiny.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/shiny.png\"></a>I’m back, after my day job related hiatus! This time I’m looking at installing RStudio’s open source version of Shiny Server on Linux Mint. If you aren’t familiar with Shiny, its a product from the <a href=\"https://shiny.rstudio.com/\">RStudio team</a> that provides a web framework to host interactive data analysis apps using a variety of different technologies including but not limited to CSS, HTMLWidgets and Javascript actions. You built your apps directly from the R programming language for data analytics and visualizations to embed R Markdown documents to present your analysis with notes, put together dashboards and develop other interactive data apps.</p>\n<h5 id=\"SYSTEM-PREPARATION\"><a href=\"#SYSTEM-PREPARATION\" class=\"headerlink\" title=\"SYSTEM PREPARATION\"></a>SYSTEM PREPARATION</h5><p>My development environment is running Linux Mint 19.1 Cinnamon 64-bit with 4GB of RAM. When I started I removed most of the default applications to have just a basic GUI system and patched it to the most current versions of everything that remained. Then, I followed my previous post for <a href=\"http://edpflager.com/2018/12/18/rstudio-on-linux-mint-19/\">installing R and RStudio on Linux Mint</a> to get a basic R installation up and running.</p>","more":"<p>I am assuming that if you are installing a server application, then you have root access via <strong>sudo</strong> to your machine. If you don’t then this tutorial won’t work for you. You are very likely to  get an error message like this:</p>\n<p>Warning in install.packages :<br>  ‘lib = “/usr/local/lib/R/site-library”‘ is not writable</p>\n<h5 id=\"INSTALLATION\"><a href=\"#INSTALLATION\" class=\"headerlink\" title=\"INSTALLATION\"></a>INSTALLATION</h5><p>Open a terminal prompt and run RStudio as root:</p>\n<p>sudo /usr/bin/rstudio</p>\n<p>Shiny server runs under a user account called Shiny. When you install packages in R on Linux Mint by default, it installs those packages under <strong>your</strong> user account’s R package library. But they need to be under the Shiny account’s package library or preferably under the system-wide R package library so the Shiny account can access them. When RStudio comes up, in the console pane, install the Shiny package using the syntax below. The <strong>repos</strong> clause will force R to get the Shiny package from the RStudio Cran mirror and the <strong>lib</strong> clause will install it into the system library instead of your personal one so it is accessible to the Shiny account:</p>\n<p>install.packages(“shiny”, repos = “<a href=\"https://cran.us.r-project.org&quot;/\">https://cran.us.r-project.org&quot;</a>, lib = “/usr/local/lib/R/site-library”)</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/ShinyPackage.png\"></a>As part of the installation, a number of dependencies will be installed as well if they are not already present so it may take awhile to complete.</p>\n<p>After the shiny installation completes, repeat the process with RMarkdown. This package is required for the default startup page for Shiny, and you’ll probably be using it a lot with Shiny:</p>\n<p>install.packages(“rmarkdown”, repos = “<a href=\"https://cran.us.r-project.org&quot;/\">https://cran.us.r-project.org&quot;</a>, lib = “/usr/local/lib/R/site-library”)</p>\n<p>Once completed, if you switch to the Packages tab in RStudio, you’ll see a System Library header with Shiny and RMarkdown included along with a number of other packages. If you deploy any code to your server that uses packages that are not listed in the System Library, be sure to install them using the method outlined here.</p>\n<p>BTW, if you’d prefer there is another way to do the package installation strictly via the command line. Because you are running this as <strong>sudo</strong> it will automatically push the packages in to the system library for R:</p>\n<p>sudo su -<br>-c “R -e “install.packages(‘shiny’, repos=’<a href=\"https://cran.rstudio.com/&#39;)&quot;&quot;\">https://cran.rstudio.com/&#39;)&quot;&quot;</a></p>\n<p>Now exit R Studio and from a browser check the <a href=\"https://www.rstudio.com/products/shiny/download-server/\">RStudio Shiny download site</a> for the latest version of the Shiny server. Because Linux Mint is a derivative of Ubuntu I used that. As of this writing, the most current version is for Ubuntu 14.04 and the Shiny version is 1.5.9.923. From a command line use <strong>wget</strong> to download the installation package.</p>\n<p>wget <a href=\"https://download3.rstudio.org/ubuntu-14.04/x86/_64/shiny-server-1.5.9.923-amd64.deb\">https://download3.rstudio.org/ubuntu-14.04/x86\\_64/shiny-server-1.5.9.923-amd64.deb</a></p>\n<p>To install the downloaded package, gdebi is required which is installed by default on my version of Linux Mint. If its not on your system install it from the command line with:</p>\n<p>sudo apt-get install gdebi-core</p>\n<p>Then as root again, install the Shiny application.</p>\n<p>sudo gdebi shiny-server-1.5.9.923-amd64.deb</p>\n<p>It will prompt you to accept the install and then take several minutes to complete. After that, if everything goes OK, you’ll see a message indicating the server is up and running! Launch a browser and point it to the default URL to see the Welcome page for Shiny Server!</p>\n<p><a href=\"http://localhost:3838/\">http://localhost:3838</a></p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/03/WelcomePage.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/03/WelcomePage-248x300.png\"></a>There are two small web apps on the right side of this page, and both should be active when you open the Welcome screen. If not, go back and check to make sure there weren’t any problems. A final reminder here: When you install packages in R on Linux Mint by default, it installs those packages under <strong>your</strong> user account’s R package library. But they need to be under the Shiny account’s package library or preferably under the system-wide R package library so the Shiny account can access them. Be sure to use the syntax I have supplied above to make sure they go into the correct location.</p>"},{"title":"Install the Saiku Analytics plugin in Pentaho BIServer CE","id":"3322","comments":0,"date":"2016-06-05T09:52:15.000Z","_content":"\n[![meterorite](http://edpflager.com/wp-content/uploads/2016/06/meterorite-300x180.jpg)](http://edpflager.com/?attachment_id=3324#main) I've been working with Mondrian and Pentaho's Schema Workbench lately and attempted to add Meteorite Consulting's Saiku Analytic plugin to my installation of Pentaho BI Server community edition, to process some MDX queries. MDX is a query language similar to SQL that is used for processing database cubes. Mondrian is a OLAP engine that implements the MDX language and is incorporated into the Saiku Analytic software. It differs from other OLAP engines in that the cubes are built on the fly as the query processes, rather than having the cube data stored on a server. For simpler cubes, the trade off between a slightly slower build time and disk space is negligible. Here is the process I followed to get Saiku enabled in my BI Server:\n<!-- more -->\n1.  Access the [Pentaho Marketplace](http://www.pentaho.com/marketplace/) and search for the Saiku Analytic plugin.\n2.  Click on the plugin from the results window and in the popup window, click the **DOWNLOAD PLUGIN** button.\n3.  After it completes downloading, extract the ZIP file. It should generate its own folder called \"saiku\".\n4.  Move the \"saiku\" folder to the **biserver-ce/pentaho-solutions/system** folder. In my case that was under **/opt/pentaho**. Your version may be different.\n5.  Access the [Meteorite Consulting's license website](http://licensing.meteorite.bi/)  and signup for a new account (its free). For company, I just used my address.\n6.  Once you have validated your account, login to the system, and click the **CREATE NEW LICENSE** button.\n7.  On the new license page, enter the hostname of the machine that biserver runs on. Enter one for the the maximum number of users (I believe it is irrelevant for the Community version). If you are not comfortable with that, enter how many actual users you think will be using it. Set the license type to **COMMUNITY EDITION**. Your user name and company(address) info will already be filled in.\n8.  Click the **SAVE** button, and the page will update with a link to **Download License**. Click it, and a license file will download to your system, as the hostname you specified and the extension of \".lic\".\n9.  Rename the file to \"license.lic\" and then copy it to the \"saiku\" folder that you moved under bi-server/pentaho/system previously.\n10.  Restart your biserver.\n11.  Login to your biserver as a user with analytics permissions.\n12.  Click FILE -> NEW and you should have an option for SAIKU ANALYTICS.\n13.  Click it and if all was done correctly, the SAIKU ANALYTICS page will load.","source":"_posts/install-the-saiku-analytics-plugin-in-pentaho-biserver-ce.md","raw":"---\ntitle: Install the Saiku Analytics plugin in Pentaho BIServer CE\ntags:\n  - Big Data\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - SysAdmin\n  - technical\nid: '3322'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2016-06-05 05:52:15\n---\n\n[![meterorite](http://edpflager.com/wp-content/uploads/2016/06/meterorite-300x180.jpg)](http://edpflager.com/?attachment_id=3324#main) I've been working with Mondrian and Pentaho's Schema Workbench lately and attempted to add Meteorite Consulting's Saiku Analytic plugin to my installation of Pentaho BI Server community edition, to process some MDX queries. MDX is a query language similar to SQL that is used for processing database cubes. Mondrian is a OLAP engine that implements the MDX language and is incorporated into the Saiku Analytic software. It differs from other OLAP engines in that the cubes are built on the fly as the query processes, rather than having the cube data stored on a server. For simpler cubes, the trade off between a slightly slower build time and disk space is negligible. Here is the process I followed to get Saiku enabled in my BI Server:\n<!-- more -->\n1.  Access the [Pentaho Marketplace](http://www.pentaho.com/marketplace/) and search for the Saiku Analytic plugin.\n2.  Click on the plugin from the results window and in the popup window, click the **DOWNLOAD PLUGIN** button.\n3.  After it completes downloading, extract the ZIP file. It should generate its own folder called \"saiku\".\n4.  Move the \"saiku\" folder to the **biserver-ce/pentaho-solutions/system** folder. In my case that was under **/opt/pentaho**. Your version may be different.\n5.  Access the [Meteorite Consulting's license website](http://licensing.meteorite.bi/)  and signup for a new account (its free). For company, I just used my address.\n6.  Once you have validated your account, login to the system, and click the **CREATE NEW LICENSE** button.\n7.  On the new license page, enter the hostname of the machine that biserver runs on. Enter one for the the maximum number of users (I believe it is irrelevant for the Community version). If you are not comfortable with that, enter how many actual users you think will be using it. Set the license type to **COMMUNITY EDITION**. Your user name and company(address) info will already be filled in.\n8.  Click the **SAVE** button, and the page will update with a link to **Download License**. Click it, and a license file will download to your system, as the hostname you specified and the extension of \".lic\".\n9.  Rename the file to \"license.lic\" and then copy it to the \"saiku\" folder that you moved under bi-server/pentaho/system previously.\n10.  Restart your biserver.\n11.  Login to your biserver as a user with analytics permissions.\n12.  Click FILE -> NEW and you should have an option for SAIKU ANALYTICS.\n13.  Click it and if all was done correctly, the SAIKU ANALYTICS page will load.","slug":"install-the-saiku-analytics-plugin-in-pentaho-biserver-ce","published":1,"updated":"2020-08-23T20:54:35.058Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a21004wsdjxb9qm30lk","content":"<p><a href=\"http://edpflager.com/?attachment_id=3324#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/06/meterorite-300x180.jpg\" alt=\"meterorite\"></a> I’ve been working with Mondrian and Pentaho’s Schema Workbench lately and attempted to add Meteorite Consulting’s Saiku Analytic plugin to my installation of Pentaho BI Server community edition, to process some MDX queries. MDX is a query language similar to SQL that is used for processing database cubes. Mondrian is a OLAP engine that implements the MDX language and is incorporated into the Saiku Analytic software. It differs from other OLAP engines in that the cubes are built on the fly as the query processes, rather than having the cube data stored on a server. For simpler cubes, the trade off between a slightly slower build time and disk space is negligible. Here is the process I followed to get Saiku enabled in my BI Server:</p>\n<a id=\"more\"></a>\n<ol>\n<li>Access the <a href=\"http://www.pentaho.com/marketplace/\">Pentaho Marketplace</a> and search for the Saiku Analytic plugin.</li>\n<li>Click on the plugin from the results window and in the popup window, click the <strong>DOWNLOAD PLUGIN</strong> button.</li>\n<li>After it completes downloading, extract the ZIP file. It should generate its own folder called “saiku”.</li>\n<li>Move the “saiku” folder to the <strong>biserver-ce/pentaho-solutions/system</strong> folder. In my case that was under <strong>/opt/pentaho</strong>. Your version may be different.</li>\n<li>Access the <a href=\"http://licensing.meteorite.bi/\">Meteorite Consulting’s license website</a>  and signup for a new account (its free). For company, I just used my address.</li>\n<li>Once you have validated your account, login to the system, and click the <strong>CREATE NEW LICENSE</strong> button.</li>\n<li>On the new license page, enter the hostname of the machine that biserver runs on. Enter one for the the maximum number of users (I believe it is irrelevant for the Community version). If you are not comfortable with that, enter how many actual users you think will be using it. Set the license type to <strong>COMMUNITY EDITION</strong>. Your user name and company(address) info will already be filled in.</li>\n<li>Click the <strong>SAVE</strong> button, and the page will update with a link to <strong>Download License</strong>. Click it, and a license file will download to your system, as the hostname you specified and the extension of “.lic”.</li>\n<li>Rename the file to “license.lic” and then copy it to the “saiku” folder that you moved under bi-server/pentaho/system previously.</li>\n<li>Restart your biserver.</li>\n<li>Login to your biserver as a user with analytics permissions.</li>\n<li>Click FILE -&gt; NEW and you should have an option for SAIKU ANALYTICS.</li>\n<li>Click it and if all was done correctly, the SAIKU ANALYTICS page will load.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3324#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/06/meterorite-300x180.jpg\" alt=\"meterorite\"></a> I’ve been working with Mondrian and Pentaho’s Schema Workbench lately and attempted to add Meteorite Consulting’s Saiku Analytic plugin to my installation of Pentaho BI Server community edition, to process some MDX queries. MDX is a query language similar to SQL that is used for processing database cubes. Mondrian is a OLAP engine that implements the MDX language and is incorporated into the Saiku Analytic software. It differs from other OLAP engines in that the cubes are built on the fly as the query processes, rather than having the cube data stored on a server. For simpler cubes, the trade off between a slightly slower build time and disk space is negligible. Here is the process I followed to get Saiku enabled in my BI Server:</p>","more":"<ol>\n<li>Access the <a href=\"http://www.pentaho.com/marketplace/\">Pentaho Marketplace</a> and search for the Saiku Analytic plugin.</li>\n<li>Click on the plugin from the results window and in the popup window, click the <strong>DOWNLOAD PLUGIN</strong> button.</li>\n<li>After it completes downloading, extract the ZIP file. It should generate its own folder called “saiku”.</li>\n<li>Move the “saiku” folder to the <strong>biserver-ce/pentaho-solutions/system</strong> folder. In my case that was under <strong>/opt/pentaho</strong>. Your version may be different.</li>\n<li>Access the <a href=\"http://licensing.meteorite.bi/\">Meteorite Consulting’s license website</a>  and signup for a new account (its free). For company, I just used my address.</li>\n<li>Once you have validated your account, login to the system, and click the <strong>CREATE NEW LICENSE</strong> button.</li>\n<li>On the new license page, enter the hostname of the machine that biserver runs on. Enter one for the the maximum number of users (I believe it is irrelevant for the Community version). If you are not comfortable with that, enter how many actual users you think will be using it. Set the license type to <strong>COMMUNITY EDITION</strong>. Your user name and company(address) info will already be filled in.</li>\n<li>Click the <strong>SAVE</strong> button, and the page will update with a link to <strong>Download License</strong>. Click it, and a license file will download to your system, as the hostname you specified and the extension of “.lic”.</li>\n<li>Rename the file to “license.lic” and then copy it to the “saiku” folder that you moved under bi-server/pentaho/system previously.</li>\n<li>Restart your biserver.</li>\n<li>Login to your biserver as a user with analytics permissions.</li>\n<li>Click FILE -&gt; NEW and you should have an option for SAIKU ANALYTICS.</li>\n<li>Click it and if all was done correctly, the SAIKU ANALYTICS page will load.</li>\n</ol>"},{"title":"Install the Tor Browser on Linux","id":"3220","comments":0,"date":"2016-01-10T12:34:59.000Z","_content":"\n[![DCF 1.0](http://edpflager.com/wp-content/uploads/2016/01/onions-1463141-225x300.jpg)](http://edpflager.com/?attachment_id=3228#main) The TOR project was started by United States Naval Research Lab employees in the early part of the 21st century as a way to protect intelligence communications  online. It was open sourced in 2004, and continues to be supported by the Electronic Frontier Foundation. As privacy as continued to decline on the Internet, interest in the Tor Browser and Tor project has increased. For more information on TOR see this recent [article at Salon.com](http://www.salon.com/2016/01/08/this_is_the_web_browser_you_should_be_using_if_you_care_at_all_about_security_partner/). I would like to note that I don't support or condone the use of TOR for illicit or illegal purposes, but for those who feel that their privacy is important , I am providing these instructions for how to install the Tor Browser on Linux. These instructions should work for most Linux distros.\n<!-- more -->\n1.  Download the appropriate file for your O/S from the [TorBrowser website.](https://www.torproject.org/projects/torbrowser.html.en#downloads)  In my case, I downloaded the Linux 5.0.7 Linux 64 bit version for English to my Downloads folder.\n2.  Extract the file at the terminal with this command:\n    \n    tar -xvf tor-browser-linux64-5.0.7\\_en-US.tar.xz\n    \n3.  Move the tor folder to the /opt folder with this command:\n    \n    sudo mv tor-browser\\_en-US /opt\n    \n4.  Switch to the /opt/tor-browser\\_en-US and check the file permissions on the TOR folder. Make sure everything is owned by your user name:\n    \n    ls -la tor\\*\n    \n5.  In the tor-browser\\_en-US folder, execute the start-tor-browser.desktop file. This file is designed to update itself with the location of where it is installed.:\n    \n    ./start-tor-browser.desktop\n    \n6.  You'll see a response Launching './Browser/start-tor-browser --detach'...\n7.  The first time you start TOR you will be prompted to select how you are connecting to the Internet. Unless you know for sure you are using a proxy, select the default option of connecting directly to the Tor network. You can always change this option later by clicking the Open Setting button on the Tor browser startup window.[![tor startup](http://edpflager.com/wp-content/uploads/2016/01/tor-startup-242x300.png)](http://edpflager.com/?attachment_id=3225#main)\n8.  Tor will connect to the Internet, and the browser will open.[![torconfig](http://edpflager.com/wp-content/uploads/2016/01/torconfig-300x193.png)](http://edpflager.com/?attachment_id=3226#main)\n9.  Close the Tor Browser momentarily and copy the start-tor-browser.desktop file to your personal desktop:\n    \n    cp ./start-tor-browser.desktop ~/Desktop\n    \n10.  Now you can start the program up from a desktop shortcut. If you would prefer to add the application shortcut to your menu, for Linux Mint there is a good tutorial [here](http://community.linuxmint.com/tutorial/view/1504). For other distros, please check Google.","source":"_posts/install-the-tor-browser-on-linux.md","raw":"---\ntitle: Install the Tor Browser on Linux\ntags:\n  - guides\n  - How-to\n  - howto\n  - install\n  - Mint\n  - SysAdmin\n  - technical\nid: '3220'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2016-01-10 07:34:59\n---\n\n[![DCF 1.0](http://edpflager.com/wp-content/uploads/2016/01/onions-1463141-225x300.jpg)](http://edpflager.com/?attachment_id=3228#main) The TOR project was started by United States Naval Research Lab employees in the early part of the 21st century as a way to protect intelligence communications  online. It was open sourced in 2004, and continues to be supported by the Electronic Frontier Foundation. As privacy as continued to decline on the Internet, interest in the Tor Browser and Tor project has increased. For more information on TOR see this recent [article at Salon.com](http://www.salon.com/2016/01/08/this_is_the_web_browser_you_should_be_using_if_you_care_at_all_about_security_partner/). I would like to note that I don't support or condone the use of TOR for illicit or illegal purposes, but for those who feel that their privacy is important , I am providing these instructions for how to install the Tor Browser on Linux. These instructions should work for most Linux distros.\n<!-- more -->\n1.  Download the appropriate file for your O/S from the [TorBrowser website.](https://www.torproject.org/projects/torbrowser.html.en#downloads)  In my case, I downloaded the Linux 5.0.7 Linux 64 bit version for English to my Downloads folder.\n2.  Extract the file at the terminal with this command:\n    \n    tar -xvf tor-browser-linux64-5.0.7\\_en-US.tar.xz\n    \n3.  Move the tor folder to the /opt folder with this command:\n    \n    sudo mv tor-browser\\_en-US /opt\n    \n4.  Switch to the /opt/tor-browser\\_en-US and check the file permissions on the TOR folder. Make sure everything is owned by your user name:\n    \n    ls -la tor\\*\n    \n5.  In the tor-browser\\_en-US folder, execute the start-tor-browser.desktop file. This file is designed to update itself with the location of where it is installed.:\n    \n    ./start-tor-browser.desktop\n    \n6.  You'll see a response Launching './Browser/start-tor-browser --detach'...\n7.  The first time you start TOR you will be prompted to select how you are connecting to the Internet. Unless you know for sure you are using a proxy, select the default option of connecting directly to the Tor network. You can always change this option later by clicking the Open Setting button on the Tor browser startup window.[![tor startup](http://edpflager.com/wp-content/uploads/2016/01/tor-startup-242x300.png)](http://edpflager.com/?attachment_id=3225#main)\n8.  Tor will connect to the Internet, and the browser will open.[![torconfig](http://edpflager.com/wp-content/uploads/2016/01/torconfig-300x193.png)](http://edpflager.com/?attachment_id=3226#main)\n9.  Close the Tor Browser momentarily and copy the start-tor-browser.desktop file to your personal desktop:\n    \n    cp ./start-tor-browser.desktop ~/Desktop\n    \n10.  Now you can start the program up from a desktop shortcut. If you would prefer to add the application shortcut to your menu, for Linux Mint there is a good tutorial [here](http://community.linuxmint.com/tutorial/view/1504). For other distros, please check Google.","slug":"install-the-tor-browser-on-linux","published":1,"updated":"2020-08-23T20:54:35.010Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a26004zsdjxcjvy8hbj","content":"<p><a href=\"http://edpflager.com/?attachment_id=3228#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/onions-1463141-225x300.jpg\" alt=\"DCF 1.0\"></a> The TOR project was started by United States Naval Research Lab employees in the early part of the 21st century as a way to protect intelligence communications  online. It was open sourced in 2004, and continues to be supported by the Electronic Frontier Foundation. As privacy as continued to decline on the Internet, interest in the Tor Browser and Tor project has increased. For more information on TOR see this recent <a href=\"http://www.salon.com/2016/01/08/this_is_the_web_browser_you_should_be_using_if_you_care_at_all_about_security_partner/\">article at Salon.com</a>. I would like to note that I don’t support or condone the use of TOR for illicit or illegal purposes, but for those who feel that their privacy is important , I am providing these instructions for how to install the Tor Browser on Linux. These instructions should work for most Linux distros.</p>\n<a id=\"more\"></a>\n<ol>\n<li><p>Download the appropriate file for your O/S from the <a href=\"https://www.torproject.org/projects/torbrowser.html.en#downloads\">TorBrowser website.</a>  In my case, I downloaded the Linux 5.0.7 Linux 64 bit version for English to my Downloads folder.</p>\n</li>\n<li><p>Extract the file at the terminal with this command:</p>\n<p>tar -xvf tor-browser-linux64-5.0.7_en-US.tar.xz</p>\n</li>\n<li><p>Move the tor folder to the /opt folder with this command:</p>\n<p>sudo mv tor-browser_en-US /opt</p>\n</li>\n<li><p>Switch to the /opt/tor-browser_en-US and check the file permissions on the TOR folder. Make sure everything is owned by your user name:</p>\n<p>ls -la tor*</p>\n</li>\n<li><p>In the tor-browser_en-US folder, execute the start-tor-browser.desktop file. This file is designed to update itself with the location of where it is installed.:</p>\n<p>./start-tor-browser.desktop</p>\n</li>\n<li><p>You’ll see a response Launching ‘./Browser/start-tor-browser –detach’…</p>\n</li>\n<li><p>The first time you start TOR you will be prompted to select how you are connecting to the Internet. Unless you know for sure you are using a proxy, select the default option of connecting directly to the Tor network. You can always change this option later by clicking the Open Setting button on the Tor browser startup window.<a href=\"http://edpflager.com/?attachment_id=3225#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/tor-startup-242x300.png\" alt=\"tor startup\"></a></p>\n</li>\n<li><p>Tor will connect to the Internet, and the browser will open.<a href=\"http://edpflager.com/?attachment_id=3226#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/torconfig-300x193.png\" alt=\"torconfig\"></a></p>\n</li>\n<li><p>Close the Tor Browser momentarily and copy the start-tor-browser.desktop file to your personal desktop:</p>\n<p>cp ./start-tor-browser.desktop ~/Desktop</p>\n</li>\n<li><p>Now you can start the program up from a desktop shortcut. If you would prefer to add the application shortcut to your menu, for Linux Mint there is a good tutorial <a href=\"http://community.linuxmint.com/tutorial/view/1504\">here</a>. For other distros, please check Google.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3228#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/onions-1463141-225x300.jpg\" alt=\"DCF 1.0\"></a> The TOR project was started by United States Naval Research Lab employees in the early part of the 21st century as a way to protect intelligence communications  online. It was open sourced in 2004, and continues to be supported by the Electronic Frontier Foundation. As privacy as continued to decline on the Internet, interest in the Tor Browser and Tor project has increased. For more information on TOR see this recent <a href=\"http://www.salon.com/2016/01/08/this_is_the_web_browser_you_should_be_using_if_you_care_at_all_about_security_partner/\">article at Salon.com</a>. I would like to note that I don’t support or condone the use of TOR for illicit or illegal purposes, but for those who feel that their privacy is important , I am providing these instructions for how to install the Tor Browser on Linux. These instructions should work for most Linux distros.</p>","more":"<ol>\n<li><p>Download the appropriate file for your O/S from the <a href=\"https://www.torproject.org/projects/torbrowser.html.en#downloads\">TorBrowser website.</a>  In my case, I downloaded the Linux 5.0.7 Linux 64 bit version for English to my Downloads folder.</p>\n</li>\n<li><p>Extract the file at the terminal with this command:</p>\n<p>tar -xvf tor-browser-linux64-5.0.7_en-US.tar.xz</p>\n</li>\n<li><p>Move the tor folder to the /opt folder with this command:</p>\n<p>sudo mv tor-browser_en-US /opt</p>\n</li>\n<li><p>Switch to the /opt/tor-browser_en-US and check the file permissions on the TOR folder. Make sure everything is owned by your user name:</p>\n<p>ls -la tor*</p>\n</li>\n<li><p>In the tor-browser_en-US folder, execute the start-tor-browser.desktop file. This file is designed to update itself with the location of where it is installed.:</p>\n<p>./start-tor-browser.desktop</p>\n</li>\n<li><p>You’ll see a response Launching ‘./Browser/start-tor-browser –detach’…</p>\n</li>\n<li><p>The first time you start TOR you will be prompted to select how you are connecting to the Internet. Unless you know for sure you are using a proxy, select the default option of connecting directly to the Tor network. You can always change this option later by clicking the Open Setting button on the Tor browser startup window.<a href=\"http://edpflager.com/?attachment_id=3225#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/tor-startup-242x300.png\" alt=\"tor startup\"></a></p>\n</li>\n<li><p>Tor will connect to the Internet, and the browser will open.<a href=\"http://edpflager.com/?attachment_id=3226#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/01/torconfig-300x193.png\" alt=\"torconfig\"></a></p>\n</li>\n<li><p>Close the Tor Browser momentarily and copy the start-tor-browser.desktop file to your personal desktop:</p>\n<p>cp ./start-tor-browser.desktop ~/Desktop</p>\n</li>\n<li><p>Now you can start the program up from a desktop shortcut. If you would prefer to add the application shortcut to your menu, for Linux Mint there is a good tutorial <a href=\"http://community.linuxmint.com/tutorial/view/1504\">here</a>. For other distros, please check Google.</p>\n</li>\n</ol>"},{"title":"Interesting Docker Containers (and some tips on running them)","id":"3376","comments":0,"date":"2016-07-31T17:10:13.000Z","_content":"\n[![shipping-82339](http://edpflager.com/wp-content/uploads/2016/07/shipping-82339-300x225.jpg)](http://edpflager.com/?attachment_id=3385#main)I've been learning how to use Docker for the last couple of months. Part of that experience has been downloading and working with various freely available containers from the Docker Hub. Since an ever increasing number applications are web based (i.e using a website as a UI tool) porting many open source projects to use Docker is becoming less complex. While not all open source applications can benefit yet from containerization, I think its only a matter of time before this technology starts to really gain in popularity.\n\nMy area of interest tends to fall more towards Business Intelligence and related technologies, so what I've been experimenting with are containers in that realm. This time around, I'll discuss these containers and my experience getting them running on a native Docker installation on Ubuntu:\n\n*   Rstudio\n*   Cloudera Hadoop\n*   ODOO with Postgresql\n<!-- more -->\n##### RStudio\n\nMy latest experimenting has been with RStudio which is an IDE that allows you to execute code with the data analytics language R. To pull the image from the Docker Hub execute:\n\ndocker pull rocker/rstudio\n\nRunning the container with the command given in the documentation will assign an IP address from within the Docker created subnet. On my system this caused a significant delay and then the browser on my client machine timed out. To assign the Host server's IP address and make the container available without setting up additional routing on my client I needed to modify the provided command and insert the Docker host's IP address into the command like this:\n\ndocker run -d -p <host IP address>:8787:8787 rocker/rstudio\n\nAccessing the address: **<host IP address>:8787** brings up the RStudio interface quickly and you can login with the default UserID and password: rstudio/rstudio\n\n#####  CLOUDERA HADOOP\n\nCloudera - developers of one of the most popular Hadoop distributions, offers a quick start Docker version of their Hadoop platform on their [website](http://www.cloudera.com/downloads/quickstart_vms/5-7.html). Since my server doesn't have a GUI and the Cloudera website doesn't provide a link (that I could use **wget** with) unless you provide all of your information to them, I opted to pull a container from the Docker hub with:\n\npull cloudera/quickstart:latest\n\nEither way, once you have it downloaded, you can start the container with a series of options. For ease of use, I dropped the command line into a BASH script and just execute that to start it :\n\ndocker run --hostname\\=quickstart.cloudera --privileged\\=true -t -i -p 8888:8888 cloudera/quickstart /usr/bin/docker-quickstart\n\nSwitching over to my client, in a web browser, I can access **<host IP address>:8888** to bring up the Cloudera HUE interface and login with the default User ID and password: cloudera/cloudera\n\n##### ODOO WITH POSTGRESQL\n\nODOO or OpenERP as it was called initially, is a huge suite of business applications that can be installed to meet most needs for small to very large enterprises (ODOO says on their website that Toyota uses their software for some of its needs). They pride themselves on ease of use, which in my experience tends to be accurate. The Community Edition lacks some of the features in the paid edition, but for a smaller company, or one testing ODOO out, it should be sufficient for most needs (no support however).\n\nThe ODOO container is available on the Docker Hub and can be pulled with:\n\n pull odoo\n\nOdoo requires a PostgreSQL 9.4 database server as a backend, and their online documentation provides a command line that sets up the PostgreSQL container with the expected container name and two environment variables that Odoo will be attempting to use:\n\ndocker run -d -e POSTGRES\\_USER=odoo -e POSTGRES\\_PASSWORD=odoo --name db postgres:9.4\n\nOnce you have the database server running, you can run the ODOO container with a fairly straightforward Docker command that maps the container port to 8069, and links to the PostgreSQL container with the --link switch:\n\ndocker run -p 8069:8069 --name odoo --link db:db -t odoo\n\nThis command uses a -t switch to show you on the terminal what is happening within the ODOO container.  For my purposes, I would prefer not to show that, but instead would use this command to run ODOO as a daemon:\n\ndocker run -d -p 8069:8069 --name odoo --link db:db odoo \n\nIf I need to see the screen responses, I can use the Docker logs command with any switches necessary to trim down the output. To see whats happened on screen since the container was started up for example:\n\ndocker logs odoo\n\nSwitching over to my client, in a web browser, I can access **<host IP address>:8069** to bring up the ODOO database creation website. Enter in the requested info and click CREATE DATABASE. After a few minutes, the database will be created, and the webpage will change and prompt you to login using the credentials you supplied in the first screen. Login and you are ready to start configuring and using ODOO. One thing to watch for with ODOO: If you shutdown the ODOO container, be sure to restart the PostgreSQL container before restarting ODOO.\n\nThe best part of this process is I can have all three (four if you count the PostgreSQL one) up and running at the same time. While it was possible to do this previously using VMs there is considerably less overhead on my server running Docker with these containers. Hopefully that illustrates why I am excited about Docker and what it brings to the open source landscape.","source":"_posts/interesting-docker-containers-and-some-tips-on-running-them.md","raw":"---\ntitle: Interesting Docker Containers (and some tips on running them)\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - How-to\n  - howto\n  - install\n  - Linux\n  - technical\nid: '3376'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Docker\ncomments: false\ndate: 2016-07-31 13:10:13\n---\n\n[![shipping-82339](http://edpflager.com/wp-content/uploads/2016/07/shipping-82339-300x225.jpg)](http://edpflager.com/?attachment_id=3385#main)I've been learning how to use Docker for the last couple of months. Part of that experience has been downloading and working with various freely available containers from the Docker Hub. Since an ever increasing number applications are web based (i.e using a website as a UI tool) porting many open source projects to use Docker is becoming less complex. While not all open source applications can benefit yet from containerization, I think its only a matter of time before this technology starts to really gain in popularity.\n\nMy area of interest tends to fall more towards Business Intelligence and related technologies, so what I've been experimenting with are containers in that realm. This time around, I'll discuss these containers and my experience getting them running on a native Docker installation on Ubuntu:\n\n*   Rstudio\n*   Cloudera Hadoop\n*   ODOO with Postgresql\n<!-- more -->\n##### RStudio\n\nMy latest experimenting has been with RStudio which is an IDE that allows you to execute code with the data analytics language R. To pull the image from the Docker Hub execute:\n\ndocker pull rocker/rstudio\n\nRunning the container with the command given in the documentation will assign an IP address from within the Docker created subnet. On my system this caused a significant delay and then the browser on my client machine timed out. To assign the Host server's IP address and make the container available without setting up additional routing on my client I needed to modify the provided command and insert the Docker host's IP address into the command like this:\n\ndocker run -d -p <host IP address>:8787:8787 rocker/rstudio\n\nAccessing the address: **<host IP address>:8787** brings up the RStudio interface quickly and you can login with the default UserID and password: rstudio/rstudio\n\n#####  CLOUDERA HADOOP\n\nCloudera - developers of one of the most popular Hadoop distributions, offers a quick start Docker version of their Hadoop platform on their [website](http://www.cloudera.com/downloads/quickstart_vms/5-7.html). Since my server doesn't have a GUI and the Cloudera website doesn't provide a link (that I could use **wget** with) unless you provide all of your information to them, I opted to pull a container from the Docker hub with:\n\npull cloudera/quickstart:latest\n\nEither way, once you have it downloaded, you can start the container with a series of options. For ease of use, I dropped the command line into a BASH script and just execute that to start it :\n\ndocker run --hostname\\=quickstart.cloudera --privileged\\=true -t -i -p 8888:8888 cloudera/quickstart /usr/bin/docker-quickstart\n\nSwitching over to my client, in a web browser, I can access **<host IP address>:8888** to bring up the Cloudera HUE interface and login with the default User ID and password: cloudera/cloudera\n\n##### ODOO WITH POSTGRESQL\n\nODOO or OpenERP as it was called initially, is a huge suite of business applications that can be installed to meet most needs for small to very large enterprises (ODOO says on their website that Toyota uses their software for some of its needs). They pride themselves on ease of use, which in my experience tends to be accurate. The Community Edition lacks some of the features in the paid edition, but for a smaller company, or one testing ODOO out, it should be sufficient for most needs (no support however).\n\nThe ODOO container is available on the Docker Hub and can be pulled with:\n\n pull odoo\n\nOdoo requires a PostgreSQL 9.4 database server as a backend, and their online documentation provides a command line that sets up the PostgreSQL container with the expected container name and two environment variables that Odoo will be attempting to use:\n\ndocker run -d -e POSTGRES\\_USER=odoo -e POSTGRES\\_PASSWORD=odoo --name db postgres:9.4\n\nOnce you have the database server running, you can run the ODOO container with a fairly straightforward Docker command that maps the container port to 8069, and links to the PostgreSQL container with the --link switch:\n\ndocker run -p 8069:8069 --name odoo --link db:db -t odoo\n\nThis command uses a -t switch to show you on the terminal what is happening within the ODOO container.  For my purposes, I would prefer not to show that, but instead would use this command to run ODOO as a daemon:\n\ndocker run -d -p 8069:8069 --name odoo --link db:db odoo \n\nIf I need to see the screen responses, I can use the Docker logs command with any switches necessary to trim down the output. To see whats happened on screen since the container was started up for example:\n\ndocker logs odoo\n\nSwitching over to my client, in a web browser, I can access **<host IP address>:8069** to bring up the ODOO database creation website. Enter in the requested info and click CREATE DATABASE. After a few minutes, the database will be created, and the webpage will change and prompt you to login using the credentials you supplied in the first screen. Login and you are ready to start configuring and using ODOO. One thing to watch for with ODOO: If you shutdown the ODOO container, be sure to restart the PostgreSQL container before restarting ODOO.\n\nThe best part of this process is I can have all three (four if you count the PostgreSQL one) up and running at the same time. While it was possible to do this previously using VMs there is considerably less overhead on my server running Docker with these containers. Hopefully that illustrates why I am excited about Docker and what it brings to the open source landscape.","slug":"interesting-docker-containers-and-some-tips-on-running-them","published":1,"updated":"2020-08-23T20:54:35.066Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a2j0054sdjxgskz75wp","content":"<p><a href=\"http://edpflager.com/?attachment_id=3385#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/shipping-82339-300x225.jpg\" alt=\"shipping-82339\"></a>I’ve been learning how to use Docker for the last couple of months. Part of that experience has been downloading and working with various freely available containers from the Docker Hub. Since an ever increasing number applications are web based (i.e using a website as a UI tool) porting many open source projects to use Docker is becoming less complex. While not all open source applications can benefit yet from containerization, I think its only a matter of time before this technology starts to really gain in popularity.</p>\n<p>My area of interest tends to fall more towards Business Intelligence and related technologies, so what I’ve been experimenting with are containers in that realm. This time around, I’ll discuss these containers and my experience getting them running on a native Docker installation on Ubuntu:</p>\n<ul>\n<li>Rstudio</li>\n<li>Cloudera Hadoop</li>\n<li>ODOO with Postgresql<a id=\"more\"></a>\n<h5 id=\"RStudio\"><a href=\"#RStudio\" class=\"headerlink\" title=\"RStudio\"></a>RStudio</h5></li>\n</ul>\n<p>My latest experimenting has been with RStudio which is an IDE that allows you to execute code with the data analytics language R. To pull the image from the Docker Hub execute:</p>\n<p>docker pull rocker/rstudio</p>\n<p>Running the container with the command given in the documentation will assign an IP address from within the Docker created subnet. On my system this caused a significant delay and then the browser on my client machine timed out. To assign the Host server’s IP address and make the container available without setting up additional routing on my client I needed to modify the provided command and insert the Docker host’s IP address into the command like this:</p>\n<p>docker run -d -p <host IP address>:8787:8787 rocker/rstudio</p>\n<p>Accessing the address: <strong><host IP address>:8787</strong> brings up the RStudio interface quickly and you can login with the default UserID and password: rstudio/rstudio</p>\n<h5 id=\"CLOUDERA-HADOOP\"><a href=\"#CLOUDERA-HADOOP\" class=\"headerlink\" title=\" CLOUDERA HADOOP\"></a> CLOUDERA HADOOP</h5><p>Cloudera - developers of one of the most popular Hadoop distributions, offers a quick start Docker version of their Hadoop platform on their <a href=\"http://www.cloudera.com/downloads/quickstart_vms/5-7.html\">website</a>. Since my server doesn’t have a GUI and the Cloudera website doesn’t provide a link (that I could use <strong>wget</strong> with) unless you provide all of your information to them, I opted to pull a container from the Docker hub with:</p>\n<p>pull cloudera/quickstart:latest</p>\n<p>Either way, once you have it downloaded, you can start the container with a series of options. For ease of use, I dropped the command line into a BASH script and just execute that to start it :</p>\n<p>docker run –hostname=quickstart.cloudera –privileged=true -t -i -p 8888:8888 cloudera/quickstart /usr/bin/docker-quickstart</p>\n<p>Switching over to my client, in a web browser, I can access <strong><host IP address>:8888</strong> to bring up the Cloudera HUE interface and login with the default User ID and password: cloudera/cloudera</p>\n<h5 id=\"ODOO-WITH-POSTGRESQL\"><a href=\"#ODOO-WITH-POSTGRESQL\" class=\"headerlink\" title=\"ODOO WITH POSTGRESQL\"></a>ODOO WITH POSTGRESQL</h5><p>ODOO or OpenERP as it was called initially, is a huge suite of business applications that can be installed to meet most needs for small to very large enterprises (ODOO says on their website that Toyota uses their software for some of its needs). They pride themselves on ease of use, which in my experience tends to be accurate. The Community Edition lacks some of the features in the paid edition, but for a smaller company, or one testing ODOO out, it should be sufficient for most needs (no support however).</p>\n<p>The ODOO container is available on the Docker Hub and can be pulled with:</p>\n<p> pull odoo</p>\n<p>Odoo requires a PostgreSQL 9.4 database server as a backend, and their online documentation provides a command line that sets up the PostgreSQL container with the expected container name and two environment variables that Odoo will be attempting to use:</p>\n<p>docker run -d -e POSTGRES_USER=odoo -e POSTGRES_PASSWORD=odoo –name db postgres:9.4</p>\n<p>Once you have the database server running, you can run the ODOO container with a fairly straightforward Docker command that maps the container port to 8069, and links to the PostgreSQL container with the –link switch:</p>\n<p>docker run -p 8069:8069 –name odoo –link db:db -t odoo</p>\n<p>This command uses a -t switch to show you on the terminal what is happening within the ODOO container.  For my purposes, I would prefer not to show that, but instead would use this command to run ODOO as a daemon:</p>\n<p>docker run -d -p 8069:8069 –name odoo –link db:db odoo </p>\n<p>If I need to see the screen responses, I can use the Docker logs command with any switches necessary to trim down the output. To see whats happened on screen since the container was started up for example:</p>\n<p>docker logs odoo</p>\n<p>Switching over to my client, in a web browser, I can access <strong><host IP address>:8069</strong> to bring up the ODOO database creation website. Enter in the requested info and click CREATE DATABASE. After a few minutes, the database will be created, and the webpage will change and prompt you to login using the credentials you supplied in the first screen. Login and you are ready to start configuring and using ODOO. One thing to watch for with ODOO: If you shutdown the ODOO container, be sure to restart the PostgreSQL container before restarting ODOO.</p>\n<p>The best part of this process is I can have all three (four if you count the PostgreSQL one) up and running at the same time. While it was possible to do this previously using VMs there is considerably less overhead on my server running Docker with these containers. Hopefully that illustrates why I am excited about Docker and what it brings to the open source landscape.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3385#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/07/shipping-82339-300x225.jpg\" alt=\"shipping-82339\"></a>I’ve been learning how to use Docker for the last couple of months. Part of that experience has been downloading and working with various freely available containers from the Docker Hub. Since an ever increasing number applications are web based (i.e using a website as a UI tool) porting many open source projects to use Docker is becoming less complex. While not all open source applications can benefit yet from containerization, I think its only a matter of time before this technology starts to really gain in popularity.</p>\n<p>My area of interest tends to fall more towards Business Intelligence and related technologies, so what I’ve been experimenting with are containers in that realm. This time around, I’ll discuss these containers and my experience getting them running on a native Docker installation on Ubuntu:</p>\n<ul>\n<li>Rstudio</li>\n<li>Cloudera Hadoop</li>\n<li>ODOO with Postgresql","more":"<h5 id=\"RStudio\"><a href=\"#RStudio\" class=\"headerlink\" title=\"RStudio\"></a>RStudio</h5></li>\n</ul>\n<p>My latest experimenting has been with RStudio which is an IDE that allows you to execute code with the data analytics language R. To pull the image from the Docker Hub execute:</p>\n<p>docker pull rocker/rstudio</p>\n<p>Running the container with the command given in the documentation will assign an IP address from within the Docker created subnet. On my system this caused a significant delay and then the browser on my client machine timed out. To assign the Host server’s IP address and make the container available without setting up additional routing on my client I needed to modify the provided command and insert the Docker host’s IP address into the command like this:</p>\n<p>docker run -d -p <host IP address>:8787:8787 rocker/rstudio</p>\n<p>Accessing the address: <strong><host IP address>:8787</strong> brings up the RStudio interface quickly and you can login with the default UserID and password: rstudio/rstudio</p>\n<h5 id=\"CLOUDERA-HADOOP\"><a href=\"#CLOUDERA-HADOOP\" class=\"headerlink\" title=\" CLOUDERA HADOOP\"></a> CLOUDERA HADOOP</h5><p>Cloudera - developers of one of the most popular Hadoop distributions, offers a quick start Docker version of their Hadoop platform on their <a href=\"http://www.cloudera.com/downloads/quickstart_vms/5-7.html\">website</a>. Since my server doesn’t have a GUI and the Cloudera website doesn’t provide a link (that I could use <strong>wget</strong> with) unless you provide all of your information to them, I opted to pull a container from the Docker hub with:</p>\n<p>pull cloudera/quickstart:latest</p>\n<p>Either way, once you have it downloaded, you can start the container with a series of options. For ease of use, I dropped the command line into a BASH script and just execute that to start it :</p>\n<p>docker run –hostname=quickstart.cloudera –privileged=true -t -i -p 8888:8888 cloudera/quickstart /usr/bin/docker-quickstart</p>\n<p>Switching over to my client, in a web browser, I can access <strong><host IP address>:8888</strong> to bring up the Cloudera HUE interface and login with the default User ID and password: cloudera/cloudera</p>\n<h5 id=\"ODOO-WITH-POSTGRESQL\"><a href=\"#ODOO-WITH-POSTGRESQL\" class=\"headerlink\" title=\"ODOO WITH POSTGRESQL\"></a>ODOO WITH POSTGRESQL</h5><p>ODOO or OpenERP as it was called initially, is a huge suite of business applications that can be installed to meet most needs for small to very large enterprises (ODOO says on their website that Toyota uses their software for some of its needs). They pride themselves on ease of use, which in my experience tends to be accurate. The Community Edition lacks some of the features in the paid edition, but for a smaller company, or one testing ODOO out, it should be sufficient for most needs (no support however).</p>\n<p>The ODOO container is available on the Docker Hub and can be pulled with:</p>\n<p> pull odoo</p>\n<p>Odoo requires a PostgreSQL 9.4 database server as a backend, and their online documentation provides a command line that sets up the PostgreSQL container with the expected container name and two environment variables that Odoo will be attempting to use:</p>\n<p>docker run -d -e POSTGRES_USER=odoo -e POSTGRES_PASSWORD=odoo –name db postgres:9.4</p>\n<p>Once you have the database server running, you can run the ODOO container with a fairly straightforward Docker command that maps the container port to 8069, and links to the PostgreSQL container with the –link switch:</p>\n<p>docker run -p 8069:8069 –name odoo –link db:db -t odoo</p>\n<p>This command uses a -t switch to show you on the terminal what is happening within the ODOO container.  For my purposes, I would prefer not to show that, but instead would use this command to run ODOO as a daemon:</p>\n<p>docker run -d -p 8069:8069 –name odoo –link db:db odoo </p>\n<p>If I need to see the screen responses, I can use the Docker logs command with any switches necessary to trim down the output. To see whats happened on screen since the container was started up for example:</p>\n<p>docker logs odoo</p>\n<p>Switching over to my client, in a web browser, I can access <strong><host IP address>:8069</strong> to bring up the ODOO database creation website. Enter in the requested info and click CREATE DATABASE. After a few minutes, the database will be created, and the webpage will change and prompt you to login using the credentials you supplied in the first screen. Login and you are ready to start configuring and using ODOO. One thing to watch for with ODOO: If you shutdown the ODOO container, be sure to restart the PostgreSQL container before restarting ODOO.</p>\n<p>The best part of this process is I can have all three (four if you count the PostgreSQL one) up and running at the same time. While it was possible to do this previously using VMs there is considerably less overhead on my server running Docker with these containers. Hopefully that illustrates why I am excited about Docker and what it brings to the open source landscape.</p>"},{"title":"Interesting stuff from around the Internet","id":"2997","comments":0,"date":"2015-11-01T11:02:38.000Z","_content":"\n[![internet-300px](http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png)](http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png)Been a crazy busy month personally, lost one family member, added a new one, celebrated some stuff, and Halloween was here. I've been looking into a variety of information on the web so here is a round up of some interesting topics until I have some time to devote to normal articles and tutorials. **RethinkDB** - this newer Big Data database platform seems to be getting some traction. Its aimed at realtime web applications, and uses a JSON type data structure (similar to MongoDB), but also provides support for JOINs (which MongoDB doesn't). PacktPub's blog posted an article on [Learning RethinkDB](https://www.packtpub.com/books/content/learning-rethinkdb?utm_source=Sentori&utm_medium=Email&utm_campaign=Data+Dispatch+-+End+October+15). The documentation on the platform's [website](http://rethinkdb.com/) is very good compared to what is normally associated with Open Source projects.\n<!-- more -->\n**Python IDEs -** At a previous job, the JAVA developers used Eclipse and were pretty passionate about it. I've tried it for somethings, but never saw the appeal. OpenSource.com published an article this week looking at the [Top 3 Open Source Python IDE](http://opensource.com/business/15/10/top-open-source-python-ides)s and included Eclipse as one of their choices. An interesting omission was IDLE - the standard **Python** development environment.  [PyCharm](https://www.jetbrains.com/pycharm/) was one of the three the article wrote about, and the one I have been most impressed with. PyCharm comes in two versions, a paid one that will support multiple development languages (like Eclipse), and a free community version that only works for Python. **Pentaho** released version 6 of their [Business Intelligence community edition](http://sourceforge.net/projects/pentaho/files/Data%20Integration/6.0/) suite. I've downloaded and installed Pentaho Data Integration (Kettle), but haven't had time to put it through its paces yet. Most of the new features from what I can tell are aimed at system performance rather than new functionality, which is unusual in this day and age, but a welcome thing to see. Check out the [What's New](http://community.pentaho.com/projects/data-integration/) page for more information. **Seagate Hybrid SSD** - I have switched over to using the [Dell laptop](http://edpflager.com/?p=2883) I talked about previously almost all the time. I've swapped out the hard drive to a 500GB hybrid SSD from Seagate that [Amazon has for $58](http://www.amazon.com/gp/product/B00B99JU5M?psc=1&redirect=true&ref_=oh_aui_detailpage_o00_s00). Although advertised as having similar performance as an SSD, I will say it is slightly slower than the older 240GB SSD I had, but still considerably faster than the 5400RPM HDD that came with the laptop. Definitely a worth while lower end option to SSDs. **DualBoot Linux and Windows 10** - Finally, with the expanded hard drive, I've set the laptop up to dual boot with Windows 10 home edition. [LinuxBSDOS.com](http://linuxbsdos.com/2015/09/07/how-to-dual-boot-ubuntu-15-04-with-windows-10-on-a-single-hard-drive/) has a good tutorial on getting this working correctly. The main motivation for that was occasionally I need to fire up Lightroom to do some photo editing or to access my library of older photos. While I have GIMP and Corel AfterShot installed on the Linux Mint partition, I'm still much more comfortable in Lightroom. I will report that Win10 works very well on this laptop, so if I every decide to sell it off, I will be able to include it that on the old hard drive.","source":"_posts/interesting-stuff-from-around-the-internet.md","raw":"---\ntitle: Interesting stuff from around the Internet\ntags:\n  - external article\n  - guides\n  - How-to\n  - install\n  - technical\nid: '2997'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2015-11-01 06:02:38\n---\n\n[![internet-300px](http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png)](http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png)Been a crazy busy month personally, lost one family member, added a new one, celebrated some stuff, and Halloween was here. I've been looking into a variety of information on the web so here is a round up of some interesting topics until I have some time to devote to normal articles and tutorials. **RethinkDB** - this newer Big Data database platform seems to be getting some traction. Its aimed at realtime web applications, and uses a JSON type data structure (similar to MongoDB), but also provides support for JOINs (which MongoDB doesn't). PacktPub's blog posted an article on [Learning RethinkDB](https://www.packtpub.com/books/content/learning-rethinkdb?utm_source=Sentori&utm_medium=Email&utm_campaign=Data+Dispatch+-+End+October+15). The documentation on the platform's [website](http://rethinkdb.com/) is very good compared to what is normally associated with Open Source projects.\n<!-- more -->\n**Python IDEs -** At a previous job, the JAVA developers used Eclipse and were pretty passionate about it. I've tried it for somethings, but never saw the appeal. OpenSource.com published an article this week looking at the [Top 3 Open Source Python IDE](http://opensource.com/business/15/10/top-open-source-python-ides)s and included Eclipse as one of their choices. An interesting omission was IDLE - the standard **Python** development environment.  [PyCharm](https://www.jetbrains.com/pycharm/) was one of the three the article wrote about, and the one I have been most impressed with. PyCharm comes in two versions, a paid one that will support multiple development languages (like Eclipse), and a free community version that only works for Python. **Pentaho** released version 6 of their [Business Intelligence community edition](http://sourceforge.net/projects/pentaho/files/Data%20Integration/6.0/) suite. I've downloaded and installed Pentaho Data Integration (Kettle), but haven't had time to put it through its paces yet. Most of the new features from what I can tell are aimed at system performance rather than new functionality, which is unusual in this day and age, but a welcome thing to see. Check out the [What's New](http://community.pentaho.com/projects/data-integration/) page for more information. **Seagate Hybrid SSD** - I have switched over to using the [Dell laptop](http://edpflager.com/?p=2883) I talked about previously almost all the time. I've swapped out the hard drive to a 500GB hybrid SSD from Seagate that [Amazon has for $58](http://www.amazon.com/gp/product/B00B99JU5M?psc=1&redirect=true&ref_=oh_aui_detailpage_o00_s00). Although advertised as having similar performance as an SSD, I will say it is slightly slower than the older 240GB SSD I had, but still considerably faster than the 5400RPM HDD that came with the laptop. Definitely a worth while lower end option to SSDs. **DualBoot Linux and Windows 10** - Finally, with the expanded hard drive, I've set the laptop up to dual boot with Windows 10 home edition. [LinuxBSDOS.com](http://linuxbsdos.com/2015/09/07/how-to-dual-boot-ubuntu-15-04-with-windows-10-on-a-single-hard-drive/) has a good tutorial on getting this working correctly. The main motivation for that was occasionally I need to fire up Lightroom to do some photo editing or to access my library of older photos. While I have GIMP and Corel AfterShot installed on the Linux Mint partition, I'm still much more comfortable in Lightroom. I will report that Win10 works very well on this laptop, so if I every decide to sell it off, I will be able to include it that on the old hard drive.","slug":"interesting-stuff-from-around-the-internet","published":1,"updated":"2020-08-23T20:54:34.982Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a2p0057sdjx2oa5e9rj","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png\" alt=\"internet-300px\"></a>Been a crazy busy month personally, lost one family member, added a new one, celebrated some stuff, and Halloween was here. I’ve been looking into a variety of information on the web so here is a round up of some interesting topics until I have some time to devote to normal articles and tutorials. <strong>RethinkDB</strong> - this newer Big Data database platform seems to be getting some traction. Its aimed at realtime web applications, and uses a JSON type data structure (similar to MongoDB), but also provides support for JOINs (which MongoDB doesn’t). PacktPub’s blog posted an article on <a href=\"https://www.packtpub.com/books/content/learning-rethinkdb?utm_source=Sentori&utm_medium=Email&utm_campaign=Data+Dispatch+-+End+October+15\">Learning RethinkDB</a>. The documentation on the platform’s <a href=\"http://rethinkdb.com/\">website</a> is very good compared to what is normally associated with Open Source projects.</p>\n<a id=\"more\"></a>\n<p><strong>Python IDEs -</strong> At a previous job, the JAVA developers used Eclipse and were pretty passionate about it. I’ve tried it for somethings, but never saw the appeal. OpenSource.com published an article this week looking at the <a href=\"http://opensource.com/business/15/10/top-open-source-python-ides\">Top 3 Open Source Python IDE</a>s and included Eclipse as one of their choices. An interesting omission was IDLE - the standard <strong>Python</strong> development environment.  <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> was one of the three the article wrote about, and the one I have been most impressed with. PyCharm comes in two versions, a paid one that will support multiple development languages (like Eclipse), and a free community version that only works for Python. <strong>Pentaho</strong> released version 6 of their <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/6.0/\">Business Intelligence community edition</a> suite. I’ve downloaded and installed Pentaho Data Integration (Kettle), but haven’t had time to put it through its paces yet. Most of the new features from what I can tell are aimed at system performance rather than new functionality, which is unusual in this day and age, but a welcome thing to see. Check out the <a href=\"http://community.pentaho.com/projects/data-integration/\">What’s New</a> page for more information. <strong>Seagate Hybrid SSD</strong> - I have switched over to using the <a href=\"http://edpflager.com/?p=2883\">Dell laptop</a> I talked about previously almost all the time. I’ve swapped out the hard drive to a 500GB hybrid SSD from Seagate that <a href=\"http://www.amazon.com/gp/product/B00B99JU5M?psc=1&redirect=true&ref_=oh_aui_detailpage_o00_s00\">Amazon has for $58</a>. Although advertised as having similar performance as an SSD, I will say it is slightly slower than the older 240GB SSD I had, but still considerably faster than the 5400RPM HDD that came with the laptop. Definitely a worth while lower end option to SSDs. <strong>DualBoot Linux and Windows 10</strong> - Finally, with the expanded hard drive, I’ve set the laptop up to dual boot with Windows 10 home edition. <a href=\"http://linuxbsdos.com/2015/09/07/how-to-dual-boot-ubuntu-15-04-with-windows-10-on-a-single-hard-drive/\">LinuxBSDOS.com</a> has a good tutorial on getting this working correctly. The main motivation for that was occasionally I need to fire up Lightroom to do some photo editing or to access my library of older photos. While I have GIMP and Corel AfterShot installed on the Linux Mint partition, I’m still much more comfortable in Lightroom. I will report that Win10 works very well on this laptop, so if I every decide to sell it off, I will be able to include it that on the old hard drive.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/internet-300px.png\" alt=\"internet-300px\"></a>Been a crazy busy month personally, lost one family member, added a new one, celebrated some stuff, and Halloween was here. I’ve been looking into a variety of information on the web so here is a round up of some interesting topics until I have some time to devote to normal articles and tutorials. <strong>RethinkDB</strong> - this newer Big Data database platform seems to be getting some traction. Its aimed at realtime web applications, and uses a JSON type data structure (similar to MongoDB), but also provides support for JOINs (which MongoDB doesn’t). PacktPub’s blog posted an article on <a href=\"https://www.packtpub.com/books/content/learning-rethinkdb?utm_source=Sentori&utm_medium=Email&utm_campaign=Data+Dispatch+-+End+October+15\">Learning RethinkDB</a>. The documentation on the platform’s <a href=\"http://rethinkdb.com/\">website</a> is very good compared to what is normally associated with Open Source projects.</p>","more":"<p><strong>Python IDEs -</strong> At a previous job, the JAVA developers used Eclipse and were pretty passionate about it. I’ve tried it for somethings, but never saw the appeal. OpenSource.com published an article this week looking at the <a href=\"http://opensource.com/business/15/10/top-open-source-python-ides\">Top 3 Open Source Python IDE</a>s and included Eclipse as one of their choices. An interesting omission was IDLE - the standard <strong>Python</strong> development environment.  <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> was one of the three the article wrote about, and the one I have been most impressed with. PyCharm comes in two versions, a paid one that will support multiple development languages (like Eclipse), and a free community version that only works for Python. <strong>Pentaho</strong> released version 6 of their <a href=\"http://sourceforge.net/projects/pentaho/files/Data%20Integration/6.0/\">Business Intelligence community edition</a> suite. I’ve downloaded and installed Pentaho Data Integration (Kettle), but haven’t had time to put it through its paces yet. Most of the new features from what I can tell are aimed at system performance rather than new functionality, which is unusual in this day and age, but a welcome thing to see. Check out the <a href=\"http://community.pentaho.com/projects/data-integration/\">What’s New</a> page for more information. <strong>Seagate Hybrid SSD</strong> - I have switched over to using the <a href=\"http://edpflager.com/?p=2883\">Dell laptop</a> I talked about previously almost all the time. I’ve swapped out the hard drive to a 500GB hybrid SSD from Seagate that <a href=\"http://www.amazon.com/gp/product/B00B99JU5M?psc=1&redirect=true&ref_=oh_aui_detailpage_o00_s00\">Amazon has for $58</a>. Although advertised as having similar performance as an SSD, I will say it is slightly slower than the older 240GB SSD I had, but still considerably faster than the 5400RPM HDD that came with the laptop. Definitely a worth while lower end option to SSDs. <strong>DualBoot Linux and Windows 10</strong> - Finally, with the expanded hard drive, I’ve set the laptop up to dual boot with Windows 10 home edition. <a href=\"http://linuxbsdos.com/2015/09/07/how-to-dual-boot-ubuntu-15-04-with-windows-10-on-a-single-hard-drive/\">LinuxBSDOS.com</a> has a good tutorial on getting this working correctly. The main motivation for that was occasionally I need to fire up Lightroom to do some photo editing or to access my library of older photos. While I have GIMP and Corel AfterShot installed on the Linux Mint partition, I’m still much more comfortable in Lightroom. I will report that Win10 works very well on this laptop, so if I every decide to sell it off, I will be able to include it that on the old hard drive.</p>"},{"title":"Just Work!","id":"1045","comments":0,"date":"2013-04-04T21:36:42.000Z","_content":"\nFor those who don't know me, a little background. I have a BA and an MBA, I have worked in the IT field for over a dozen years, on various operating systems, applications and hardware platforms. I think I'm a fairly sharp guy. But one of the things that continuously annoys me about Linux and Open Source software in general is the sheer kludginess (is that a word) of trying to get software to JUST WORK! Microsoft and Apple have not carved out their market share because they have superior products, but because for the average layperson, getting the system to work, and installing software is pretty simple to do. I read something a few weeks back from a columnist (wish I could remember who - so I could link to him), who said that for 90% of the population, they don't care how their computer and operating system works, they just want it to work so they can get stuff done. Just as most people don't care how their car engine works, or how household appliances work.  They want them to JUST WORK!\n<!-- more -->\nSo what brought all of this up? For various professional reasons, I decided to download and install an unnamed database package to try and learn it. Off I went to their website to download it. First thing I discovered was, that if you wanted to use it in a Linux environment, it had to be a specific version of Linux. They were not supporting a large number of distros anymore (including the one I had already installed and was comfortable with). I backtracked a little bit and installed a couple of the distros they are supporting in VMs to try it out. Then I had to download and install several different non-standard packages that the program installer needed. Not the actual application, but THE INSTALLER! Got that done, and tried to install the database. It did a system check, and errored out, because I didn't have the OS configured correctly. It needed the VM to have a domain name associated with it. Localhost wasn't enough. It had to be localhost.localdomain. So I fixed that, and tried to run it again. Still didn't work. I didn't have some other environment variable setup OK. (Starting to see a pattern here?). Anyway, after trying for several hours over a weekend, hitting multiple snags and fixing them on multiple VMs, I finally got it installed.... On a Windows VM in about 15 minutes. :P Now technically its supposed to run better on Linux than Windows, but if I can't get it running on Linux because the process is pretty clunky and cumbersome, I'm left to fall back on Windows. And that is what a lot of consumers and companies are doing. Despite the inherent benefits of using an open source platform, it came down to I wanted it to JUST WORK! And that folks, is the problem that Linux vendors, developers and users have to confront and overcome. Make the software easy to use, easy to install and easy to maintain so people can get their stuff done. Get it to JUST WORK, and people will use it.","source":"_posts/just-work.md","raw":"---\ntitle: Just Work!\ntags: []\nid: '1045'\ncategories:\n  - - Linux\ncomments: false\ndate: 2013-04-04 17:36:42\n---\n\nFor those who don't know me, a little background. I have a BA and an MBA, I have worked in the IT field for over a dozen years, on various operating systems, applications and hardware platforms. I think I'm a fairly sharp guy. But one of the things that continuously annoys me about Linux and Open Source software in general is the sheer kludginess (is that a word) of trying to get software to JUST WORK! Microsoft and Apple have not carved out their market share because they have superior products, but because for the average layperson, getting the system to work, and installing software is pretty simple to do. I read something a few weeks back from a columnist (wish I could remember who - so I could link to him), who said that for 90% of the population, they don't care how their computer and operating system works, they just want it to work so they can get stuff done. Just as most people don't care how their car engine works, or how household appliances work.  They want them to JUST WORK!\n<!-- more -->\nSo what brought all of this up? For various professional reasons, I decided to download and install an unnamed database package to try and learn it. Off I went to their website to download it. First thing I discovered was, that if you wanted to use it in a Linux environment, it had to be a specific version of Linux. They were not supporting a large number of distros anymore (including the one I had already installed and was comfortable with). I backtracked a little bit and installed a couple of the distros they are supporting in VMs to try it out. Then I had to download and install several different non-standard packages that the program installer needed. Not the actual application, but THE INSTALLER! Got that done, and tried to install the database. It did a system check, and errored out, because I didn't have the OS configured correctly. It needed the VM to have a domain name associated with it. Localhost wasn't enough. It had to be localhost.localdomain. So I fixed that, and tried to run it again. Still didn't work. I didn't have some other environment variable setup OK. (Starting to see a pattern here?). Anyway, after trying for several hours over a weekend, hitting multiple snags and fixing them on multiple VMs, I finally got it installed.... On a Windows VM in about 15 minutes. :P Now technically its supposed to run better on Linux than Windows, but if I can't get it running on Linux because the process is pretty clunky and cumbersome, I'm left to fall back on Windows. And that is what a lot of consumers and companies are doing. Despite the inherent benefits of using an open source platform, it came down to I wanted it to JUST WORK! And that folks, is the problem that Linux vendors, developers and users have to confront and overcome. Make the software easy to use, easy to install and easy to maintain so people can get their stuff done. Get it to JUST WORK, and people will use it.","slug":"just-work","published":1,"updated":"2020-08-23T20:54:34.722Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a2w005csdjx6qxg58dl","content":"<p>For those who don’t know me, a little background. I have a BA and an MBA, I have worked in the IT field for over a dozen years, on various operating systems, applications and hardware platforms. I think I’m a fairly sharp guy. But one of the things that continuously annoys me about Linux and Open Source software in general is the sheer kludginess (is that a word) of trying to get software to JUST WORK! Microsoft and Apple have not carved out their market share because they have superior products, but because for the average layperson, getting the system to work, and installing software is pretty simple to do. I read something a few weeks back from a columnist (wish I could remember who - so I could link to him), who said that for 90% of the population, they don’t care how their computer and operating system works, they just want it to work so they can get stuff done. Just as most people don’t care how their car engine works, or how household appliances work.  They want them to JUST WORK!</p>\n<a id=\"more\"></a>\n<p>So what brought all of this up? For various professional reasons, I decided to download and install an unnamed database package to try and learn it. Off I went to their website to download it. First thing I discovered was, that if you wanted to use it in a Linux environment, it had to be a specific version of Linux. They were not supporting a large number of distros anymore (including the one I had already installed and was comfortable with). I backtracked a little bit and installed a couple of the distros they are supporting in VMs to try it out. Then I had to download and install several different non-standard packages that the program installer needed. Not the actual application, but THE INSTALLER! Got that done, and tried to install the database. It did a system check, and errored out, because I didn’t have the OS configured correctly. It needed the VM to have a domain name associated with it. Localhost wasn’t enough. It had to be localhost.localdomain. So I fixed that, and tried to run it again. Still didn’t work. I didn’t have some other environment variable setup OK. (Starting to see a pattern here?). Anyway, after trying for several hours over a weekend, hitting multiple snags and fixing them on multiple VMs, I finally got it installed…. On a Windows VM in about 15 minutes. :P Now technically its supposed to run better on Linux than Windows, but if I can’t get it running on Linux because the process is pretty clunky and cumbersome, I’m left to fall back on Windows. And that is what a lot of consumers and companies are doing. Despite the inherent benefits of using an open source platform, it came down to I wanted it to JUST WORK! And that folks, is the problem that Linux vendors, developers and users have to confront and overcome. Make the software easy to use, easy to install and easy to maintain so people can get their stuff done. Get it to JUST WORK, and people will use it.</p>\n","site":{"data":{}},"excerpt":"<p>For those who don’t know me, a little background. I have a BA and an MBA, I have worked in the IT field for over a dozen years, on various operating systems, applications and hardware platforms. I think I’m a fairly sharp guy. But one of the things that continuously annoys me about Linux and Open Source software in general is the sheer kludginess (is that a word) of trying to get software to JUST WORK! Microsoft and Apple have not carved out their market share because they have superior products, but because for the average layperson, getting the system to work, and installing software is pretty simple to do. I read something a few weeks back from a columnist (wish I could remember who - so I could link to him), who said that for 90% of the population, they don’t care how their computer and operating system works, they just want it to work so they can get stuff done. Just as most people don’t care how their car engine works, or how household appliances work.  They want them to JUST WORK!</p>","more":"<p>So what brought all of this up? For various professional reasons, I decided to download and install an unnamed database package to try and learn it. Off I went to their website to download it. First thing I discovered was, that if you wanted to use it in a Linux environment, it had to be a specific version of Linux. They were not supporting a large number of distros anymore (including the one I had already installed and was comfortable with). I backtracked a little bit and installed a couple of the distros they are supporting in VMs to try it out. Then I had to download and install several different non-standard packages that the program installer needed. Not the actual application, but THE INSTALLER! Got that done, and tried to install the database. It did a system check, and errored out, because I didn’t have the OS configured correctly. It needed the VM to have a domain name associated with it. Localhost wasn’t enough. It had to be localhost.localdomain. So I fixed that, and tried to run it again. Still didn’t work. I didn’t have some other environment variable setup OK. (Starting to see a pattern here?). Anyway, after trying for several hours over a weekend, hitting multiple snags and fixing them on multiple VMs, I finally got it installed…. On a Windows VM in about 15 minutes. :P Now technically its supposed to run better on Linux than Windows, but if I can’t get it running on Linux because the process is pretty clunky and cumbersome, I’m left to fall back on Windows. And that is what a lot of consumers and companies are doing. Despite the inherent benefits of using an open source platform, it came down to I wanted it to JUST WORK! And that folks, is the problem that Linux vendors, developers and users have to confront and overcome. Make the software easy to use, easy to install and easy to maintain so people can get their stuff done. Get it to JUST WORK, and people will use it.</p>"},{"title":"Pentaho Kettle Time and Date Manipulations - Part 1","id":"2158","comments":0,"date":"2014-06-30T20:04:12.000Z","_content":"\n[![Screenshot](http://edpflager.com/wp-content/uploads/2014/06/Screenshot-300x152.png)](http://edpflager.com/wp-content/uploads/2014/06/Screenshot.png)When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. When pulling data from a database, its usually easier and more efficient to use any built-in functions that the database provides. But if you are performing some work in Kettle (AKA Pentaho PDI) jobs that are not part of queries that manipulate dates, you should be able to handle it easily with a couple of Kettle's transform steps. In this part of  this series, we'll look into the GET SYSTEM INFO/DATA function. (The label in the Design | Input tree is GET SYSTEM INFO but when you open the function on the canvas the window is labeled Get System Data.)\n<!-- more -->\n##### Get System Info\n\nThe Get System Info function resides under the INPUT node in the Design tab of the Kettle interface. Drag it onto the canvas and double click it. Its fairly plan, but that hides the abundance of options that are available. Use the name field to provide a name for the item you will be using, generally something meaningful in the context of your transformation. You can add a number of items here, so give them names that can provide some hints to their purpose or what they contain. For my example, I used Yesterday-EndOfDay and Today-StartofDay. Next, click in the Type column and you will be presented with a large list of system information you can use in your transformation. The information falls into several categories: date and time values, machine information, Kettle information, job information and command line arguments. The date and time values are useful when you are creating transformations that may be run regularly because Kettle will substitute the actual value in place of the Information Type you selected. For example:\n\n*   You can define two different values each for Yesterday, Today and Tomorrow - one with a 00:00:00 time component  and the other with a 23:59:59 time component.\n*   With the variance in the number of days in a month,  information types for first and last day of last month, this month and next month again with two time components are especially useful.\n*   First and last day of last week, this week and next week (with the time components) make manipulating information in week specific chunks easier.\n*   Quarter values are also useful, again with first and last day of last quarter, this quarter and next quarter along with the beginning time of 00:00:00 for the quarter and 23:59:59 ending time for the last day of the quarter.\n*   First and last day of the previous year, the current year and next year are also handy, allowing you to code transformations that may be used for several years. (And they include the beginning time of 00:00:00 for the year and 23:59:59 for the ending time for the year.)\n*   One interesting option is the first day of last week, this week and next week and the last day of the same which has two different versions, one for US and the other blank. In my testing they did not return different results, so I am not sure what the difference is.\n*   Finally, there is the Last Working Day of Last week, this Week and Next week. A word of warning - if you use these, be aware that they return Thursday for the last working day of the week, not Friday as it is in the US.\n\nFrom the example screen shot above, you can see the difference between the start of day and end of day components. The two fields I have created are only a second apart, but the usage of Kettle's defined time variables means I can do some finer grained manipulation without needing to add unnecessary code. Once you have a field defined (or multiple fields), click on the Preview button, and Kettle will show you the results so you can verify you are getting what you expected.\n\n##### COMING UP...\n\nAs I stated above, the Get System Info component also provides a large number of other predefined values that may be useful in your ETL workflows. We'll delve in to those at a future date. Next time we'll look at another component that Pentaho provides to allow you to define and manipulate date and time functions, and how to perform calculations using the fields we've defined here, and more.","source":"_posts/kettle-time-and-date-manipulations-part-1.md","raw":"---\ntitle: Pentaho Kettle Time and Date Manipulations - Part 1\ntags:\n  - ETL\n  - How-to\n  - kettle\n  - technical\nid: '2158'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-06-30 16:04:12\n---\n\n[![Screenshot](http://edpflager.com/wp-content/uploads/2014/06/Screenshot-300x152.png)](http://edpflager.com/wp-content/uploads/2014/06/Screenshot.png)When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. When pulling data from a database, its usually easier and more efficient to use any built-in functions that the database provides. But if you are performing some work in Kettle (AKA Pentaho PDI) jobs that are not part of queries that manipulate dates, you should be able to handle it easily with a couple of Kettle's transform steps. In this part of  this series, we'll look into the GET SYSTEM INFO/DATA function. (The label in the Design | Input tree is GET SYSTEM INFO but when you open the function on the canvas the window is labeled Get System Data.)\n<!-- more -->\n##### Get System Info\n\nThe Get System Info function resides under the INPUT node in the Design tab of the Kettle interface. Drag it onto the canvas and double click it. Its fairly plan, but that hides the abundance of options that are available. Use the name field to provide a name for the item you will be using, generally something meaningful in the context of your transformation. You can add a number of items here, so give them names that can provide some hints to their purpose or what they contain. For my example, I used Yesterday-EndOfDay and Today-StartofDay. Next, click in the Type column and you will be presented with a large list of system information you can use in your transformation. The information falls into several categories: date and time values, machine information, Kettle information, job information and command line arguments. The date and time values are useful when you are creating transformations that may be run regularly because Kettle will substitute the actual value in place of the Information Type you selected. For example:\n\n*   You can define two different values each for Yesterday, Today and Tomorrow - one with a 00:00:00 time component  and the other with a 23:59:59 time component.\n*   With the variance in the number of days in a month,  information types for first and last day of last month, this month and next month again with two time components are especially useful.\n*   First and last day of last week, this week and next week (with the time components) make manipulating information in week specific chunks easier.\n*   Quarter values are also useful, again with first and last day of last quarter, this quarter and next quarter along with the beginning time of 00:00:00 for the quarter and 23:59:59 ending time for the last day of the quarter.\n*   First and last day of the previous year, the current year and next year are also handy, allowing you to code transformations that may be used for several years. (And they include the beginning time of 00:00:00 for the year and 23:59:59 for the ending time for the year.)\n*   One interesting option is the first day of last week, this week and next week and the last day of the same which has two different versions, one for US and the other blank. In my testing they did not return different results, so I am not sure what the difference is.\n*   Finally, there is the Last Working Day of Last week, this Week and Next week. A word of warning - if you use these, be aware that they return Thursday for the last working day of the week, not Friday as it is in the US.\n\nFrom the example screen shot above, you can see the difference between the start of day and end of day components. The two fields I have created are only a second apart, but the usage of Kettle's defined time variables means I can do some finer grained manipulation without needing to add unnecessary code. Once you have a field defined (or multiple fields), click on the Preview button, and Kettle will show you the results so you can verify you are getting what you expected.\n\n##### COMING UP...\n\nAs I stated above, the Get System Info component also provides a large number of other predefined values that may be useful in your ETL workflows. We'll delve in to those at a future date. Next time we'll look at another component that Pentaho provides to allow you to define and manipulate date and time functions, and how to perform calculations using the fields we've defined here, and more.","slug":"kettle-time-and-date-manipulations-part-1","published":1,"updated":"2020-08-23T20:54:34.862Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a34005fsdjx641tcwyo","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/06/Screenshot.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/Screenshot-300x152.png\" alt=\"Screenshot\"></a>When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. When pulling data from a database, its usually easier and more efficient to use any built-in functions that the database provides. But if you are performing some work in Kettle (AKA Pentaho PDI) jobs that are not part of queries that manipulate dates, you should be able to handle it easily with a couple of Kettle’s transform steps. In this part of  this series, we’ll look into the GET SYSTEM INFO/DATA function. (The label in the Design | Input tree is GET SYSTEM INFO but when you open the function on the canvas the window is labeled Get System Data.)</p>\n<a id=\"more\"></a>\n<h5 id=\"Get-System-Info\"><a href=\"#Get-System-Info\" class=\"headerlink\" title=\"Get System Info\"></a>Get System Info</h5><p>The Get System Info function resides under the INPUT node in the Design tab of the Kettle interface. Drag it onto the canvas and double click it. Its fairly plan, but that hides the abundance of options that are available. Use the name field to provide a name for the item you will be using, generally something meaningful in the context of your transformation. You can add a number of items here, so give them names that can provide some hints to their purpose or what they contain. For my example, I used Yesterday-EndOfDay and Today-StartofDay. Next, click in the Type column and you will be presented with a large list of system information you can use in your transformation. The information falls into several categories: date and time values, machine information, Kettle information, job information and command line arguments. The date and time values are useful when you are creating transformations that may be run regularly because Kettle will substitute the actual value in place of the Information Type you selected. For example:</p>\n<ul>\n<li>You can define two different values each for Yesterday, Today and Tomorrow - one with a 00:00:00 time component  and the other with a 23:59:59 time component.</li>\n<li>With the variance in the number of days in a month,  information types for first and last day of last month, this month and next month again with two time components are especially useful.</li>\n<li>First and last day of last week, this week and next week (with the time components) make manipulating information in week specific chunks easier.</li>\n<li>Quarter values are also useful, again with first and last day of last quarter, this quarter and next quarter along with the beginning time of 00:00:00 for the quarter and 23:59:59 ending time for the last day of the quarter.</li>\n<li>First and last day of the previous year, the current year and next year are also handy, allowing you to code transformations that may be used for several years. (And they include the beginning time of 00:00:00 for the year and 23:59:59 for the ending time for the year.)</li>\n<li>One interesting option is the first day of last week, this week and next week and the last day of the same which has two different versions, one for US and the other blank. In my testing they did not return different results, so I am not sure what the difference is.</li>\n<li>Finally, there is the Last Working Day of Last week, this Week and Next week. A word of warning - if you use these, be aware that they return Thursday for the last working day of the week, not Friday as it is in the US.</li>\n</ul>\n<p>From the example screen shot above, you can see the difference between the start of day and end of day components. The two fields I have created are only a second apart, but the usage of Kettle’s defined time variables means I can do some finer grained manipulation without needing to add unnecessary code. Once you have a field defined (or multiple fields), click on the Preview button, and Kettle will show you the results so you can verify you are getting what you expected.</p>\n<h5 id=\"COMING-UP…\"><a href=\"#COMING-UP…\" class=\"headerlink\" title=\"COMING UP…\"></a>COMING UP…</h5><p>As I stated above, the Get System Info component also provides a large number of other predefined values that may be useful in your ETL workflows. We’ll delve in to those at a future date. Next time we’ll look at another component that Pentaho provides to allow you to define and manipulate date and time functions, and how to perform calculations using the fields we’ve defined here, and more.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/06/Screenshot.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/Screenshot-300x152.png\" alt=\"Screenshot\"></a>When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. When pulling data from a database, its usually easier and more efficient to use any built-in functions that the database provides. But if you are performing some work in Kettle (AKA Pentaho PDI) jobs that are not part of queries that manipulate dates, you should be able to handle it easily with a couple of Kettle’s transform steps. In this part of  this series, we’ll look into the GET SYSTEM INFO/DATA function. (The label in the Design | Input tree is GET SYSTEM INFO but when you open the function on the canvas the window is labeled Get System Data.)</p>","more":"<h5 id=\"Get-System-Info\"><a href=\"#Get-System-Info\" class=\"headerlink\" title=\"Get System Info\"></a>Get System Info</h5><p>The Get System Info function resides under the INPUT node in the Design tab of the Kettle interface. Drag it onto the canvas and double click it. Its fairly plan, but that hides the abundance of options that are available. Use the name field to provide a name for the item you will be using, generally something meaningful in the context of your transformation. You can add a number of items here, so give them names that can provide some hints to their purpose or what they contain. For my example, I used Yesterday-EndOfDay and Today-StartofDay. Next, click in the Type column and you will be presented with a large list of system information you can use in your transformation. The information falls into several categories: date and time values, machine information, Kettle information, job information and command line arguments. The date and time values are useful when you are creating transformations that may be run regularly because Kettle will substitute the actual value in place of the Information Type you selected. For example:</p>\n<ul>\n<li>You can define two different values each for Yesterday, Today and Tomorrow - one with a 00:00:00 time component  and the other with a 23:59:59 time component.</li>\n<li>With the variance in the number of days in a month,  information types for first and last day of last month, this month and next month again with two time components are especially useful.</li>\n<li>First and last day of last week, this week and next week (with the time components) make manipulating information in week specific chunks easier.</li>\n<li>Quarter values are also useful, again with first and last day of last quarter, this quarter and next quarter along with the beginning time of 00:00:00 for the quarter and 23:59:59 ending time for the last day of the quarter.</li>\n<li>First and last day of the previous year, the current year and next year are also handy, allowing you to code transformations that may be used for several years. (And they include the beginning time of 00:00:00 for the year and 23:59:59 for the ending time for the year.)</li>\n<li>One interesting option is the first day of last week, this week and next week and the last day of the same which has two different versions, one for US and the other blank. In my testing they did not return different results, so I am not sure what the difference is.</li>\n<li>Finally, there is the Last Working Day of Last week, this Week and Next week. A word of warning - if you use these, be aware that they return Thursday for the last working day of the week, not Friday as it is in the US.</li>\n</ul>\n<p>From the example screen shot above, you can see the difference between the start of day and end of day components. The two fields I have created are only a second apart, but the usage of Kettle’s defined time variables means I can do some finer grained manipulation without needing to add unnecessary code. Once you have a field defined (or multiple fields), click on the Preview button, and Kettle will show you the results so you can verify you are getting what you expected.</p>\n<h5 id=\"COMING-UP…\"><a href=\"#COMING-UP…\" class=\"headerlink\" title=\"COMING UP…\"></a>COMING UP…</h5><p>As I stated above, the Get System Info component also provides a large number of other predefined values that may be useful in your ETL workflows. We’ll delve in to those at a future date. Next time we’ll look at another component that Pentaho provides to allow you to define and manipulate date and time functions, and how to perform calculations using the fields we’ve defined here, and more.</p>"},{"title":"Launch Pentaho Spoon from the Desktop","id":"2322","comments":0,"date":"2014-07-29T20:08:47.000Z","_content":"\n[![launch](http://edpflager.com/wp-content/uploads/2014/07/launch-300x201.jpg)](http://edpflager.com/wp-content/uploads/2014/07/launch.jpg)Coming from a Windows/Mac background, I got in the habit of having shortcuts to applications I use frequently on my desktop or in a task bar. I've continued that practice when switching to Linux as well. Unfortunately, Pentaho Spoon - the GUI tool for designing transformations and jobs for Pentaho Data Integration (aka Kettle) is started from a command line. When I tried to create a desktop launcher on my CentOS laptop, but that only resulted in an error message:\n\n###### Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar\n\nI searched on the Internet, and apparently this was a common question, so I decided to come up with a quick and easy solution. Here it is:\n<!-- more -->\n1.  In your home folder, create a text file. I called mine \"start-pentaho.sh\", but feel free to call it whatever you like, as long as you have the .sh extension.\n2.  Edit the file and add these three lines: #!/bin/sh cd <path to where you extracted PDI>. On my laptop, its: cd /opt/pentaho/data-integration ./spoon.sh\n3.  Save the file.\n4.  Open up the file browser, and go to your home folder. Locate the \"start-pentaho.sh\" file you just created, and right click on it and choose properties from the menu.\n5.  In the Properties window that appears, switch to the Permissions tab. For the owner and group, make sure the Access drop down is set to Read and Write and for others it is set to Read-only. Finally at the bottom of the window, check the box next to \"Allow executing file as program\".[![properties](http://edpflager.com/wp-content/uploads/2014/07/properties-282x300.png)](http://edpflager.com/wp-content/uploads/2014/07/properties.png)\n6.  Click Close to save your changes and return to the desktop.\n7.  On your desktop, right click and choose: Create Launcher\n8.  In the type field, choose \"Application in Terminal\". Enter whatever you would like in the Name field. Finally, for the Command field, you can click Browse to where you created your startup file and choose it, or enter the path manually. When you are finished the results should look similar to this:![launcher](http://edpflager.com/wp-content/uploads/2014/07/launcher-300x130.png)\n9.  Before you click OK, you can click the spring icon in the upper left corner, and navigate to the data-integration folder on your system to substitute a Pentaho icon for the spring.\n10.  Once you have changed the icon, click OK, and you should now have a desktop shortcut to start Pentaho.\n11.  Double click the window and a terminal will open with the first line of the script showing: cd /opt/pentaho/data-integration. After a few seconds, the Pentaho splash screen will appear, and then the application will load up after that!\n\n**Update:** If you are running Linux Mint Mate (have not tried this on Cinnamon), you need to install Gnome-terminal for it to work. Launch the Software Manager application, search for gnome-terminal and install it. Then the above will work. PENTAHO is a trademark of Penatho, Inc.","source":"_posts/launch-pentaho-spoon-from-the-desktop.md","raw":"---\ntitle: Launch Pentaho Spoon from the Desktop\ntags:\n  - ETL\n  - How-to\n  - kettle\n  - PDI\n  - technical\nid: '2322'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-07-29 16:08:47\n---\n\n[![launch](http://edpflager.com/wp-content/uploads/2014/07/launch-300x201.jpg)](http://edpflager.com/wp-content/uploads/2014/07/launch.jpg)Coming from a Windows/Mac background, I got in the habit of having shortcuts to applications I use frequently on my desktop or in a task bar. I've continued that practice when switching to Linux as well. Unfortunately, Pentaho Spoon - the GUI tool for designing transformations and jobs for Pentaho Data Integration (aka Kettle) is started from a command line. When I tried to create a desktop launcher on my CentOS laptop, but that only resulted in an error message:\n\n###### Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar\n\nI searched on the Internet, and apparently this was a common question, so I decided to come up with a quick and easy solution. Here it is:\n<!-- more -->\n1.  In your home folder, create a text file. I called mine \"start-pentaho.sh\", but feel free to call it whatever you like, as long as you have the .sh extension.\n2.  Edit the file and add these three lines: #!/bin/sh cd <path to where you extracted PDI>. On my laptop, its: cd /opt/pentaho/data-integration ./spoon.sh\n3.  Save the file.\n4.  Open up the file browser, and go to your home folder. Locate the \"start-pentaho.sh\" file you just created, and right click on it and choose properties from the menu.\n5.  In the Properties window that appears, switch to the Permissions tab. For the owner and group, make sure the Access drop down is set to Read and Write and for others it is set to Read-only. Finally at the bottom of the window, check the box next to \"Allow executing file as program\".[![properties](http://edpflager.com/wp-content/uploads/2014/07/properties-282x300.png)](http://edpflager.com/wp-content/uploads/2014/07/properties.png)\n6.  Click Close to save your changes and return to the desktop.\n7.  On your desktop, right click and choose: Create Launcher\n8.  In the type field, choose \"Application in Terminal\". Enter whatever you would like in the Name field. Finally, for the Command field, you can click Browse to where you created your startup file and choose it, or enter the path manually. When you are finished the results should look similar to this:![launcher](http://edpflager.com/wp-content/uploads/2014/07/launcher-300x130.png)\n9.  Before you click OK, you can click the spring icon in the upper left corner, and navigate to the data-integration folder on your system to substitute a Pentaho icon for the spring.\n10.  Once you have changed the icon, click OK, and you should now have a desktop shortcut to start Pentaho.\n11.  Double click the window and a terminal will open with the first line of the script showing: cd /opt/pentaho/data-integration. After a few seconds, the Pentaho splash screen will appear, and then the application will load up after that!\n\n**Update:** If you are running Linux Mint Mate (have not tried this on Cinnamon), you need to install Gnome-terminal for it to work. Launch the Software Manager application, search for gnome-terminal and install it. Then the above will work. PENTAHO is a trademark of Penatho, Inc.","slug":"launch-pentaho-spoon-from-the-desktop","published":1,"updated":"2020-08-23T20:54:34.886Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a3b005jsdjx8t2c3999","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/launch.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/launch-300x201.jpg\" alt=\"launch\"></a>Coming from a Windows/Mac background, I got in the habit of having shortcuts to applications I use frequently on my desktop or in a task bar. I’ve continued that practice when switching to Linux as well. Unfortunately, Pentaho Spoon - the GUI tool for designing transformations and jobs for Pentaho Data Integration (aka Kettle) is started from a command line. When I tried to create a desktop launcher on my CentOS laptop, but that only resulted in an error message:</p>\n<h6 id=\"Unable-to-access-jarfile-launcher-pentaho-launcher-5-1-0-0-752-jar\"><a href=\"#Unable-to-access-jarfile-launcher-pentaho-launcher-5-1-0-0-752-jar\" class=\"headerlink\" title=\"Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar\"></a>Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar</h6><p>I searched on the Internet, and apparently this was a common question, so I decided to come up with a quick and easy solution. Here it is:</p>\n<a id=\"more\"></a>\n<ol>\n<li>In your home folder, create a text file. I called mine “start-pentaho.sh”, but feel free to call it whatever you like, as long as you have the .sh extension.</li>\n<li>Edit the file and add these three lines: #!/bin/sh cd <path to where you extracted PDI>. On my laptop, its: cd /opt/pentaho/data-integration ./spoon.sh</li>\n<li>Save the file.</li>\n<li>Open up the file browser, and go to your home folder. Locate the “start-pentaho.sh” file you just created, and right click on it and choose properties from the menu.</li>\n<li>In the Properties window that appears, switch to the Permissions tab. For the owner and group, make sure the Access drop down is set to Read and Write and for others it is set to Read-only. Finally at the bottom of the window, check the box next to “Allow executing file as program”.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/properties.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/properties-282x300.png\" alt=\"properties\"></a></li>\n<li>Click Close to save your changes and return to the desktop.</li>\n<li>On your desktop, right click and choose: Create Launcher</li>\n<li>In the type field, choose “Application in Terminal”. Enter whatever you would like in the Name field. Finally, for the Command field, you can click Browse to where you created your startup file and choose it, or enter the path manually. When you are finished the results should look similar to this:<img src=\"http://edpflager.com/wp-content/uploads/2014/07/launcher-300x130.png\" alt=\"launcher\"></li>\n<li>Before you click OK, you can click the spring icon in the upper left corner, and navigate to the data-integration folder on your system to substitute a Pentaho icon for the spring.</li>\n<li>Once you have changed the icon, click OK, and you should now have a desktop shortcut to start Pentaho.</li>\n<li>Double click the window and a terminal will open with the first line of the script showing: cd /opt/pentaho/data-integration. After a few seconds, the Pentaho splash screen will appear, and then the application will load up after that!</li>\n</ol>\n<p><strong>Update:</strong> If you are running Linux Mint Mate (have not tried this on Cinnamon), you need to install Gnome-terminal for it to work. Launch the Software Manager application, search for gnome-terminal and install it. Then the above will work. PENTAHO is a trademark of Penatho, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/launch.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/launch-300x201.jpg\" alt=\"launch\"></a>Coming from a Windows/Mac background, I got in the habit of having shortcuts to applications I use frequently on my desktop or in a task bar. I’ve continued that practice when switching to Linux as well. Unfortunately, Pentaho Spoon - the GUI tool for designing transformations and jobs for Pentaho Data Integration (aka Kettle) is started from a command line. When I tried to create a desktop launcher on my CentOS laptop, but that only resulted in an error message:</p>\n<h6 id=\"Unable-to-access-jarfile-launcher-pentaho-launcher-5-1-0-0-752-jar\"><a href=\"#Unable-to-access-jarfile-launcher-pentaho-launcher-5-1-0-0-752-jar\" class=\"headerlink\" title=\"Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar\"></a>Unable to access jarfile launcher/pentaho-launcher-5.1.0.0-752.jar</h6><p>I searched on the Internet, and apparently this was a common question, so I decided to come up with a quick and easy solution. Here it is:</p>","more":"<ol>\n<li>In your home folder, create a text file. I called mine “start-pentaho.sh”, but feel free to call it whatever you like, as long as you have the .sh extension.</li>\n<li>Edit the file and add these three lines: #!/bin/sh cd <path to where you extracted PDI>. On my laptop, its: cd /opt/pentaho/data-integration ./spoon.sh</li>\n<li>Save the file.</li>\n<li>Open up the file browser, and go to your home folder. Locate the “start-pentaho.sh” file you just created, and right click on it and choose properties from the menu.</li>\n<li>In the Properties window that appears, switch to the Permissions tab. For the owner and group, make sure the Access drop down is set to Read and Write and for others it is set to Read-only. Finally at the bottom of the window, check the box next to “Allow executing file as program”.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/properties.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/properties-282x300.png\" alt=\"properties\"></a></li>\n<li>Click Close to save your changes and return to the desktop.</li>\n<li>On your desktop, right click and choose: Create Launcher</li>\n<li>In the type field, choose “Application in Terminal”. Enter whatever you would like in the Name field. Finally, for the Command field, you can click Browse to where you created your startup file and choose it, or enter the path manually. When you are finished the results should look similar to this:<img src=\"http://edpflager.com/wp-content/uploads/2014/07/launcher-300x130.png\" alt=\"launcher\"></li>\n<li>Before you click OK, you can click the spring icon in the upper left corner, and navigate to the data-integration folder on your system to substitute a Pentaho icon for the spring.</li>\n<li>Once you have changed the icon, click OK, and you should now have a desktop shortcut to start Pentaho.</li>\n<li>Double click the window and a terminal will open with the first line of the script showing: cd /opt/pentaho/data-integration. After a few seconds, the Pentaho splash screen will appear, and then the application will load up after that!</li>\n</ol>\n<p><strong>Update:</strong> If you are running Linux Mint Mate (have not tried this on Cinnamon), you need to install Gnome-terminal for it to work. Launch the Software Manager application, search for gnome-terminal and install it. Then the above will work. PENTAHO is a trademark of Penatho, Inc.</p>"},{"title":"Linux bootable USB drives on Mac OS X Yosemite - Solved!","id":"2762","comments":0,"date":"2015-05-14T22:52:27.000Z","_content":"\n[![boots](http://edpflager.com/wp-content/uploads/2015/05/boots.jpg)](http://edpflager.com/wp-content/uploads/2015/05/boots.jpg)In my twenty plus years of working on PCs, I have seen external media formats change from 5.25 inch floppy disks (which really were floppy) to 3.5 inch (not-so) floppy discs to CD to DVD and USB (thumb) drives. I've had PCs at one time or another that have used all of those formats, but over the past couple of years, the push has to been to move away from external media whenever possible. Thus the last two PCs I have purchased did not come with anything other than USB slots. One is my ZBox computer that I use for development, and I change operating systems on it fairly frequently. Rather than burn a DVD of an ISO every time I want to reconfigure the development box, its should be easier to make a boot-able USB. Using Windows there are a host of different GUI tools to create bootable USB drives, and most Linux distributions are similar. If you are on a Mac running OS X Yosemite, the  solution isn't quite as simple. A number of websites include directions, but generally I've found they don't work. But this week, I found a successful method, and created both Fedora and CentOS bootable USB drives (unfortunately it doesn't seem to work with Windows ISO's). Because this requires enabling the root user account on your system, be sure to exercise caution when following these steps. Read on for more information.\n<!-- more -->\n1.  Find a USB drive that you are OK with erasing and insert it into one of your Mac's USB slots. Make sure the drive is formatted as Fat32 with the Boot Loader option in the partition window.\n2.  Open a terminal prompt (its in your Utilities folder), and run this command: **diskutil list**\n3.  Look for the USB drive you inserted, and WRITE IT DOWN! In my case it was mounted as /dev/disk3 which I was able to tell by looking at the SIZE column. In the following commands, substitute your mount point for the one I am using:[![thumbdrive](http://edpflager.com/wp-content/uploads/2015/05/thumbdrive-300x63.png)](http://edpflager.com/wp-content/uploads/2015/05/thumbdrive.png)\n4.  Run this command to unmount the thumb drive, but don't remove it from the USB slot: **diskutil unmountDisk /dev/disk3**\n5.  The system should respond that: **Unmount of all volumes on disk3 was successful.**\n6.  Stay with me now, this next bit is convoluted! From the Apple icon in top menu bar, open System Preferences and click on **Users & Groups**. A new window will appear, showing the user accounts on your system.\n7.  There is likely a padlock icon at the bottom of the window, that is closed. Click on the lock and enter your administrator user ID and password and click the unlock button to open it.\n8.  At the bottom of the Users pane on the left, click on the Login Options button next to the house.[![loginoptions](http://edpflager.com/wp-content/uploads/2015/05/loginoptions-300x109.png)](http://edpflager.com/wp-content/uploads/2015/05/loginoptions.png)\n9.  In the new window that appears, click the JOIN button.  The Directory Utility window will appear. Click on the OPEN DIRECTORY UTILITY button. In the new window that appears, if the padlock is closed, open it like you did previously.\n10.  From the menu bar, make you are accessing the Directory Utility menu, and click on Edit, and then click Enable Root User.  (This can be a very dangerous step, since by default, Root access is disabled on a Mac! Be sure to take precautions, and once your are finished creating your bootable thumb drive, disable the root user.)\n11.  You will be prompted for a password for the root account and have to enter it twice. Click OK once you are done.\n12.  Switch back to the Terminal prompt and enter **su -** to switch to the root user account. When prompted, enter the password your entered for the root account in step 11.\n13.  Type **dd if=** then drag and drop the iso file to the terminal window to append the file name and path.\n14.  To the end of the command enter **`of=/dev/disk3 bs=1m`** substituting the correct mount point you noted in step 3. You should wind up with something like: **`dd if=/Users/Desktop/distribution.iso of=/dev/rdisk3 bs=1m`**\n15.  Take a deep breath, review what you entered and if everything looks good, hit ENTER.\n16.  At this point,  you'll need to wait while your USB drive is erased and the disk image is written to the drive. You'll know once its complete when the command prompt returns in your terminal windows.\n17.  Once the terminal prompt returns, enter EXIT to leave root access.\n18.  Switch back to the Directory Utility window, and from the menu, choose Disable Root User. Click the padlock to lock it, and then Quit Directory Utility from the menu bar.\n19.  Back at the Users & Groups window, lock the padlock and then Quit System Preferences  from the menu bar.\n20.   Finally, switch back to the Terminal window, and enter Exit to logout of the terminal. Quit the terminal application from the menu bar.\n21.  Remove the USB drive and go boot your other PC!","source":"_posts/linux-bootable-usb-drives-on-mac-os-x-yosemite-solved.md","raw":"---\ntitle: Linux bootable USB drives on Mac OS X Yosemite - Solved!\ntags:\n  - How-to\n  - howto\n  - Mac\nid: '2762'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-05-14 18:52:27\n---\n\n[![boots](http://edpflager.com/wp-content/uploads/2015/05/boots.jpg)](http://edpflager.com/wp-content/uploads/2015/05/boots.jpg)In my twenty plus years of working on PCs, I have seen external media formats change from 5.25 inch floppy disks (which really were floppy) to 3.5 inch (not-so) floppy discs to CD to DVD and USB (thumb) drives. I've had PCs at one time or another that have used all of those formats, but over the past couple of years, the push has to been to move away from external media whenever possible. Thus the last two PCs I have purchased did not come with anything other than USB slots. One is my ZBox computer that I use for development, and I change operating systems on it fairly frequently. Rather than burn a DVD of an ISO every time I want to reconfigure the development box, its should be easier to make a boot-able USB. Using Windows there are a host of different GUI tools to create bootable USB drives, and most Linux distributions are similar. If you are on a Mac running OS X Yosemite, the  solution isn't quite as simple. A number of websites include directions, but generally I've found they don't work. But this week, I found a successful method, and created both Fedora and CentOS bootable USB drives (unfortunately it doesn't seem to work with Windows ISO's). Because this requires enabling the root user account on your system, be sure to exercise caution when following these steps. Read on for more information.\n<!-- more -->\n1.  Find a USB drive that you are OK with erasing and insert it into one of your Mac's USB slots. Make sure the drive is formatted as Fat32 with the Boot Loader option in the partition window.\n2.  Open a terminal prompt (its in your Utilities folder), and run this command: **diskutil list**\n3.  Look for the USB drive you inserted, and WRITE IT DOWN! In my case it was mounted as /dev/disk3 which I was able to tell by looking at the SIZE column. In the following commands, substitute your mount point for the one I am using:[![thumbdrive](http://edpflager.com/wp-content/uploads/2015/05/thumbdrive-300x63.png)](http://edpflager.com/wp-content/uploads/2015/05/thumbdrive.png)\n4.  Run this command to unmount the thumb drive, but don't remove it from the USB slot: **diskutil unmountDisk /dev/disk3**\n5.  The system should respond that: **Unmount of all volumes on disk3 was successful.**\n6.  Stay with me now, this next bit is convoluted! From the Apple icon in top menu bar, open System Preferences and click on **Users & Groups**. A new window will appear, showing the user accounts on your system.\n7.  There is likely a padlock icon at the bottom of the window, that is closed. Click on the lock and enter your administrator user ID and password and click the unlock button to open it.\n8.  At the bottom of the Users pane on the left, click on the Login Options button next to the house.[![loginoptions](http://edpflager.com/wp-content/uploads/2015/05/loginoptions-300x109.png)](http://edpflager.com/wp-content/uploads/2015/05/loginoptions.png)\n9.  In the new window that appears, click the JOIN button.  The Directory Utility window will appear. Click on the OPEN DIRECTORY UTILITY button. In the new window that appears, if the padlock is closed, open it like you did previously.\n10.  From the menu bar, make you are accessing the Directory Utility menu, and click on Edit, and then click Enable Root User.  (This can be a very dangerous step, since by default, Root access is disabled on a Mac! Be sure to take precautions, and once your are finished creating your bootable thumb drive, disable the root user.)\n11.  You will be prompted for a password for the root account and have to enter it twice. Click OK once you are done.\n12.  Switch back to the Terminal prompt and enter **su -** to switch to the root user account. When prompted, enter the password your entered for the root account in step 11.\n13.  Type **dd if=** then drag and drop the iso file to the terminal window to append the file name and path.\n14.  To the end of the command enter **`of=/dev/disk3 bs=1m`** substituting the correct mount point you noted in step 3. You should wind up with something like: **`dd if=/Users/Desktop/distribution.iso of=/dev/rdisk3 bs=1m`**\n15.  Take a deep breath, review what you entered and if everything looks good, hit ENTER.\n16.  At this point,  you'll need to wait while your USB drive is erased and the disk image is written to the drive. You'll know once its complete when the command prompt returns in your terminal windows.\n17.  Once the terminal prompt returns, enter EXIT to leave root access.\n18.  Switch back to the Directory Utility window, and from the menu, choose Disable Root User. Click the padlock to lock it, and then Quit Directory Utility from the menu bar.\n19.  Back at the Users & Groups window, lock the padlock and then Quit System Preferences  from the menu bar.\n20.   Finally, switch back to the Terminal window, and enter Exit to logout of the terminal. Quit the terminal application from the menu bar.\n21.  Remove the USB drive and go boot your other PC!","slug":"linux-bootable-usb-drives-on-mac-os-x-yosemite-solved","published":1,"updated":"2020-08-23T20:54:34.942Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a3h005msdjxextf1ons","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/05/boots.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/boots.jpg\" alt=\"boots\"></a>In my twenty plus years of working on PCs, I have seen external media formats change from 5.25 inch floppy disks (which really were floppy) to 3.5 inch (not-so) floppy discs to CD to DVD and USB (thumb) drives. I’ve had PCs at one time or another that have used all of those formats, but over the past couple of years, the push has to been to move away from external media whenever possible. Thus the last two PCs I have purchased did not come with anything other than USB slots. One is my ZBox computer that I use for development, and I change operating systems on it fairly frequently. Rather than burn a DVD of an ISO every time I want to reconfigure the development box, its should be easier to make a boot-able USB. Using Windows there are a host of different GUI tools to create bootable USB drives, and most Linux distributions are similar. If you are on a Mac running OS X Yosemite, the  solution isn’t quite as simple. A number of websites include directions, but generally I’ve found they don’t work. But this week, I found a successful method, and created both Fedora and CentOS bootable USB drives (unfortunately it doesn’t seem to work with Windows ISO’s). Because this requires enabling the root user account on your system, be sure to exercise caution when following these steps. Read on for more information.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Find a USB drive that you are OK with erasing and insert it into one of your Mac’s USB slots. Make sure the drive is formatted as Fat32 with the Boot Loader option in the partition window.</li>\n<li>Open a terminal prompt (its in your Utilities folder), and run this command: <strong>diskutil list</strong></li>\n<li>Look for the USB drive you inserted, and WRITE IT DOWN! In my case it was mounted as /dev/disk3 which I was able to tell by looking at the SIZE column. In the following commands, substitute your mount point for the one I am using:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/thumbdrive.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/thumbdrive-300x63.png\" alt=\"thumbdrive\"></a></li>\n<li>Run this command to unmount the thumb drive, but don’t remove it from the USB slot: <strong>diskutil unmountDisk /dev/disk3</strong></li>\n<li>The system should respond that: <strong>Unmount of all volumes on disk3 was successful.</strong></li>\n<li>Stay with me now, this next bit is convoluted! From the Apple icon in top menu bar, open System Preferences and click on <strong>Users &amp; Groups</strong>. A new window will appear, showing the user accounts on your system.</li>\n<li>There is likely a padlock icon at the bottom of the window, that is closed. Click on the lock and enter your administrator user ID and password and click the unlock button to open it.</li>\n<li>At the bottom of the Users pane on the left, click on the Login Options button next to the house.<a href=\"http://edpflager.com/wp-content/uploads/2015/05/loginoptions.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/loginoptions-300x109.png\" alt=\"loginoptions\"></a></li>\n<li>In the new window that appears, click the JOIN button.  The Directory Utility window will appear. Click on the OPEN DIRECTORY UTILITY button. In the new window that appears, if the padlock is closed, open it like you did previously.</li>\n<li>From the menu bar, make you are accessing the Directory Utility menu, and click on Edit, and then click Enable Root User.  (This can be a very dangerous step, since by default, Root access is disabled on a Mac! Be sure to take precautions, and once your are finished creating your bootable thumb drive, disable the root user.)</li>\n<li>You will be prompted for a password for the root account and have to enter it twice. Click OK once you are done.</li>\n<li>Switch back to the Terminal prompt and enter <strong>su -</strong> to switch to the root user account. When prompted, enter the password your entered for the root account in step 11.</li>\n<li>Type <strong>dd if=</strong> then drag and drop the iso file to the terminal window to append the file name and path.</li>\n<li>To the end of the command enter <strong><code>of=/dev/disk3 bs=1m</code></strong> substituting the correct mount point you noted in step 3. You should wind up with something like: <strong><code>dd if=/Users/Desktop/distribution.iso of=/dev/rdisk3 bs=1m</code></strong></li>\n<li>Take a deep breath, review what you entered and if everything looks good, hit ENTER.</li>\n<li>At this point,  you’ll need to wait while your USB drive is erased and the disk image is written to the drive. You’ll know once its complete when the command prompt returns in your terminal windows.</li>\n<li>Once the terminal prompt returns, enter EXIT to leave root access.</li>\n<li>Switch back to the Directory Utility window, and from the menu, choose Disable Root User. Click the padlock to lock it, and then Quit Directory Utility from the menu bar.</li>\n<li>Back at the Users &amp; Groups window, lock the padlock and then Quit System Preferences  from the menu bar.</li>\n<li> Finally, switch back to the Terminal window, and enter Exit to logout of the terminal. Quit the terminal application from the menu bar.</li>\n<li>Remove the USB drive and go boot your other PC!</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/05/boots.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/boots.jpg\" alt=\"boots\"></a>In my twenty plus years of working on PCs, I have seen external media formats change from 5.25 inch floppy disks (which really were floppy) to 3.5 inch (not-so) floppy discs to CD to DVD and USB (thumb) drives. I’ve had PCs at one time or another that have used all of those formats, but over the past couple of years, the push has to been to move away from external media whenever possible. Thus the last two PCs I have purchased did not come with anything other than USB slots. One is my ZBox computer that I use for development, and I change operating systems on it fairly frequently. Rather than burn a DVD of an ISO every time I want to reconfigure the development box, its should be easier to make a boot-able USB. Using Windows there are a host of different GUI tools to create bootable USB drives, and most Linux distributions are similar. If you are on a Mac running OS X Yosemite, the  solution isn’t quite as simple. A number of websites include directions, but generally I’ve found they don’t work. But this week, I found a successful method, and created both Fedora and CentOS bootable USB drives (unfortunately it doesn’t seem to work with Windows ISO’s). Because this requires enabling the root user account on your system, be sure to exercise caution when following these steps. Read on for more information.</p>","more":"<ol>\n<li>Find a USB drive that you are OK with erasing and insert it into one of your Mac’s USB slots. Make sure the drive is formatted as Fat32 with the Boot Loader option in the partition window.</li>\n<li>Open a terminal prompt (its in your Utilities folder), and run this command: <strong>diskutil list</strong></li>\n<li>Look for the USB drive you inserted, and WRITE IT DOWN! In my case it was mounted as /dev/disk3 which I was able to tell by looking at the SIZE column. In the following commands, substitute your mount point for the one I am using:<a href=\"http://edpflager.com/wp-content/uploads/2015/05/thumbdrive.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/thumbdrive-300x63.png\" alt=\"thumbdrive\"></a></li>\n<li>Run this command to unmount the thumb drive, but don’t remove it from the USB slot: <strong>diskutil unmountDisk /dev/disk3</strong></li>\n<li>The system should respond that: <strong>Unmount of all volumes on disk3 was successful.</strong></li>\n<li>Stay with me now, this next bit is convoluted! From the Apple icon in top menu bar, open System Preferences and click on <strong>Users &amp; Groups</strong>. A new window will appear, showing the user accounts on your system.</li>\n<li>There is likely a padlock icon at the bottom of the window, that is closed. Click on the lock and enter your administrator user ID and password and click the unlock button to open it.</li>\n<li>At the bottom of the Users pane on the left, click on the Login Options button next to the house.<a href=\"http://edpflager.com/wp-content/uploads/2015/05/loginoptions.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/05/loginoptions-300x109.png\" alt=\"loginoptions\"></a></li>\n<li>In the new window that appears, click the JOIN button.  The Directory Utility window will appear. Click on the OPEN DIRECTORY UTILITY button. In the new window that appears, if the padlock is closed, open it like you did previously.</li>\n<li>From the menu bar, make you are accessing the Directory Utility menu, and click on Edit, and then click Enable Root User.  (This can be a very dangerous step, since by default, Root access is disabled on a Mac! Be sure to take precautions, and once your are finished creating your bootable thumb drive, disable the root user.)</li>\n<li>You will be prompted for a password for the root account and have to enter it twice. Click OK once you are done.</li>\n<li>Switch back to the Terminal prompt and enter <strong>su -</strong> to switch to the root user account. When prompted, enter the password your entered for the root account in step 11.</li>\n<li>Type <strong>dd if=</strong> then drag and drop the iso file to the terminal window to append the file name and path.</li>\n<li>To the end of the command enter <strong><code>of=/dev/disk3 bs=1m</code></strong> substituting the correct mount point you noted in step 3. You should wind up with something like: <strong><code>dd if=/Users/Desktop/distribution.iso of=/dev/rdisk3 bs=1m</code></strong></li>\n<li>Take a deep breath, review what you entered and if everything looks good, hit ENTER.</li>\n<li>At this point,  you’ll need to wait while your USB drive is erased and the disk image is written to the drive. You’ll know once its complete when the command prompt returns in your terminal windows.</li>\n<li>Once the terminal prompt returns, enter EXIT to leave root access.</li>\n<li>Switch back to the Directory Utility window, and from the menu, choose Disable Root User. Click the padlock to lock it, and then Quit Directory Utility from the menu bar.</li>\n<li>Back at the Users &amp; Groups window, lock the padlock and then Quit System Preferences  from the menu bar.</li>\n<li> Finally, switch back to the Terminal window, and enter Exit to logout of the terminal. Quit the terminal application from the menu bar.</li>\n<li>Remove the USB drive and go boot your other PC!</li>\n</ol>"},{"title":"Linux Partitioning","id":"2960","comments":0,"date":"2015-09-15T22:14:14.000Z","_content":"\n[![shoji-screens-1416865](http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865-1024x768.jpg)](http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865.jpg)When installing a Linux distro, one of the things you have to decide on is how to partition your hard drive to store various components of the Linux system. For those new to Linux, you can let the installer decide for you, and as with most default settings the outcome may not be the best but it will work. The system's default layout generally will define a boot partition and a swap location, and then a root partition for everything else. Not optimal, but it will work. Once you've worked with Linux for a while, and have installed a few distros or upgraded, you realize that those default partitions can cause some problems. Specifically your personal files from your home folder will get overwritten and you may lose any personalized configuration settings that are stored in home hidden folders and files. But defining a partition scheme can be a daunting task. So here are some suggestions on how to partition your drive, using my current setup as an example.\n<!-- more -->\nFor more recent PCs, you will probably need an EFI partition if the computer is designed to comply with the Unified Extensible Firmware Interface. This partition stores files that are used to start installed operating systems or other utilities. To allow for multiple operating systems or other additions, set aside 20GB for the EFI partition. Boot partition - I have seen a number of recommendations for sizing the boot partition, but generally they fall in the 300MB to 500MB range. Kernel images and other files that are actually used to boot the operating system are stored here. If you plan to dual or multiboot on your system, go on the high side of that recommendation. Root or system partition - the files from the boot partition access the system partition. Here is where the actual operating system files are stored. For most current Linux distros, a system partition of 15-30GB should be sufficient. Its also possible to have the Boot and System partitions as one partition if you want. If you plan to install a large amount of software on your machine, you may want to make this partition larger. Home partition - as I stated above, the Home partition is where your files and configuration settings are stored. Generally you want this to  be the largest partition, since you will have your files here. On my system, this partition is about 180GB because I usually work with several virtual machines which are stored in my home folder. SWAP space - a good rule of thumb for sizing your swap space partition is to take the amount of RAM in  your system and double it. So for my system, with 8GB of RAM, I setup a 16GB swap partition.  Summary:\n\n*   /EFI Boot       20GB\n*   /Boot            300MB\n*   /  (Root)          20GB\n*   /Home          180GB\n*   /Swap             16GB\n\nTotal ~                             236GB That's it!","source":"_posts/linux-partitioning-my-thoughts.md","raw":"---\ntitle: Linux Partitioning\ntags:\n  - guides\n  - How-to\n  - install\n  - Mint\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '2960'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-09-15 18:14:14\n---\n\n[![shoji-screens-1416865](http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865-1024x768.jpg)](http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865.jpg)When installing a Linux distro, one of the things you have to decide on is how to partition your hard drive to store various components of the Linux system. For those new to Linux, you can let the installer decide for you, and as with most default settings the outcome may not be the best but it will work. The system's default layout generally will define a boot partition and a swap location, and then a root partition for everything else. Not optimal, but it will work. Once you've worked with Linux for a while, and have installed a few distros or upgraded, you realize that those default partitions can cause some problems. Specifically your personal files from your home folder will get overwritten and you may lose any personalized configuration settings that are stored in home hidden folders and files. But defining a partition scheme can be a daunting task. So here are some suggestions on how to partition your drive, using my current setup as an example.\n<!-- more -->\nFor more recent PCs, you will probably need an EFI partition if the computer is designed to comply with the Unified Extensible Firmware Interface. This partition stores files that are used to start installed operating systems or other utilities. To allow for multiple operating systems or other additions, set aside 20GB for the EFI partition. Boot partition - I have seen a number of recommendations for sizing the boot partition, but generally they fall in the 300MB to 500MB range. Kernel images and other files that are actually used to boot the operating system are stored here. If you plan to dual or multiboot on your system, go on the high side of that recommendation. Root or system partition - the files from the boot partition access the system partition. Here is where the actual operating system files are stored. For most current Linux distros, a system partition of 15-30GB should be sufficient. Its also possible to have the Boot and System partitions as one partition if you want. If you plan to install a large amount of software on your machine, you may want to make this partition larger. Home partition - as I stated above, the Home partition is where your files and configuration settings are stored. Generally you want this to  be the largest partition, since you will have your files here. On my system, this partition is about 180GB because I usually work with several virtual machines which are stored in my home folder. SWAP space - a good rule of thumb for sizing your swap space partition is to take the amount of RAM in  your system and double it. So for my system, with 8GB of RAM, I setup a 16GB swap partition.  Summary:\n\n*   /EFI Boot       20GB\n*   /Boot            300MB\n*   /  (Root)          20GB\n*   /Home          180GB\n*   /Swap             16GB\n\nTotal ~                             236GB That's it!","slug":"linux-partitioning-my-thoughts","published":1,"updated":"2020-08-23T20:54:34.970Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a3o005qsdjxexk20abe","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865-1024x768.jpg\" alt=\"shoji-screens-1416865\"></a>When installing a Linux distro, one of the things you have to decide on is how to partition your hard drive to store various components of the Linux system. For those new to Linux, you can let the installer decide for you, and as with most default settings the outcome may not be the best but it will work. The system’s default layout generally will define a boot partition and a swap location, and then a root partition for everything else. Not optimal, but it will work. Once you’ve worked with Linux for a while, and have installed a few distros or upgraded, you realize that those default partitions can cause some problems. Specifically your personal files from your home folder will get overwritten and you may lose any personalized configuration settings that are stored in home hidden folders and files. But defining a partition scheme can be a daunting task. So here are some suggestions on how to partition your drive, using my current setup as an example.</p>\n<a id=\"more\"></a>\n<p>For more recent PCs, you will probably need an EFI partition if the computer is designed to comply with the Unified Extensible Firmware Interface. This partition stores files that are used to start installed operating systems or other utilities. To allow for multiple operating systems or other additions, set aside 20GB for the EFI partition. Boot partition - I have seen a number of recommendations for sizing the boot partition, but generally they fall in the 300MB to 500MB range. Kernel images and other files that are actually used to boot the operating system are stored here. If you plan to dual or multiboot on your system, go on the high side of that recommendation. Root or system partition - the files from the boot partition access the system partition. Here is where the actual operating system files are stored. For most current Linux distros, a system partition of 15-30GB should be sufficient. Its also possible to have the Boot and System partitions as one partition if you want. If you plan to install a large amount of software on your machine, you may want to make this partition larger. Home partition - as I stated above, the Home partition is where your files and configuration settings are stored. Generally you want this to  be the largest partition, since you will have your files here. On my system, this partition is about 180GB because I usually work with several virtual machines which are stored in my home folder. SWAP space - a good rule of thumb for sizing your swap space partition is to take the amount of RAM in  your system and double it. So for my system, with 8GB of RAM, I setup a 16GB swap partition.  Summary:</p>\n<ul>\n<li>/EFI Boot       20GB</li>\n<li>/Boot            300MB</li>\n<li>/  (Root)          20GB</li>\n<li>/Home          180GB</li>\n<li>/Swap             16GB</li>\n</ul>\n<p>Total ~                             236GB That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/09/shoji-screens-1416865-1024x768.jpg\" alt=\"shoji-screens-1416865\"></a>When installing a Linux distro, one of the things you have to decide on is how to partition your hard drive to store various components of the Linux system. For those new to Linux, you can let the installer decide for you, and as with most default settings the outcome may not be the best but it will work. The system’s default layout generally will define a boot partition and a swap location, and then a root partition for everything else. Not optimal, but it will work. Once you’ve worked with Linux for a while, and have installed a few distros or upgraded, you realize that those default partitions can cause some problems. Specifically your personal files from your home folder will get overwritten and you may lose any personalized configuration settings that are stored in home hidden folders and files. But defining a partition scheme can be a daunting task. So here are some suggestions on how to partition your drive, using my current setup as an example.</p>","more":"<p>For more recent PCs, you will probably need an EFI partition if the computer is designed to comply with the Unified Extensible Firmware Interface. This partition stores files that are used to start installed operating systems or other utilities. To allow for multiple operating systems or other additions, set aside 20GB for the EFI partition. Boot partition - I have seen a number of recommendations for sizing the boot partition, but generally they fall in the 300MB to 500MB range. Kernel images and other files that are actually used to boot the operating system are stored here. If you plan to dual or multiboot on your system, go on the high side of that recommendation. Root or system partition - the files from the boot partition access the system partition. Here is where the actual operating system files are stored. For most current Linux distros, a system partition of 15-30GB should be sufficient. Its also possible to have the Boot and System partitions as one partition if you want. If you plan to install a large amount of software on your machine, you may want to make this partition larger. Home partition - as I stated above, the Home partition is where your files and configuration settings are stored. Generally you want this to  be the largest partition, since you will have your files here. On my system, this partition is about 180GB because I usually work with several virtual machines which are stored in my home folder. SWAP space - a good rule of thumb for sizing your swap space partition is to take the amount of RAM in  your system and double it. So for my system, with 8GB of RAM, I setup a 16GB swap partition.  Summary:</p>\n<ul>\n<li>/EFI Boot       20GB</li>\n<li>/Boot            300MB</li>\n<li>/  (Root)          20GB</li>\n<li>/Home          180GB</li>\n<li>/Swap             16GB</li>\n</ul>\n<p>Total ~                             236GB That’s it!</p>"},{"title":"Map Windows host location to Linux container in Docker","id":"3628","comments":0,"date":"2017-10-04T18:05:15.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2017/10/map-300x225.jpg)](http://edpflager.com/?attachment_id=3640#main)I've been working more with Docker on a Windows PC lately. With the more recent versions of Docker, the application runs much better and there is task bar control panel for managing the processes. If you are interested in trying out Docker and don't have a Linux machine to work with, go download the Windows [Stable Community edition](https://store.docker.com/editions/community/docker-ce-desktop-windows) which was recently updated to 17.09.0.\n<!-- more -->\nThis post is the result of something I have been trying to find solve for a while. If you are running an all Linux Docker ecosystem, mapping a volume from host to container is very quick and easy. Just include the volume command line switch: -v <host location>:<container location>. So for example, if you were starting an NGINX webserver container, you could create a directory under your home location called **~/nginx/web**  where you can put HTML files. Then when you start the NGINX container, you add a volume map switch and point the NGINX container location for HTML to your local directory like this:\n\n```\n-v ~/nginx/web:/usr/share/nginx/html\n```\n\nIf you place an INDEX.HTML file in that folder, NGINX will serve that page up rather than the default page it normally does. [![](http://edpflager.com/wp-content/uploads/2017/10/SharedDrives-300x196.png)](http://edpflager.com/?attachment_id=3637#main)But on Docker for Windows, its not quite that easy. First in the Docker Control Panel, you need to enable local access to Docker. Right click the Docker whale icon, and choose settings from the menu that appears. The Settings window will appear, and you will need to select the Shared Drives option on the left. In the list of Drives check the ones you want to share in Docker. Click the Apply button at the bottom, and you will be prompted for your credentials to make sure you have authority to allow this. If you are working on a corporate domain and are using system that may be used away from the domain (like a laptop), you may want to setup a local system account that has permission to do this, and use that account. Now once you have shared the drive you want Docker containers to have access to, you still need to deal with the odd notation Docker uses in Windows  to map the drive. Let me illustrate with an example. From the root of your Windows C: drive (or another drive if one is available), create a folder called nginx, and then a subfolder called web. Add a simple index.html file to that folder. Doesn't need to be much beyond a Hello World type entry, like this\n\n<html>\n  <head>\n    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==\" crossorigin=\"anonymous\">\n    <title>Docker NGINX Sample page</title>\n  </head>\n  <body>\n    <div class=\"container\">\n      <h1>Hello! This page brought to you by Docker! </h1>\n    </div>\n   </body>\n</html>\n\nFinally, start your NGINX container with this volume map switch: -v /c/nginx/web:/usr/share/nginx/html So the complete Docker RUN command would be (updated) :\n\ndocker run --name NginxTest -d nginx --rm -p 80:80 -v /c/nginx/web:/usr/share/nginx/html\n\n(that's two hyphens before name, and rm) Open a web browser and go to http://localhost and you should see the NGINX website, with your simple Hello World page!  Follow the same notation whenever you want to map a Windows folder to a Linux container.","source":"_posts/map-windows-host-location-to-linux-container-in-docker.md","raw":"---\ntitle: Map Windows host location to Linux container in Docker\ntags:\n  - external article\n  - How-to\n  - howto\n  - Linux\n  - nginx\n  - technical\n  - Windows\nid: '3628'\ncategories:\n  - - Blog\n  - - Docker\ncomments: false\ndate: 2017-10-04 14:05:15\n---\n\n[![](http://edpflager.com/wp-content/uploads/2017/10/map-300x225.jpg)](http://edpflager.com/?attachment_id=3640#main)I've been working more with Docker on a Windows PC lately. With the more recent versions of Docker, the application runs much better and there is task bar control panel for managing the processes. If you are interested in trying out Docker and don't have a Linux machine to work with, go download the Windows [Stable Community edition](https://store.docker.com/editions/community/docker-ce-desktop-windows) which was recently updated to 17.09.0.\n<!-- more -->\nThis post is the result of something I have been trying to find solve for a while. If you are running an all Linux Docker ecosystem, mapping a volume from host to container is very quick and easy. Just include the volume command line switch: -v <host location>:<container location>. So for example, if you were starting an NGINX webserver container, you could create a directory under your home location called **~/nginx/web**  where you can put HTML files. Then when you start the NGINX container, you add a volume map switch and point the NGINX container location for HTML to your local directory like this:\n\n```\n-v ~/nginx/web:/usr/share/nginx/html\n```\n\nIf you place an INDEX.HTML file in that folder, NGINX will serve that page up rather than the default page it normally does. [![](http://edpflager.com/wp-content/uploads/2017/10/SharedDrives-300x196.png)](http://edpflager.com/?attachment_id=3637#main)But on Docker for Windows, its not quite that easy. First in the Docker Control Panel, you need to enable local access to Docker. Right click the Docker whale icon, and choose settings from the menu that appears. The Settings window will appear, and you will need to select the Shared Drives option on the left. In the list of Drives check the ones you want to share in Docker. Click the Apply button at the bottom, and you will be prompted for your credentials to make sure you have authority to allow this. If you are working on a corporate domain and are using system that may be used away from the domain (like a laptop), you may want to setup a local system account that has permission to do this, and use that account. Now once you have shared the drive you want Docker containers to have access to, you still need to deal with the odd notation Docker uses in Windows  to map the drive. Let me illustrate with an example. From the root of your Windows C: drive (or another drive if one is available), create a folder called nginx, and then a subfolder called web. Add a simple index.html file to that folder. Doesn't need to be much beyond a Hello World type entry, like this\n\n<html>\n  <head>\n    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==\" crossorigin=\"anonymous\">\n    <title>Docker NGINX Sample page</title>\n  </head>\n  <body>\n    <div class=\"container\">\n      <h1>Hello! This page brought to you by Docker! </h1>\n    </div>\n   </body>\n</html>\n\nFinally, start your NGINX container with this volume map switch: -v /c/nginx/web:/usr/share/nginx/html So the complete Docker RUN command would be (updated) :\n\ndocker run --name NginxTest -d nginx --rm -p 80:80 -v /c/nginx/web:/usr/share/nginx/html\n\n(that's two hyphens before name, and rm) Open a web browser and go to http://localhost and you should see the NGINX website, with your simple Hello World page!  Follow the same notation whenever you want to map a Windows folder to a Linux container.","slug":"map-windows-host-location-to-linux-container-in-docker","published":1,"updated":"2020-08-23T20:54:35.118Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a3y005tsdjx1zrf6sfk","content":"<p><a href=\"http://edpflager.com/?attachment_id=3640#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/10/map-300x225.jpg\"></a>I’ve been working more with Docker on a Windows PC lately. With the more recent versions of Docker, the application runs much better and there is task bar control panel for managing the processes. If you are interested in trying out Docker and don’t have a Linux machine to work with, go download the Windows <a href=\"https://store.docker.com/editions/community/docker-ce-desktop-windows\">Stable Community edition</a> which was recently updated to 17.09.0.</p>\n<a id=\"more\"></a>\n<p>This post is the result of something I have been trying to find solve for a while. If you are running an all Linux Docker ecosystem, mapping a volume from host to container is very quick and easy. Just include the volume command line switch: -v <host location>:<container location>. So for example, if you were starting an NGINX webserver container, you could create a directory under your home location called <strong>~/nginx/web</strong>  where you can put HTML files. Then when you start the NGINX container, you add a volume map switch and point the NGINX container location for HTML to your local directory like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-v ~&#x2F;nginx&#x2F;web:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br></pre></td></tr></table></figure>\n\n<p>If you place an INDEX.HTML file in that folder, NGINX will serve that page up rather than the default page it normally does. <a href=\"http://edpflager.com/?attachment_id=3637#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/10/SharedDrives-300x196.png\"></a>But on Docker for Windows, its not quite that easy. First in the Docker Control Panel, you need to enable local access to Docker. Right click the Docker whale icon, and choose settings from the menu that appears. The Settings window will appear, and you will need to select the Shared Drives option on the left. In the list of Drives check the ones you want to share in Docker. Click the Apply button at the bottom, and you will be prompted for your credentials to make sure you have authority to allow this. If you are working on a corporate domain and are using system that may be used away from the domain (like a laptop), you may want to setup a local system account that has permission to do this, and use that account. Now once you have shared the drive you want Docker containers to have access to, you still need to deal with the odd notation Docker uses in Windows  to map the drive. Let me illustrate with an example. From the root of your Windows C: drive (or another drive if one is available), create a folder called nginx, and then a subfolder called web. Add a simple index.html file to that folder. Doesn’t need to be much beyond a Hello World type entry, like this</p>\n<html>\n  <head>\n    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==\" crossorigin=\"anonymous\">\n    <title>Docker NGINX Sample page</title>\n  </head>\n  <body>\n    <div class=\"container\">\n      <h1>Hello! This page brought to you by Docker! </h1>\n    </div>\n   </body>\n</html>\n\n<p>Finally, start your NGINX container with this volume map switch: -v /c/nginx/web:/usr/share/nginx/html So the complete Docker RUN command would be (updated) :</p>\n<p>docker run –name NginxTest -d nginx –rm -p 80:80 -v /c/nginx/web:/usr/share/nginx/html</p>\n<p>(that’s two hyphens before name, and rm) Open a web browser and go to <a href=\"http://localhost/\">http://localhost</a> and you should see the NGINX website, with your simple Hello World page!  Follow the same notation whenever you want to map a Windows folder to a Linux container.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3640#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/10/map-300x225.jpg\"></a>I’ve been working more with Docker on a Windows PC lately. With the more recent versions of Docker, the application runs much better and there is task bar control panel for managing the processes. If you are interested in trying out Docker and don’t have a Linux machine to work with, go download the Windows <a href=\"https://store.docker.com/editions/community/docker-ce-desktop-windows\">Stable Community edition</a> which was recently updated to 17.09.0.</p>","more":"<p>This post is the result of something I have been trying to find solve for a while. If you are running an all Linux Docker ecosystem, mapping a volume from host to container is very quick and easy. Just include the volume command line switch: -v <host location>:<container location>. So for example, if you were starting an NGINX webserver container, you could create a directory under your home location called <strong>~/nginx/web</strong>  where you can put HTML files. Then when you start the NGINX container, you add a volume map switch and point the NGINX container location for HTML to your local directory like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-v ~&#x2F;nginx&#x2F;web:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br></pre></td></tr></table></figure>\n\n<p>If you place an INDEX.HTML file in that folder, NGINX will serve that page up rather than the default page it normally does. <a href=\"http://edpflager.com/?attachment_id=3637#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/10/SharedDrives-300x196.png\"></a>But on Docker for Windows, its not quite that easy. First in the Docker Control Panel, you need to enable local access to Docker. Right click the Docker whale icon, and choose settings from the menu that appears. The Settings window will appear, and you will need to select the Shared Drives option on the left. In the list of Drives check the ones you want to share in Docker. Click the Apply button at the bottom, and you will be prompted for your credentials to make sure you have authority to allow this. If you are working on a corporate domain and are using system that may be used away from the domain (like a laptop), you may want to setup a local system account that has permission to do this, and use that account. Now once you have shared the drive you want Docker containers to have access to, you still need to deal with the odd notation Docker uses in Windows  to map the drive. Let me illustrate with an example. From the root of your Windows C: drive (or another drive if one is available), create a folder called nginx, and then a subfolder called web. Add a simple index.html file to that folder. Doesn’t need to be much beyond a Hello World type entry, like this</p>\n<html>\n  <head>\n    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==\" crossorigin=\"anonymous\">\n    <title>Docker NGINX Sample page</title>\n  </head>\n  <body>\n    <div class=\"container\">\n      <h1>Hello! This page brought to you by Docker! </h1>\n    </div>\n   </body>\n</html>\n\n<p>Finally, start your NGINX container with this volume map switch: -v /c/nginx/web:/usr/share/nginx/html So the complete Docker RUN command would be (updated) :</p>\n<p>docker run –name NginxTest -d nginx –rm -p 80:80 -v /c/nginx/web:/usr/share/nginx/html</p>\n<p>(that’s two hyphens before name, and rm) Open a web browser and go to <a href=\"http://localhost/\">http://localhost</a> and you should see the NGINX website, with your simple Hello World page!  Follow the same notation whenever you want to map a Windows folder to a Linux container.</p>"},{"title":"MariaDB/MySQL start on demand","id":"1618","comments":0,"date":"2013-08-08T16:47:01.000Z","_content":"\n###### [![Mariadb](http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png)](http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png)Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article [here](http://wp.me/p1BUkU-KE).\n\nBy default when MariaDB or MySQL is installed in Ubuntu, it is set to run automatically when the system starts up. For a server that makes sense, but if you are working on a development machine, like a laptop, you may not want it to always be running. (BTW MariaDB is a fork of MySQL started by one of the original MySQL developers). To disable the automatic start of MySQL on Ubuntu, open a terminal session and enter this command: **ls -l /etc/rc?.d/S\\*mysql\\***\n<!-- more -->\nThis will give you a list of various runlevel folders with mysql symbolic links in them that point back to the MySQL startup scripts. (A runlevel defines how the system should start up - single user mode, multi-user with GUI but no networking, etc). You are only interested in the MySQL processes that start with an S. Go to each of those folders, and using sudo rename the file, replacing the S with a K. Once you have finished, restart your system, and MySQL won't be running. To get it to start up, open a terminal session, and enter this command: **sudo service mysql start** Another way is by using the MySQL Workbench application. Create a connection to your localhost under Server Administration. Open the connection (you'll see a screen indicating that your MySQL server is not running). Navigate to the Startup/Shutdown node under the management node, and on the right side of the screen, click the Start Server button. Your MariaDB or MySQL instance will startup.","source":"_posts/mariadbmysql-start-on-demand.md","raw":"---\ntitle: MariaDB/MySQL start on demand\ntags: []\nid: '1618'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2013-08-08 12:47:01\n---\n\n###### [![Mariadb](http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png)](http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png)Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article [here](http://wp.me/p1BUkU-KE).\n\nBy default when MariaDB or MySQL is installed in Ubuntu, it is set to run automatically when the system starts up. For a server that makes sense, but if you are working on a development machine, like a laptop, you may not want it to always be running. (BTW MariaDB is a fork of MySQL started by one of the original MySQL developers). To disable the automatic start of MySQL on Ubuntu, open a terminal session and enter this command: **ls -l /etc/rc?.d/S\\*mysql\\***\n<!-- more -->\nThis will give you a list of various runlevel folders with mysql symbolic links in them that point back to the MySQL startup scripts. (A runlevel defines how the system should start up - single user mode, multi-user with GUI but no networking, etc). You are only interested in the MySQL processes that start with an S. Go to each of those folders, and using sudo rename the file, replacing the S with a K. Once you have finished, restart your system, and MySQL won't be running. To get it to start up, open a terminal session, and enter this command: **sudo service mysql start** Another way is by using the MySQL Workbench application. Create a connection to your localhost under Server Administration. Open the connection (you'll see a screen indicating that your MySQL server is not running). Navigate to the Startup/Shutdown node under the management node, and on the right side of the screen, click the Start Server button. Your MariaDB or MySQL instance will startup.","slug":"mariadbmysql-start-on-demand","published":1,"updated":"2020-08-23T20:54:34.746Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a44005xsdjxgye25som","content":"<h6 id=\"Note-This-method-no-longer-works-with-the-newer-versions-of-Ubuntu-running-systemd-instead-of-upstart-for-service-control-Please-see-the-updated-article-here\"><a href=\"#Note-This-method-no-longer-works-with-the-newer-versions-of-Ubuntu-running-systemd-instead-of-upstart-for-service-control-Please-see-the-updated-article-here\" class=\"headerlink\" title=\"Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article here.\"></a><a href=\"http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png\" alt=\"Mariadb\"></a>Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article <a href=\"http://wp.me/p1BUkU-KE\">here</a>.</h6><p>By default when MariaDB or MySQL is installed in Ubuntu, it is set to run automatically when the system starts up. For a server that makes sense, but if you are working on a development machine, like a laptop, you may not want it to always be running. (BTW MariaDB is a fork of MySQL started by one of the original MySQL developers). To disable the automatic start of MySQL on Ubuntu, open a terminal session and enter this command: <strong>ls -l /etc/rc?.d/S*mysql*</strong></p>\n<a id=\"more\"></a>\n<p>This will give you a list of various runlevel folders with mysql symbolic links in them that point back to the MySQL startup scripts. (A runlevel defines how the system should start up - single user mode, multi-user with GUI but no networking, etc). You are only interested in the MySQL processes that start with an S. Go to each of those folders, and using sudo rename the file, replacing the S with a K. Once you have finished, restart your system, and MySQL won’t be running. To get it to start up, open a terminal session, and enter this command: <strong>sudo service mysql start</strong> Another way is by using the MySQL Workbench application. Create a connection to your localhost under Server Administration. Open the connection (you’ll see a screen indicating that your MySQL server is not running). Navigate to the Startup/Shutdown node under the management node, and on the right side of the screen, click the Start Server button. Your MariaDB or MySQL instance will startup.</p>\n","site":{"data":{}},"excerpt":"<h6 id=\"Note-This-method-no-longer-works-with-the-newer-versions-of-Ubuntu-running-systemd-instead-of-upstart-for-service-control-Please-see-the-updated-article-here\"><a href=\"#Note-This-method-no-longer-works-with-the-newer-versions-of-Ubuntu-running-systemd-instead-of-upstart-for-service-control-Please-see-the-updated-article-here\" class=\"headerlink\" title=\"Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article here.\"></a><a href=\"http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/Mariadb.png\" alt=\"Mariadb\"></a>Note: This method no longer works with the newer versions of Ubuntu, running systemd instead of upstart for service control. Please see the updated article <a href=\"http://wp.me/p1BUkU-KE\">here</a>.</h6><p>By default when MariaDB or MySQL is installed in Ubuntu, it is set to run automatically when the system starts up. For a server that makes sense, but if you are working on a development machine, like a laptop, you may not want it to always be running. (BTW MariaDB is a fork of MySQL started by one of the original MySQL developers). To disable the automatic start of MySQL on Ubuntu, open a terminal session and enter this command: <strong>ls -l /etc/rc?.d/S*mysql*</strong></p>","more":"<p>This will give you a list of various runlevel folders with mysql symbolic links in them that point back to the MySQL startup scripts. (A runlevel defines how the system should start up - single user mode, multi-user with GUI but no networking, etc). You are only interested in the MySQL processes that start with an S. Go to each of those folders, and using sudo rename the file, replacing the S with a K. Once you have finished, restart your system, and MySQL won’t be running. To get it to start up, open a terminal session, and enter this command: <strong>sudo service mysql start</strong> Another way is by using the MySQL Workbench application. Create a connection to your localhost under Server Administration. Open the connection (you’ll see a screen indicating that your MySQL server is not running). Navigate to the Startup/Shutdown node under the management node, and on the right side of the screen, click the Start Server button. Your MariaDB or MySQL instance will startup.</p>"},{"title":"Microsoft SQL Server on Docker (part 2) - Use AdventureWorks","id":"3451","comments":0,"date":"2016-11-30T23:58:33.000Z","_content":"\n[![sql_on_docker](http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg)](http://edpflager.com/?attachment_id=3463#main)In [part one of this series](http://edpflager.com/?p=3447), I provided some info on Microsoft's implementation of Sql Server on Docker and provided a method to have your SQL Server databases saved on your Docker host system so that they would remain persistent if the SQL Server container was shutdown. This time around, we'll look at how to restore Microsoft's sample database AdventureWorks to your SQL Server container.\n\n##### DOWNLOAD ADVENTUREWORKS\n\nIf you followed my instructions from part 1 of this article you have a SQL Server container up and running with a data folder on your Docker host. Within that folder, you have several subfolders that SQL Server uses. You need to download a copy of AdventureWorks and save it in that folder. There are multiple versions available, as Microsoft provides a new version with every upgrade to SQL Server. The 2014 version should work fine for this purpose (If you download the 2016CTP version the restore command below will not work because that version includes FILESTREAM as part of the backup. An addition MOVE command needs to be supplied to include that in the RESTORE). Go to this [website](https://msftdbprodsamples.codeplex.com/releases/view/125550), and download Adventure Works 2014 Full Database Backup.zip. I renamed the file to just AdventureWorks.zip to make it easier to transfer to my Docker host system and used scp to transfer it:\n<!-- more -->\nscp AdventureWorks.zip <username>@<Docker Host IP>:~/AdventureWorks.zip\n\nNow unzip the file to the shared data folder that was created previously. SUDO is required because the folder is protected:\n\nsudo unzip AdventureWorks.zip -d ~/mssql/data\n\n \n\n##### RESTORE ADVENTUREWORKS\n\nBefore we can restore a database, we need a Linux or Mac system with the Microsoft SQL Server command line tools installed or a Windows machine with the SQL Server Management Studio installed. The instructions below are just for using the command line tool however. Attempting to install the tools on the SQL Server container currently fails with several unmet dependency issues, and installing the tools in an Ubuntu container doesn't appear to work either, resulting in an error when attempting to access the sqlcmd tool. So on another Linux machine, execute the following commands to install the SQL Server command line tools. If you Docker host machine is running Ubuntu you can install the tools there with these instructions.\n\nRefresh the repository information in your container with:\n\napt-get update\n\nThen install CURL if you don't have it:\n\napt-get install curl\n\nFetch the Microsoft GPG registry key and install it:\n\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n\nRegister the Microsoft repository for Ubuntu (since that is what the container is built on)\n\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | tee /etc/apt/sources.list.d/msprod.list\n\nInstall the https transport component for Ubuntu:\n\n```\napt-get install apt-transport-https\n```\n\nAnd update your repository cache again:\n\napt-get update\n\nFinally, install the SQL Server command line tools:\n\n```\napt-get install mssql-tools\n```\n\nDuring the installation you will prompted to accept the license terms twice. Enter Yes both times if you accept and want to proceed. Once the installation completes, access SQLCMD and connect to your SQL Server machine:\n\nsqlcmd -S <SQL Server IP> -U SA -P <password>\n\nThe display should respond with:  1>\n\nYou can then restore the AdventureWorks database with this command, entering each line with a RETURN at the end:\n\nRESTORE DATABASE AdventureWorks FROM DISK = 'C:varoptmssqldataAdventureWorks.BAK'\nWITH MOVE 'AdventureWorks\\_Data' TO 'C:varoptmssqldataAdventureWorks2014\\_Data.mdf',\nMOVE 'AdventureWorks\\_Log2014' TO 'C:varoptmssqldataAdventureWorks\\_Log.ldf'\nGO\n\nIf you typed everything correctly you should see a number of rows displayed as the database is restored and after a second or two a success message. [![resotore_start](http://edpflager.com/wp-content/uploads/2016/11/resotore_start-1024x484.png)](http://edpflager.com/?attachment_id=3467#main)Switch to the new database with:\n\nUSE AdventureWorks\nGO\n\nThe response should be Changed Database context to 'AdventureWorks'! You have restored AdventureWorks, and can experiment with it!","source":"_posts/microsoft-sql-server-on-docker-part-2-use-adventureworks.md","raw":"---\ntitle: Microsoft SQL Server on Docker (part 2) - Use AdventureWorks\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - SQL Server\n  - SysAdmin\nid: '3451'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-11-30 18:58:33\n---\n\n[![sql_on_docker](http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg)](http://edpflager.com/?attachment_id=3463#main)In [part one of this series](http://edpflager.com/?p=3447), I provided some info on Microsoft's implementation of Sql Server on Docker and provided a method to have your SQL Server databases saved on your Docker host system so that they would remain persistent if the SQL Server container was shutdown. This time around, we'll look at how to restore Microsoft's sample database AdventureWorks to your SQL Server container.\n\n##### DOWNLOAD ADVENTUREWORKS\n\nIf you followed my instructions from part 1 of this article you have a SQL Server container up and running with a data folder on your Docker host. Within that folder, you have several subfolders that SQL Server uses. You need to download a copy of AdventureWorks and save it in that folder. There are multiple versions available, as Microsoft provides a new version with every upgrade to SQL Server. The 2014 version should work fine for this purpose (If you download the 2016CTP version the restore command below will not work because that version includes FILESTREAM as part of the backup. An addition MOVE command needs to be supplied to include that in the RESTORE). Go to this [website](https://msftdbprodsamples.codeplex.com/releases/view/125550), and download Adventure Works 2014 Full Database Backup.zip. I renamed the file to just AdventureWorks.zip to make it easier to transfer to my Docker host system and used scp to transfer it:\n<!-- more -->\nscp AdventureWorks.zip <username>@<Docker Host IP>:~/AdventureWorks.zip\n\nNow unzip the file to the shared data folder that was created previously. SUDO is required because the folder is protected:\n\nsudo unzip AdventureWorks.zip -d ~/mssql/data\n\n \n\n##### RESTORE ADVENTUREWORKS\n\nBefore we can restore a database, we need a Linux or Mac system with the Microsoft SQL Server command line tools installed or a Windows machine with the SQL Server Management Studio installed. The instructions below are just for using the command line tool however. Attempting to install the tools on the SQL Server container currently fails with several unmet dependency issues, and installing the tools in an Ubuntu container doesn't appear to work either, resulting in an error when attempting to access the sqlcmd tool. So on another Linux machine, execute the following commands to install the SQL Server command line tools. If you Docker host machine is running Ubuntu you can install the tools there with these instructions.\n\nRefresh the repository information in your container with:\n\napt-get update\n\nThen install CURL if you don't have it:\n\napt-get install curl\n\nFetch the Microsoft GPG registry key and install it:\n\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n\nRegister the Microsoft repository for Ubuntu (since that is what the container is built on)\n\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | tee /etc/apt/sources.list.d/msprod.list\n\nInstall the https transport component for Ubuntu:\n\n```\napt-get install apt-transport-https\n```\n\nAnd update your repository cache again:\n\napt-get update\n\nFinally, install the SQL Server command line tools:\n\n```\napt-get install mssql-tools\n```\n\nDuring the installation you will prompted to accept the license terms twice. Enter Yes both times if you accept and want to proceed. Once the installation completes, access SQLCMD and connect to your SQL Server machine:\n\nsqlcmd -S <SQL Server IP> -U SA -P <password>\n\nThe display should respond with:  1>\n\nYou can then restore the AdventureWorks database with this command, entering each line with a RETURN at the end:\n\nRESTORE DATABASE AdventureWorks FROM DISK = 'C:varoptmssqldataAdventureWorks.BAK'\nWITH MOVE 'AdventureWorks\\_Data' TO 'C:varoptmssqldataAdventureWorks2014\\_Data.mdf',\nMOVE 'AdventureWorks\\_Log2014' TO 'C:varoptmssqldataAdventureWorks\\_Log.ldf'\nGO\n\nIf you typed everything correctly you should see a number of rows displayed as the database is restored and after a second or two a success message. [![resotore_start](http://edpflager.com/wp-content/uploads/2016/11/resotore_start-1024x484.png)](http://edpflager.com/?attachment_id=3467#main)Switch to the new database with:\n\nUSE AdventureWorks\nGO\n\nThe response should be Changed Database context to 'AdventureWorks'! You have restored AdventureWorks, and can experiment with it!","slug":"microsoft-sql-server-on-docker-part-2-use-adventureworks","published":1,"updated":"2020-08-23T20:54:35.082Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a480060sdjx4kvnevnu","content":"<p><a href=\"http://edpflager.com/?attachment_id=3463#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg\" alt=\"sql_on_docker\"></a>In <a href=\"http://edpflager.com/?p=3447\">part one of this series</a>, I provided some info on Microsoft’s implementation of Sql Server on Docker and provided a method to have your SQL Server databases saved on your Docker host system so that they would remain persistent if the SQL Server container was shutdown. This time around, we’ll look at how to restore Microsoft’s sample database AdventureWorks to your SQL Server container.</p>\n<h5 id=\"DOWNLOAD-ADVENTUREWORKS\"><a href=\"#DOWNLOAD-ADVENTUREWORKS\" class=\"headerlink\" title=\"DOWNLOAD ADVENTUREWORKS\"></a>DOWNLOAD ADVENTUREWORKS</h5><p>If you followed my instructions from part 1 of this article you have a SQL Server container up and running with a data folder on your Docker host. Within that folder, you have several subfolders that SQL Server uses. You need to download a copy of AdventureWorks and save it in that folder. There are multiple versions available, as Microsoft provides a new version with every upgrade to SQL Server. The 2014 version should work fine for this purpose (If you download the 2016CTP version the restore command below will not work because that version includes FILESTREAM as part of the backup. An addition MOVE command needs to be supplied to include that in the RESTORE). Go to this <a href=\"https://msftdbprodsamples.codeplex.com/releases/view/125550\">website</a>, and download Adventure Works 2014 Full Database Backup.zip. I renamed the file to just AdventureWorks.zip to make it easier to transfer to my Docker host system and used scp to transfer it:</p>\n<a id=\"more\"></a>\n<p>scp AdventureWorks.zip <username>@<Docker Host IP>:~/AdventureWorks.zip</p>\n<p>Now unzip the file to the shared data folder that was created previously. SUDO is required because the folder is protected:</p>\n<p>sudo unzip AdventureWorks.zip -d ~/mssql/data</p>\n<p> </p>\n<h5 id=\"RESTORE-ADVENTUREWORKS\"><a href=\"#RESTORE-ADVENTUREWORKS\" class=\"headerlink\" title=\"RESTORE ADVENTUREWORKS\"></a>RESTORE ADVENTUREWORKS</h5><p>Before we can restore a database, we need a Linux or Mac system with the Microsoft SQL Server command line tools installed or a Windows machine with the SQL Server Management Studio installed. The instructions below are just for using the command line tool however. Attempting to install the tools on the SQL Server container currently fails with several unmet dependency issues, and installing the tools in an Ubuntu container doesn’t appear to work either, resulting in an error when attempting to access the sqlcmd tool. So on another Linux machine, execute the following commands to install the SQL Server command line tools. If you Docker host machine is running Ubuntu you can install the tools there with these instructions.</p>\n<p>Refresh the repository information in your container with:</p>\n<p>apt-get update</p>\n<p>Then install CURL if you don’t have it:</p>\n<p>apt-get install curl</p>\n<p>Fetch the Microsoft GPG registry key and install it:</p>\n<p>curl <a href=\"https://packages.microsoft.com/keys/microsoft.asc\">https://packages.microsoft.com/keys/microsoft.asc</a> | apt-key add -</p>\n<p>Register the Microsoft repository for Ubuntu (since that is what the container is built on)</p>\n<p>curl <a href=\"https://packages.microsoft.com/config/ubuntu/16.04/prod.list\">https://packages.microsoft.com/config/ubuntu/16.04/prod.list</a> | tee /etc/apt/sources.list.d/msprod.list</p>\n<p>Install the https transport component for Ubuntu:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt-get install apt-transport-https</span><br></pre></td></tr></table></figure>\n\n<p>And update your repository cache again:</p>\n<p>apt-get update</p>\n<p>Finally, install the SQL Server command line tools:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt-get install mssql-tools</span><br></pre></td></tr></table></figure>\n\n<p>During the installation you will prompted to accept the license terms twice. Enter Yes both times if you accept and want to proceed. Once the installation completes, access SQLCMD and connect to your SQL Server machine:</p>\n<p>sqlcmd -S <SQL Server IP> -U SA -P <password></p>\n<p>The display should respond with:  1&gt;</p>\n<p>You can then restore the AdventureWorks database with this command, entering each line with a RETURN at the end:</p>\n<p>RESTORE DATABASE AdventureWorks FROM DISK = ‘C:varoptmssqldataAdventureWorks.BAK’<br>WITH MOVE ‘AdventureWorks_Data’ TO ‘C:varoptmssqldataAdventureWorks2014_Data.mdf’,<br>MOVE ‘AdventureWorks_Log2014’ TO ‘C:varoptmssqldataAdventureWorks_Log.ldf’<br>GO</p>\n<p>If you typed everything correctly you should see a number of rows displayed as the database is restored and after a second or two a success message. <a href=\"http://edpflager.com/?attachment_id=3467#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/resotore_start-1024x484.png\" alt=\"resotore_start\"></a>Switch to the new database with:</p>\n<p>USE AdventureWorks<br>GO</p>\n<p>The response should be Changed Database context to ‘AdventureWorks’! You have restored AdventureWorks, and can experiment with it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3463#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg\" alt=\"sql_on_docker\"></a>In <a href=\"http://edpflager.com/?p=3447\">part one of this series</a>, I provided some info on Microsoft’s implementation of Sql Server on Docker and provided a method to have your SQL Server databases saved on your Docker host system so that they would remain persistent if the SQL Server container was shutdown. This time around, we’ll look at how to restore Microsoft’s sample database AdventureWorks to your SQL Server container.</p>\n<h5 id=\"DOWNLOAD-ADVENTUREWORKS\"><a href=\"#DOWNLOAD-ADVENTUREWORKS\" class=\"headerlink\" title=\"DOWNLOAD ADVENTUREWORKS\"></a>DOWNLOAD ADVENTUREWORKS</h5><p>If you followed my instructions from part 1 of this article you have a SQL Server container up and running with a data folder on your Docker host. Within that folder, you have several subfolders that SQL Server uses. You need to download a copy of AdventureWorks and save it in that folder. There are multiple versions available, as Microsoft provides a new version with every upgrade to SQL Server. The 2014 version should work fine for this purpose (If you download the 2016CTP version the restore command below will not work because that version includes FILESTREAM as part of the backup. An addition MOVE command needs to be supplied to include that in the RESTORE). Go to this <a href=\"https://msftdbprodsamples.codeplex.com/releases/view/125550\">website</a>, and download Adventure Works 2014 Full Database Backup.zip. I renamed the file to just AdventureWorks.zip to make it easier to transfer to my Docker host system and used scp to transfer it:</p>","more":"<p>scp AdventureWorks.zip <username>@<Docker Host IP>:~/AdventureWorks.zip</p>\n<p>Now unzip the file to the shared data folder that was created previously. SUDO is required because the folder is protected:</p>\n<p>sudo unzip AdventureWorks.zip -d ~/mssql/data</p>\n<p> </p>\n<h5 id=\"RESTORE-ADVENTUREWORKS\"><a href=\"#RESTORE-ADVENTUREWORKS\" class=\"headerlink\" title=\"RESTORE ADVENTUREWORKS\"></a>RESTORE ADVENTUREWORKS</h5><p>Before we can restore a database, we need a Linux or Mac system with the Microsoft SQL Server command line tools installed or a Windows machine with the SQL Server Management Studio installed. The instructions below are just for using the command line tool however. Attempting to install the tools on the SQL Server container currently fails with several unmet dependency issues, and installing the tools in an Ubuntu container doesn’t appear to work either, resulting in an error when attempting to access the sqlcmd tool. So on another Linux machine, execute the following commands to install the SQL Server command line tools. If you Docker host machine is running Ubuntu you can install the tools there with these instructions.</p>\n<p>Refresh the repository information in your container with:</p>\n<p>apt-get update</p>\n<p>Then install CURL if you don’t have it:</p>\n<p>apt-get install curl</p>\n<p>Fetch the Microsoft GPG registry key and install it:</p>\n<p>curl <a href=\"https://packages.microsoft.com/keys/microsoft.asc\">https://packages.microsoft.com/keys/microsoft.asc</a> | apt-key add -</p>\n<p>Register the Microsoft repository for Ubuntu (since that is what the container is built on)</p>\n<p>curl <a href=\"https://packages.microsoft.com/config/ubuntu/16.04/prod.list\">https://packages.microsoft.com/config/ubuntu/16.04/prod.list</a> | tee /etc/apt/sources.list.d/msprod.list</p>\n<p>Install the https transport component for Ubuntu:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt-get install apt-transport-https</span><br></pre></td></tr></table></figure>\n\n<p>And update your repository cache again:</p>\n<p>apt-get update</p>\n<p>Finally, install the SQL Server command line tools:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apt-get install mssql-tools</span><br></pre></td></tr></table></figure>\n\n<p>During the installation you will prompted to accept the license terms twice. Enter Yes both times if you accept and want to proceed. Once the installation completes, access SQLCMD and connect to your SQL Server machine:</p>\n<p>sqlcmd -S <SQL Server IP> -U SA -P <password></p>\n<p>The display should respond with:  1&gt;</p>\n<p>You can then restore the AdventureWorks database with this command, entering each line with a RETURN at the end:</p>\n<p>RESTORE DATABASE AdventureWorks FROM DISK = ‘C:varoptmssqldataAdventureWorks.BAK’<br>WITH MOVE ‘AdventureWorks_Data’ TO ‘C:varoptmssqldataAdventureWorks2014_Data.mdf’,<br>MOVE ‘AdventureWorks_Log2014’ TO ‘C:varoptmssqldataAdventureWorks_Log.ldf’<br>GO</p>\n<p>If you typed everything correctly you should see a number of rows displayed as the database is restored and after a second or two a success message. <a href=\"http://edpflager.com/?attachment_id=3467#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/resotore_start-1024x484.png\" alt=\"resotore_start\"></a>Switch to the new database with:</p>\n<p>USE AdventureWorks<br>GO</p>\n<p>The response should be Changed Database context to ‘AdventureWorks’! You have restored AdventureWorks, and can experiment with it!</p>"},{"title":"Microsoft SQL Server on Docker (part 1)","id":"3447","comments":0,"date":"2016-11-28T15:45:09.000Z","_content":"\n[![sql_on_docker](http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg)](http://edpflager.com/?attachment_id=3463#main)After last week's announcement that Microsoft has released a Public Preview version of SQL Server that would run on Linux platforms (currently Ubuntu 16.04 and Red Hat Enterprise 7.2), I wondered if a Docker version was going to be made available as well. Checking out Microsoft's [SQL Server v.Next website](https://www.microsoft.com/en-us/sql-server/sql-server-vnext-including-Linux?wt.mc_id=AID533228_EML_4683749#resources), I quickly saw that **Lo and behold!** there was! As I stated previously, a good portion of my professional career has been spent working with Microsoft SQL Server and a lot of my spare time is spent experimenting with Linux and Open Source applications. The merging of the two is hugely exciting to me and the ability to run SQL Server in a Docker container is amazing! Previous Docker containers of Microsoft products have been a mixed bag of requirements. Some required you to run the Docker engine on a Windows platform in order for it to work, while others could be run on different Linux variants. Even the previously released version of SQL Server Express, the free version of SQL would only work on Windows hosted containers. But so far it seems with this version of SQL Server, Microsoft may be starting to explore the open source world possibilities. There are some caveats though:\n<!-- more -->\n*   So far, the Linux based version of SQL Server only has command line tools available. If you want to use the standard SQL Server Management Studio GUI tool, you'll need a Windows system to connect to your Linux SQL Server.\n*   The Linux version of SQL Server does not support Active Directory authentication, a big part of the Microsoft security model. With AD, you can use the same user account across multiple machines, and don't need to login to each one, and your credentials are maintained in a central location.\n*   The Linux version does not include Reporting Services or Analysis Services, two other components of the Microsoft BI Stack. While not essential for many organizations, these tools are included as part of a fully licensed version of SQL Server on a Windows platform.\n*   And finally, there is a big issue of licensing. This Preview edition of SQL Server is only operational for a few months. Normal practice for Microsoft's preview versions to keep folks from running it in perpetuity. But what is Microsoft's license model for SQL Server running on Linux? By not requiring a Windows platform they are taking part of their revenue stream away, but without the other features of the BI stack is it a non-starter for many organizations?\n\nAll of this may change, and is likely to as development continues, but for now, lets look at how to get a SQL Server Docker container running. Connect to your Docker host, and create a folder where you'd like your database files to persist. This way, when your container shuts down for whatever reason, you will stay have you data files. In my case, I created it under my user folder:\n\nmkdir mssql\n\nThen enter the following command to start the container. You'll need to include a strong system administrator password as part of the command using the enviroment setting -e SA\\_PASSWORD. The default port that SQL Server uses is 1433. If you want to change that, you can alter the -p option. Substitute the path to your data folder after the -v flag. For my case, the command to start my container looks something like this:\n\ndocker run -e ACCEPT\\_EULA=Y -e SA\\_PASSWORD=string1password -p 1433:1433 -v /home/me/mssql:/var/opt/mssql -d microsoft/mssql-server-linux\n\nOn  your host machine in your data folder, you'll now see several new folders created: data, etc, log, and secrets. The data folder containers the four databases that SQL Server needs to run: master, model, msdbdata and tempdb. The etc folder is probably empty. The log folder is where logs are written to by the SQL Server engine, and the secrets folder contains a file identifying your machine.\n\nNext time I'll walk through installing the SQL Server command line tools and restoring a copy of AdventureWorks to your Docker container.","source":"_posts/microsoft-sql-server-on-docker.md","raw":"---\ntitle: Microsoft SQL Server on Docker (part 1)\ntags:\n  - How-to\n  - howto\n  - install\n  - SQL Server\n  - SysAdmin\n  - technical\nid: '3447'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-11-28 10:45:09\n---\n\n[![sql_on_docker](http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg)](http://edpflager.com/?attachment_id=3463#main)After last week's announcement that Microsoft has released a Public Preview version of SQL Server that would run on Linux platforms (currently Ubuntu 16.04 and Red Hat Enterprise 7.2), I wondered if a Docker version was going to be made available as well. Checking out Microsoft's [SQL Server v.Next website](https://www.microsoft.com/en-us/sql-server/sql-server-vnext-including-Linux?wt.mc_id=AID533228_EML_4683749#resources), I quickly saw that **Lo and behold!** there was! As I stated previously, a good portion of my professional career has been spent working with Microsoft SQL Server and a lot of my spare time is spent experimenting with Linux and Open Source applications. The merging of the two is hugely exciting to me and the ability to run SQL Server in a Docker container is amazing! Previous Docker containers of Microsoft products have been a mixed bag of requirements. Some required you to run the Docker engine on a Windows platform in order for it to work, while others could be run on different Linux variants. Even the previously released version of SQL Server Express, the free version of SQL would only work on Windows hosted containers. But so far it seems with this version of SQL Server, Microsoft may be starting to explore the open source world possibilities. There are some caveats though:\n<!-- more -->\n*   So far, the Linux based version of SQL Server only has command line tools available. If you want to use the standard SQL Server Management Studio GUI tool, you'll need a Windows system to connect to your Linux SQL Server.\n*   The Linux version of SQL Server does not support Active Directory authentication, a big part of the Microsoft security model. With AD, you can use the same user account across multiple machines, and don't need to login to each one, and your credentials are maintained in a central location.\n*   The Linux version does not include Reporting Services or Analysis Services, two other components of the Microsoft BI Stack. While not essential for many organizations, these tools are included as part of a fully licensed version of SQL Server on a Windows platform.\n*   And finally, there is a big issue of licensing. This Preview edition of SQL Server is only operational for a few months. Normal practice for Microsoft's preview versions to keep folks from running it in perpetuity. But what is Microsoft's license model for SQL Server running on Linux? By not requiring a Windows platform they are taking part of their revenue stream away, but without the other features of the BI stack is it a non-starter for many organizations?\n\nAll of this may change, and is likely to as development continues, but for now, lets look at how to get a SQL Server Docker container running. Connect to your Docker host, and create a folder where you'd like your database files to persist. This way, when your container shuts down for whatever reason, you will stay have you data files. In my case, I created it under my user folder:\n\nmkdir mssql\n\nThen enter the following command to start the container. You'll need to include a strong system administrator password as part of the command using the enviroment setting -e SA\\_PASSWORD. The default port that SQL Server uses is 1433. If you want to change that, you can alter the -p option. Substitute the path to your data folder after the -v flag. For my case, the command to start my container looks something like this:\n\ndocker run -e ACCEPT\\_EULA=Y -e SA\\_PASSWORD=string1password -p 1433:1433 -v /home/me/mssql:/var/opt/mssql -d microsoft/mssql-server-linux\n\nOn  your host machine in your data folder, you'll now see several new folders created: data, etc, log, and secrets. The data folder containers the four databases that SQL Server needs to run: master, model, msdbdata and tempdb. The etc folder is probably empty. The log folder is where logs are written to by the SQL Server engine, and the secrets folder contains a file identifying your machine.\n\nNext time I'll walk through installing the SQL Server command line tools and restoring a copy of AdventureWorks to your Docker container.","slug":"microsoft-sql-server-on-docker","published":1,"updated":"2020-08-23T20:54:35.078Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4b0064sdjxg3tq4sun","content":"<p><a href=\"http://edpflager.com/?attachment_id=3463#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg\" alt=\"sql_on_docker\"></a>After last week’s announcement that Microsoft has released a Public Preview version of SQL Server that would run on Linux platforms (currently Ubuntu 16.04 and Red Hat Enterprise 7.2), I wondered if a Docker version was going to be made available as well. Checking out Microsoft’s <a href=\"https://www.microsoft.com/en-us/sql-server/sql-server-vnext-including-Linux?wt.mc_id=AID533228_EML_4683749#resources\">SQL Server v.Next website</a>, I quickly saw that <strong>Lo and behold!</strong> there was! As I stated previously, a good portion of my professional career has been spent working with Microsoft SQL Server and a lot of my spare time is spent experimenting with Linux and Open Source applications. The merging of the two is hugely exciting to me and the ability to run SQL Server in a Docker container is amazing! Previous Docker containers of Microsoft products have been a mixed bag of requirements. Some required you to run the Docker engine on a Windows platform in order for it to work, while others could be run on different Linux variants. Even the previously released version of SQL Server Express, the free version of SQL would only work on Windows hosted containers. But so far it seems with this version of SQL Server, Microsoft may be starting to explore the open source world possibilities. There are some caveats though:</p>\n<a id=\"more\"></a>\n<ul>\n<li>So far, the Linux based version of SQL Server only has command line tools available. If you want to use the standard SQL Server Management Studio GUI tool, you’ll need a Windows system to connect to your Linux SQL Server.</li>\n<li>The Linux version of SQL Server does not support Active Directory authentication, a big part of the Microsoft security model. With AD, you can use the same user account across multiple machines, and don’t need to login to each one, and your credentials are maintained in a central location.</li>\n<li>The Linux version does not include Reporting Services or Analysis Services, two other components of the Microsoft BI Stack. While not essential for many organizations, these tools are included as part of a fully licensed version of SQL Server on a Windows platform.</li>\n<li>And finally, there is a big issue of licensing. This Preview edition of SQL Server is only operational for a few months. Normal practice for Microsoft’s preview versions to keep folks from running it in perpetuity. But what is Microsoft’s license model for SQL Server running on Linux? By not requiring a Windows platform they are taking part of their revenue stream away, but without the other features of the BI stack is it a non-starter for many organizations?</li>\n</ul>\n<p>All of this may change, and is likely to as development continues, but for now, lets look at how to get a SQL Server Docker container running. Connect to your Docker host, and create a folder where you’d like your database files to persist. This way, when your container shuts down for whatever reason, you will stay have you data files. In my case, I created it under my user folder:</p>\n<p>mkdir mssql</p>\n<p>Then enter the following command to start the container. You’ll need to include a strong system administrator password as part of the command using the enviroment setting -e SA_PASSWORD. The default port that SQL Server uses is 1433. If you want to change that, you can alter the -p option. Substitute the path to your data folder after the -v flag. For my case, the command to start my container looks something like this:</p>\n<p>docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=string1password -p 1433:1433 -v /home/me/mssql:/var/opt/mssql -d microsoft/mssql-server-linux</p>\n<p>On  your host machine in your data folder, you’ll now see several new folders created: data, etc, log, and secrets. The data folder containers the four databases that SQL Server needs to run: master, model, msdbdata and tempdb. The etc folder is probably empty. The log folder is where logs are written to by the SQL Server engine, and the secrets folder contains a file identifying your machine.</p>\n<p>Next time I’ll walk through installing the SQL Server command line tools and restoring a copy of AdventureWorks to your Docker container.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3463#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/11/sql_on_docker-300x229.jpg\" alt=\"sql_on_docker\"></a>After last week’s announcement that Microsoft has released a Public Preview version of SQL Server that would run on Linux platforms (currently Ubuntu 16.04 and Red Hat Enterprise 7.2), I wondered if a Docker version was going to be made available as well. Checking out Microsoft’s <a href=\"https://www.microsoft.com/en-us/sql-server/sql-server-vnext-including-Linux?wt.mc_id=AID533228_EML_4683749#resources\">SQL Server v.Next website</a>, I quickly saw that <strong>Lo and behold!</strong> there was! As I stated previously, a good portion of my professional career has been spent working with Microsoft SQL Server and a lot of my spare time is spent experimenting with Linux and Open Source applications. The merging of the two is hugely exciting to me and the ability to run SQL Server in a Docker container is amazing! Previous Docker containers of Microsoft products have been a mixed bag of requirements. Some required you to run the Docker engine on a Windows platform in order for it to work, while others could be run on different Linux variants. Even the previously released version of SQL Server Express, the free version of SQL would only work on Windows hosted containers. But so far it seems with this version of SQL Server, Microsoft may be starting to explore the open source world possibilities. There are some caveats though:</p>","more":"<ul>\n<li>So far, the Linux based version of SQL Server only has command line tools available. If you want to use the standard SQL Server Management Studio GUI tool, you’ll need a Windows system to connect to your Linux SQL Server.</li>\n<li>The Linux version of SQL Server does not support Active Directory authentication, a big part of the Microsoft security model. With AD, you can use the same user account across multiple machines, and don’t need to login to each one, and your credentials are maintained in a central location.</li>\n<li>The Linux version does not include Reporting Services or Analysis Services, two other components of the Microsoft BI Stack. While not essential for many organizations, these tools are included as part of a fully licensed version of SQL Server on a Windows platform.</li>\n<li>And finally, there is a big issue of licensing. This Preview edition of SQL Server is only operational for a few months. Normal practice for Microsoft’s preview versions to keep folks from running it in perpetuity. But what is Microsoft’s license model for SQL Server running on Linux? By not requiring a Windows platform they are taking part of their revenue stream away, but without the other features of the BI stack is it a non-starter for many organizations?</li>\n</ul>\n<p>All of this may change, and is likely to as development continues, but for now, lets look at how to get a SQL Server Docker container running. Connect to your Docker host, and create a folder where you’d like your database files to persist. This way, when your container shuts down for whatever reason, you will stay have you data files. In my case, I created it under my user folder:</p>\n<p>mkdir mssql</p>\n<p>Then enter the following command to start the container. You’ll need to include a strong system administrator password as part of the command using the enviroment setting -e SA_PASSWORD. The default port that SQL Server uses is 1433. If you want to change that, you can alter the -p option. Substitute the path to your data folder after the -v flag. For my case, the command to start my container looks something like this:</p>\n<p>docker run -e ACCEPT_EULA=Y -e SA_PASSWORD=string1password -p 1433:1433 -v /home/me/mssql:/var/opt/mssql -d microsoft/mssql-server-linux</p>\n<p>On  your host machine in your data folder, you’ll now see several new folders created: data, etc, log, and secrets. The data folder containers the four databases that SQL Server needs to run: master, model, msdbdata and tempdb. The etc folder is probably empty. The log folder is where logs are written to by the SQL Server engine, and the secrets folder contains a file identifying your machine.</p>\n<p>Next time I’ll walk through installing the SQL Server command line tools and restoring a copy of AdventureWorks to your Docker container.</p>"},{"title":"MongoDB data loading using Kettle","id":"1642","comments":0,"date":"2013-08-30T19:35:29.000Z","_content":"\n[![etl](http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png)](http://edpflager.com/wp-content/uploads/2013/08/etl.png) In this tutorial I will show you how to use [Pentaho Data Integration](http://kettle.pentaho.com/) (aka Kettle or PDI) to load data into a [MongoDB database](http://www.mongodb.org/). Before we get started let's define those two items:\n\n*   **PDI** is an open source tool used to extract data from one source, transform it into other formats, and then load it into a destination. This process is called ETL, and its a big part of my day to day job. We'll use PDI because its free open source software, and because it supports connecting to big data systems.\n*   **MongoDB** is an open source document NoSQL database system. Rather than storing data in rows and columns like a traditional database system, Mongo stores information in documents. This allows for quicker retrieval of information, but does require some getting used to how information needs to be formatted.\n<!-- more -->\nPrerequisites for this tutorial:\n\n*   MongoDB installed locally or on another server you have access to with an empty MongoDB database available.\n*   MySQL or MariaDB installed locally or on another server you have access to with the MySQL version of AdventureWorks available from [SourceForge](http://sourceforge.net/projects/awmysql/).\n*   Pentaho DI (Kettle) installed locally.\n\n## Pull data from a source table\n\nThe first part of the process is to define where the data we want to move is currently located. For our example, we are using the Customer table from the AdventureWorks database.\n\n1.  Open up Pentaho DI and create a new Transformation, and add a database connection to your MySQL database. If you're not sure how to do this, check out my previous tutorial on setting up a Kettle repository. Its very similar.\n2.  From the Design tab, drag a Table Input object from the Input node into your workspace. Double click on it to open it. Assign a name to the step (always a good idea for help in fixing bugs). Then from the connection drop down, chose your database connection that you created previously. In the SQL box, type in this statement to retrieve all the columns from the contact table: **SELECT  \\* FROM customer**![MySQLtableinput](http://edpflager.com/wp-content/uploads/2013/08/MySQLtableinput-300x199.png)\n3.  Click Preview at the bottom of the window, and then click OK in the window where you are prompted for the number of rows to retrieve. A sample of the data you will be working with will be displayed.\n4.  If everything looks good, click Close to be returned to the Table Input window, and then OK to be returned to the Transform Editor. At this point we have a step in place to pull records, but they aren't going anywhere.\n\n## Create a data destination\n\nNow we need to setup the destination for the data we are pulling from AdventureWorks.\n\n1.  From the Design Tab in PDI, open the Big Data node, and drag a MongoDB Output object to the workspace. Connect the Table Input object to the MongoDB Output object by holding Shift down while you click and drag from the Table Input to the MongoDB Output (or if you have a scroll wheel on your mouse, try clicking with the wheel and dragging between them).![MongoDB Destination](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Destination-300x233.png)\n2.  Double click on the MongoDB Output object and in the Configure Connection tab, enter the connection information for your MongoDB server. You can use a host name or an IP address with the default port 27017. In my setup, entering a username and password caused me to get an \"unable to authenticate error\". Leaving those fields blank let me connect.\n3.  If you already have a destination database and collection created, you can select them by clicking the Get DBs and Get Collections buttons to populate drop down lists. You can enter a new database and collection name in the appropriate boxes as well, in which case the database and collection will be created on the fly. Check the Truncate Collection button if that is appropriate for you and leave the rest of the options set to their defaults.[![MongoDB Connection](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection-300x216.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection.png)\n4.  Click on the second tab at the top left, labeled Mongo Document fields. This window should currently be blank. Click the Get Fields button near the bottom left, and a list of fields will be populated from the Customer table. You can modify these as needed (change the fields names, etc). For our purposes, we'll leave them as is.[![MongoDB Fields](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields-300x98.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields.png)\n5.  Click OK at the bottom of the screen, and you'll be back at the PDI workspace. Save your transformation, and then click the RUN TRANSFORM button (or press F9). In the Execute a Transformation window, click the LAUNCH button at the bottom.\n6.  If all goes well, the data from the Contact table will be read into PDI and passed into MongoDB. At the bottom of the Kettle window, you will see a multi-tabbed execution results window appear.\n7.  Check the Step Metrics tab to see a breakdown of the steps in your process, and  the records that were read and written as part of it. Click the Logging tab and you will see a number of batches that were loaded from the source and pushed to the destination.[![MongoDB Logging](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging-300x124.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging.png)\n8.  Connect to your MongoDB server and do a count on the customer collection. The results should equal the value shown at the end of the Logging tab, and the written records value from the Step Metrics tab.![MongoDB Count](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Count.png)\n9.  Congratulations! You have imported your first data set into MongoDB using an ETL tool!","source":"_posts/mongodb-data-loading-using-kettle.md","raw":"---\ntitle: MongoDB data loading using Kettle\ntags:\n  - ETL\n  - kettle\n  - PDI\nid: '1642'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2013-08-30 15:35:29\n---\n\n[![etl](http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png)](http://edpflager.com/wp-content/uploads/2013/08/etl.png) In this tutorial I will show you how to use [Pentaho Data Integration](http://kettle.pentaho.com/) (aka Kettle or PDI) to load data into a [MongoDB database](http://www.mongodb.org/). Before we get started let's define those two items:\n\n*   **PDI** is an open source tool used to extract data from one source, transform it into other formats, and then load it into a destination. This process is called ETL, and its a big part of my day to day job. We'll use PDI because its free open source software, and because it supports connecting to big data systems.\n*   **MongoDB** is an open source document NoSQL database system. Rather than storing data in rows and columns like a traditional database system, Mongo stores information in documents. This allows for quicker retrieval of information, but does require some getting used to how information needs to be formatted.\n<!-- more -->\nPrerequisites for this tutorial:\n\n*   MongoDB installed locally or on another server you have access to with an empty MongoDB database available.\n*   MySQL or MariaDB installed locally or on another server you have access to with the MySQL version of AdventureWorks available from [SourceForge](http://sourceforge.net/projects/awmysql/).\n*   Pentaho DI (Kettle) installed locally.\n\n## Pull data from a source table\n\nThe first part of the process is to define where the data we want to move is currently located. For our example, we are using the Customer table from the AdventureWorks database.\n\n1.  Open up Pentaho DI and create a new Transformation, and add a database connection to your MySQL database. If you're not sure how to do this, check out my previous tutorial on setting up a Kettle repository. Its very similar.\n2.  From the Design tab, drag a Table Input object from the Input node into your workspace. Double click on it to open it. Assign a name to the step (always a good idea for help in fixing bugs). Then from the connection drop down, chose your database connection that you created previously. In the SQL box, type in this statement to retrieve all the columns from the contact table: **SELECT  \\* FROM customer**![MySQLtableinput](http://edpflager.com/wp-content/uploads/2013/08/MySQLtableinput-300x199.png)\n3.  Click Preview at the bottom of the window, and then click OK in the window where you are prompted for the number of rows to retrieve. A sample of the data you will be working with will be displayed.\n4.  If everything looks good, click Close to be returned to the Table Input window, and then OK to be returned to the Transform Editor. At this point we have a step in place to pull records, but they aren't going anywhere.\n\n## Create a data destination\n\nNow we need to setup the destination for the data we are pulling from AdventureWorks.\n\n1.  From the Design Tab in PDI, open the Big Data node, and drag a MongoDB Output object to the workspace. Connect the Table Input object to the MongoDB Output object by holding Shift down while you click and drag from the Table Input to the MongoDB Output (or if you have a scroll wheel on your mouse, try clicking with the wheel and dragging between them).![MongoDB Destination](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Destination-300x233.png)\n2.  Double click on the MongoDB Output object and in the Configure Connection tab, enter the connection information for your MongoDB server. You can use a host name or an IP address with the default port 27017. In my setup, entering a username and password caused me to get an \"unable to authenticate error\". Leaving those fields blank let me connect.\n3.  If you already have a destination database and collection created, you can select them by clicking the Get DBs and Get Collections buttons to populate drop down lists. You can enter a new database and collection name in the appropriate boxes as well, in which case the database and collection will be created on the fly. Check the Truncate Collection button if that is appropriate for you and leave the rest of the options set to their defaults.[![MongoDB Connection](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection-300x216.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection.png)\n4.  Click on the second tab at the top left, labeled Mongo Document fields. This window should currently be blank. Click the Get Fields button near the bottom left, and a list of fields will be populated from the Customer table. You can modify these as needed (change the fields names, etc). For our purposes, we'll leave them as is.[![MongoDB Fields](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields-300x98.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields.png)\n5.  Click OK at the bottom of the screen, and you'll be back at the PDI workspace. Save your transformation, and then click the RUN TRANSFORM button (or press F9). In the Execute a Transformation window, click the LAUNCH button at the bottom.\n6.  If all goes well, the data from the Contact table will be read into PDI and passed into MongoDB. At the bottom of the Kettle window, you will see a multi-tabbed execution results window appear.\n7.  Check the Step Metrics tab to see a breakdown of the steps in your process, and  the records that were read and written as part of it. Click the Logging tab and you will see a number of batches that were loaded from the source and pushed to the destination.[![MongoDB Logging](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging-300x124.png)](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging.png)\n8.  Connect to your MongoDB server and do a count on the customer collection. The results should equal the value shown at the end of the Logging tab, and the written records value from the Step Metrics tab.![MongoDB Count](http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Count.png)\n9.  Congratulations! You have imported your first data set into MongoDB using an ETL tool!","slug":"mongodb-data-loading-using-kettle","published":1,"updated":"2020-08-23T20:54:34.750Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4e0067sdjx1pvm99t8","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/etl.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png\" alt=\"etl\"></a> In this tutorial I will show you how to use <a href=\"http://kettle.pentaho.com/\">Pentaho Data Integration</a> (aka Kettle or PDI) to load data into a <a href=\"http://www.mongodb.org/\">MongoDB database</a>. Before we get started let’s define those two items:</p>\n<ul>\n<li><p><strong>PDI</strong> is an open source tool used to extract data from one source, transform it into other formats, and then load it into a destination. This process is called ETL, and its a big part of my day to day job. We’ll use PDI because its free open source software, and because it supports connecting to big data systems.</p>\n</li>\n<li><p><strong>MongoDB</strong> is an open source document NoSQL database system. Rather than storing data in rows and columns like a traditional database system, Mongo stores information in documents. This allows for quicker retrieval of information, but does require some getting used to how information needs to be formatted.</p>\n<a id=\"more\"></a>\n<p>Prerequisites for this tutorial:</p>\n</li>\n<li><p>MongoDB installed locally or on another server you have access to with an empty MongoDB database available.</p>\n</li>\n<li><p>MySQL or MariaDB installed locally or on another server you have access to with the MySQL version of AdventureWorks available from <a href=\"http://sourceforge.net/projects/awmysql/\">SourceForge</a>.</p>\n</li>\n<li><p>Pentaho DI (Kettle) installed locally.</p>\n</li>\n</ul>\n<h2 id=\"Pull-data-from-a-source-table\"><a href=\"#Pull-data-from-a-source-table\" class=\"headerlink\" title=\"Pull data from a source table\"></a>Pull data from a source table</h2><p>The first part of the process is to define where the data we want to move is currently located. For our example, we are using the Customer table from the AdventureWorks database.</p>\n<ol>\n<li>Open up Pentaho DI and create a new Transformation, and add a database connection to your MySQL database. If you’re not sure how to do this, check out my previous tutorial on setting up a Kettle repository. Its very similar.</li>\n<li>From the Design tab, drag a Table Input object from the Input node into your workspace. Double click on it to open it. Assign a name to the step (always a good idea for help in fixing bugs). Then from the connection drop down, chose your database connection that you created previously. In the SQL box, type in this statement to retrieve all the columns from the contact table: <strong>SELECT  * FROM customer</strong><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MySQLtableinput-300x199.png\" alt=\"MySQLtableinput\"></li>\n<li>Click Preview at the bottom of the window, and then click OK in the window where you are prompted for the number of rows to retrieve. A sample of the data you will be working with will be displayed.</li>\n<li>If everything looks good, click Close to be returned to the Table Input window, and then OK to be returned to the Transform Editor. At this point we have a step in place to pull records, but they aren’t going anywhere.</li>\n</ol>\n<h2 id=\"Create-a-data-destination\"><a href=\"#Create-a-data-destination\" class=\"headerlink\" title=\"Create a data destination\"></a>Create a data destination</h2><p>Now we need to setup the destination for the data we are pulling from AdventureWorks.</p>\n<ol>\n<li>From the Design Tab in PDI, open the Big Data node, and drag a MongoDB Output object to the workspace. Connect the Table Input object to the MongoDB Output object by holding Shift down while you click and drag from the Table Input to the MongoDB Output (or if you have a scroll wheel on your mouse, try clicking with the wheel and dragging between them).<img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Destination-300x233.png\" alt=\"MongoDB Destination\"></li>\n<li>Double click on the MongoDB Output object and in the Configure Connection tab, enter the connection information for your MongoDB server. You can use a host name or an IP address with the default port 27017. In my setup, entering a username and password caused me to get an “unable to authenticate error”. Leaving those fields blank let me connect.</li>\n<li>If you already have a destination database and collection created, you can select them by clicking the Get DBs and Get Collections buttons to populate drop down lists. You can enter a new database and collection name in the appropriate boxes as well, in which case the database and collection will be created on the fly. Check the Truncate Collection button if that is appropriate for you and leave the rest of the options set to their defaults.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection-300x216.png\" alt=\"MongoDB Connection\"></a></li>\n<li>Click on the second tab at the top left, labeled Mongo Document fields. This window should currently be blank. Click the Get Fields button near the bottom left, and a list of fields will be populated from the Customer table. You can modify these as needed (change the fields names, etc). For our purposes, we’ll leave them as is.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields-300x98.png\" alt=\"MongoDB Fields\"></a></li>\n<li>Click OK at the bottom of the screen, and you’ll be back at the PDI workspace. Save your transformation, and then click the RUN TRANSFORM button (or press F9). In the Execute a Transformation window, click the LAUNCH button at the bottom.</li>\n<li>If all goes well, the data from the Contact table will be read into PDI and passed into MongoDB. At the bottom of the Kettle window, you will see a multi-tabbed execution results window appear.</li>\n<li>Check the Step Metrics tab to see a breakdown of the steps in your process, and  the records that were read and written as part of it. Click the Logging tab and you will see a number of batches that were loaded from the source and pushed to the destination.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging-300x124.png\" alt=\"MongoDB Logging\"></a></li>\n<li>Connect to your MongoDB server and do a count on the customer collection. The results should equal the value shown at the end of the Logging tab, and the written records value from the Step Metrics tab.<img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Count.png\" alt=\"MongoDB Count\"></li>\n<li>Congratulations! You have imported your first data set into MongoDB using an ETL tool!</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/etl.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png\" alt=\"etl\"></a> In this tutorial I will show you how to use <a href=\"http://kettle.pentaho.com/\">Pentaho Data Integration</a> (aka Kettle or PDI) to load data into a <a href=\"http://www.mongodb.org/\">MongoDB database</a>. Before we get started let’s define those two items:</p>\n<ul>\n<li><p><strong>PDI</strong> is an open source tool used to extract data from one source, transform it into other formats, and then load it into a destination. This process is called ETL, and its a big part of my day to day job. We’ll use PDI because its free open source software, and because it supports connecting to big data systems.</p>\n</li>\n<li><p><strong>MongoDB</strong> is an open source document NoSQL database system. Rather than storing data in rows and columns like a traditional database system, Mongo stores information in documents. This allows for quicker retrieval of information, but does require some getting used to how information needs to be formatted.</p>","more":"<p>Prerequisites for this tutorial:</p>\n</li>\n<li><p>MongoDB installed locally or on another server you have access to with an empty MongoDB database available.</p>\n</li>\n<li><p>MySQL or MariaDB installed locally or on another server you have access to with the MySQL version of AdventureWorks available from <a href=\"http://sourceforge.net/projects/awmysql/\">SourceForge</a>.</p>\n</li>\n<li><p>Pentaho DI (Kettle) installed locally.</p>\n</li>\n</ul>\n<h2 id=\"Pull-data-from-a-source-table\"><a href=\"#Pull-data-from-a-source-table\" class=\"headerlink\" title=\"Pull data from a source table\"></a>Pull data from a source table</h2><p>The first part of the process is to define where the data we want to move is currently located. For our example, we are using the Customer table from the AdventureWorks database.</p>\n<ol>\n<li>Open up Pentaho DI and create a new Transformation, and add a database connection to your MySQL database. If you’re not sure how to do this, check out my previous tutorial on setting up a Kettle repository. Its very similar.</li>\n<li>From the Design tab, drag a Table Input object from the Input node into your workspace. Double click on it to open it. Assign a name to the step (always a good idea for help in fixing bugs). Then from the connection drop down, chose your database connection that you created previously. In the SQL box, type in this statement to retrieve all the columns from the contact table: <strong>SELECT  * FROM customer</strong><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MySQLtableinput-300x199.png\" alt=\"MySQLtableinput\"></li>\n<li>Click Preview at the bottom of the window, and then click OK in the window where you are prompted for the number of rows to retrieve. A sample of the data you will be working with will be displayed.</li>\n<li>If everything looks good, click Close to be returned to the Table Input window, and then OK to be returned to the Transform Editor. At this point we have a step in place to pull records, but they aren’t going anywhere.</li>\n</ol>\n<h2 id=\"Create-a-data-destination\"><a href=\"#Create-a-data-destination\" class=\"headerlink\" title=\"Create a data destination\"></a>Create a data destination</h2><p>Now we need to setup the destination for the data we are pulling from AdventureWorks.</p>\n<ol>\n<li>From the Design Tab in PDI, open the Big Data node, and drag a MongoDB Output object to the workspace. Connect the Table Input object to the MongoDB Output object by holding Shift down while you click and drag from the Table Input to the MongoDB Output (or if you have a scroll wheel on your mouse, try clicking with the wheel and dragging between them).<img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Destination-300x233.png\" alt=\"MongoDB Destination\"></li>\n<li>Double click on the MongoDB Output object and in the Configure Connection tab, enter the connection information for your MongoDB server. You can use a host name or an IP address with the default port 27017. In my setup, entering a username and password caused me to get an “unable to authenticate error”. Leaving those fields blank let me connect.</li>\n<li>If you already have a destination database and collection created, you can select them by clicking the Get DBs and Get Collections buttons to populate drop down lists. You can enter a new database and collection name in the appropriate boxes as well, in which case the database and collection will be created on the fly. Check the Truncate Collection button if that is appropriate for you and leave the rest of the options set to their defaults.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Connection-300x216.png\" alt=\"MongoDB Connection\"></a></li>\n<li>Click on the second tab at the top left, labeled Mongo Document fields. This window should currently be blank. Click the Get Fields button near the bottom left, and a list of fields will be populated from the Customer table. You can modify these as needed (change the fields names, etc). For our purposes, we’ll leave them as is.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Fields-300x98.png\" alt=\"MongoDB Fields\"></a></li>\n<li>Click OK at the bottom of the screen, and you’ll be back at the PDI workspace. Save your transformation, and then click the RUN TRANSFORM button (or press F9). In the Execute a Transformation window, click the LAUNCH button at the bottom.</li>\n<li>If all goes well, the data from the Contact table will be read into PDI and passed into MongoDB. At the bottom of the Kettle window, you will see a multi-tabbed execution results window appear.</li>\n<li>Check the Step Metrics tab to see a breakdown of the steps in your process, and  the records that were read and written as part of it. Click the Logging tab and you will see a number of batches that were loaded from the source and pushed to the destination.<a href=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Logging-300x124.png\" alt=\"MongoDB Logging\"></a></li>\n<li>Connect to your MongoDB server and do a count on the customer collection. The results should equal the value shown at the end of the Logging tab, and the written records value from the Step Metrics tab.<img src=\"http://edpflager.com/wp-content/uploads/2013/08/MongoDB-Count.png\" alt=\"MongoDB Count\"></li>\n<li>Congratulations! You have imported your first data set into MongoDB using an ETL tool!</li>\n</ol>"},{"title":"More Control over Footnotes in R Markdown","id":"4319","comments":0,"date":"2019-01-22T00:17:03.000Z","_content":"\nI[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)n R Markdown include footnotes in documents to provide a reference or comment that may provide additional context or an aside to the topic. Traditionally, these numbered notes are placed at the bottom of the page and a corresponding superscript number is added to the text body at the relevant text. R Markdown supports footnotes natively without the need for any additional packages.I'll cover that first, and then show how to gain more control over footnotes using an additional LaTeX package. The native footnote syntax is simple, and requires just a paired set of footnote tags. **\\[^1\\]** - denotes where the footnote superscript notation should be placed. **\\[^1\\]: {footnote text}** - is the corresponding footnote text. The character within the brackets has to be match the superscript tag and be followed by the trailing the colon. This footnote text can be placed at any spot in the document, but for clarity's sake, place it close to the text it refers to.\n<!-- more -->\nTo illustrate more clearly, here is an example RMarkdown document: [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode-1024x274.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode.png) And the rendered document from RMarkdown: [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal-1024x360.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal.png) R Markdown and Knitr will generate appropriate footnote numbers in sequence according to the placement of the opening footnote tags regardless of the tags being used. Character notation works as well as numbers.\n\n##### USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL\n\nUse the LaTeX package [**FOOTNOTE**](https://ctan.org/pkg/footnote) from Mark Wood­ing by adding this line to the YAML section of the R Markdown document:\n\n usepackage{footnote}\n\nThe footnote package provides features to generate footnotes using roman numerals (lower and upper case), alphabetic characters (lower and upper case) or even the sequence of nine keyboard symbols pictured here. [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols-150x150.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols.png)Be careful using the symbols because more than nine footnotes will generate an error message. To select the footnote option to use, at the beginning of the R Markdown document, or at least before the first footnote, add the appropriate command from this list:\n\nrenewcommand{thefootnote}{fnsymbol{footnote}} - symbols in the picture above\nrenewcommand{thefootnote}{arabic{footnote}} - the standard footnote pattern: 1,2,3,etc\nrenewcommand{thefootnote}{Roman{footnote}} - upper case Roman numerals (I,II, III, IV, etc)\nrenewcommand{thefootnote}{roman{footnote}} - lower case Roman numerals (i,ii,iii,iv, etc)\nrenewcommand{thefootnote}{Alph{footnote}} - upper case characters (A,B,C,etc)\nrenewcommand{thefootnote}{alph{footnote}} - lower case characters (a,b,c, etc)\n\nTurning off the superscript for footnotes completely can be accomplished by including this command at the beginning of the document:\n\nletthefootnoterelaxfootnote\n\n##### LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS\n\nOriginally created by Robin Fair­bairns, the [**FOOTMISC**](https://ctan.org/pkg/footmisc) package in LaTeX modifies the standard footnote options, and adds some new features. Some of the more interesting ones are highlighted below, but check out the documentation at the link above for more. Suppress the printing of the traditional horizontal line at the bottom of the page before footnotes can be turned on by inserting the following near the top of the R Markdown document:\n\nrenewcommandfootnoterule{}\n\nAnother interesting option in **FOOTMISC** is the inclusion of several new symbol sets for footnote notation. To use one of the predefined symbol sets (brighthurst, chicago, or wiley) that removes the duplicate entries from the original set and adds new ones, use these commands:\n\nsetfnsymbol{symbol set name, e.g. chicago}\nrenewcommand{thefootnote}{fnsymbol{footnote}}\n\nIn LaTeX, the package allows the definition of new sets, although I have not been able to get it to work in R Markdown. Keep in mind that the formatting options chosen for the footnote will remain consistent throughout the document. There are also additional packages available in LaTeX to provide more control over footnote formatting, including: footbib and footmisx. Check the CTAN.ORG website for more information.","source":"_posts/more-control-over-footnotes-in-r-markdown.md","raw":"---\ntitle: More Control over Footnotes in R Markdown\ntags:\n  - howto\n  - R Markdown\n  - technical\nid: '4319'\ncategories:\n  - - Blog\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2019-01-21 19:17:03\n---\n\nI[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)n R Markdown include footnotes in documents to provide a reference or comment that may provide additional context or an aside to the topic. Traditionally, these numbered notes are placed at the bottom of the page and a corresponding superscript number is added to the text body at the relevant text. R Markdown supports footnotes natively without the need for any additional packages.I'll cover that first, and then show how to gain more control over footnotes using an additional LaTeX package. The native footnote syntax is simple, and requires just a paired set of footnote tags. **\\[^1\\]** - denotes where the footnote superscript notation should be placed. **\\[^1\\]: {footnote text}** - is the corresponding footnote text. The character within the brackets has to be match the superscript tag and be followed by the trailing the colon. This footnote text can be placed at any spot in the document, but for clarity's sake, place it close to the text it refers to.\n<!-- more -->\nTo illustrate more clearly, here is an example RMarkdown document: [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode-1024x274.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode.png) And the rendered document from RMarkdown: [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal-1024x360.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal.png) R Markdown and Knitr will generate appropriate footnote numbers in sequence according to the placement of the opening footnote tags regardless of the tags being used. Character notation works as well as numbers.\n\n##### USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL\n\nUse the LaTeX package [**FOOTNOTE**](https://ctan.org/pkg/footnote) from Mark Wood­ing by adding this line to the YAML section of the R Markdown document:\n\n usepackage{footnote}\n\nThe footnote package provides features to generate footnotes using roman numerals (lower and upper case), alphabetic characters (lower and upper case) or even the sequence of nine keyboard symbols pictured here. [![](http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols-150x150.png)](http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols.png)Be careful using the symbols because more than nine footnotes will generate an error message. To select the footnote option to use, at the beginning of the R Markdown document, or at least before the first footnote, add the appropriate command from this list:\n\nrenewcommand{thefootnote}{fnsymbol{footnote}} - symbols in the picture above\nrenewcommand{thefootnote}{arabic{footnote}} - the standard footnote pattern: 1,2,3,etc\nrenewcommand{thefootnote}{Roman{footnote}} - upper case Roman numerals (I,II, III, IV, etc)\nrenewcommand{thefootnote}{roman{footnote}} - lower case Roman numerals (i,ii,iii,iv, etc)\nrenewcommand{thefootnote}{Alph{footnote}} - upper case characters (A,B,C,etc)\nrenewcommand{thefootnote}{alph{footnote}} - lower case characters (a,b,c, etc)\n\nTurning off the superscript for footnotes completely can be accomplished by including this command at the beginning of the document:\n\nletthefootnoterelaxfootnote\n\n##### LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS\n\nOriginally created by Robin Fair­bairns, the [**FOOTMISC**](https://ctan.org/pkg/footmisc) package in LaTeX modifies the standard footnote options, and adds some new features. Some of the more interesting ones are highlighted below, but check out the documentation at the link above for more. Suppress the printing of the traditional horizontal line at the bottom of the page before footnotes can be turned on by inserting the following near the top of the R Markdown document:\n\nrenewcommandfootnoterule{}\n\nAnother interesting option in **FOOTMISC** is the inclusion of several new symbol sets for footnote notation. To use one of the predefined symbol sets (brighthurst, chicago, or wiley) that removes the duplicate entries from the original set and adds new ones, use these commands:\n\nsetfnsymbol{symbol set name, e.g. chicago}\nrenewcommand{thefootnote}{fnsymbol{footnote}}\n\nIn LaTeX, the package allows the definition of new sets, although I have not been able to get it to work in R Markdown. Keep in mind that the formatting options chosen for the footnote will remain consistent throughout the document. There are also additional packages available in LaTeX to provide more control over footnote formatting, including: footbib and footmisx. Check the CTAN.ORG website for more information.","slug":"more-control-over-footnotes-in-r-markdown","published":1,"updated":"2020-08-23T20:54:35.210Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4i006bsdjx2bw02r0z","content":"<p>I<a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>n R Markdown include footnotes in documents to provide a reference or comment that may provide additional context or an aside to the topic. Traditionally, these numbered notes are placed at the bottom of the page and a corresponding superscript number is added to the text body at the relevant text. R Markdown supports footnotes natively without the need for any additional packages.I’ll cover that first, and then show how to gain more control over footnotes using an additional LaTeX package. The native footnote syntax is simple, and requires just a paired set of footnote tags. <strong>[^1]</strong> - denotes where the footnote superscript notation should be placed. <strong>[^1]: {footnote text}</strong> - is the corresponding footnote text. The character within the brackets has to be match the superscript tag and be followed by the trailing the colon. This footnote text can be placed at any spot in the document, but for clarity’s sake, place it close to the text it refers to.</p>\n<a id=\"more\"></a>\n<p>To illustrate more clearly, here is an example RMarkdown document: <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode-1024x274.png\"></a> And the rendered document from RMarkdown: <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal-1024x360.png\"></a> R Markdown and Knitr will generate appropriate footnote numbers in sequence according to the placement of the opening footnote tags regardless of the tags being used. Character notation works as well as numbers.</p>\n<h5 id=\"USE-LATEX-PACKAGE-FOOTNOTE-FOR-MORE-CONTROL\"><a href=\"#USE-LATEX-PACKAGE-FOOTNOTE-FOR-MORE-CONTROL\" class=\"headerlink\" title=\"USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL\"></a>USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL</h5><p>Use the LaTeX package <a href=\"https://ctan.org/pkg/footnote\"><strong>FOOTNOTE</strong></a> from Mark Wood­ing by adding this line to the YAML section of the R Markdown document:</p>\n<p> usepackage{footnote}</p>\n<p>The footnote package provides features to generate footnotes using roman numerals (lower and upper case), alphabetic characters (lower and upper case) or even the sequence of nine keyboard symbols pictured here. <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols-150x150.png\"></a>Be careful using the symbols because more than nine footnotes will generate an error message. To select the footnote option to use, at the beginning of the R Markdown document, or at least before the first footnote, add the appropriate command from this list:</p>\n<p>renewcommand{thefootnote}{fnsymbol{footnote}} - symbols in the picture above<br>renewcommand{thefootnote}{arabic{footnote}} - the standard footnote pattern: 1,2,3,etc<br>renewcommand{thefootnote}{Roman{footnote}} - upper case Roman numerals (I,II, III, IV, etc)<br>renewcommand{thefootnote}{roman{footnote}} - lower case Roman numerals (i,ii,iii,iv, etc)<br>renewcommand{thefootnote}{Alph{footnote}} - upper case characters (A,B,C,etc)<br>renewcommand{thefootnote}{alph{footnote}} - lower case characters (a,b,c, etc)</p>\n<p>Turning off the superscript for footnotes completely can be accomplished by including this command at the beginning of the document:</p>\n<p>letthefootnoterelaxfootnote</p>\n<h5 id=\"LATEX-PACKAGE-FOOTMISC-PROVIDES-MORE-OPTIONS\"><a href=\"#LATEX-PACKAGE-FOOTMISC-PROVIDES-MORE-OPTIONS\" class=\"headerlink\" title=\"LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS\"></a>LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS</h5><p>Originally created by Robin Fair­bairns, the <a href=\"https://ctan.org/pkg/footmisc\"><strong>FOOTMISC</strong></a> package in LaTeX modifies the standard footnote options, and adds some new features. Some of the more interesting ones are highlighted below, but check out the documentation at the link above for more. Suppress the printing of the traditional horizontal line at the bottom of the page before footnotes can be turned on by inserting the following near the top of the R Markdown document:</p>\n<p>renewcommandfootnoterule{}</p>\n<p>Another interesting option in <strong>FOOTMISC</strong> is the inclusion of several new symbol sets for footnote notation. To use one of the predefined symbol sets (brighthurst, chicago, or wiley) that removes the duplicate entries from the original set and adds new ones, use these commands:</p>\n<p>setfnsymbol{symbol set name, e.g. chicago}<br>renewcommand{thefootnote}{fnsymbol{footnote}}</p>\n<p>In LaTeX, the package allows the definition of new sets, although I have not been able to get it to work in R Markdown. Keep in mind that the formatting options chosen for the footnote will remain consistent throughout the document. There are also additional packages available in LaTeX to provide more control over footnote formatting, including: footbib and footmisx. Check the CTAN.ORG website for more information.</p>\n","site":{"data":{}},"excerpt":"<p>I<a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>n R Markdown include footnotes in documents to provide a reference or comment that may provide additional context or an aside to the topic. Traditionally, these numbered notes are placed at the bottom of the page and a corresponding superscript number is added to the text body at the relevant text. R Markdown supports footnotes natively without the need for any additional packages.I’ll cover that first, and then show how to gain more control over footnotes using an additional LaTeX package. The native footnote syntax is simple, and requires just a paired set of footnote tags. <strong>[^1]</strong> - denotes where the footnote superscript notation should be placed. <strong>[^1]: {footnote text}</strong> - is the corresponding footnote text. The character within the brackets has to be match the superscript tag and be followed by the trailing the colon. This footnote text can be placed at any spot in the document, but for clarity’s sake, place it close to the text it refers to.</p>","more":"<p>To illustrate more clearly, here is an example RMarkdown document: <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormalCode-1024x274.png\"></a> And the rendered document from RMarkdown: <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteNormal-1024x360.png\"></a> R Markdown and Knitr will generate appropriate footnote numbers in sequence according to the placement of the opening footnote tags regardless of the tags being used. Character notation works as well as numbers.</p>\n<h5 id=\"USE-LATEX-PACKAGE-FOOTNOTE-FOR-MORE-CONTROL\"><a href=\"#USE-LATEX-PACKAGE-FOOTNOTE-FOR-MORE-CONTROL\" class=\"headerlink\" title=\"USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL\"></a>USE LATEX PACKAGE FOOTNOTE FOR MORE CONTROL</h5><p>Use the LaTeX package <a href=\"https://ctan.org/pkg/footnote\"><strong>FOOTNOTE</strong></a> from Mark Wood­ing by adding this line to the YAML section of the R Markdown document:</p>\n<p> usepackage{footnote}</p>\n<p>The footnote package provides features to generate footnotes using roman numerals (lower and upper case), alphabetic characters (lower and upper case) or even the sequence of nine keyboard symbols pictured here. <a href=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/FootnoteSymbols-150x150.png\"></a>Be careful using the symbols because more than nine footnotes will generate an error message. To select the footnote option to use, at the beginning of the R Markdown document, or at least before the first footnote, add the appropriate command from this list:</p>\n<p>renewcommand{thefootnote}{fnsymbol{footnote}} - symbols in the picture above<br>renewcommand{thefootnote}{arabic{footnote}} - the standard footnote pattern: 1,2,3,etc<br>renewcommand{thefootnote}{Roman{footnote}} - upper case Roman numerals (I,II, III, IV, etc)<br>renewcommand{thefootnote}{roman{footnote}} - lower case Roman numerals (i,ii,iii,iv, etc)<br>renewcommand{thefootnote}{Alph{footnote}} - upper case characters (A,B,C,etc)<br>renewcommand{thefootnote}{alph{footnote}} - lower case characters (a,b,c, etc)</p>\n<p>Turning off the superscript for footnotes completely can be accomplished by including this command at the beginning of the document:</p>\n<p>letthefootnoterelaxfootnote</p>\n<h5 id=\"LATEX-PACKAGE-FOOTMISC-PROVIDES-MORE-OPTIONS\"><a href=\"#LATEX-PACKAGE-FOOTMISC-PROVIDES-MORE-OPTIONS\" class=\"headerlink\" title=\"LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS\"></a>LATEX PACKAGE FOOTMISC PROVIDES MORE OPTIONS</h5><p>Originally created by Robin Fair­bairns, the <a href=\"https://ctan.org/pkg/footmisc\"><strong>FOOTMISC</strong></a> package in LaTeX modifies the standard footnote options, and adds some new features. Some of the more interesting ones are highlighted below, but check out the documentation at the link above for more. Suppress the printing of the traditional horizontal line at the bottom of the page before footnotes can be turned on by inserting the following near the top of the R Markdown document:</p>\n<p>renewcommandfootnoterule{}</p>\n<p>Another interesting option in <strong>FOOTMISC</strong> is the inclusion of several new symbol sets for footnote notation. To use one of the predefined symbol sets (brighthurst, chicago, or wiley) that removes the duplicate entries from the original set and adds new ones, use these commands:</p>\n<p>setfnsymbol{symbol set name, e.g. chicago}<br>renewcommand{thefootnote}{fnsymbol{footnote}}</p>\n<p>In LaTeX, the package allows the definition of new sets, although I have not been able to get it to work in R Markdown. Keep in mind that the formatting options chosen for the footnote will remain consistent throughout the document. There are also additional packages available in LaTeX to provide more control over footnote formatting, including: footbib and footmisx. Check the CTAN.ORG website for more information.</p>"},{"title":"NAT:HNS error with Docker SQL Server container on Windows10","id":"3599","comments":0,"date":"2017-08-04T18:29:27.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png)](http://edpflager.com/?attachment_id=3290#main)As with many things I post here, this article was the result of a problem I encountered, and how to resolve or work around it. I have been working with Docker on Windows and was attempting to run a container provided by Microsoft that included SQL Server. The process is fairly easy, just pull the image and run it with the appropriate command options. Unfortunately, that didn't work. When I used the supplied command to run the container, I would get this error message: Error response from daemon: failed to create endpoint <container name> on network nat: HNS failed with error. Unspecified error. I spent several hours working this out over the past few days, doing web searches and trying various ideas. Lots of people experiencing the problem, but no solutions. Eventually I figured out a solution. Hopefully this helps someone else. The error above I found out indicates a problem with the default 'nat' network that Docker creates when you install it.  By using the default NAT network, you basically have a private network on your docker host. You can attempt to delete that network by opening PowerShell as Administrator. (Microsoft has a good write-up on networking for Windows containers [here](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-networking).) Enter the following command to see a list of the virtual networks defined on your PC:\n\nGet-NetNat\n\nStop Docker using the whale icon on your notification area, by right clicking on the whale and choosing Quit Docker from the menu. Then run this command:\n\nRemove-NetNat\n\nEnter \"Y\" at the prompt to remove all networks. Now restart Docker.  If this doesn't work for you, read on.\n<!-- more -->\nSo to work around the issue with the default network, we can create a new network for the container to use. The transparent driver type that I specify here starts your container directly connected to the same physical network your host is on. You can assign a static IP address to containers running on this network, or allow DHCP to assign them an IP address. Be aware of the security implications of doing this, including the possibility of malware and viruses. To create the new network, first make sure your Docker environment is setup to run Windows containers and not Linux (the default option). Check this by right clicking the whale icon in your notification panel on the task bar, and looking for this option:[![](http://edpflager.com/wp-content/uploads/2017/08/environment.png)](http://edpflager.com/?attachment_id=3605#main) If it says Switch to Windows containers, select it before you go any further. If it shows like above, you are good to continue. Run this command to create your new network:\n\ndocker network create -d transparent <name for network>\n\nVery quickly, you see a long character string that was assigned to your new network. Enter this command and make sure your new network shows up in the list (in my case, the new network is named NEWNET):\n\ndocker network ls\n\n[![](http://edpflager.com/wp-content/uploads/2017/08/newnet.png)](http://edpflager.com/?attachment_id=3606#main) Now pull a copy of the container down with:\n\ndocker pull microsoft/mssql-server-windows-developer\n\nThen run it with this command to have it use your new network name :\n\ndocker run -d --network=transparent -p 1433:1433 -e sa=<your\\_sa\\_password> -e ACCEPT\\_EULA= Y --name=<NET NETWORK NAME>  microsoft/mssql-server-windows-developer\n\nOnce it completes starting up, get the container ID with the PS command (It would nice if you could use the name you supplied in the start up command for this next step, but I've had no luck with it):\n\ndocker ps -a\n\n[![](http://edpflager.com/wp-content/uploads/2017/08/container_id.png)](http://edpflager.com/?attachment_id=3601#main) With the container ID noted or copied to the clipboard, open up SQL Server Management Studio and input the container ID for the server name, switch Authentication to SQL Server Authentication, enter sa in the Login box, and then enter the password you defined as part of the Docker run command. Click the Connect button, and if all goes well, Management Studio will connect to the SQL Server in your container. [ ![](http://edpflager.com/wp-content/uploads/2017/08/mantstudio.png)](http://edpflager.com/?attachment_id=3602#main) There are additional command switched you can add to make additional databases available outside of your container, but I leave that for you to work out on your own,","source":"_posts/nathns-error-with-docker-sql-server-container-on-windows10.md","raw":"---\ntitle: 'NAT:HNS error with Docker SQL Server container on Windows10'\ntags:\n  - guides\n  - How-to\n  - howto\n  - 'nat: HNS failed'\n  - technical\n  - Windows\nid: '3599'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Docker\ncomments: false\ndate: 2017-08-04 14:29:27\n---\n\n[![](http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png)](http://edpflager.com/?attachment_id=3290#main)As with many things I post here, this article was the result of a problem I encountered, and how to resolve or work around it. I have been working with Docker on Windows and was attempting to run a container provided by Microsoft that included SQL Server. The process is fairly easy, just pull the image and run it with the appropriate command options. Unfortunately, that didn't work. When I used the supplied command to run the container, I would get this error message: Error response from daemon: failed to create endpoint <container name> on network nat: HNS failed with error. Unspecified error. I spent several hours working this out over the past few days, doing web searches and trying various ideas. Lots of people experiencing the problem, but no solutions. Eventually I figured out a solution. Hopefully this helps someone else. The error above I found out indicates a problem with the default 'nat' network that Docker creates when you install it.  By using the default NAT network, you basically have a private network on your docker host. You can attempt to delete that network by opening PowerShell as Administrator. (Microsoft has a good write-up on networking for Windows containers [here](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-networking).) Enter the following command to see a list of the virtual networks defined on your PC:\n\nGet-NetNat\n\nStop Docker using the whale icon on your notification area, by right clicking on the whale and choosing Quit Docker from the menu. Then run this command:\n\nRemove-NetNat\n\nEnter \"Y\" at the prompt to remove all networks. Now restart Docker.  If this doesn't work for you, read on.\n<!-- more -->\nSo to work around the issue with the default network, we can create a new network for the container to use. The transparent driver type that I specify here starts your container directly connected to the same physical network your host is on. You can assign a static IP address to containers running on this network, or allow DHCP to assign them an IP address. Be aware of the security implications of doing this, including the possibility of malware and viruses. To create the new network, first make sure your Docker environment is setup to run Windows containers and not Linux (the default option). Check this by right clicking the whale icon in your notification panel on the task bar, and looking for this option:[![](http://edpflager.com/wp-content/uploads/2017/08/environment.png)](http://edpflager.com/?attachment_id=3605#main) If it says Switch to Windows containers, select it before you go any further. If it shows like above, you are good to continue. Run this command to create your new network:\n\ndocker network create -d transparent <name for network>\n\nVery quickly, you see a long character string that was assigned to your new network. Enter this command and make sure your new network shows up in the list (in my case, the new network is named NEWNET):\n\ndocker network ls\n\n[![](http://edpflager.com/wp-content/uploads/2017/08/newnet.png)](http://edpflager.com/?attachment_id=3606#main) Now pull a copy of the container down with:\n\ndocker pull microsoft/mssql-server-windows-developer\n\nThen run it with this command to have it use your new network name :\n\ndocker run -d --network=transparent -p 1433:1433 -e sa=<your\\_sa\\_password> -e ACCEPT\\_EULA= Y --name=<NET NETWORK NAME>  microsoft/mssql-server-windows-developer\n\nOnce it completes starting up, get the container ID with the PS command (It would nice if you could use the name you supplied in the start up command for this next step, but I've had no luck with it):\n\ndocker ps -a\n\n[![](http://edpflager.com/wp-content/uploads/2017/08/container_id.png)](http://edpflager.com/?attachment_id=3601#main) With the container ID noted or copied to the clipboard, open up SQL Server Management Studio and input the container ID for the server name, switch Authentication to SQL Server Authentication, enter sa in the Login box, and then enter the password you defined as part of the Docker run command. Click the Connect button, and if all goes well, Management Studio will connect to the SQL Server in your container. [ ![](http://edpflager.com/wp-content/uploads/2017/08/mantstudio.png)](http://edpflager.com/?attachment_id=3602#main) There are additional command switched you can add to make additional databases available outside of your container, but I leave that for you to work out on your own,","slug":"nathns-error-with-docker-sql-server-container-on-windows10","published":1,"updated":"2020-08-23T20:54:35.114Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4m006esdjxck5kfsph","content":"<p><a href=\"http://edpflager.com/?attachment_id=3290#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png\"></a>As with many things I post here, this article was the result of a problem I encountered, and how to resolve or work around it. I have been working with Docker on Windows and was attempting to run a container provided by Microsoft that included SQL Server. The process is fairly easy, just pull the image and run it with the appropriate command options. Unfortunately, that didn’t work. When I used the supplied command to run the container, I would get this error message: Error response from daemon: failed to create endpoint <container name> on network nat: HNS failed with error. Unspecified error. I spent several hours working this out over the past few days, doing web searches and trying various ideas. Lots of people experiencing the problem, but no solutions. Eventually I figured out a solution. Hopefully this helps someone else. The error above I found out indicates a problem with the default ‘nat’ network that Docker creates when you install it.  By using the default NAT network, you basically have a private network on your docker host. You can attempt to delete that network by opening PowerShell as Administrator. (Microsoft has a good write-up on networking for Windows containers <a href=\"https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-networking\">here</a>.) Enter the following command to see a list of the virtual networks defined on your PC:</p>\n<p>Get-NetNat</p>\n<p>Stop Docker using the whale icon on your notification area, by right clicking on the whale and choosing Quit Docker from the menu. Then run this command:</p>\n<p>Remove-NetNat</p>\n<p>Enter “Y” at the prompt to remove all networks. Now restart Docker.  If this doesn’t work for you, read on.</p>\n<a id=\"more\"></a>\n<p>So to work around the issue with the default network, we can create a new network for the container to use. The transparent driver type that I specify here starts your container directly connected to the same physical network your host is on. You can assign a static IP address to containers running on this network, or allow DHCP to assign them an IP address. Be aware of the security implications of doing this, including the possibility of malware and viruses. To create the new network, first make sure your Docker environment is setup to run Windows containers and not Linux (the default option). Check this by right clicking the whale icon in your notification panel on the task bar, and looking for this option:<a href=\"http://edpflager.com/?attachment_id=3605#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/environment.png\"></a> If it says Switch to Windows containers, select it before you go any further. If it shows like above, you are good to continue. Run this command to create your new network:</p>\n<p>docker network create -d transparent <name for network></p>\n<p>Very quickly, you see a long character string that was assigned to your new network. Enter this command and make sure your new network shows up in the list (in my case, the new network is named NEWNET):</p>\n<p>docker network ls</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3606#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/newnet.png\"></a> Now pull a copy of the container down with:</p>\n<p>docker pull microsoft/mssql-server-windows-developer</p>\n<p>Then run it with this command to have it use your new network name :</p>\n<p>docker run -d –network=transparent -p 1433:1433 -e sa=&lt;your_sa_password&gt; -e ACCEPT_EULA= Y –name=<NET NETWORK NAME>  microsoft/mssql-server-windows-developer</p>\n<p>Once it completes starting up, get the container ID with the PS command (It would nice if you could use the name you supplied in the start up command for this next step, but I’ve had no luck with it):</p>\n<p>docker ps -a</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3601#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/container_id.png\"></a> With the container ID noted or copied to the clipboard, open up SQL Server Management Studio and input the container ID for the server name, switch Authentication to SQL Server Authentication, enter sa in the Login box, and then enter the password you defined as part of the Docker run command. Click the Connect button, and if all goes well, Management Studio will connect to the SQL Server in your container. <a href=\"http://edpflager.com/?attachment_id=3602#main\"> <img src=\"http://edpflager.com/wp-content/uploads/2017/08/mantstudio.png\"></a> There are additional command switched you can add to make additional databases available outside of your container, but I leave that for you to work out on your own,</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3290#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png\"></a>As with many things I post here, this article was the result of a problem I encountered, and how to resolve or work around it. I have been working with Docker on Windows and was attempting to run a container provided by Microsoft that included SQL Server. The process is fairly easy, just pull the image and run it with the appropriate command options. Unfortunately, that didn’t work. When I used the supplied command to run the container, I would get this error message: Error response from daemon: failed to create endpoint <container name> on network nat: HNS failed with error. Unspecified error. I spent several hours working this out over the past few days, doing web searches and trying various ideas. Lots of people experiencing the problem, but no solutions. Eventually I figured out a solution. Hopefully this helps someone else. The error above I found out indicates a problem with the default ‘nat’ network that Docker creates when you install it.  By using the default NAT network, you basically have a private network on your docker host. You can attempt to delete that network by opening PowerShell as Administrator. (Microsoft has a good write-up on networking for Windows containers <a href=\"https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-networking\">here</a>.) Enter the following command to see a list of the virtual networks defined on your PC:</p>\n<p>Get-NetNat</p>\n<p>Stop Docker using the whale icon on your notification area, by right clicking on the whale and choosing Quit Docker from the menu. Then run this command:</p>\n<p>Remove-NetNat</p>\n<p>Enter “Y” at the prompt to remove all networks. Now restart Docker.  If this doesn’t work for you, read on.</p>","more":"<p>So to work around the issue with the default network, we can create a new network for the container to use. The transparent driver type that I specify here starts your container directly connected to the same physical network your host is on. You can assign a static IP address to containers running on this network, or allow DHCP to assign them an IP address. Be aware of the security implications of doing this, including the possibility of malware and viruses. To create the new network, first make sure your Docker environment is setup to run Windows containers and not Linux (the default option). Check this by right clicking the whale icon in your notification panel on the task bar, and looking for this option:<a href=\"http://edpflager.com/?attachment_id=3605#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/environment.png\"></a> If it says Switch to Windows containers, select it before you go any further. If it shows like above, you are good to continue. Run this command to create your new network:</p>\n<p>docker network create -d transparent <name for network></p>\n<p>Very quickly, you see a long character string that was assigned to your new network. Enter this command and make sure your new network shows up in the list (in my case, the new network is named NEWNET):</p>\n<p>docker network ls</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3606#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/newnet.png\"></a> Now pull a copy of the container down with:</p>\n<p>docker pull microsoft/mssql-server-windows-developer</p>\n<p>Then run it with this command to have it use your new network name :</p>\n<p>docker run -d –network=transparent -p 1433:1433 -e sa=&lt;your_sa_password&gt; -e ACCEPT_EULA= Y –name=<NET NETWORK NAME>  microsoft/mssql-server-windows-developer</p>\n<p>Once it completes starting up, get the container ID with the PS command (It would nice if you could use the name you supplied in the start up command for this next step, but I’ve had no luck with it):</p>\n<p>docker ps -a</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3601#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/08/container_id.png\"></a> With the container ID noted or copied to the clipboard, open up SQL Server Management Studio and input the container ID for the server name, switch Authentication to SQL Server Authentication, enter sa in the Login box, and then enter the password you defined as part of the Docker run command. Click the Connect button, and if all goes well, Management Studio will connect to the SQL Server in your container. <a href=\"http://edpflager.com/?attachment_id=3602#main\"> <img src=\"http://edpflager.com/wp-content/uploads/2017/08/mantstudio.png\"></a> There are additional command switched you can add to make additional databases available outside of your container, but I leave that for you to work out on your own,</p>"},{"title":"NixNote (Evernote client) on Linux","id":"2072","comments":0,"date":"2014-05-12T07:57:10.000Z","_content":"\n[![Nixnote](http://edpflager.com/wp-content/uploads/2014/05/Nixnote-300x115.png)](http://edpflager.com/wp-content/uploads/2014/05/Nixnote.png)Evernote is a cloud based service that allows you to take and share notes across all of your devices - Android, iPhones and iPads, Windows and Windows phones, and Mac OSX. Its very handy, allowing you to jot down notes when you think of them and pull them up later when you need them. I often use it to make notes for this blog on my phone and then pull them up later on my laptop when I want to work on posts. But did you notice the glaring omission in that list of products? Linux. If you are running Ubuntu, you can install [Everpad](https://github.com/nvbn/everpad) to provide a client to access your Evernote account. Everpad's wiki provides some instructions on getting it to run on other distros. You can access your account via the Everpad website as well, but I'd still rather have a dedicated client. I keep a lot of links and instructions for installations in my Everpad account, and copying and pasting is much handier than typing everything. Recently I found [NixNote](http://nevernote.sourceforge.net/) (aka NeverNote), an open source client for accessing Evernote. I experimented with it, and got it running in CentOS with only a couple of dependent packages. Below are the instructions for running NixNote on CentOS 6.5.\n<!-- more -->\n1.  Open the Add/Remove Software application and search for \"TermReadKey\". Install the one package that should result - perl-TermReadKey-2.30-13.el6 (x86-64) currently.\n2.  In Add/Remove Software search for openssl. In the results, locate these two packages and install them, if they aren't already installed: \"General purpose cryptography library with TLS implementation (openssl-1.0.1e-16.el6\\_5.7 (x86\\_64)\" \"Files for development of applications which use OpenSSL (openssl-devel-1.0.1e-16.el6\\_5.7 (x86\\_64)\" - this will require 6 additional packages to be installed as well.\n3.  Download the nixnote rpm package from the [NixNote sourceforge site](http://sourceforge.net/projects/nevernote/files/). Version 1.6 is the most current stable version.\n4.  Open a terminal window, and switch to the root user account (su -).\n5.  Navigate to where you downloaded the RPM package and run this command: **rpm -ivh nixnote-1.6-2.x86\\_64.rpm**\n6.  It will install very quickly, and in your menu bar, under Applications | Internet you should see an option for NixNote.\n7.  Start NixNote and in the menu bar click on Tools -> Connect. A window will appear asking you to grant access to Evernote for NixNote.\n8.  If you already have an Evernote account, sign in. If not click the option to create a new account.\n9.  You now have access to your Evernote account from your Linux machine!","source":"_posts/nixnote-evernote-client-on-linux.md","raw":"---\ntitle: NixNote (Evernote client) on Linux\ntags:\n  - howto\n  - install\nid: '2072'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2014-05-12 03:57:10\n---\n\n[![Nixnote](http://edpflager.com/wp-content/uploads/2014/05/Nixnote-300x115.png)](http://edpflager.com/wp-content/uploads/2014/05/Nixnote.png)Evernote is a cloud based service that allows you to take and share notes across all of your devices - Android, iPhones and iPads, Windows and Windows phones, and Mac OSX. Its very handy, allowing you to jot down notes when you think of them and pull them up later when you need them. I often use it to make notes for this blog on my phone and then pull them up later on my laptop when I want to work on posts. But did you notice the glaring omission in that list of products? Linux. If you are running Ubuntu, you can install [Everpad](https://github.com/nvbn/everpad) to provide a client to access your Evernote account. Everpad's wiki provides some instructions on getting it to run on other distros. You can access your account via the Everpad website as well, but I'd still rather have a dedicated client. I keep a lot of links and instructions for installations in my Everpad account, and copying and pasting is much handier than typing everything. Recently I found [NixNote](http://nevernote.sourceforge.net/) (aka NeverNote), an open source client for accessing Evernote. I experimented with it, and got it running in CentOS with only a couple of dependent packages. Below are the instructions for running NixNote on CentOS 6.5.\n<!-- more -->\n1.  Open the Add/Remove Software application and search for \"TermReadKey\". Install the one package that should result - perl-TermReadKey-2.30-13.el6 (x86-64) currently.\n2.  In Add/Remove Software search for openssl. In the results, locate these two packages and install them, if they aren't already installed: \"General purpose cryptography library with TLS implementation (openssl-1.0.1e-16.el6\\_5.7 (x86\\_64)\" \"Files for development of applications which use OpenSSL (openssl-devel-1.0.1e-16.el6\\_5.7 (x86\\_64)\" - this will require 6 additional packages to be installed as well.\n3.  Download the nixnote rpm package from the [NixNote sourceforge site](http://sourceforge.net/projects/nevernote/files/). Version 1.6 is the most current stable version.\n4.  Open a terminal window, and switch to the root user account (su -).\n5.  Navigate to where you downloaded the RPM package and run this command: **rpm -ivh nixnote-1.6-2.x86\\_64.rpm**\n6.  It will install very quickly, and in your menu bar, under Applications | Internet you should see an option for NixNote.\n7.  Start NixNote and in the menu bar click on Tools -> Connect. A window will appear asking you to grant access to Evernote for NixNote.\n8.  If you already have an Evernote account, sign in. If not click the option to create a new account.\n9.  You now have access to your Evernote account from your Linux machine!","slug":"nixnote-evernote-client-on-linux","published":1,"updated":"2020-08-23T20:54:34.842Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4r006isdjx04r0cg0z","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/Nixnote.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/Nixnote-300x115.png\" alt=\"Nixnote\"></a>Evernote is a cloud based service that allows you to take and share notes across all of your devices - Android, iPhones and iPads, Windows and Windows phones, and Mac OSX. Its very handy, allowing you to jot down notes when you think of them and pull them up later when you need them. I often use it to make notes for this blog on my phone and then pull them up later on my laptop when I want to work on posts. But did you notice the glaring omission in that list of products? Linux. If you are running Ubuntu, you can install <a href=\"https://github.com/nvbn/everpad\">Everpad</a> to provide a client to access your Evernote account. Everpad’s wiki provides some instructions on getting it to run on other distros. You can access your account via the Everpad website as well, but I’d still rather have a dedicated client. I keep a lot of links and instructions for installations in my Everpad account, and copying and pasting is much handier than typing everything. Recently I found <a href=\"http://nevernote.sourceforge.net/\">NixNote</a> (aka NeverNote), an open source client for accessing Evernote. I experimented with it, and got it running in CentOS with only a couple of dependent packages. Below are the instructions for running NixNote on CentOS 6.5.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Open the Add/Remove Software application and search for “TermReadKey”. Install the one package that should result - perl-TermReadKey-2.30-13.el6 (x86-64) currently.</li>\n<li>In Add/Remove Software search for openssl. In the results, locate these two packages and install them, if they aren’t already installed: “General purpose cryptography library with TLS implementation (openssl-1.0.1e-16.el6_5.7 (x86_64)” “Files for development of applications which use OpenSSL (openssl-devel-1.0.1e-16.el6_5.7 (x86_64)” - this will require 6 additional packages to be installed as well.</li>\n<li>Download the nixnote rpm package from the <a href=\"http://sourceforge.net/projects/nevernote/files/\">NixNote sourceforge site</a>. Version 1.6 is the most current stable version.</li>\n<li>Open a terminal window, and switch to the root user account (su -).</li>\n<li>Navigate to where you downloaded the RPM package and run this command: <strong>rpm -ivh nixnote-1.6-2.x86_64.rpm</strong></li>\n<li>It will install very quickly, and in your menu bar, under Applications | Internet you should see an option for NixNote.</li>\n<li>Start NixNote and in the menu bar click on Tools -&gt; Connect. A window will appear asking you to grant access to Evernote for NixNote.</li>\n<li>If you already have an Evernote account, sign in. If not click the option to create a new account.</li>\n<li>You now have access to your Evernote account from your Linux machine!</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/Nixnote.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/Nixnote-300x115.png\" alt=\"Nixnote\"></a>Evernote is a cloud based service that allows you to take and share notes across all of your devices - Android, iPhones and iPads, Windows and Windows phones, and Mac OSX. Its very handy, allowing you to jot down notes when you think of them and pull them up later when you need them. I often use it to make notes for this blog on my phone and then pull them up later on my laptop when I want to work on posts. But did you notice the glaring omission in that list of products? Linux. If you are running Ubuntu, you can install <a href=\"https://github.com/nvbn/everpad\">Everpad</a> to provide a client to access your Evernote account. Everpad’s wiki provides some instructions on getting it to run on other distros. You can access your account via the Everpad website as well, but I’d still rather have a dedicated client. I keep a lot of links and instructions for installations in my Everpad account, and copying and pasting is much handier than typing everything. Recently I found <a href=\"http://nevernote.sourceforge.net/\">NixNote</a> (aka NeverNote), an open source client for accessing Evernote. I experimented with it, and got it running in CentOS with only a couple of dependent packages. Below are the instructions for running NixNote on CentOS 6.5.</p>","more":"<ol>\n<li>Open the Add/Remove Software application and search for “TermReadKey”. Install the one package that should result - perl-TermReadKey-2.30-13.el6 (x86-64) currently.</li>\n<li>In Add/Remove Software search for openssl. In the results, locate these two packages and install them, if they aren’t already installed: “General purpose cryptography library with TLS implementation (openssl-1.0.1e-16.el6_5.7 (x86_64)” “Files for development of applications which use OpenSSL (openssl-devel-1.0.1e-16.el6_5.7 (x86_64)” - this will require 6 additional packages to be installed as well.</li>\n<li>Download the nixnote rpm package from the <a href=\"http://sourceforge.net/projects/nevernote/files/\">NixNote sourceforge site</a>. Version 1.6 is the most current stable version.</li>\n<li>Open a terminal window, and switch to the root user account (su -).</li>\n<li>Navigate to where you downloaded the RPM package and run this command: <strong>rpm -ivh nixnote-1.6-2.x86_64.rpm</strong></li>\n<li>It will install very quickly, and in your menu bar, under Applications | Internet you should see an option for NixNote.</li>\n<li>Start NixNote and in the menu bar click on Tools -&gt; Connect. A window will appear asking you to grant access to Evernote for NixNote.</li>\n<li>If you already have an Evernote account, sign in. If not click the option to create a new account.</li>\n<li>You now have access to your Evernote account from your Linux machine!</li>\n</ol>"},{"title":"Not dead but dreaming.. :)","id":"2957","comments":0,"date":"2015-09-06T17:20:27.000Z","_content":"\nGreetings! Its been almost a month since I posted anything. That's not because I've been inordinately busy at work or at home, just didn't have anything ready to post. Just as a teaser, I've been experimenting with Docker, using Pentaho and MongoDB together, and switched my Dell Inspiron from Ubuntu to Linux Mint. So hopefully, I'll have something useful to post soon.","source":"_posts/not-dead-but-dreaming.md","raw":"---\ntitle: 'Not dead but dreaming.. :)'\ntags: []\nid: '2957'\ncategories:\n  - - Blog\ncomments: false\ndate: 2015-09-06 13:20:27\n---\n\nGreetings! Its been almost a month since I posted anything. That's not because I've been inordinately busy at work or at home, just didn't have anything ready to post. Just as a teaser, I've been experimenting with Docker, using Pentaho and MongoDB together, and switched my Dell Inspiron from Ubuntu to Linux Mint. So hopefully, I'll have something useful to post soon.","slug":"not-dead-but-dreaming","published":1,"updated":"2020-08-23T20:54:34.966Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a4w006lsdjx9qn73o3h","content":"<p>Greetings! Its been almost a month since I posted anything. That’s not because I’ve been inordinately busy at work or at home, just didn’t have anything ready to post. Just as a teaser, I’ve been experimenting with Docker, using Pentaho and MongoDB together, and switched my Dell Inspiron from Ubuntu to Linux Mint. So hopefully, I’ll have something useful to post soon.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Greetings! Its been almost a month since I posted anything. That’s not because I’ve been inordinately busy at work or at home, just didn’t have anything ready to post. Just as a teaser, I’ve been experimenting with Docker, using Pentaho and MongoDB together, and switched my Dell Inspiron from Ubuntu to Linux Mint. So hopefully, I’ll have something useful to post soon.</p>\n"},{"title":"Off the Grid","id":"291","comments":0,"date":"2009-05-27T12:43:28.000Z","_content":"\nI am off the Internet starting later today for the better part of two weeks. Traveling to Ethiopia! I'll post pictures when I can, folks. :) Thanks for stopping by.","source":"_posts/off-the-grid.md","raw":"---\ntitle: Off the Grid\ntags: []\nid: '291'\ncategories:\n  - - Blog\ncomments: false\ndate: 2009-05-27 08:43:28\n---\n\nI am off the Internet starting later today for the better part of two weeks. Traveling to Ethiopia! I'll post pictures when I can, folks. :) Thanks for stopping by.","slug":"off-the-grid","published":1,"updated":"2020-08-23T20:54:34.702Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a50006psdjx1bmedvym","content":"<p>I am off the Internet starting later today for the better part of two weeks. Traveling to Ethiopia! I’ll post pictures when I can, folks. :) Thanks for stopping by.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>I am off the Internet starting later today for the better part of two weeks. Traveling to Ethiopia! I’ll post pictures when I can, folks. :) Thanks for stopping by.</p>\n"},{"title":"PDI Kettle - Delete File(s)","id":"1918","comments":0,"date":"2014-04-04T20:06:44.000Z","_content":"\n[![deletefilejob](http://edpflager.com/wp-content/uploads/2014/04/deletefilejob-300x76.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefilejob.png)When working on ETL flows, its sometimes useful to store information in temporary files as long as you clean those files up when you are finished. Pentaho Data Integration (aka Kettle or PDI) has two steps for deleting file(s) - one handles a single file, and one handles multiple files. Both are in the File Management section of the Design node in the Job designer.\n<!-- more -->\n### Single files\n\n[![deletefile](http://edpflager.com/wp-content/uploads/2014/04/deletefile-300x100.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefile.png)If you have a single file that you need to delete as part of your workflow and you use the same name over and over every time your job runs, then deleting it is pretty straightforward. For a single file, drag and drop the Delete a file... job entry onto your canvas. Open the step up, and enter a name for it. If the file is created/stored on the computer you are developing on, you can navigate to it by clicking the Browse button. If you are developing on one computer, but the file will be stored somewhere else, you can supply a UNC path to the folder in the format of: \\\\computernamesharefoldersubfolderfilename. (the folder and subfolder are optional if the file is in the root of the share).  The exact notation will vary depending on whether you are working on Windows or Linux or Mac. Finally, you may choose to fail the step if the file does not exist by checking the box supplied. Click OK and wire up the job with a START and Success job entry, and you are finished.\n\n###  Delete multiple distinct files\n\n[![deletefiles](http://edpflager.com/wp-content/uploads/2014/04/deletefiles-300x215.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefiles.png) On some of the workflows I create, there may be multiple files of a specific file type created, or the date is appended to the file name because the recipient of the output may store them, or there may be a combination of the two. In that case,  you'll want to use the Delete files... job entry in your work flow. Drag and drop the Delete files... job entry onto your canvas. Open the step up, and enter a name for it. Below that there are a couple of check boxes here that you may use on some occasions:\n\n*   Checking the \"Include Subfolders\" box will delete subfolders as well as the top level folder. This makes it much easier to clean up nested folders.\n*   \"Copy previous results to args?\" will allow you to use filenames that are created as part of a  previous job entry (as part of the result files).\n\nIf your workflow is designed to create a file or multiple files with the same names in the same locations repeatedly, click on the FILE button on the right side, and an Open file window will appear. Navigate to where the file is stored (whether its stored locally or somewhere else on your network), select it and click the Open button. Back in the Delete Files... window, click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each individual file and clicking Add after each.\n\n### Delete multiple files with a pattern\n\nAs I said earlier some of the workflows I create generate temporary files with the same basic name, but they append a date to the end of the file. For example, there may be a file called \"testfile20140403.txt\" generated on one day and one called\"testfile20140404.txt\" generated on the next. In order to have a reusable workflow, I need to use the Wildcard box in the Delete Files... process task. You'll notice that the box is labeled Wildcard (RegExp). This is because it the wildcard expressions you enter in this box have to conform to Regular Expression syntax. I won't delve into the intricacies of RegEx here, other than to provide one brief example. If you need help with the syntax, there are numerous websites that can walk you through it. Returning to my example of \"testfile20140403.txt\" and  \"testfile20140404.txt\", I would first use the Folder button to locate the folder the files are or will be stored in. This lets me navigate through my local machine, or if you are using a remote machine, you can use the UNC path to get to the folder. Once you have it open in the window, click the Open button to return to the Delete Files... job entry window. The path to the folder should now be entered in the FileFolder box. In the box underneath it, you can now enter your RegEx file name. For the example above, I would enter: ^.\\*txt Now click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each folder, adding your RegEx and clicking Add after each. Once you are finished, click OK and wire up the job with a START and Success job entry, and your job is done.","source":"_posts/pdi-kettle-delete-files.md","raw":"---\ntitle: PDI Kettle - Delete File(s)\ntags:\n  - ETL\n  - kettle\n  - PDI\nid: '1918'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-04-04 16:06:44\n---\n\n[![deletefilejob](http://edpflager.com/wp-content/uploads/2014/04/deletefilejob-300x76.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefilejob.png)When working on ETL flows, its sometimes useful to store information in temporary files as long as you clean those files up when you are finished. Pentaho Data Integration (aka Kettle or PDI) has two steps for deleting file(s) - one handles a single file, and one handles multiple files. Both are in the File Management section of the Design node in the Job designer.\n<!-- more -->\n### Single files\n\n[![deletefile](http://edpflager.com/wp-content/uploads/2014/04/deletefile-300x100.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefile.png)If you have a single file that you need to delete as part of your workflow and you use the same name over and over every time your job runs, then deleting it is pretty straightforward. For a single file, drag and drop the Delete a file... job entry onto your canvas. Open the step up, and enter a name for it. If the file is created/stored on the computer you are developing on, you can navigate to it by clicking the Browse button. If you are developing on one computer, but the file will be stored somewhere else, you can supply a UNC path to the folder in the format of: \\\\computernamesharefoldersubfolderfilename. (the folder and subfolder are optional if the file is in the root of the share).  The exact notation will vary depending on whether you are working on Windows or Linux or Mac. Finally, you may choose to fail the step if the file does not exist by checking the box supplied. Click OK and wire up the job with a START and Success job entry, and you are finished.\n\n###  Delete multiple distinct files\n\n[![deletefiles](http://edpflager.com/wp-content/uploads/2014/04/deletefiles-300x215.png)](http://edpflager.com/wp-content/uploads/2014/04/deletefiles.png) On some of the workflows I create, there may be multiple files of a specific file type created, or the date is appended to the file name because the recipient of the output may store them, or there may be a combination of the two. In that case,  you'll want to use the Delete files... job entry in your work flow. Drag and drop the Delete files... job entry onto your canvas. Open the step up, and enter a name for it. Below that there are a couple of check boxes here that you may use on some occasions:\n\n*   Checking the \"Include Subfolders\" box will delete subfolders as well as the top level folder. This makes it much easier to clean up nested folders.\n*   \"Copy previous results to args?\" will allow you to use filenames that are created as part of a  previous job entry (as part of the result files).\n\nIf your workflow is designed to create a file or multiple files with the same names in the same locations repeatedly, click on the FILE button on the right side, and an Open file window will appear. Navigate to where the file is stored (whether its stored locally or somewhere else on your network), select it and click the Open button. Back in the Delete Files... window, click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each individual file and clicking Add after each.\n\n### Delete multiple files with a pattern\n\nAs I said earlier some of the workflows I create generate temporary files with the same basic name, but they append a date to the end of the file. For example, there may be a file called \"testfile20140403.txt\" generated on one day and one called\"testfile20140404.txt\" generated on the next. In order to have a reusable workflow, I need to use the Wildcard box in the Delete Files... process task. You'll notice that the box is labeled Wildcard (RegExp). This is because it the wildcard expressions you enter in this box have to conform to Regular Expression syntax. I won't delve into the intricacies of RegEx here, other than to provide one brief example. If you need help with the syntax, there are numerous websites that can walk you through it. Returning to my example of \"testfile20140403.txt\" and  \"testfile20140404.txt\", I would first use the Folder button to locate the folder the files are or will be stored in. This lets me navigate through my local machine, or if you are using a remote machine, you can use the UNC path to get to the folder. Once you have it open in the window, click the Open button to return to the Delete Files... job entry window. The path to the folder should now be entered in the FileFolder box. In the box underneath it, you can now enter your RegEx file name. For the example above, I would enter: ^.\\*txt Now click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each folder, adding your RegEx and clicking Add after each. Once you are finished, click OK and wire up the job with a START and Success job entry, and your job is done.","slug":"pdi-kettle-delete-files","published":1,"updated":"2020-08-23T20:54:34.810Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a55006ssdjxbs8d52ng","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefilejob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefilejob-300x76.png\" alt=\"deletefilejob\"></a>When working on ETL flows, its sometimes useful to store information in temporary files as long as you clean those files up when you are finished. Pentaho Data Integration (aka Kettle or PDI) has two steps for deleting file(s) - one handles a single file, and one handles multiple files. Both are in the File Management section of the Design node in the Job designer.</p>\n<a id=\"more\"></a>\n<h3 id=\"Single-files\"><a href=\"#Single-files\" class=\"headerlink\" title=\"Single files\"></a>Single files</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefile.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefile-300x100.png\" alt=\"deletefile\"></a>If you have a single file that you need to delete as part of your workflow and you use the same name over and over every time your job runs, then deleting it is pretty straightforward. For a single file, drag and drop the Delete a file… job entry onto your canvas. Open the step up, and enter a name for it. If the file is created/stored on the computer you are developing on, you can navigate to it by clicking the Browse button. If you are developing on one computer, but the file will be stored somewhere else, you can supply a UNC path to the folder in the format of: \\computernamesharefoldersubfolderfilename. (the folder and subfolder are optional if the file is in the root of the share).  The exact notation will vary depending on whether you are working on Windows or Linux or Mac. Finally, you may choose to fail the step if the file does not exist by checking the box supplied. Click OK and wire up the job with a START and Success job entry, and you are finished.</p>\n<h3 id=\"Delete-multiple-distinct-files\"><a href=\"#Delete-multiple-distinct-files\" class=\"headerlink\" title=\" Delete multiple distinct files\"></a> Delete multiple distinct files</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefiles-300x215.png\" alt=\"deletefiles\"></a> On some of the workflows I create, there may be multiple files of a specific file type created, or the date is appended to the file name because the recipient of the output may store them, or there may be a combination of the two. In that case,  you’ll want to use the Delete files… job entry in your work flow. Drag and drop the Delete files… job entry onto your canvas. Open the step up, and enter a name for it. Below that there are a couple of check boxes here that you may use on some occasions:</p>\n<ul>\n<li>Checking the “Include Subfolders” box will delete subfolders as well as the top level folder. This makes it much easier to clean up nested folders.</li>\n<li>“Copy previous results to args?” will allow you to use filenames that are created as part of a  previous job entry (as part of the result files).</li>\n</ul>\n<p>If your workflow is designed to create a file or multiple files with the same names in the same locations repeatedly, click on the FILE button on the right side, and an Open file window will appear. Navigate to where the file is stored (whether its stored locally or somewhere else on your network), select it and click the Open button. Back in the Delete Files… window, click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each individual file and clicking Add after each.</p>\n<h3 id=\"Delete-multiple-files-with-a-pattern\"><a href=\"#Delete-multiple-files-with-a-pattern\" class=\"headerlink\" title=\"Delete multiple files with a pattern\"></a>Delete multiple files with a pattern</h3><p>As I said earlier some of the workflows I create generate temporary files with the same basic name, but they append a date to the end of the file. For example, there may be a file called “testfile20140403.txt” generated on one day and one called”testfile20140404.txt” generated on the next. In order to have a reusable workflow, I need to use the Wildcard box in the Delete Files… process task. You’ll notice that the box is labeled Wildcard (RegExp). This is because it the wildcard expressions you enter in this box have to conform to Regular Expression syntax. I won’t delve into the intricacies of RegEx here, other than to provide one brief example. If you need help with the syntax, there are numerous websites that can walk you through it. Returning to my example of “testfile20140403.txt” and  “testfile20140404.txt”, I would first use the Folder button to locate the folder the files are or will be stored in. This lets me navigate through my local machine, or if you are using a remote machine, you can use the UNC path to get to the folder. Once you have it open in the window, click the Open button to return to the Delete Files… job entry window. The path to the folder should now be entered in the FileFolder box. In the box underneath it, you can now enter your RegEx file name. For the example above, I would enter: ^.*txt Now click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each folder, adding your RegEx and clicking Add after each. Once you are finished, click OK and wire up the job with a START and Success job entry, and your job is done.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefilejob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefilejob-300x76.png\" alt=\"deletefilejob\"></a>When working on ETL flows, its sometimes useful to store information in temporary files as long as you clean those files up when you are finished. Pentaho Data Integration (aka Kettle or PDI) has two steps for deleting file(s) - one handles a single file, and one handles multiple files. Both are in the File Management section of the Design node in the Job designer.</p>","more":"<h3 id=\"Single-files\"><a href=\"#Single-files\" class=\"headerlink\" title=\"Single files\"></a>Single files</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefile.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefile-300x100.png\" alt=\"deletefile\"></a>If you have a single file that you need to delete as part of your workflow and you use the same name over and over every time your job runs, then deleting it is pretty straightforward. For a single file, drag and drop the Delete a file… job entry onto your canvas. Open the step up, and enter a name for it. If the file is created/stored on the computer you are developing on, you can navigate to it by clicking the Browse button. If you are developing on one computer, but the file will be stored somewhere else, you can supply a UNC path to the folder in the format of: \\computernamesharefoldersubfolderfilename. (the folder and subfolder are optional if the file is in the root of the share).  The exact notation will vary depending on whether you are working on Windows or Linux or Mac. Finally, you may choose to fail the step if the file does not exist by checking the box supplied. Click OK and wire up the job with a START and Success job entry, and you are finished.</p>\n<h3 id=\"Delete-multiple-distinct-files\"><a href=\"#Delete-multiple-distinct-files\" class=\"headerlink\" title=\" Delete multiple distinct files\"></a> Delete multiple distinct files</h3><p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/deletefiles.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/deletefiles-300x215.png\" alt=\"deletefiles\"></a> On some of the workflows I create, there may be multiple files of a specific file type created, or the date is appended to the file name because the recipient of the output may store them, or there may be a combination of the two. In that case,  you’ll want to use the Delete files… job entry in your work flow. Drag and drop the Delete files… job entry onto your canvas. Open the step up, and enter a name for it. Below that there are a couple of check boxes here that you may use on some occasions:</p>\n<ul>\n<li>Checking the “Include Subfolders” box will delete subfolders as well as the top level folder. This makes it much easier to clean up nested folders.</li>\n<li>“Copy previous results to args?” will allow you to use filenames that are created as part of a  previous job entry (as part of the result files).</li>\n</ul>\n<p>If your workflow is designed to create a file or multiple files with the same names in the same locations repeatedly, click on the FILE button on the right side, and an Open file window will appear. Navigate to where the file is stored (whether its stored locally or somewhere else on your network), select it and click the Open button. Back in the Delete Files… window, click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each individual file and clicking Add after each.</p>\n<h3 id=\"Delete-multiple-files-with-a-pattern\"><a href=\"#Delete-multiple-files-with-a-pattern\" class=\"headerlink\" title=\"Delete multiple files with a pattern\"></a>Delete multiple files with a pattern</h3><p>As I said earlier some of the workflows I create generate temporary files with the same basic name, but they append a date to the end of the file. For example, there may be a file called “testfile20140403.txt” generated on one day and one called”testfile20140404.txt” generated on the next. In order to have a reusable workflow, I need to use the Wildcard box in the Delete Files… process task. You’ll notice that the box is labeled Wildcard (RegExp). This is because it the wildcard expressions you enter in this box have to conform to Regular Expression syntax. I won’t delve into the intricacies of RegEx here, other than to provide one brief example. If you need help with the syntax, there are numerous websites that can walk you through it. Returning to my example of “testfile20140403.txt” and  “testfile20140404.txt”, I would first use the Folder button to locate the folder the files are or will be stored in. This lets me navigate through my local machine, or if you are using a remote machine, you can use the UNC path to get to the folder. Once you have it open in the window, click the Open button to return to the Delete Files… job entry window. The path to the folder should now be entered in the FileFolder box. In the box underneath it, you can now enter your RegEx file name. For the example above, I would enter: ^.*txt Now click the Add button to move your selection down to the list of files to process. If you have more files to add, repeat the process, navigating to each folder, adding your RegEx and clicking Add after each. Once you are finished, click OK and wire up the job with a START and Success job entry, and your job is done.</p>"},{"title":"PDI - Kettle updated to version 5.1","id":"2169","comments":0,"date":"2014-06-27T20:01:35.000Z","_content":"\n[![pdi51](http://edpflager.com/wp-content/uploads/2014/06/pdi51-259x300.png)](http://edpflager.com/wp-content/uploads/2014/06/pdi51.png)While I was lounging around the back roads of Texas last week, Pentaho, Inc. was busy - releasing version 5.1 of Kettle in both Enterprise and Community editions. I've downloaded and installed the CE version on my laptop, and plan to put it through its passes this weekend. [Pentaho's website](https://help.pentaho.com/Documentation/5.1/0T0/0Z0/000) lists the improvements in more detail but here are some of the highlights that look interesting to me. There are new Big Data features including:\n<!-- more -->\n*   support for current versions of a couple of NoSQL platforms - MongoDB (2.6) and Cassandra (2.0),\n*   improved YARN (Hadoop v2) functionality allowing PDI to interact with  more recent versions of several Hadoop platforms Cloudera (version 5), MapR (version 3.1) and HortonWorks (version 2.1) as well as with over a dozen other Hadoop platforms\n*   and the ability to execute transforms on YARN clusters in parallel using the data nodes of the Hadoop clusters, rather than processing the data on the PDI box. This should provide a large performance boast since the data will be processed where it resides, rather than pulling it across the network.\n\nDocumentation is being improved! This faces one of the biggest issues I see with open source software. While functionality of OSS many times is head and shoulders above its commercial counterparts, the poor quality or complete lack of documentation turns off a lot of potential users. Lets hope it continues! Security features have been added, and one of the biggest is the ability to grant or restrict permission to run transformations/jobs by role. In many large scale environments subject to Sarbanes Oxley or other regulations, developers may not be given the ability to run code on production data systems. Now Pentaho has implemented that level of security for ETL developers as well. There are quite a few other improvements, that I won't go into here, but do check out Pentaho's website for more information.","source":"_posts/pdi-kettle-updated-to-version-5-1.md","raw":"---\ntitle: PDI - Kettle updated to version 5.1\ntags:\n  - ETL\n  - external article\n  - HortonWorks\n  - PDI\n  - technical\nid: '2169'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-06-27 16:01:35\n---\n\n[![pdi51](http://edpflager.com/wp-content/uploads/2014/06/pdi51-259x300.png)](http://edpflager.com/wp-content/uploads/2014/06/pdi51.png)While I was lounging around the back roads of Texas last week, Pentaho, Inc. was busy - releasing version 5.1 of Kettle in both Enterprise and Community editions. I've downloaded and installed the CE version on my laptop, and plan to put it through its passes this weekend. [Pentaho's website](https://help.pentaho.com/Documentation/5.1/0T0/0Z0/000) lists the improvements in more detail but here are some of the highlights that look interesting to me. There are new Big Data features including:\n<!-- more -->\n*   support for current versions of a couple of NoSQL platforms - MongoDB (2.6) and Cassandra (2.0),\n*   improved YARN (Hadoop v2) functionality allowing PDI to interact with  more recent versions of several Hadoop platforms Cloudera (version 5), MapR (version 3.1) and HortonWorks (version 2.1) as well as with over a dozen other Hadoop platforms\n*   and the ability to execute transforms on YARN clusters in parallel using the data nodes of the Hadoop clusters, rather than processing the data on the PDI box. This should provide a large performance boast since the data will be processed where it resides, rather than pulling it across the network.\n\nDocumentation is being improved! This faces one of the biggest issues I see with open source software. While functionality of OSS many times is head and shoulders above its commercial counterparts, the poor quality or complete lack of documentation turns off a lot of potential users. Lets hope it continues! Security features have been added, and one of the biggest is the ability to grant or restrict permission to run transformations/jobs by role. In many large scale environments subject to Sarbanes Oxley or other regulations, developers may not be given the ability to run code on production data systems. Now Pentaho has implemented that level of security for ETL developers as well. There are quite a few other improvements, that I won't go into here, but do check out Pentaho's website for more information.","slug":"pdi-kettle-updated-to-version-5-1","published":1,"updated":"2020-08-23T20:54:34.870Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a5a006wsdjxgkqcaymw","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/06/pdi51.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/pdi51-259x300.png\" alt=\"pdi51\"></a>While I was lounging around the back roads of Texas last week, Pentaho, Inc. was busy - releasing version 5.1 of Kettle in both Enterprise and Community editions. I’ve downloaded and installed the CE version on my laptop, and plan to put it through its passes this weekend. <a href=\"https://help.pentaho.com/Documentation/5.1/0T0/0Z0/000\">Pentaho’s website</a> lists the improvements in more detail but here are some of the highlights that look interesting to me. There are new Big Data features including:</p>\n<a id=\"more\"></a>\n<ul>\n<li>support for current versions of a couple of NoSQL platforms - MongoDB (2.6) and Cassandra (2.0),</li>\n<li>improved YARN (Hadoop v2) functionality allowing PDI to interact with  more recent versions of several Hadoop platforms Cloudera (version 5), MapR (version 3.1) and HortonWorks (version 2.1) as well as with over a dozen other Hadoop platforms</li>\n<li>and the ability to execute transforms on YARN clusters in parallel using the data nodes of the Hadoop clusters, rather than processing the data on the PDI box. This should provide a large performance boast since the data will be processed where it resides, rather than pulling it across the network.</li>\n</ul>\n<p>Documentation is being improved! This faces one of the biggest issues I see with open source software. While functionality of OSS many times is head and shoulders above its commercial counterparts, the poor quality or complete lack of documentation turns off a lot of potential users. Lets hope it continues! Security features have been added, and one of the biggest is the ability to grant or restrict permission to run transformations/jobs by role. In many large scale environments subject to Sarbanes Oxley or other regulations, developers may not be given the ability to run code on production data systems. Now Pentaho has implemented that level of security for ETL developers as well. There are quite a few other improvements, that I won’t go into here, but do check out Pentaho’s website for more information.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/06/pdi51.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/pdi51-259x300.png\" alt=\"pdi51\"></a>While I was lounging around the back roads of Texas last week, Pentaho, Inc. was busy - releasing version 5.1 of Kettle in both Enterprise and Community editions. I’ve downloaded and installed the CE version on my laptop, and plan to put it through its passes this weekend. <a href=\"https://help.pentaho.com/Documentation/5.1/0T0/0Z0/000\">Pentaho’s website</a> lists the improvements in more detail but here are some of the highlights that look interesting to me. There are new Big Data features including:</p>","more":"<ul>\n<li>support for current versions of a couple of NoSQL platforms - MongoDB (2.6) and Cassandra (2.0),</li>\n<li>improved YARN (Hadoop v2) functionality allowing PDI to interact with  more recent versions of several Hadoop platforms Cloudera (version 5), MapR (version 3.1) and HortonWorks (version 2.1) as well as with over a dozen other Hadoop platforms</li>\n<li>and the ability to execute transforms on YARN clusters in parallel using the data nodes of the Hadoop clusters, rather than processing the data on the PDI box. This should provide a large performance boast since the data will be processed where it resides, rather than pulling it across the network.</li>\n</ul>\n<p>Documentation is being improved! This faces one of the biggest issues I see with open source software. While functionality of OSS many times is head and shoulders above its commercial counterparts, the poor quality or complete lack of documentation turns off a lot of potential users. Lets hope it continues! Security features have been added, and one of the biggest is the ability to grant or restrict permission to run transformations/jobs by role. In many large scale environments subject to Sarbanes Oxley or other regulations, developers may not be given the ability to run code on production data systems. Now Pentaho has implemented that level of security for ETL developers as well. There are quite a few other improvements, that I won’t go into here, but do check out Pentaho’s website for more information.</p>"},{"title":"Pentaho access to SQL Server with AD Authentication","id":"2334","comments":0,"date":"2014-08-04T19:24:20.000Z","_content":"\n[![keys3](http://edpflager.com/wp-content/uploads/2014/08/keys3-200x300.jpg)](http://edpflager.com/wp-content/uploads/2014/08/keys3.jpg)Back in June, I posted on how to [Use SQL Server with Pentaho Data Integrator](http://edpflager.com/?p=2130). That article covered using Microsoft SQL Server (MSSQL) authentication to connect to your MSSQL databases. This time around, we'll look at the other type of authentication that you can encounter when working with MSSQL - Active Directory authentication (AD for short). If you are not familiar with AD, its a centralized authentication mechanism allowing access to the various hardware and services in the network. By centralizing the authentication process, the same user account can be used to access multiple resources, and it eliminates some of the setup needed to enable those users on various systems. Most DBA's prefer to use AD authentication for those reasons, and if you will be using PDI to access multiple MSSQL systems, you'll probably want to become familiar with setting it up.\n<!-- more -->\n1.  Although Microsoft provides their own JDBC driver, which I covered in the previous post, this time around we will be using an open source driver called jTDS. You can download the driver from SourceForge using this [link](http://sourceforge.net/projects/jtds/). Download the most current version (at this writing it was version 1.3.1).\n2.  Extract the archive file and open it. Copy the jtds-1.3.1.jar file to the Pentaho lib folder on your system. On my system that is currently /opt/pentaho.data-integration/lib.\n3.  In the folder where you extract the archive, locate the subfolder matching your systems architecture (x64, x86 or IA64). Open it, and open the SSO subfolder.\n4.  Copy the ntlmauth.dll file to a folder on your path. (On CentOS from a command prompt enter: ECHO $PATH$ to see the current path). On my system, I copied the file (as root) to the /usr/local/bin folder.\n5.  Open the Pentaho DI GUI (aka Spoon) and start a new job. Click on the VIEW tab in the Explorer panel.\n6.  Right click on Database Connections, and choose NEW to open the Database Connection window.\n7.  Enter a name in the Connection Name box to identify it.\n8.  Scroll down in Connection Type and choose MS SQL Server.\n9.  In the Access panel, make sure Native (JDBC) is selected.\n10.  In the Settings panel, enter your server's hostname or IP address, the database you want to connect to, the port SQL Server is using (by default its 1433), and the user name and password in the appropriate fields. You can leave Instance Name empty unless your DBA tells you the server is using a named instance. It should look something like this:[![Screenshot-Database Connection -1](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1-300x278.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1.png)\n11.  In the left most panel, select Options. The right panel will refresh, and will probably only have one value entered: \"instance\". Leave the value as is.\n12.  Add a parameter called \"integratedSecurity\" (watch the text case), and set the value to true.\n13.  Add another parameter called \"domain\" and set the value to your network's domain name. (You can use the full domain name or the shorthand one).[![Screenshot-Database Connection -2](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2-300x278.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2.png)\n14.  Click the TEST button at the bottom of the screen, and you should be rewarded with a successful connection window. Click OK and you are done.\n\n[![Screenshot-Database Connection](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection--300x164.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-.png) Pentaho is a trademark of Pentaho, Inc.","source":"_posts/pentaho-access-to-sql-server-with-ad-authentication.md","raw":"---\ntitle: Pentaho access to SQL Server with AD Authentication\ntags:\n  - ETL\n  - How-to\n  - PDI\n  - technical\nid: '2334'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-08-04 15:24:20\n---\n\n[![keys3](http://edpflager.com/wp-content/uploads/2014/08/keys3-200x300.jpg)](http://edpflager.com/wp-content/uploads/2014/08/keys3.jpg)Back in June, I posted on how to [Use SQL Server with Pentaho Data Integrator](http://edpflager.com/?p=2130). That article covered using Microsoft SQL Server (MSSQL) authentication to connect to your MSSQL databases. This time around, we'll look at the other type of authentication that you can encounter when working with MSSQL - Active Directory authentication (AD for short). If you are not familiar with AD, its a centralized authentication mechanism allowing access to the various hardware and services in the network. By centralizing the authentication process, the same user account can be used to access multiple resources, and it eliminates some of the setup needed to enable those users on various systems. Most DBA's prefer to use AD authentication for those reasons, and if you will be using PDI to access multiple MSSQL systems, you'll probably want to become familiar with setting it up.\n<!-- more -->\n1.  Although Microsoft provides their own JDBC driver, which I covered in the previous post, this time around we will be using an open source driver called jTDS. You can download the driver from SourceForge using this [link](http://sourceforge.net/projects/jtds/). Download the most current version (at this writing it was version 1.3.1).\n2.  Extract the archive file and open it. Copy the jtds-1.3.1.jar file to the Pentaho lib folder on your system. On my system that is currently /opt/pentaho.data-integration/lib.\n3.  In the folder where you extract the archive, locate the subfolder matching your systems architecture (x64, x86 or IA64). Open it, and open the SSO subfolder.\n4.  Copy the ntlmauth.dll file to a folder on your path. (On CentOS from a command prompt enter: ECHO $PATH$ to see the current path). On my system, I copied the file (as root) to the /usr/local/bin folder.\n5.  Open the Pentaho DI GUI (aka Spoon) and start a new job. Click on the VIEW tab in the Explorer panel.\n6.  Right click on Database Connections, and choose NEW to open the Database Connection window.\n7.  Enter a name in the Connection Name box to identify it.\n8.  Scroll down in Connection Type and choose MS SQL Server.\n9.  In the Access panel, make sure Native (JDBC) is selected.\n10.  In the Settings panel, enter your server's hostname or IP address, the database you want to connect to, the port SQL Server is using (by default its 1433), and the user name and password in the appropriate fields. You can leave Instance Name empty unless your DBA tells you the server is using a named instance. It should look something like this:[![Screenshot-Database Connection -1](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1-300x278.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1.png)\n11.  In the left most panel, select Options. The right panel will refresh, and will probably only have one value entered: \"instance\". Leave the value as is.\n12.  Add a parameter called \"integratedSecurity\" (watch the text case), and set the value to true.\n13.  Add another parameter called \"domain\" and set the value to your network's domain name. (You can use the full domain name or the shorthand one).[![Screenshot-Database Connection -2](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2-300x278.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2.png)\n14.  Click the TEST button at the bottom of the screen, and you should be rewarded with a successful connection window. Click OK and you are done.\n\n[![Screenshot-Database Connection](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection--300x164.png)](http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-.png) Pentaho is a trademark of Pentaho, Inc.","slug":"pentaho-access-to-sql-server-with-ad-authentication","published":1,"updated":"2020-08-23T20:54:34.890Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a5f006zsdjx4qke1yty","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/keys3.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/keys3-200x300.jpg\" alt=\"keys3\"></a>Back in June, I posted on how to <a href=\"http://edpflager.com/?p=2130\">Use SQL Server with Pentaho Data Integrator</a>. That article covered using Microsoft SQL Server (MSSQL) authentication to connect to your MSSQL databases. This time around, we’ll look at the other type of authentication that you can encounter when working with MSSQL - Active Directory authentication (AD for short). If you are not familiar with AD, its a centralized authentication mechanism allowing access to the various hardware and services in the network. By centralizing the authentication process, the same user account can be used to access multiple resources, and it eliminates some of the setup needed to enable those users on various systems. Most DBA’s prefer to use AD authentication for those reasons, and if you will be using PDI to access multiple MSSQL systems, you’ll probably want to become familiar with setting it up.</p>\n<a id=\"more\"></a>\n<ol>\n<li>Although Microsoft provides their own JDBC driver, which I covered in the previous post, this time around we will be using an open source driver called jTDS. You can download the driver from SourceForge using this <a href=\"http://sourceforge.net/projects/jtds/\">link</a>. Download the most current version (at this writing it was version 1.3.1).</li>\n<li>Extract the archive file and open it. Copy the jtds-1.3.1.jar file to the Pentaho lib folder on your system. On my system that is currently /opt/pentaho.data-integration/lib.</li>\n<li>In the folder where you extract the archive, locate the subfolder matching your systems architecture (x64, x86 or IA64). Open it, and open the SSO subfolder.</li>\n<li>Copy the ntlmauth.dll file to a folder on your path. (On CentOS from a command prompt enter: ECHO $PATH$ to see the current path). On my system, I copied the file (as root) to the /usr/local/bin folder.</li>\n<li>Open the Pentaho DI GUI (aka Spoon) and start a new job. Click on the VIEW tab in the Explorer panel.</li>\n<li>Right click on Database Connections, and choose NEW to open the Database Connection window.</li>\n<li>Enter a name in the Connection Name box to identify it.</li>\n<li>Scroll down in Connection Type and choose MS SQL Server.</li>\n<li>In the Access panel, make sure Native (JDBC) is selected.</li>\n<li>In the Settings panel, enter your server’s hostname or IP address, the database you want to connect to, the port SQL Server is using (by default its 1433), and the user name and password in the appropriate fields. You can leave Instance Name empty unless your DBA tells you the server is using a named instance. It should look something like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1-300x278.png\" alt=\"Screenshot-Database Connection -1\"></a></li>\n<li>In the left most panel, select Options. The right panel will refresh, and will probably only have one value entered: “instance”. Leave the value as is.</li>\n<li>Add a parameter called “integratedSecurity” (watch the text case), and set the value to true.</li>\n<li>Add another parameter called “domain” and set the value to your network’s domain name. (You can use the full domain name or the shorthand one).<a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2-300x278.png\" alt=\"Screenshot-Database Connection -2\"></a></li>\n<li>Click the TEST button at the bottom of the screen, and you should be rewarded with a successful connection window. Click OK and you are done.</li>\n</ol>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection--300x164.png\" alt=\"Screenshot-Database Connection\"></a> Pentaho is a trademark of Pentaho, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/keys3.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/keys3-200x300.jpg\" alt=\"keys3\"></a>Back in June, I posted on how to <a href=\"http://edpflager.com/?p=2130\">Use SQL Server with Pentaho Data Integrator</a>. That article covered using Microsoft SQL Server (MSSQL) authentication to connect to your MSSQL databases. This time around, we’ll look at the other type of authentication that you can encounter when working with MSSQL - Active Directory authentication (AD for short). If you are not familiar with AD, its a centralized authentication mechanism allowing access to the various hardware and services in the network. By centralizing the authentication process, the same user account can be used to access multiple resources, and it eliminates some of the setup needed to enable those users on various systems. Most DBA’s prefer to use AD authentication for those reasons, and if you will be using PDI to access multiple MSSQL systems, you’ll probably want to become familiar with setting it up.</p>","more":"<ol>\n<li>Although Microsoft provides their own JDBC driver, which I covered in the previous post, this time around we will be using an open source driver called jTDS. You can download the driver from SourceForge using this <a href=\"http://sourceforge.net/projects/jtds/\">link</a>. Download the most current version (at this writing it was version 1.3.1).</li>\n<li>Extract the archive file and open it. Copy the jtds-1.3.1.jar file to the Pentaho lib folder on your system. On my system that is currently /opt/pentaho.data-integration/lib.</li>\n<li>In the folder where you extract the archive, locate the subfolder matching your systems architecture (x64, x86 or IA64). Open it, and open the SSO subfolder.</li>\n<li>Copy the ntlmauth.dll file to a folder on your path. (On CentOS from a command prompt enter: ECHO $PATH$ to see the current path). On my system, I copied the file (as root) to the /usr/local/bin folder.</li>\n<li>Open the Pentaho DI GUI (aka Spoon) and start a new job. Click on the VIEW tab in the Explorer panel.</li>\n<li>Right click on Database Connections, and choose NEW to open the Database Connection window.</li>\n<li>Enter a name in the Connection Name box to identify it.</li>\n<li>Scroll down in Connection Type and choose MS SQL Server.</li>\n<li>In the Access panel, make sure Native (JDBC) is selected.</li>\n<li>In the Settings panel, enter your server’s hostname or IP address, the database you want to connect to, the port SQL Server is using (by default its 1433), and the user name and password in the appropriate fields. You can leave Instance Name empty unless your DBA tells you the server is using a named instance. It should look something like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-1-300x278.png\" alt=\"Screenshot-Database Connection -1\"></a></li>\n<li>In the left most panel, select Options. The right panel will refresh, and will probably only have one value entered: “instance”. Leave the value as is.</li>\n<li>Add a parameter called “integratedSecurity” (watch the text case), and set the value to true.</li>\n<li>Add another parameter called “domain” and set the value to your network’s domain name. (You can use the full domain name or the shorthand one).<a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-2-300x278.png\" alt=\"Screenshot-Database Connection -2\"></a></li>\n<li>Click the TEST button at the bottom of the screen, and you should be rewarded with a successful connection window. Click OK and you are done.</li>\n</ol>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection-.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/08/Screenshot-Database-Connection--300x164.png\" alt=\"Screenshot-Database Connection\"></a> Pentaho is a trademark of Pentaho, Inc.</p>"},{"title":"Pentaho - Using Database Lookups","id":"2122","comments":0,"date":"2014-11-16T00:34:48.000Z","_content":"\n[![Kettle](http://edpflager.com/wp-content/uploads/2014/11/Kettle1-300x72.png)](http://edpflager.com/wp-content/uploads/2014/11/Kettle1.png)At my previous day job, we were often tasked with producing data extracts for distribution to outside companies who sold our products. To accomplish this we used a well known reporting platform to produce either Excel or CSV format files. While it worked, it was like using a hammer to drive in a screw. Its just not the best way to accomplish your goal. So during a rare period of calm, I dissected a couple of our extract reports and attempted to convert them to Pentaho Data Integration (aka Kettle). Because we were pulling data from multiple sources and combining it into one unified output file,  the extracts were made up of several queries gathering data from multiple sources. The first query would pull the bulk of the data, and then specific fields from those results were used as filters to pull additional fields from tables in other data sources.  This concept can be used in Pentaho as well, making it unnecessary to temporarily store the data from the first extract and join it later with data from subsequent extracts. An additional benefit is that processing in this manner tends to be much quicker. This type of processing can also be used for denormalizing data when building a data warehouse and in many other scenarios when working with ETL processes.\n<!-- more -->\nFor an example, I’ll combine a [US population by zip code data set](http://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/) with an edited 2010 [crime statistics data set from the city of Austin, TX](http://assets.austintexas.gov/police/zipcode/zipcode/indx_nindx_zip_1210.pdf). If they were combined in the same database, it would be easy enough to do a join between the two tables and combine them that way. Instead I loaded the data into two different MySQL databases, but they could just as easily have been loaded in two different RDBMS. If you would like to follow along, here is the queries to create the tables in MySQL: CREATE DATABASE CRIME; USE CRIME; CREATE TABLE Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT); CREATE DATABASE POPULATION; USE POPULATION; CREATE TABLE Austin ( Zip INT, Population INT ) CREATE DATABASE CRIME\\_BY\\_POPULATION; USE CRIME\\_BY\\_POPULATION; CREATE TABLE Combined\\_Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT, Population INT ); The demographic data set includes only zip codes and the population of that zip code from the 2010 US Census. To setup this extract, I could write either data set to a temporary table in the other database. While that approach will work it does add an unnecessary step to the process. And in real-world instances, you may not have the luxury of creating temporary tables. ![ReadCrime](http://edpflager.com/wp-content/uploads/2014/11/ReadCrime-237x300.png)So instead, I am going to pull data from both databases and write the combined data to a third database. To begin, start a new Pentaho transformation, and add a Table Input object to the workspace (its under Input in the Design palette). Open the object up, make a connection to your database, and construct  a query to pull the bulk of the data. This first query gets the bulk of the data, in this instance the crime data for various zip codes in the Austin area. ![Lookup](http://edpflager.com/wp-content/uploads/2014/11/Lookup-300x287.png)Add a Lookup step from the Lookup node in the Design panel, and connect the Table Input object to it. The data from the Table Input step is then passed to the Lookup step where the database connection and the table that holds data we want to lookup are defined. To make it work, key fields are defined (basically a join between the two tables). In this instance, from the Table Input step we have a ZipCode field. In the Population table on another database, we have  a similar field called Zip. In the key(s) to lookup section, you can click a drop down in the Table Field area to choose fields from the Lookup table. Select a Comparator operator in the next field. The equals operator will be the most common one used, but others such as LIKE, BETWEEN, ISNULL, IS NOT NULL, and various Greater Than and Less Than combinations are also available. In the Values to return from the lookup table section, once again you can choose various fields from your Lookup table from drop downs. You can provide a new name for the field if you like, set a default value, and specify the data type. Finally at the bottom of the window,  you have two options that can be used to fine tune the Database Value Lookup and filter the results. Check the first option if you don't want the row to be passed on if no results are returned from the lookup. Check the second option if you want the lookup to fail if more than one result is returned from the lookup. [![WriteCrime1](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1-300x263.png)](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1.png) Drag a Table Output step on to the canvas and connect the Lookup step to it. Open the step and define your destination connection information. In this window,  I generally use the \"Specify database fields\" option in order to have more control over the mapping process. Specify the \"Truncate table\" if you want to remove anything in your destination table before writing to it. If you chose the \"Specify database fields\" option, click on the Database Fields tab, and click the Get fields button on the right. The \"Fields to insert\" table should populate showing the fields from the processing stream on the right, and the destination table fields on the left. If you want to make any changes, click the Enter field mapping button and adjust the mappings. [![WriteCrime2](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2-300x264.png)](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2.png) Click OK to close the window, and save your transformation. Run it and in your third database, you should now have a combined dataset of crime by zip code with the population of each zip code. Pentaho and Kettle are products of Pentaho Inc.","source":"_posts/pentaho-data-extracts-with-lookups.md","raw":"---\ntitle: Pentaho - Using Database Lookups\ntags:\n  - ETL\n  - How-to\n  - kettle\n  - PDI\n  - technical\nid: '2122'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-11-15 19:34:48\n---\n\n[![Kettle](http://edpflager.com/wp-content/uploads/2014/11/Kettle1-300x72.png)](http://edpflager.com/wp-content/uploads/2014/11/Kettle1.png)At my previous day job, we were often tasked with producing data extracts for distribution to outside companies who sold our products. To accomplish this we used a well known reporting platform to produce either Excel or CSV format files. While it worked, it was like using a hammer to drive in a screw. Its just not the best way to accomplish your goal. So during a rare period of calm, I dissected a couple of our extract reports and attempted to convert them to Pentaho Data Integration (aka Kettle). Because we were pulling data from multiple sources and combining it into one unified output file,  the extracts were made up of several queries gathering data from multiple sources. The first query would pull the bulk of the data, and then specific fields from those results were used as filters to pull additional fields from tables in other data sources.  This concept can be used in Pentaho as well, making it unnecessary to temporarily store the data from the first extract and join it later with data from subsequent extracts. An additional benefit is that processing in this manner tends to be much quicker. This type of processing can also be used for denormalizing data when building a data warehouse and in many other scenarios when working with ETL processes.\n<!-- more -->\nFor an example, I’ll combine a [US population by zip code data set](http://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/) with an edited 2010 [crime statistics data set from the city of Austin, TX](http://assets.austintexas.gov/police/zipcode/zipcode/indx_nindx_zip_1210.pdf). If they were combined in the same database, it would be easy enough to do a join between the two tables and combine them that way. Instead I loaded the data into two different MySQL databases, but they could just as easily have been loaded in two different RDBMS. If you would like to follow along, here is the queries to create the tables in MySQL: CREATE DATABASE CRIME; USE CRIME; CREATE TABLE Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT); CREATE DATABASE POPULATION; USE POPULATION; CREATE TABLE Austin ( Zip INT, Population INT ) CREATE DATABASE CRIME\\_BY\\_POPULATION; USE CRIME\\_BY\\_POPULATION; CREATE TABLE Combined\\_Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT, Population INT ); The demographic data set includes only zip codes and the population of that zip code from the 2010 US Census. To setup this extract, I could write either data set to a temporary table in the other database. While that approach will work it does add an unnecessary step to the process. And in real-world instances, you may not have the luxury of creating temporary tables. ![ReadCrime](http://edpflager.com/wp-content/uploads/2014/11/ReadCrime-237x300.png)So instead, I am going to pull data from both databases and write the combined data to a third database. To begin, start a new Pentaho transformation, and add a Table Input object to the workspace (its under Input in the Design palette). Open the object up, make a connection to your database, and construct  a query to pull the bulk of the data. This first query gets the bulk of the data, in this instance the crime data for various zip codes in the Austin area. ![Lookup](http://edpflager.com/wp-content/uploads/2014/11/Lookup-300x287.png)Add a Lookup step from the Lookup node in the Design panel, and connect the Table Input object to it. The data from the Table Input step is then passed to the Lookup step where the database connection and the table that holds data we want to lookup are defined. To make it work, key fields are defined (basically a join between the two tables). In this instance, from the Table Input step we have a ZipCode field. In the Population table on another database, we have  a similar field called Zip. In the key(s) to lookup section, you can click a drop down in the Table Field area to choose fields from the Lookup table. Select a Comparator operator in the next field. The equals operator will be the most common one used, but others such as LIKE, BETWEEN, ISNULL, IS NOT NULL, and various Greater Than and Less Than combinations are also available. In the Values to return from the lookup table section, once again you can choose various fields from your Lookup table from drop downs. You can provide a new name for the field if you like, set a default value, and specify the data type. Finally at the bottom of the window,  you have two options that can be used to fine tune the Database Value Lookup and filter the results. Check the first option if you don't want the row to be passed on if no results are returned from the lookup. Check the second option if you want the lookup to fail if more than one result is returned from the lookup. [![WriteCrime1](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1-300x263.png)](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1.png) Drag a Table Output step on to the canvas and connect the Lookup step to it. Open the step and define your destination connection information. In this window,  I generally use the \"Specify database fields\" option in order to have more control over the mapping process. Specify the \"Truncate table\" if you want to remove anything in your destination table before writing to it. If you chose the \"Specify database fields\" option, click on the Database Fields tab, and click the Get fields button on the right. The \"Fields to insert\" table should populate showing the fields from the processing stream on the right, and the destination table fields on the left. If you want to make any changes, click the Enter field mapping button and adjust the mappings. [![WriteCrime2](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2-300x264.png)](http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2.png) Click OK to close the window, and save your transformation. Run it and in your third database, you should now have a combined dataset of crime by zip code with the population of each zip code. Pentaho and Kettle are products of Pentaho Inc.","slug":"pentaho-data-extracts-with-lookups","published":1,"updated":"2020-08-23T20:54:34.894Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a5h0073sdjxgpg6ehpm","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/11/Kettle1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/Kettle1-300x72.png\" alt=\"Kettle\"></a>At my previous day job, we were often tasked with producing data extracts for distribution to outside companies who sold our products. To accomplish this we used a well known reporting platform to produce either Excel or CSV format files. While it worked, it was like using a hammer to drive in a screw. Its just not the best way to accomplish your goal. So during a rare period of calm, I dissected a couple of our extract reports and attempted to convert them to Pentaho Data Integration (aka Kettle). Because we were pulling data from multiple sources and combining it into one unified output file,  the extracts were made up of several queries gathering data from multiple sources. The first query would pull the bulk of the data, and then specific fields from those results were used as filters to pull additional fields from tables in other data sources.  This concept can be used in Pentaho as well, making it unnecessary to temporarily store the data from the first extract and join it later with data from subsequent extracts. An additional benefit is that processing in this manner tends to be much quicker. This type of processing can also be used for denormalizing data when building a data warehouse and in many other scenarios when working with ETL processes.</p>\n<a id=\"more\"></a>\n<p>For an example, I’ll combine a <a href=\"http://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/\">US population by zip code data set</a> with an edited 2010 <a href=\"http://assets.austintexas.gov/police/zipcode/zipcode/indx_nindx_zip_1210.pdf\">crime statistics data set from the city of Austin, TX</a>. If they were combined in the same database, it would be easy enough to do a join between the two tables and combine them that way. Instead I loaded the data into two different MySQL databases, but they could just as easily have been loaded in two different RDBMS. If you would like to follow along, here is the queries to create the tables in MySQL: CREATE DATABASE CRIME; USE CRIME; CREATE TABLE Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT); CREATE DATABASE POPULATION; USE POPULATION; CREATE TABLE Austin ( Zip INT, Population INT ) CREATE DATABASE CRIME_BY_POPULATION; USE CRIME_BY_POPULATION; CREATE TABLE Combined_Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT, Population INT ); The demographic data set includes only zip codes and the population of that zip code from the 2010 US Census. To setup this extract, I could write either data set to a temporary table in the other database. While that approach will work it does add an unnecessary step to the process. And in real-world instances, you may not have the luxury of creating temporary tables. <img src=\"http://edpflager.com/wp-content/uploads/2014/11/ReadCrime-237x300.png\" alt=\"ReadCrime\">So instead, I am going to pull data from both databases and write the combined data to a third database. To begin, start a new Pentaho transformation, and add a Table Input object to the workspace (its under Input in the Design palette). Open the object up, make a connection to your database, and construct  a query to pull the bulk of the data. This first query gets the bulk of the data, in this instance the crime data for various zip codes in the Austin area. <img src=\"http://edpflager.com/wp-content/uploads/2014/11/Lookup-300x287.png\" alt=\"Lookup\">Add a Lookup step from the Lookup node in the Design panel, and connect the Table Input object to it. The data from the Table Input step is then passed to the Lookup step where the database connection and the table that holds data we want to lookup are defined. To make it work, key fields are defined (basically a join between the two tables). In this instance, from the Table Input step we have a ZipCode field. In the Population table on another database, we have  a similar field called Zip. In the key(s) to lookup section, you can click a drop down in the Table Field area to choose fields from the Lookup table. Select a Comparator operator in the next field. The equals operator will be the most common one used, but others such as LIKE, BETWEEN, ISNULL, IS NOT NULL, and various Greater Than and Less Than combinations are also available. In the Values to return from the lookup table section, once again you can choose various fields from your Lookup table from drop downs. You can provide a new name for the field if you like, set a default value, and specify the data type. Finally at the bottom of the window,  you have two options that can be used to fine tune the Database Value Lookup and filter the results. Check the first option if you don’t want the row to be passed on if no results are returned from the lookup. Check the second option if you want the lookup to fail if more than one result is returned from the lookup. <a href=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1-300x263.png\" alt=\"WriteCrime1\"></a> Drag a Table Output step on to the canvas and connect the Lookup step to it. Open the step and define your destination connection information. In this window,  I generally use the “Specify database fields” option in order to have more control over the mapping process. Specify the “Truncate table” if you want to remove anything in your destination table before writing to it. If you chose the “Specify database fields” option, click on the Database Fields tab, and click the Get fields button on the right. The “Fields to insert” table should populate showing the fields from the processing stream on the right, and the destination table fields on the left. If you want to make any changes, click the Enter field mapping button and adjust the mappings. <a href=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2-300x264.png\" alt=\"WriteCrime2\"></a> Click OK to close the window, and save your transformation. Run it and in your third database, you should now have a combined dataset of crime by zip code with the population of each zip code. Pentaho and Kettle are products of Pentaho Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/11/Kettle1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/Kettle1-300x72.png\" alt=\"Kettle\"></a>At my previous day job, we were often tasked with producing data extracts for distribution to outside companies who sold our products. To accomplish this we used a well known reporting platform to produce either Excel or CSV format files. While it worked, it was like using a hammer to drive in a screw. Its just not the best way to accomplish your goal. So during a rare period of calm, I dissected a couple of our extract reports and attempted to convert them to Pentaho Data Integration (aka Kettle). Because we were pulling data from multiple sources and combining it into one unified output file,  the extracts were made up of several queries gathering data from multiple sources. The first query would pull the bulk of the data, and then specific fields from those results were used as filters to pull additional fields from tables in other data sources.  This concept can be used in Pentaho as well, making it unnecessary to temporarily store the data from the first extract and join it later with data from subsequent extracts. An additional benefit is that processing in this manner tends to be much quicker. This type of processing can also be used for denormalizing data when building a data warehouse and in many other scenarios when working with ETL processes.</p>","more":"<p>For an example, I’ll combine a <a href=\"http://blog.splitwise.com/2013/09/18/the-2010-us-census-population-by-zip-code-totally-free/\">US population by zip code data set</a> with an edited 2010 <a href=\"http://assets.austintexas.gov/police/zipcode/zipcode/indx_nindx_zip_1210.pdf\">crime statistics data set from the city of Austin, TX</a>. If they were combined in the same database, it would be easy enough to do a join between the two tables and combine them that way. Instead I loaded the data into two different MySQL databases, but they could just as easily have been loaded in two different RDBMS. If you would like to follow along, here is the queries to create the tables in MySQL: CREATE DATABASE CRIME; USE CRIME; CREATE TABLE Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT); CREATE DATABASE POPULATION; USE POPULATION; CREATE TABLE Austin ( Zip INT, Population INT ) CREATE DATABASE CRIME_BY_POPULATION; USE CRIME_BY_POPULATION; CREATE TABLE Combined_Crime2010 ( ZIPCODE INT, Murder INT, Rape INT, Robbery INT, AggAssault INT, Burglary INT, Theft INT, VehicleTheft INT, Arson INT, TotalCrime INT, Population INT ); The demographic data set includes only zip codes and the population of that zip code from the 2010 US Census. To setup this extract, I could write either data set to a temporary table in the other database. While that approach will work it does add an unnecessary step to the process. And in real-world instances, you may not have the luxury of creating temporary tables. <img src=\"http://edpflager.com/wp-content/uploads/2014/11/ReadCrime-237x300.png\" alt=\"ReadCrime\">So instead, I am going to pull data from both databases and write the combined data to a third database. To begin, start a new Pentaho transformation, and add a Table Input object to the workspace (its under Input in the Design palette). Open the object up, make a connection to your database, and construct  a query to pull the bulk of the data. This first query gets the bulk of the data, in this instance the crime data for various zip codes in the Austin area. <img src=\"http://edpflager.com/wp-content/uploads/2014/11/Lookup-300x287.png\" alt=\"Lookup\">Add a Lookup step from the Lookup node in the Design panel, and connect the Table Input object to it. The data from the Table Input step is then passed to the Lookup step where the database connection and the table that holds data we want to lookup are defined. To make it work, key fields are defined (basically a join between the two tables). In this instance, from the Table Input step we have a ZipCode field. In the Population table on another database, we have  a similar field called Zip. In the key(s) to lookup section, you can click a drop down in the Table Field area to choose fields from the Lookup table. Select a Comparator operator in the next field. The equals operator will be the most common one used, but others such as LIKE, BETWEEN, ISNULL, IS NOT NULL, and various Greater Than and Less Than combinations are also available. In the Values to return from the lookup table section, once again you can choose various fields from your Lookup table from drop downs. You can provide a new name for the field if you like, set a default value, and specify the data type. Finally at the bottom of the window,  you have two options that can be used to fine tune the Database Value Lookup and filter the results. Check the first option if you don’t want the row to be passed on if no results are returned from the lookup. Check the second option if you want the lookup to fail if more than one result is returned from the lookup. <a href=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime1-300x263.png\" alt=\"WriteCrime1\"></a> Drag a Table Output step on to the canvas and connect the Lookup step to it. Open the step and define your destination connection information. In this window,  I generally use the “Specify database fields” option in order to have more control over the mapping process. Specify the “Truncate table” if you want to remove anything in your destination table before writing to it. If you chose the “Specify database fields” option, click on the Database Fields tab, and click the Get fields button on the right. The “Fields to insert” table should populate showing the fields from the processing stream on the right, and the destination table fields on the left. If you want to make any changes, click the Enter field mapping button and adjust the mappings. <a href=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/11/WriteCrime2-300x264.png\" alt=\"WriteCrime2\"></a> Click OK to close the window, and save your transformation. Run it and in your third database, you should now have a combined dataset of crime by zip code with the population of each zip code. Pentaho and Kettle are products of Pentaho Inc.</p>"},{"title":"Pentaho Data Integration's Fuzzy Match","id":"2532","comments":0,"date":"2016-05-26T19:21:54.000Z","_content":"\n[![fuzzy](http://edpflager.com/wp-content/uploads/2016/05/fuzzy.jpg)](http://edpflager.com/?attachment_id=3311#main)When cleansing data, one of the biggest challenges is determining if one record is the same as another in the absence of a unique identifier. For example, if your database has a record for Terri Lee Duffy, and you get a new record for Terry Lee Duffy, is it the same person? If you have a government ID number then its possible to tell definitively, that its the same person. But what if you don't have that to distinguish the record? You could check other related data if you have it, like street address, but what if one record has 100 South Ave and the other is 100 South Road? A human looking could say yes or no that this is the same person. We don't want to have to check every discrepancy, especially if we are moving millions of rows at a time. In order to automate this process, we can use a component in Pentaho called Fuzzy Match. (For a longer discussion of Fuzzy Matching, [Melissa Data Corporation](https://www.melissadata.com/deduplication/what-is-fuzzy-matching.htm) has a good overview.) While the results of a Fuzzy Match process are not 100% perfect, you can set an allowance threshold so that similarities have to be within a certain range or you can show only the closest match as a result of your Fuzzy Match. Finally, the Fuzzy Match component can use one of several algorithms to determine if one field is a match for another.  The [Pentaho Wiki](http://wiki.pentaho.com/display/EAI/Fuzzy+match) discusses the nuances of these algorithms and has some discussion on the best times to use them.\n<!-- more -->\nFuzzy Match Example [![FuzzyTranform](http://edpflager.com/wp-content/uploads/2016/05/FuzzyTranform-300x147.png)](http://edpflager.com/?attachment_id=3298#main) In this example, I am using a Person table with about twenty thousand records, derived from the Microsoft SQL Server AdventureWorks2012 database. (A version for MySQL is available on [SourceForge](https://sourceforge.net/projects/awmysql/)). My source table has a simple query, where the first name, middle name and last name are combined to form a new field, called FullName. This fields will be the one that is actually used to Match on in the component. Here is the query:\n\nSELECT PersonType, FirstName, MiddleName, LastName, \n FirstName + ' ' + Coalesce(MiddleName,'') + ' ' + LastName as FullName\nFROM Person.Person\n\nThe NewData component is a DataGrid component. I have defined Meta data of four string fields (FirstName, MiddleName, LastName, and FullName) with three rows of data, pictured below: [![Add constancts](http://edpflager.com/wp-content/uploads/2016/05/Add-constancts-300x192.png)](http://edpflager.com/?attachment_id=3301#main) The first row is an alternative of an existing record with no middle name, the last row is the example I highlighted earlier, changing Terri to Terry in the first name. The middle row is a deliberate misspelling of the last name Smith and there is no existing records with Jane Smith. [![FuzzyStringGeneral](http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringGeneral-300x300.png)](http://edpflager.com/?attachment_id=3305#main)After setting those two inputs up, I join them with the Fuzzy Match component which in PDI is under the Lookup folder in the Design tab. On the General tab of the component, I set my Person source as the Lookup stream. This is the reference side of the function where the data is treated as clean or reliable. FullName is set as my Lookup field. Since I have defined the Person source as my Lookup stream, Pentaho knows that my other input is the Main stream and populates the Main Stream drop down list with the fields available from there. [![FuzzyStringFields](http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringFields-300x298.png)](http://edpflager.com/?attachment_id=3308#main)In the Settings area, I select the algorithm to use for comparing.  In this case I am using the Jano algorithm which is designed to calculate a similarity index between two strings. I skip the case sensitive check box, and check the Get Closer value box. This limits the results for each record checked to the closest match. For minimal value, I enter a .95 because I want matches to be fairly close to my reference table to consider it a match. My maximal value I leave set to 1.0 (an exact match). Switching to the Fields tab, I changed the MatchField name to ReferenceString and the Value field to SimilarityIndex. (Not necessary - but makes it easier to tell what each is). I click the GET FIELDS button to populate the grid with the fields from my Input Stream. Click OK to return to the canvas. Finally I wire up a DUMMY step to stop the flow, and allow me to see the results. Save your transform and Run it accepting the defaults, and after a few moments it will complete. Click on the DUMMY component, and then switch to the PREVIEW DATA tab in the bottom panel will show you what values it has detected as matches for the new data and the similarity between them. [![FuzzyMatchresults](http://edpflager.com/wp-content/uploads/2016/05/FuzzyMatchresults-300x51.png)](http://edpflager.com/?attachment_id=3309#main) Notice that the first row and the last row are shown as a match of 1 (meaning a 100% match). Jane A Smithh shows no results, meaning no matches were found within the 95-100% range that was set. If you reedit the Fuzzy Match component and change the threshold to .90 instead of .95, Jane A Smith is matched to Jacob A Smith at a 90% level. From here you could use the Similarity index as a value to determine if the record is discarded or added as a new row. I'll leave that for you to work with.","source":"_posts/pentaho-fuzzy-match.md","raw":"---\ntitle: Pentaho Data Integration's Fuzzy Match\ntags:\n  - cookbook\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - SysAdmin\n  - technical\nid: '2532'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2016-05-26 15:21:54\n---\n\n[![fuzzy](http://edpflager.com/wp-content/uploads/2016/05/fuzzy.jpg)](http://edpflager.com/?attachment_id=3311#main)When cleansing data, one of the biggest challenges is determining if one record is the same as another in the absence of a unique identifier. For example, if your database has a record for Terri Lee Duffy, and you get a new record for Terry Lee Duffy, is it the same person? If you have a government ID number then its possible to tell definitively, that its the same person. But what if you don't have that to distinguish the record? You could check other related data if you have it, like street address, but what if one record has 100 South Ave and the other is 100 South Road? A human looking could say yes or no that this is the same person. We don't want to have to check every discrepancy, especially if we are moving millions of rows at a time. In order to automate this process, we can use a component in Pentaho called Fuzzy Match. (For a longer discussion of Fuzzy Matching, [Melissa Data Corporation](https://www.melissadata.com/deduplication/what-is-fuzzy-matching.htm) has a good overview.) While the results of a Fuzzy Match process are not 100% perfect, you can set an allowance threshold so that similarities have to be within a certain range or you can show only the closest match as a result of your Fuzzy Match. Finally, the Fuzzy Match component can use one of several algorithms to determine if one field is a match for another.  The [Pentaho Wiki](http://wiki.pentaho.com/display/EAI/Fuzzy+match) discusses the nuances of these algorithms and has some discussion on the best times to use them.\n<!-- more -->\nFuzzy Match Example [![FuzzyTranform](http://edpflager.com/wp-content/uploads/2016/05/FuzzyTranform-300x147.png)](http://edpflager.com/?attachment_id=3298#main) In this example, I am using a Person table with about twenty thousand records, derived from the Microsoft SQL Server AdventureWorks2012 database. (A version for MySQL is available on [SourceForge](https://sourceforge.net/projects/awmysql/)). My source table has a simple query, where the first name, middle name and last name are combined to form a new field, called FullName. This fields will be the one that is actually used to Match on in the component. Here is the query:\n\nSELECT PersonType, FirstName, MiddleName, LastName, \n FirstName + ' ' + Coalesce(MiddleName,'') + ' ' + LastName as FullName\nFROM Person.Person\n\nThe NewData component is a DataGrid component. I have defined Meta data of four string fields (FirstName, MiddleName, LastName, and FullName) with three rows of data, pictured below: [![Add constancts](http://edpflager.com/wp-content/uploads/2016/05/Add-constancts-300x192.png)](http://edpflager.com/?attachment_id=3301#main) The first row is an alternative of an existing record with no middle name, the last row is the example I highlighted earlier, changing Terri to Terry in the first name. The middle row is a deliberate misspelling of the last name Smith and there is no existing records with Jane Smith. [![FuzzyStringGeneral](http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringGeneral-300x300.png)](http://edpflager.com/?attachment_id=3305#main)After setting those two inputs up, I join them with the Fuzzy Match component which in PDI is under the Lookup folder in the Design tab. On the General tab of the component, I set my Person source as the Lookup stream. This is the reference side of the function where the data is treated as clean or reliable. FullName is set as my Lookup field. Since I have defined the Person source as my Lookup stream, Pentaho knows that my other input is the Main stream and populates the Main Stream drop down list with the fields available from there. [![FuzzyStringFields](http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringFields-300x298.png)](http://edpflager.com/?attachment_id=3308#main)In the Settings area, I select the algorithm to use for comparing.  In this case I am using the Jano algorithm which is designed to calculate a similarity index between two strings. I skip the case sensitive check box, and check the Get Closer value box. This limits the results for each record checked to the closest match. For minimal value, I enter a .95 because I want matches to be fairly close to my reference table to consider it a match. My maximal value I leave set to 1.0 (an exact match). Switching to the Fields tab, I changed the MatchField name to ReferenceString and the Value field to SimilarityIndex. (Not necessary - but makes it easier to tell what each is). I click the GET FIELDS button to populate the grid with the fields from my Input Stream. Click OK to return to the canvas. Finally I wire up a DUMMY step to stop the flow, and allow me to see the results. Save your transform and Run it accepting the defaults, and after a few moments it will complete. Click on the DUMMY component, and then switch to the PREVIEW DATA tab in the bottom panel will show you what values it has detected as matches for the new data and the similarity between them. [![FuzzyMatchresults](http://edpflager.com/wp-content/uploads/2016/05/FuzzyMatchresults-300x51.png)](http://edpflager.com/?attachment_id=3309#main) Notice that the first row and the last row are shown as a match of 1 (meaning a 100% match). Jane A Smithh shows no results, meaning no matches were found within the 95-100% range that was set. If you reedit the Fuzzy Match component and change the threshold to .90 instead of .95, Jane A Smith is matched to Jacob A Smith at a 90% level. From here you could use the Similarity index as a value to determine if the record is discarded or added as a new row. I'll leave that for you to work with.","slug":"pentaho-fuzzy-match","published":1,"updated":"2020-08-23T20:54:35.038Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a5n0076sdjx8j3ifjdz","content":"<p><a href=\"http://edpflager.com/?attachment_id=3311#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/fuzzy.jpg\" alt=\"fuzzy\"></a>When cleansing data, one of the biggest challenges is determining if one record is the same as another in the absence of a unique identifier. For example, if your database has a record for Terri Lee Duffy, and you get a new record for Terry Lee Duffy, is it the same person? If you have a government ID number then its possible to tell definitively, that its the same person. But what if you don’t have that to distinguish the record? You could check other related data if you have it, like street address, but what if one record has 100 South Ave and the other is 100 South Road? A human looking could say yes or no that this is the same person. We don’t want to have to check every discrepancy, especially if we are moving millions of rows at a time. In order to automate this process, we can use a component in Pentaho called Fuzzy Match. (For a longer discussion of Fuzzy Matching, <a href=\"https://www.melissadata.com/deduplication/what-is-fuzzy-matching.htm\">Melissa Data Corporation</a> has a good overview.) While the results of a Fuzzy Match process are not 100% perfect, you can set an allowance threshold so that similarities have to be within a certain range or you can show only the closest match as a result of your Fuzzy Match. Finally, the Fuzzy Match component can use one of several algorithms to determine if one field is a match for another.  The <a href=\"http://wiki.pentaho.com/display/EAI/Fuzzy+match\">Pentaho Wiki</a> discusses the nuances of these algorithms and has some discussion on the best times to use them.</p>\n<a id=\"more\"></a>\n<p>Fuzzy Match Example <a href=\"http://edpflager.com/?attachment_id=3298#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyTranform-300x147.png\" alt=\"FuzzyTranform\"></a> In this example, I am using a Person table with about twenty thousand records, derived from the Microsoft SQL Server AdventureWorks2012 database. (A version for MySQL is available on <a href=\"https://sourceforge.net/projects/awmysql/\">SourceForge</a>). My source table has a simple query, where the first name, middle name and last name are combined to form a new field, called FullName. This fields will be the one that is actually used to Match on in the component. Here is the query:</p>\n<p>SELECT PersonType, FirstName, MiddleName, LastName,<br> FirstName + ‘ ‘ + Coalesce(MiddleName,’’) + ‘ ‘ + LastName as FullName<br>FROM Person.Person</p>\n<p>The NewData component is a DataGrid component. I have defined Meta data of four string fields (FirstName, MiddleName, LastName, and FullName) with three rows of data, pictured below: <a href=\"http://edpflager.com/?attachment_id=3301#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/Add-constancts-300x192.png\" alt=\"Add constancts\"></a> The first row is an alternative of an existing record with no middle name, the last row is the example I highlighted earlier, changing Terri to Terry in the first name. The middle row is a deliberate misspelling of the last name Smith and there is no existing records with Jane Smith. <a href=\"http://edpflager.com/?attachment_id=3305#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringGeneral-300x300.png\" alt=\"FuzzyStringGeneral\"></a>After setting those two inputs up, I join them with the Fuzzy Match component which in PDI is under the Lookup folder in the Design tab. On the General tab of the component, I set my Person source as the Lookup stream. This is the reference side of the function where the data is treated as clean or reliable. FullName is set as my Lookup field. Since I have defined the Person source as my Lookup stream, Pentaho knows that my other input is the Main stream and populates the Main Stream drop down list with the fields available from there. <a href=\"http://edpflager.com/?attachment_id=3308#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringFields-300x298.png\" alt=\"FuzzyStringFields\"></a>In the Settings area, I select the algorithm to use for comparing.  In this case I am using the Jano algorithm which is designed to calculate a similarity index between two strings. I skip the case sensitive check box, and check the Get Closer value box. This limits the results for each record checked to the closest match. For minimal value, I enter a .95 because I want matches to be fairly close to my reference table to consider it a match. My maximal value I leave set to 1.0 (an exact match). Switching to the Fields tab, I changed the MatchField name to ReferenceString and the Value field to SimilarityIndex. (Not necessary - but makes it easier to tell what each is). I click the GET FIELDS button to populate the grid with the fields from my Input Stream. Click OK to return to the canvas. Finally I wire up a DUMMY step to stop the flow, and allow me to see the results. Save your transform and Run it accepting the defaults, and after a few moments it will complete. Click on the DUMMY component, and then switch to the PREVIEW DATA tab in the bottom panel will show you what values it has detected as matches for the new data and the similarity between them. <a href=\"http://edpflager.com/?attachment_id=3309#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyMatchresults-300x51.png\" alt=\"FuzzyMatchresults\"></a> Notice that the first row and the last row are shown as a match of 1 (meaning a 100% match). Jane A Smithh shows no results, meaning no matches were found within the 95-100% range that was set. If you reedit the Fuzzy Match component and change the threshold to .90 instead of .95, Jane A Smith is matched to Jacob A Smith at a 90% level. From here you could use the Similarity index as a value to determine if the record is discarded or added as a new row. I’ll leave that for you to work with.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3311#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/fuzzy.jpg\" alt=\"fuzzy\"></a>When cleansing data, one of the biggest challenges is determining if one record is the same as another in the absence of a unique identifier. For example, if your database has a record for Terri Lee Duffy, and you get a new record for Terry Lee Duffy, is it the same person? If you have a government ID number then its possible to tell definitively, that its the same person. But what if you don’t have that to distinguish the record? You could check other related data if you have it, like street address, but what if one record has 100 South Ave and the other is 100 South Road? A human looking could say yes or no that this is the same person. We don’t want to have to check every discrepancy, especially if we are moving millions of rows at a time. In order to automate this process, we can use a component in Pentaho called Fuzzy Match. (For a longer discussion of Fuzzy Matching, <a href=\"https://www.melissadata.com/deduplication/what-is-fuzzy-matching.htm\">Melissa Data Corporation</a> has a good overview.) While the results of a Fuzzy Match process are not 100% perfect, you can set an allowance threshold so that similarities have to be within a certain range or you can show only the closest match as a result of your Fuzzy Match. Finally, the Fuzzy Match component can use one of several algorithms to determine if one field is a match for another.  The <a href=\"http://wiki.pentaho.com/display/EAI/Fuzzy+match\">Pentaho Wiki</a> discusses the nuances of these algorithms and has some discussion on the best times to use them.</p>","more":"<p>Fuzzy Match Example <a href=\"http://edpflager.com/?attachment_id=3298#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyTranform-300x147.png\" alt=\"FuzzyTranform\"></a> In this example, I am using a Person table with about twenty thousand records, derived from the Microsoft SQL Server AdventureWorks2012 database. (A version for MySQL is available on <a href=\"https://sourceforge.net/projects/awmysql/\">SourceForge</a>). My source table has a simple query, where the first name, middle name and last name are combined to form a new field, called FullName. This fields will be the one that is actually used to Match on in the component. Here is the query:</p>\n<p>SELECT PersonType, FirstName, MiddleName, LastName,<br> FirstName + ‘ ‘ + Coalesce(MiddleName,’’) + ‘ ‘ + LastName as FullName<br>FROM Person.Person</p>\n<p>The NewData component is a DataGrid component. I have defined Meta data of four string fields (FirstName, MiddleName, LastName, and FullName) with three rows of data, pictured below: <a href=\"http://edpflager.com/?attachment_id=3301#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/Add-constancts-300x192.png\" alt=\"Add constancts\"></a> The first row is an alternative of an existing record with no middle name, the last row is the example I highlighted earlier, changing Terri to Terry in the first name. The middle row is a deliberate misspelling of the last name Smith and there is no existing records with Jane Smith. <a href=\"http://edpflager.com/?attachment_id=3305#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringGeneral-300x300.png\" alt=\"FuzzyStringGeneral\"></a>After setting those two inputs up, I join them with the Fuzzy Match component which in PDI is under the Lookup folder in the Design tab. On the General tab of the component, I set my Person source as the Lookup stream. This is the reference side of the function where the data is treated as clean or reliable. FullName is set as my Lookup field. Since I have defined the Person source as my Lookup stream, Pentaho knows that my other input is the Main stream and populates the Main Stream drop down list with the fields available from there. <a href=\"http://edpflager.com/?attachment_id=3308#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyStringFields-300x298.png\" alt=\"FuzzyStringFields\"></a>In the Settings area, I select the algorithm to use for comparing.  In this case I am using the Jano algorithm which is designed to calculate a similarity index between two strings. I skip the case sensitive check box, and check the Get Closer value box. This limits the results for each record checked to the closest match. For minimal value, I enter a .95 because I want matches to be fairly close to my reference table to consider it a match. My maximal value I leave set to 1.0 (an exact match). Switching to the Fields tab, I changed the MatchField name to ReferenceString and the Value field to SimilarityIndex. (Not necessary - but makes it easier to tell what each is). I click the GET FIELDS button to populate the grid with the fields from my Input Stream. Click OK to return to the canvas. Finally I wire up a DUMMY step to stop the flow, and allow me to see the results. Save your transform and Run it accepting the defaults, and after a few moments it will complete. Click on the DUMMY component, and then switch to the PREVIEW DATA tab in the bottom panel will show you what values it has detected as matches for the new data and the similarity between them. <a href=\"http://edpflager.com/?attachment_id=3309#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/FuzzyMatchresults-300x51.png\" alt=\"FuzzyMatchresults\"></a> Notice that the first row and the last row are shown as a match of 1 (meaning a 100% match). Jane A Smithh shows no results, meaning no matches were found within the 95-100% range that was set. If you reedit the Fuzzy Match component and change the threshold to .90 instead of .95, Jane A Smith is matched to Jacob A Smith at a 90% level. From here you could use the Similarity index as a value to determine if the record is discarded or added as a new row. I’ll leave that for you to work with.</p>"},{"title":"Pentaho Kettle and DB2 - Truncate (updated)","id":"2214","comments":0,"date":"2014-07-19T16:11:32.000Z","_content":"\n[![truncatedisk](http://edpflager.com/wp-content/uploads/2014/07/truncatedisk-150x150.jpg)](http://edpflager.com/wp-content/uploads/2014/07/truncatedisk.jpg)**Updated to cover error on truncating empty table** Recently while working on a data transformation to move records on a regular basis from a PostgreSQL database system to a DB2 mainframe system, I ran across an interesting problem.  The scope of the project called for a complete refresh of each of the tables rather than just updating old records and inserting new ones, so I would need to clear each table out prior to writing the refreshed records. Normally I would use the Truncate Table option in the Table Output step to handle this, but  I found that it caused an error in the workflow. Apparently this problem is due to DB2 not supporting the Truncate SQL command.\n<!-- more -->\nIf you have this problem, here is a fix to resolve it.\n\n1.  Prior to the workflow step where you populate the table with new data, add a transform and drag an Execute SQL task on to the canvas.\n2.  Double click the Execute SQL Script and configure it like this. Add a one line command DELETE FROM <schema.tablename>; and select the Execute as a Single Statement option. ![SQLScript](http://edpflager.com/wp-content/uploads/2014/07/SQLScript-300x203.png)\n3.  Save your workflow and run it to test.\n\nThere are a couple of gotchas with this process:\n\n*   Be sure the account you are using to access DB2 has permissions to Delete from the table you are working with.\n*   If the table you are attempting to truncate is empty, it will generate an error in DB2. To get around this, modify your transform to get a row count of the table and use a switch statement to run your truncate script or go to a Dummy statement, depending on whether or not the table is already empty. The transform should look like this:[![image001](http://edpflager.com/wp-content/uploads/2014/07/image001-300x119.png)](http://edpflager.com/wp-content/uploads/2014/07/image001.png)\n\nPENTAHO is a registered trademark of Pentaho, Inc.","source":"_posts/pentaho-kettle-and-db2-truncate.md","raw":"---\ntitle: Pentaho Kettle and DB2 - Truncate (updated)\ntags:\n  - Big Data\n  - ETL\n  - How-to\n  - kettle\n  - PDI\nid: '2214'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-07-19 12:11:32\n---\n\n[![truncatedisk](http://edpflager.com/wp-content/uploads/2014/07/truncatedisk-150x150.jpg)](http://edpflager.com/wp-content/uploads/2014/07/truncatedisk.jpg)**Updated to cover error on truncating empty table** Recently while working on a data transformation to move records on a regular basis from a PostgreSQL database system to a DB2 mainframe system, I ran across an interesting problem.  The scope of the project called for a complete refresh of each of the tables rather than just updating old records and inserting new ones, so I would need to clear each table out prior to writing the refreshed records. Normally I would use the Truncate Table option in the Table Output step to handle this, but  I found that it caused an error in the workflow. Apparently this problem is due to DB2 not supporting the Truncate SQL command.\n<!-- more -->\nIf you have this problem, here is a fix to resolve it.\n\n1.  Prior to the workflow step where you populate the table with new data, add a transform and drag an Execute SQL task on to the canvas.\n2.  Double click the Execute SQL Script and configure it like this. Add a one line command DELETE FROM <schema.tablename>; and select the Execute as a Single Statement option. ![SQLScript](http://edpflager.com/wp-content/uploads/2014/07/SQLScript-300x203.png)\n3.  Save your workflow and run it to test.\n\nThere are a couple of gotchas with this process:\n\n*   Be sure the account you are using to access DB2 has permissions to Delete from the table you are working with.\n*   If the table you are attempting to truncate is empty, it will generate an error in DB2. To get around this, modify your transform to get a row count of the table and use a switch statement to run your truncate script or go to a Dummy statement, depending on whether or not the table is already empty. The transform should look like this:[![image001](http://edpflager.com/wp-content/uploads/2014/07/image001-300x119.png)](http://edpflager.com/wp-content/uploads/2014/07/image001.png)\n\nPENTAHO is a registered trademark of Pentaho, Inc.","slug":"pentaho-kettle-and-db2-truncate","published":1,"updated":"2020-08-23T20:54:34.878Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a64007asdjxax46g6wd","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/truncatedisk.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/truncatedisk-150x150.jpg\" alt=\"truncatedisk\"></a><strong>Updated to cover error on truncating empty table</strong> Recently while working on a data transformation to move records on a regular basis from a PostgreSQL database system to a DB2 mainframe system, I ran across an interesting problem.  The scope of the project called for a complete refresh of each of the tables rather than just updating old records and inserting new ones, so I would need to clear each table out prior to writing the refreshed records. Normally I would use the Truncate Table option in the Table Output step to handle this, but  I found that it caused an error in the workflow. Apparently this problem is due to DB2 not supporting the Truncate SQL command.</p>\n<a id=\"more\"></a>\n<p>If you have this problem, here is a fix to resolve it.</p>\n<ol>\n<li>Prior to the workflow step where you populate the table with new data, add a transform and drag an Execute SQL task on to the canvas.</li>\n<li>Double click the Execute SQL Script and configure it like this. Add a one line command DELETE FROM &lt;schema.tablename&gt;; and select the Execute as a Single Statement option. <img src=\"http://edpflager.com/wp-content/uploads/2014/07/SQLScript-300x203.png\" alt=\"SQLScript\"></li>\n<li>Save your workflow and run it to test.</li>\n</ol>\n<p>There are a couple of gotchas with this process:</p>\n<ul>\n<li>Be sure the account you are using to access DB2 has permissions to Delete from the table you are working with.</li>\n<li>If the table you are attempting to truncate is empty, it will generate an error in DB2. To get around this, modify your transform to get a row count of the table and use a switch statement to run your truncate script or go to a Dummy statement, depending on whether or not the table is already empty. The transform should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/07/image001.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/image001-300x119.png\" alt=\"image001\"></a></li>\n</ul>\n<p>PENTAHO is a registered trademark of Pentaho, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/truncatedisk.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/truncatedisk-150x150.jpg\" alt=\"truncatedisk\"></a><strong>Updated to cover error on truncating empty table</strong> Recently while working on a data transformation to move records on a regular basis from a PostgreSQL database system to a DB2 mainframe system, I ran across an interesting problem.  The scope of the project called for a complete refresh of each of the tables rather than just updating old records and inserting new ones, so I would need to clear each table out prior to writing the refreshed records. Normally I would use the Truncate Table option in the Table Output step to handle this, but  I found that it caused an error in the workflow. Apparently this problem is due to DB2 not supporting the Truncate SQL command.</p>","more":"<p>If you have this problem, here is a fix to resolve it.</p>\n<ol>\n<li>Prior to the workflow step where you populate the table with new data, add a transform and drag an Execute SQL task on to the canvas.</li>\n<li>Double click the Execute SQL Script and configure it like this. Add a one line command DELETE FROM &lt;schema.tablename&gt;; and select the Execute as a Single Statement option. <img src=\"http://edpflager.com/wp-content/uploads/2014/07/SQLScript-300x203.png\" alt=\"SQLScript\"></li>\n<li>Save your workflow and run it to test.</li>\n</ol>\n<p>There are a couple of gotchas with this process:</p>\n<ul>\n<li>Be sure the account you are using to access DB2 has permissions to Delete from the table you are working with.</li>\n<li>If the table you are attempting to truncate is empty, it will generate an error in DB2. To get around this, modify your transform to get a row count of the table and use a switch statement to run your truncate script or go to a Dummy statement, depending on whether or not the table is already empty. The transform should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/07/image001.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/image001-300x119.png\" alt=\"image001\"></a></li>\n</ul>\n<p>PENTAHO is a registered trademark of Pentaho, Inc.</p>"},{"title":"Pentaho Kettle - Create Destination Tables - on the fly","id":"2089","comments":0,"date":"2014-05-15T07:29:36.000Z","_content":"\n[![fly](http://edpflager.com/wp-content/uploads/2014/05/fly-300x200.png)](http://edpflager.com/wp-content/uploads/2014/05/fly.png)A quick tip today that can save you a lot of time when using Pentaho Kettle (aka PDI) to move data from one system to another. If you don't have your tables created in your destination system, you can switch between PDI and your database system's management software to create your tables, but there is an easier way. If the account that is used in your database connection in Pentaho has permissions to create tables  on the destination database, you can create new tables from within your transformation workflow!\n<!-- more -->\n1.  To start, create a new transformation.\n2.  Add a Table Input element to your transformation. Double click the element and choose your database connection. Click the Get SQL select statement and find the table you want to copy. When prompted to include the field names, choose Yes.\n3.  Add a Table Output destination element and create a hop from the Table Input to the Table Output element.\n4.  Double click your destination element and select your destination connection. For your destination table, enter the name for a non-existent table.\n5.  Click the SQL button. You'll see a SQL statement to create the destination table. Click EXECUTE to have Pentaho create the table. That's it! Pentaho has created your output table.\n6.  Close the Execute results window and then the SQL Editor to return to the Table Output window. Close the Table Output window and run your Transformation to have your new table populated!\n\nBonus: If your source table changes, you can open the Table Output window and click the SQL button to generate an alter statement!","source":"_posts/pentaho-kettle-create-destination-tables-on-the-fly.md","raw":"---\ntitle: Pentaho Kettle - Create Destination Tables - on the fly\ntags:\n  - ETL\n  - howto\n  - kettle\n  - PDI\nid: '2089'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-05-15 03:29:36\n---\n\n[![fly](http://edpflager.com/wp-content/uploads/2014/05/fly-300x200.png)](http://edpflager.com/wp-content/uploads/2014/05/fly.png)A quick tip today that can save you a lot of time when using Pentaho Kettle (aka PDI) to move data from one system to another. If you don't have your tables created in your destination system, you can switch between PDI and your database system's management software to create your tables, but there is an easier way. If the account that is used in your database connection in Pentaho has permissions to create tables  on the destination database, you can create new tables from within your transformation workflow!\n<!-- more -->\n1.  To start, create a new transformation.\n2.  Add a Table Input element to your transformation. Double click the element and choose your database connection. Click the Get SQL select statement and find the table you want to copy. When prompted to include the field names, choose Yes.\n3.  Add a Table Output destination element and create a hop from the Table Input to the Table Output element.\n4.  Double click your destination element and select your destination connection. For your destination table, enter the name for a non-existent table.\n5.  Click the SQL button. You'll see a SQL statement to create the destination table. Click EXECUTE to have Pentaho create the table. That's it! Pentaho has created your output table.\n6.  Close the Execute results window and then the SQL Editor to return to the Table Output window. Close the Table Output window and run your Transformation to have your new table populated!\n\nBonus: If your source table changes, you can open the Table Output window and click the SQL button to generate an alter statement!","slug":"pentaho-kettle-create-destination-tables-on-the-fly","published":1,"updated":"2020-08-23T20:54:34.842Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a6b007dsdjx76n6h8jv","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/fly.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/fly-300x200.png\" alt=\"fly\"></a>A quick tip today that can save you a lot of time when using Pentaho Kettle (aka PDI) to move data from one system to another. If you don’t have your tables created in your destination system, you can switch between PDI and your database system’s management software to create your tables, but there is an easier way. If the account that is used in your database connection in Pentaho has permissions to create tables  on the destination database, you can create new tables from within your transformation workflow!</p>\n<a id=\"more\"></a>\n<ol>\n<li>To start, create a new transformation.</li>\n<li>Add a Table Input element to your transformation. Double click the element and choose your database connection. Click the Get SQL select statement and find the table you want to copy. When prompted to include the field names, choose Yes.</li>\n<li>Add a Table Output destination element and create a hop from the Table Input to the Table Output element.</li>\n<li>Double click your destination element and select your destination connection. For your destination table, enter the name for a non-existent table.</li>\n<li>Click the SQL button. You’ll see a SQL statement to create the destination table. Click EXECUTE to have Pentaho create the table. That’s it! Pentaho has created your output table.</li>\n<li>Close the Execute results window and then the SQL Editor to return to the Table Output window. Close the Table Output window and run your Transformation to have your new table populated!</li>\n</ol>\n<p>Bonus: If your source table changes, you can open the Table Output window and click the SQL button to generate an alter statement!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/fly.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/fly-300x200.png\" alt=\"fly\"></a>A quick tip today that can save you a lot of time when using Pentaho Kettle (aka PDI) to move data from one system to another. If you don’t have your tables created in your destination system, you can switch between PDI and your database system’s management software to create your tables, but there is an easier way. If the account that is used in your database connection in Pentaho has permissions to create tables  on the destination database, you can create new tables from within your transformation workflow!</p>","more":"<ol>\n<li>To start, create a new transformation.</li>\n<li>Add a Table Input element to your transformation. Double click the element and choose your database connection. Click the Get SQL select statement and find the table you want to copy. When prompted to include the field names, choose Yes.</li>\n<li>Add a Table Output destination element and create a hop from the Table Input to the Table Output element.</li>\n<li>Double click your destination element and select your destination connection. For your destination table, enter the name for a non-existent table.</li>\n<li>Click the SQL button. You’ll see a SQL statement to create the destination table. Click EXECUTE to have Pentaho create the table. That’s it! Pentaho has created your output table.</li>\n<li>Close the Execute results window and then the SQL Editor to return to the Table Output window. Close the Table Output window and run your Transformation to have your new table populated!</li>\n</ol>\n<p>Bonus: If your source table changes, you can open the Table Output window and click the SQL button to generate an alter statement!</p>"},{"title":"Pentaho Kettle Spoon - Damaged message on Mac OSX Mavericks","id":"1901","comments":0,"date":"2014-03-26T19:54:02.000Z","_content":"\n![AppDamaged](http://edpflager.com/wp-content/uploads/2014/03/AppDamaged-300x125.png)Spoon is the graphical front end for designing ETL workflows for Pentaho Data Integration also known as Kettle. The latest community edition (5.01) was released in November of 2013, with versions for Windows, Linux and Macs. On the first two platforms it works very well as soon as you extract the archive, but unfortunately on Mac OS X 10.9 (Mavericks) there are some issues. It is possible to get it run, but its not easy. I'll assume that you have Data Integration downloaded, and extracted on your system and Java 1.6 installed.  The instructions from Pentaho say you can run the Data Integration.app to launch Kettle, but on the systems I've tried this on, I get an error message that the App is damaged. If you are experiencing this, don't click \"Move to Trash!\" There is a couple of ways to get it working. :) The first method is pretty straightforward.\n\n1.  While you are clicking on the Data Integration application, hold down the Control key on your keyboard. A menu will appear and you can then click on Open near the top. You'll then see an Are You Sure warning window, where you can click Open again. The application will then start. Simple!\n<!-- more -->\nIf you're like me, and sometimes forget about using the Control key, you can bypass that functionality using this handy tutorial.\n\n1.  Go to System Preferences in OSX (hold down Command and Space for a second, then type SYS into the Spotlight window, and hit Enter).\n2.  Click on Show All if the entire System Preferences is not shown, then click on Security and Privacy.![SecurityIcon](http://edpflager.com/wp-content/uploads/2014/03/SecurityIcon-300x73.png)\n3.  In the Security and Privacy window, make sure you are on the General page, and click on the Lock icon in the bottom left of the screen. Next you'll have to enter your password to unlock the settings.\n4.  In the bottom section, under \"Allow apps downloaded from:\" select the option labeled, Anywhere. (Keep in mind this setting does make your Mac less secure, so use it at your own risk!) You'll get a pop-up message to that effect from Apple as well. Click the \"Allow from Anywhere\" button. ![SecuritySetting](http://edpflager.com/wp-content/uploads/2014/03/SecuritySetting-300x235.png)\n5.  Click the Lock icon again to disallow any other changes, close the System Preferences window, and you are good to go. Start Data Integration up and you should see the normal startup screen.","source":"_posts/pentaho-kettle-spoon-damaged-message-on-mac-osx-mavericks.md","raw":"---\ntitle: Pentaho Kettle Spoon - Damaged message on Mac OSX Mavericks\ntags:\n  - ETL\n  - kettle\n  - Mac\n  - PDI\nid: '1901'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-03-26 15:54:02\n---\n\n![AppDamaged](http://edpflager.com/wp-content/uploads/2014/03/AppDamaged-300x125.png)Spoon is the graphical front end for designing ETL workflows for Pentaho Data Integration also known as Kettle. The latest community edition (5.01) was released in November of 2013, with versions for Windows, Linux and Macs. On the first two platforms it works very well as soon as you extract the archive, but unfortunately on Mac OS X 10.9 (Mavericks) there are some issues. It is possible to get it run, but its not easy. I'll assume that you have Data Integration downloaded, and extracted on your system and Java 1.6 installed.  The instructions from Pentaho say you can run the Data Integration.app to launch Kettle, but on the systems I've tried this on, I get an error message that the App is damaged. If you are experiencing this, don't click \"Move to Trash!\" There is a couple of ways to get it working. :) The first method is pretty straightforward.\n\n1.  While you are clicking on the Data Integration application, hold down the Control key on your keyboard. A menu will appear and you can then click on Open near the top. You'll then see an Are You Sure warning window, where you can click Open again. The application will then start. Simple!\n<!-- more -->\nIf you're like me, and sometimes forget about using the Control key, you can bypass that functionality using this handy tutorial.\n\n1.  Go to System Preferences in OSX (hold down Command and Space for a second, then type SYS into the Spotlight window, and hit Enter).\n2.  Click on Show All if the entire System Preferences is not shown, then click on Security and Privacy.![SecurityIcon](http://edpflager.com/wp-content/uploads/2014/03/SecurityIcon-300x73.png)\n3.  In the Security and Privacy window, make sure you are on the General page, and click on the Lock icon in the bottom left of the screen. Next you'll have to enter your password to unlock the settings.\n4.  In the bottom section, under \"Allow apps downloaded from:\" select the option labeled, Anywhere. (Keep in mind this setting does make your Mac less secure, so use it at your own risk!) You'll get a pop-up message to that effect from Apple as well. Click the \"Allow from Anywhere\" button. ![SecuritySetting](http://edpflager.com/wp-content/uploads/2014/03/SecuritySetting-300x235.png)\n5.  Click the Lock icon again to disallow any other changes, close the System Preferences window, and you are good to go. Start Data Integration up and you should see the normal startup screen.","slug":"pentaho-kettle-spoon-damaged-message-on-mac-osx-mavericks","published":1,"updated":"2020-08-23T20:54:34.810Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a6v007hsdjxhaz3apjo","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/03/AppDamaged-300x125.png\" alt=\"AppDamaged\">Spoon is the graphical front end for designing ETL workflows for Pentaho Data Integration also known as Kettle. The latest community edition (5.01) was released in November of 2013, with versions for Windows, Linux and Macs. On the first two platforms it works very well as soon as you extract the archive, but unfortunately on Mac OS X 10.9 (Mavericks) there are some issues. It is possible to get it run, but its not easy. I’ll assume that you have Data Integration downloaded, and extracted on your system and Java 1.6 installed.  The instructions from Pentaho say you can run the Data Integration.app to launch Kettle, but on the systems I’ve tried this on, I get an error message that the App is damaged. If you are experiencing this, don’t click “Move to Trash!” There is a couple of ways to get it working. :) The first method is pretty straightforward.</p>\n<ol>\n<li><p>While you are clicking on the Data Integration application, hold down the Control key on your keyboard. A menu will appear and you can then click on Open near the top. You’ll then see an Are You Sure warning window, where you can click Open again. The application will then start. Simple!</p>\n<a id=\"more\"></a>\n<p>If you’re like me, and sometimes forget about using the Control key, you can bypass that functionality using this handy tutorial.</p>\n</li>\n<li><p>Go to System Preferences in OSX (hold down Command and Space for a second, then type SYS into the Spotlight window, and hit Enter).</p>\n</li>\n<li><p>Click on Show All if the entire System Preferences is not shown, then click on Security and Privacy.<img src=\"http://edpflager.com/wp-content/uploads/2014/03/SecurityIcon-300x73.png\" alt=\"SecurityIcon\"></p>\n</li>\n<li><p>In the Security and Privacy window, make sure you are on the General page, and click on the Lock icon in the bottom left of the screen. Next you’ll have to enter your password to unlock the settings.</p>\n</li>\n<li><p>In the bottom section, under “Allow apps downloaded from:” select the option labeled, Anywhere. (Keep in mind this setting does make your Mac less secure, so use it at your own risk!) You’ll get a pop-up message to that effect from Apple as well. Click the “Allow from Anywhere” button. <img src=\"http://edpflager.com/wp-content/uploads/2014/03/SecuritySetting-300x235.png\" alt=\"SecuritySetting\"></p>\n</li>\n<li><p>Click the Lock icon again to disallow any other changes, close the System Preferences window, and you are good to go. Start Data Integration up and you should see the normal startup screen.</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/03/AppDamaged-300x125.png\" alt=\"AppDamaged\">Spoon is the graphical front end for designing ETL workflows for Pentaho Data Integration also known as Kettle. The latest community edition (5.01) was released in November of 2013, with versions for Windows, Linux and Macs. On the first two platforms it works very well as soon as you extract the archive, but unfortunately on Mac OS X 10.9 (Mavericks) there are some issues. It is possible to get it run, but its not easy. I’ll assume that you have Data Integration downloaded, and extracted on your system and Java 1.6 installed.  The instructions from Pentaho say you can run the Data Integration.app to launch Kettle, but on the systems I’ve tried this on, I get an error message that the App is damaged. If you are experiencing this, don’t click “Move to Trash!” There is a couple of ways to get it working. :) The first method is pretty straightforward.</p>\n<ol>\n<li><p>While you are clicking on the Data Integration application, hold down the Control key on your keyboard. A menu will appear and you can then click on Open near the top. You’ll then see an Are You Sure warning window, where you can click Open again. The application will then start. Simple!</p>","more":"<p>If you’re like me, and sometimes forget about using the Control key, you can bypass that functionality using this handy tutorial.</p>\n</li>\n<li><p>Go to System Preferences in OSX (hold down Command and Space for a second, then type SYS into the Spotlight window, and hit Enter).</p>\n</li>\n<li><p>Click on Show All if the entire System Preferences is not shown, then click on Security and Privacy.<img src=\"http://edpflager.com/wp-content/uploads/2014/03/SecurityIcon-300x73.png\" alt=\"SecurityIcon\"></p>\n</li>\n<li><p>In the Security and Privacy window, make sure you are on the General page, and click on the Lock icon in the bottom left of the screen. Next you’ll have to enter your password to unlock the settings.</p>\n</li>\n<li><p>In the bottom section, under “Allow apps downloaded from:” select the option labeled, Anywhere. (Keep in mind this setting does make your Mac less secure, so use it at your own risk!) You’ll get a pop-up message to that effect from Apple as well. Click the “Allow from Anywhere” button. <img src=\"http://edpflager.com/wp-content/uploads/2014/03/SecuritySetting-300x235.png\" alt=\"SecuritySetting\"></p>\n</li>\n<li><p>Click the Lock icon again to disallow any other changes, close the System Preferences window, and you are good to go. Start Data Integration up and you should see the normal startup screen.</p>\n</li>\n</ol>"},{"title":"Pentaho Kettle Time and Date Manipulations - Part 2","id":"2178","comments":0,"date":"2014-07-04T22:50:05.000Z","_content":"\n[![Transform](http://edpflager.com/wp-content/uploads/2014/07/Transform-300x127.png)](http://edpflager.com/wp-content/uploads/2014/07/Transform.png)When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. In part 1 of this series, I looked at using the GET SYSTEM INFO function in Pentaho which pulled various date and time values based on your current system values. The other step you can add to your Kettle transformations to work with Date and Time values is the Calculator step.\n\n##### Calculator\n\nTo add the Calculator function to your ETL process, open the Transform folder in the Design tab, and drag the Calculator function onto the calculator. Double click to open it. This window is a little more complex than the Get System Info step, but not significantly, but that belies the depth of functionality it provides.\n<!-- more -->\nIn the New field box, enter a name for the result you will be creating. Again, you can define multiple items here, so give them meaningful names in the context of your transformation. Skipping the Calculation column momentarily, there are Field A, B and C columns where you specify the parameters that will be used in the calculation. These can be passed from a step in your transformation or you can use the result of a calculation from an earlier line in the window as one of the fields. So for example if you define a New field in step 1  called Test, you can use Test as Field a, B or C in a line 2 or higher. After the Field columns are various options for defining the output such as data types and formatting. There is a column called Remove as well which allows you to keep temporary values from being passed on to the next steps in your transformation. Just set it to Y and those values will be dropped after the Calculator step. The Calculation step is where the bulk of the functionality is in this component. Click on the Calculation field, and you'll be presented with a large number of options as well.  As with the GET SYSTEM INFO function, there are more than just date and time manipulations here. You'll see options for :\n\n*   Basic math functions like square root and rounding, division, multiplication, addition and subtraction.\n*   Hash calculations using a variety of checksum algorithms like CRC-32 and MD5\n*   XML and HTML manipulation and validation - especially useful is an option to check to see if your XML is well-formed!\n*   String and character concatenation, conversion and comparision - remove and add control characters, encoding back and forth with HEX and BYTE, switch case between upper and lower, and number and specific character extractions as well as several steps to compare how similar two strings are.\n\nIn a future post, I'll delve into these more in detail. But for more now, lets look at the time and date calculations that are available. They tend to fall into two categories: adding and subtracting to and from a given start date and time, or extracting portions of a given date to get a smaller portion of it such as the day of the week, the hour of the day or even the quarter of the year.\n\n##### DATE/TIME EXTRACTION EXAMPLE\n\nStart a new transformation, and drag the GET SYSTEM INFO step we discussed last time on to the canvas. Open it up, and define a field called GetNow. For the Type, choose system date (variable). Click on the Preview button, and accept the default number of rows and click OK. A new window will open showing the GetNow field has a value set to your current System time.  Click CLOSE and then OK to return to your canvas.[![GetNow](http://edpflager.com/wp-content/uploads/2014/07/GetNow-300x194.png)](http://edpflager.com/wp-content/uploads/2014/07/GetNow.png)\n\nDrag a CALCULATOR step on to your canvas. Connect the two steps by dragging a hop between them. Open the Calculator step and define eight rows, with the fields set to:\n\n*   Year,\n*   Hour,\n*   Minute,\n*   Second,\n*   MonthOfYear,\n*   DayOfWeek,\n*   DayOfMonth, and\n*   DayOfYear.\n\nIn the field A column for each of your rows, set the value to GetNow (you can click the drop down list and it should be there, or go ahead and type it in). In the Value column, set each row to Integer. The Remove column can be left as blank or set to N. For the calculation column, for the first row, set it to Year of Date A. For the second row, Hour of Day of Date A. For the remaining rows, use:\n\n*   Minute of Hour of Date A,\n*   Second of Minute of Date A,\n*   Month of Date A,\n*   Day of week of date A,\n*   Day of Month of date A, and\n*   Day of Year of Date A.\n\nWhen you are done your calculation step should look like the image below. Click OK to return to your canvas.[![Calculator](http://edpflager.com/wp-content/uploads/2014/07/Calculator-300x143.png)](http://edpflager.com/wp-content/uploads/2014/07/Calculator.png) Save your transformation, and go ahead and run it. Switch over to the Preview Data tab in your transformation results panel, and click on the Calculator step. You should see something similar to the image below, with the first column showing the time when the transformation ran and then separate columns for the various calculations.[![Results](http://edpflager.com/wp-content/uploads/2014/07/Results-300x131.png)](http://edpflager.com/wp-content/uploads/2014/07/Results.png) **A couple of things to note here is that the extracted values are integer data types and not time values including the day of the week. For Pentaho, the week starts on Sunday as day 1 and ends on Saturday as Day 7.** PENTAHO  is a registered trademark of Pentaho, Inc.","source":"_posts/pentaho-kettle-time-and-date-manipulations-part-2.md","raw":"---\ntitle: Pentaho Kettle Time and Date Manipulations - Part 2\ntags:\n  - ETL\n  - How-to\n  - kettle\n  - technical\nid: '2178'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-07-04 18:50:05\n---\n\n[![Transform](http://edpflager.com/wp-content/uploads/2014/07/Transform-300x127.png)](http://edpflager.com/wp-content/uploads/2014/07/Transform.png)When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. In part 1 of this series, I looked at using the GET SYSTEM INFO function in Pentaho which pulled various date and time values based on your current system values. The other step you can add to your Kettle transformations to work with Date and Time values is the Calculator step.\n\n##### Calculator\n\nTo add the Calculator function to your ETL process, open the Transform folder in the Design tab, and drag the Calculator function onto the calculator. Double click to open it. This window is a little more complex than the Get System Info step, but not significantly, but that belies the depth of functionality it provides.\n<!-- more -->\nIn the New field box, enter a name for the result you will be creating. Again, you can define multiple items here, so give them meaningful names in the context of your transformation. Skipping the Calculation column momentarily, there are Field A, B and C columns where you specify the parameters that will be used in the calculation. These can be passed from a step in your transformation or you can use the result of a calculation from an earlier line in the window as one of the fields. So for example if you define a New field in step 1  called Test, you can use Test as Field a, B or C in a line 2 or higher. After the Field columns are various options for defining the output such as data types and formatting. There is a column called Remove as well which allows you to keep temporary values from being passed on to the next steps in your transformation. Just set it to Y and those values will be dropped after the Calculator step. The Calculation step is where the bulk of the functionality is in this component. Click on the Calculation field, and you'll be presented with a large number of options as well.  As with the GET SYSTEM INFO function, there are more than just date and time manipulations here. You'll see options for :\n\n*   Basic math functions like square root and rounding, division, multiplication, addition and subtraction.\n*   Hash calculations using a variety of checksum algorithms like CRC-32 and MD5\n*   XML and HTML manipulation and validation - especially useful is an option to check to see if your XML is well-formed!\n*   String and character concatenation, conversion and comparision - remove and add control characters, encoding back and forth with HEX and BYTE, switch case between upper and lower, and number and specific character extractions as well as several steps to compare how similar two strings are.\n\nIn a future post, I'll delve into these more in detail. But for more now, lets look at the time and date calculations that are available. They tend to fall into two categories: adding and subtracting to and from a given start date and time, or extracting portions of a given date to get a smaller portion of it such as the day of the week, the hour of the day or even the quarter of the year.\n\n##### DATE/TIME EXTRACTION EXAMPLE\n\nStart a new transformation, and drag the GET SYSTEM INFO step we discussed last time on to the canvas. Open it up, and define a field called GetNow. For the Type, choose system date (variable). Click on the Preview button, and accept the default number of rows and click OK. A new window will open showing the GetNow field has a value set to your current System time.  Click CLOSE and then OK to return to your canvas.[![GetNow](http://edpflager.com/wp-content/uploads/2014/07/GetNow-300x194.png)](http://edpflager.com/wp-content/uploads/2014/07/GetNow.png)\n\nDrag a CALCULATOR step on to your canvas. Connect the two steps by dragging a hop between them. Open the Calculator step and define eight rows, with the fields set to:\n\n*   Year,\n*   Hour,\n*   Minute,\n*   Second,\n*   MonthOfYear,\n*   DayOfWeek,\n*   DayOfMonth, and\n*   DayOfYear.\n\nIn the field A column for each of your rows, set the value to GetNow (you can click the drop down list and it should be there, or go ahead and type it in). In the Value column, set each row to Integer. The Remove column can be left as blank or set to N. For the calculation column, for the first row, set it to Year of Date A. For the second row, Hour of Day of Date A. For the remaining rows, use:\n\n*   Minute of Hour of Date A,\n*   Second of Minute of Date A,\n*   Month of Date A,\n*   Day of week of date A,\n*   Day of Month of date A, and\n*   Day of Year of Date A.\n\nWhen you are done your calculation step should look like the image below. Click OK to return to your canvas.[![Calculator](http://edpflager.com/wp-content/uploads/2014/07/Calculator-300x143.png)](http://edpflager.com/wp-content/uploads/2014/07/Calculator.png) Save your transformation, and go ahead and run it. Switch over to the Preview Data tab in your transformation results panel, and click on the Calculator step. You should see something similar to the image below, with the first column showing the time when the transformation ran and then separate columns for the various calculations.[![Results](http://edpflager.com/wp-content/uploads/2014/07/Results-300x131.png)](http://edpflager.com/wp-content/uploads/2014/07/Results.png) **A couple of things to note here is that the extracted values are integer data types and not time values including the day of the week. For Pentaho, the week starts on Sunday as day 1 and ends on Saturday as Day 7.** PENTAHO  is a registered trademark of Pentaho, Inc.","slug":"pentaho-kettle-time-and-date-manipulations-part-2","published":1,"updated":"2020-08-23T20:54:34.870Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a6z007ksdjx3xvzcsax","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/Transform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Transform-300x127.png\" alt=\"Transform\"></a>When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. In part 1 of this series, I looked at using the GET SYSTEM INFO function in Pentaho which pulled various date and time values based on your current system values. The other step you can add to your Kettle transformations to work with Date and Time values is the Calculator step.</p>\n<h5 id=\"Calculator\"><a href=\"#Calculator\" class=\"headerlink\" title=\"Calculator\"></a>Calculator</h5><p>To add the Calculator function to your ETL process, open the Transform folder in the Design tab, and drag the Calculator function onto the calculator. Double click to open it. This window is a little more complex than the Get System Info step, but not significantly, but that belies the depth of functionality it provides.</p>\n<a id=\"more\"></a>\n<p>In the New field box, enter a name for the result you will be creating. Again, you can define multiple items here, so give them meaningful names in the context of your transformation. Skipping the Calculation column momentarily, there are Field A, B and C columns where you specify the parameters that will be used in the calculation. These can be passed from a step in your transformation or you can use the result of a calculation from an earlier line in the window as one of the fields. So for example if you define a New field in step 1  called Test, you can use Test as Field a, B or C in a line 2 or higher. After the Field columns are various options for defining the output such as data types and formatting. There is a column called Remove as well which allows you to keep temporary values from being passed on to the next steps in your transformation. Just set it to Y and those values will be dropped after the Calculator step. The Calculation step is where the bulk of the functionality is in this component. Click on the Calculation field, and you’ll be presented with a large number of options as well.  As with the GET SYSTEM INFO function, there are more than just date and time manipulations here. You’ll see options for :</p>\n<ul>\n<li>Basic math functions like square root and rounding, division, multiplication, addition and subtraction.</li>\n<li>Hash calculations using a variety of checksum algorithms like CRC-32 and MD5</li>\n<li>XML and HTML manipulation and validation - especially useful is an option to check to see if your XML is well-formed!</li>\n<li>String and character concatenation, conversion and comparision - remove and add control characters, encoding back and forth with HEX and BYTE, switch case between upper and lower, and number and specific character extractions as well as several steps to compare how similar two strings are.</li>\n</ul>\n<p>In a future post, I’ll delve into these more in detail. But for more now, lets look at the time and date calculations that are available. They tend to fall into two categories: adding and subtracting to and from a given start date and time, or extracting portions of a given date to get a smaller portion of it such as the day of the week, the hour of the day or even the quarter of the year.</p>\n<h5 id=\"DATE-TIME-EXTRACTION-EXAMPLE\"><a href=\"#DATE-TIME-EXTRACTION-EXAMPLE\" class=\"headerlink\" title=\"DATE/TIME EXTRACTION EXAMPLE\"></a>DATE/TIME EXTRACTION EXAMPLE</h5><p>Start a new transformation, and drag the GET SYSTEM INFO step we discussed last time on to the canvas. Open it up, and define a field called GetNow. For the Type, choose system date (variable). Click on the Preview button, and accept the default number of rows and click OK. A new window will open showing the GetNow field has a value set to your current System time.  Click CLOSE and then OK to return to your canvas.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/GetNow.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/GetNow-300x194.png\" alt=\"GetNow\"></a></p>\n<p>Drag a CALCULATOR step on to your canvas. Connect the two steps by dragging a hop between them. Open the Calculator step and define eight rows, with the fields set to:</p>\n<ul>\n<li>Year,</li>\n<li>Hour,</li>\n<li>Minute,</li>\n<li>Second,</li>\n<li>MonthOfYear,</li>\n<li>DayOfWeek,</li>\n<li>DayOfMonth, and</li>\n<li>DayOfYear.</li>\n</ul>\n<p>In the field A column for each of your rows, set the value to GetNow (you can click the drop down list and it should be there, or go ahead and type it in). In the Value column, set each row to Integer. The Remove column can be left as blank or set to N. For the calculation column, for the first row, set it to Year of Date A. For the second row, Hour of Day of Date A. For the remaining rows, use:</p>\n<ul>\n<li>Minute of Hour of Date A,</li>\n<li>Second of Minute of Date A,</li>\n<li>Month of Date A,</li>\n<li>Day of week of date A,</li>\n<li>Day of Month of date A, and</li>\n<li>Day of Year of Date A.</li>\n</ul>\n<p>When you are done your calculation step should look like the image below. Click OK to return to your canvas.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/Calculator.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Calculator-300x143.png\" alt=\"Calculator\"></a> Save your transformation, and go ahead and run it. Switch over to the Preview Data tab in your transformation results panel, and click on the Calculator step. You should see something similar to the image below, with the first column showing the time when the transformation ran and then separate columns for the various calculations.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/Results.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Results-300x131.png\" alt=\"Results\"></a> <strong>A couple of things to note here is that the extracted values are integer data types and not time values including the day of the week. For Pentaho, the week starts on Sunday as day 1 and ends on Saturday as Day 7.</strong> PENTAHO  is a registered trademark of Pentaho, Inc.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/07/Transform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Transform-300x127.png\" alt=\"Transform\"></a>When using an ETL tool, one requirement that you will likely run into over and over again will be dealing with dates. In part 1 of this series, I looked at using the GET SYSTEM INFO function in Pentaho which pulled various date and time values based on your current system values. The other step you can add to your Kettle transformations to work with Date and Time values is the Calculator step.</p>\n<h5 id=\"Calculator\"><a href=\"#Calculator\" class=\"headerlink\" title=\"Calculator\"></a>Calculator</h5><p>To add the Calculator function to your ETL process, open the Transform folder in the Design tab, and drag the Calculator function onto the calculator. Double click to open it. This window is a little more complex than the Get System Info step, but not significantly, but that belies the depth of functionality it provides.</p>","more":"<p>In the New field box, enter a name for the result you will be creating. Again, you can define multiple items here, so give them meaningful names in the context of your transformation. Skipping the Calculation column momentarily, there are Field A, B and C columns where you specify the parameters that will be used in the calculation. These can be passed from a step in your transformation or you can use the result of a calculation from an earlier line in the window as one of the fields. So for example if you define a New field in step 1  called Test, you can use Test as Field a, B or C in a line 2 or higher. After the Field columns are various options for defining the output such as data types and formatting. There is a column called Remove as well which allows you to keep temporary values from being passed on to the next steps in your transformation. Just set it to Y and those values will be dropped after the Calculator step. The Calculation step is where the bulk of the functionality is in this component. Click on the Calculation field, and you’ll be presented with a large number of options as well.  As with the GET SYSTEM INFO function, there are more than just date and time manipulations here. You’ll see options for :</p>\n<ul>\n<li>Basic math functions like square root and rounding, division, multiplication, addition and subtraction.</li>\n<li>Hash calculations using a variety of checksum algorithms like CRC-32 and MD5</li>\n<li>XML and HTML manipulation and validation - especially useful is an option to check to see if your XML is well-formed!</li>\n<li>String and character concatenation, conversion and comparision - remove and add control characters, encoding back and forth with HEX and BYTE, switch case between upper and lower, and number and specific character extractions as well as several steps to compare how similar two strings are.</li>\n</ul>\n<p>In a future post, I’ll delve into these more in detail. But for more now, lets look at the time and date calculations that are available. They tend to fall into two categories: adding and subtracting to and from a given start date and time, or extracting portions of a given date to get a smaller portion of it such as the day of the week, the hour of the day or even the quarter of the year.</p>\n<h5 id=\"DATE-TIME-EXTRACTION-EXAMPLE\"><a href=\"#DATE-TIME-EXTRACTION-EXAMPLE\" class=\"headerlink\" title=\"DATE/TIME EXTRACTION EXAMPLE\"></a>DATE/TIME EXTRACTION EXAMPLE</h5><p>Start a new transformation, and drag the GET SYSTEM INFO step we discussed last time on to the canvas. Open it up, and define a field called GetNow. For the Type, choose system date (variable). Click on the Preview button, and accept the default number of rows and click OK. A new window will open showing the GetNow field has a value set to your current System time.  Click CLOSE and then OK to return to your canvas.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/GetNow.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/GetNow-300x194.png\" alt=\"GetNow\"></a></p>\n<p>Drag a CALCULATOR step on to your canvas. Connect the two steps by dragging a hop between them. Open the Calculator step and define eight rows, with the fields set to:</p>\n<ul>\n<li>Year,</li>\n<li>Hour,</li>\n<li>Minute,</li>\n<li>Second,</li>\n<li>MonthOfYear,</li>\n<li>DayOfWeek,</li>\n<li>DayOfMonth, and</li>\n<li>DayOfYear.</li>\n</ul>\n<p>In the field A column for each of your rows, set the value to GetNow (you can click the drop down list and it should be there, or go ahead and type it in). In the Value column, set each row to Integer. The Remove column can be left as blank or set to N. For the calculation column, for the first row, set it to Year of Date A. For the second row, Hour of Day of Date A. For the remaining rows, use:</p>\n<ul>\n<li>Minute of Hour of Date A,</li>\n<li>Second of Minute of Date A,</li>\n<li>Month of Date A,</li>\n<li>Day of week of date A,</li>\n<li>Day of Month of date A, and</li>\n<li>Day of Year of Date A.</li>\n</ul>\n<p>When you are done your calculation step should look like the image below. Click OK to return to your canvas.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/Calculator.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Calculator-300x143.png\" alt=\"Calculator\"></a> Save your transformation, and go ahead and run it. Switch over to the Preview Data tab in your transformation results panel, and click on the Calculator step. You should see something similar to the image below, with the first column showing the time when the transformation ran and then separate columns for the various calculations.<a href=\"http://edpflager.com/wp-content/uploads/2014/07/Results.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/07/Results-300x131.png\" alt=\"Results\"></a> <strong>A couple of things to note here is that the extracted values are integer data types and not time values including the day of the week. For Pentaho, the week starts on Sunday as day 1 and ends on Saturday as Day 7.</strong> PENTAHO  is a registered trademark of Pentaho, Inc.</p>"},{"title":"Pentaho: To Lookup or Not to Lookup","id":"2683","comments":0,"date":"2015-03-17T22:19:10.000Z","_content":"\n[![faq](http://edpflager.com/wp-content/uploads/2015/03/faq-300x300.jpg)](http://edpflager.com/wp-content/uploads/2015/03/faq.jpg)Generally when developing an ETL process, if you have to replace a value from a source with a corresponding value, you should use a lookup table. For example, if you were replacing a country abbreviation with the full name of  the country, you could have a simple 2 column table with the abbreviation in one column and the full name in the other. By using a lookup table it becomes very easy to update values, enter new ones, or possibly delete obsolete ones. This also gives you the added benefit of being able to reuse your lookup table if you need to in other places. But what if you have a small group of items (say a dozen or less) that you need to replace? In that case you might want to look at using Pentaho's Value Mapper component.\n<!-- more -->\nFor this example, I am using a simple MySQL database I have, containing books I read, and the month and year when I read them (yes I am a geek). The source table can be created with this code:\n\nCREATE TABLE \\`booksread\\` (\n \\`Title\\` varchar(74) DEFAULT NULL,\n \\`Author\\` varchar(33) DEFAULT NULL,\n \\`Rating\\` varchar(6) DEFAULT NULL,\n \\`MonthRead\\` varchar(9) DEFAULT NULL,\n \\`YearRead\\` varchar(15) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\nWith the table created, I start a new Pentaho transformation with a connection defined to this database. I drag a Table Input step and a Text file output component to the work area. Between them, I drag a Value Mapper component from the Transform node in the Design panel, and connect them together like this: [![value_mapper1](http://edpflager.com/wp-content/uploads/2015/03/value_mapper1-300x88.png)](http://edpflager.com/wp-content/uploads/2015/03/value_mapper1.png)   The Table Input is fairly generic, with a Select on the various fields.  If you preview the table input, you would see the month name in the source (January, February,etc).The text file output component accepts all of the defaults except the output is written to the desktop. Opening the Value Mapper, in the header area, I identified the source field I want to replace by selecting it from the drop down list.  The target field name can remain empty if you are replacing the existing field, or a new field name can be supplied. Then a default value can be specified in the event that none of the values you define are found. Think of it as an ELSE value.  After that I have entered in the values that I would expect to see along with the value I want to replace them with. For January, replace it with 1, for February replace with 2, and so on. The finished result looks like this:[![value_mapper3](http://edpflager.com/wp-content/uploads/2015/03/value_mapper3-300x209.png)](http://edpflager.com/wp-content/uploads/2015/03/value_mapper3.png)Save the transformation and run it. As long as everything is defined correctly a new text file will be created with a number replacing the month name in the output.","source":"_posts/pentaho-to-lookup-or-not-to-lookup.md","raw":"---\ntitle: 'Pentaho: To Lookup or Not to Lookup'\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2683'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2015-03-17 18:19:10\n---\n\n[![faq](http://edpflager.com/wp-content/uploads/2015/03/faq-300x300.jpg)](http://edpflager.com/wp-content/uploads/2015/03/faq.jpg)Generally when developing an ETL process, if you have to replace a value from a source with a corresponding value, you should use a lookup table. For example, if you were replacing a country abbreviation with the full name of  the country, you could have a simple 2 column table with the abbreviation in one column and the full name in the other. By using a lookup table it becomes very easy to update values, enter new ones, or possibly delete obsolete ones. This also gives you the added benefit of being able to reuse your lookup table if you need to in other places. But what if you have a small group of items (say a dozen or less) that you need to replace? In that case you might want to look at using Pentaho's Value Mapper component.\n<!-- more -->\nFor this example, I am using a simple MySQL database I have, containing books I read, and the month and year when I read them (yes I am a geek). The source table can be created with this code:\n\nCREATE TABLE \\`booksread\\` (\n \\`Title\\` varchar(74) DEFAULT NULL,\n \\`Author\\` varchar(33) DEFAULT NULL,\n \\`Rating\\` varchar(6) DEFAULT NULL,\n \\`MonthRead\\` varchar(9) DEFAULT NULL,\n \\`YearRead\\` varchar(15) DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\nWith the table created, I start a new Pentaho transformation with a connection defined to this database. I drag a Table Input step and a Text file output component to the work area. Between them, I drag a Value Mapper component from the Transform node in the Design panel, and connect them together like this: [![value_mapper1](http://edpflager.com/wp-content/uploads/2015/03/value_mapper1-300x88.png)](http://edpflager.com/wp-content/uploads/2015/03/value_mapper1.png)   The Table Input is fairly generic, with a Select on the various fields.  If you preview the table input, you would see the month name in the source (January, February,etc).The text file output component accepts all of the defaults except the output is written to the desktop. Opening the Value Mapper, in the header area, I identified the source field I want to replace by selecting it from the drop down list.  The target field name can remain empty if you are replacing the existing field, or a new field name can be supplied. Then a default value can be specified in the event that none of the values you define are found. Think of it as an ELSE value.  After that I have entered in the values that I would expect to see along with the value I want to replace them with. For January, replace it with 1, for February replace with 2, and so on. The finished result looks like this:[![value_mapper3](http://edpflager.com/wp-content/uploads/2015/03/value_mapper3-300x209.png)](http://edpflager.com/wp-content/uploads/2015/03/value_mapper3.png)Save the transformation and run it. As long as everything is defined correctly a new text file will be created with a number replacing the month name in the output.","slug":"pentaho-to-lookup-or-not-to-lookup","published":1,"updated":"2020-08-23T20:54:34.926Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a72007osdjxe73j1oit","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/03/faq.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/faq-300x300.jpg\" alt=\"faq\"></a>Generally when developing an ETL process, if you have to replace a value from a source with a corresponding value, you should use a lookup table. For example, if you were replacing a country abbreviation with the full name of  the country, you could have a simple 2 column table with the abbreviation in one column and the full name in the other. By using a lookup table it becomes very easy to update values, enter new ones, or possibly delete obsolete ones. This also gives you the added benefit of being able to reuse your lookup table if you need to in other places. But what if you have a small group of items (say a dozen or less) that you need to replace? In that case you might want to look at using Pentaho’s Value Mapper component.</p>\n<a id=\"more\"></a>\n<p>For this example, I am using a simple MySQL database I have, containing books I read, and the month and year when I read them (yes I am a geek). The source table can be created with this code:</p>\n<p>CREATE TABLE `booksread` (<br> `Title` varchar(74) DEFAULT NULL,<br> `Author` varchar(33) DEFAULT NULL,<br> `Rating` varchar(6) DEFAULT NULL,<br> `MonthRead` varchar(9) DEFAULT NULL,<br> `YearRead` varchar(15) DEFAULT NULL<br>) ENGINE=InnoDB DEFAULT CHARSET=latin1;</p>\n<p>With the table created, I start a new Pentaho transformation with a connection defined to this database. I drag a Table Input step and a Text file output component to the work area. Between them, I drag a Value Mapper component from the Transform node in the Design panel, and connect them together like this: <a href=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper1-300x88.png\" alt=\"value_mapper1\"></a>   The Table Input is fairly generic, with a Select on the various fields.  If you preview the table input, you would see the month name in the source (January, February,etc).The text file output component accepts all of the defaults except the output is written to the desktop. Opening the Value Mapper, in the header area, I identified the source field I want to replace by selecting it from the drop down list.  The target field name can remain empty if you are replacing the existing field, or a new field name can be supplied. Then a default value can be specified in the event that none of the values you define are found. Think of it as an ELSE value.  After that I have entered in the values that I would expect to see along with the value I want to replace them with. For January, replace it with 1, for February replace with 2, and so on. The finished result looks like this:<a href=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper3.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper3-300x209.png\" alt=\"value_mapper3\"></a>Save the transformation and run it. As long as everything is defined correctly a new text file will be created with a number replacing the month name in the output.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/03/faq.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/faq-300x300.jpg\" alt=\"faq\"></a>Generally when developing an ETL process, if you have to replace a value from a source with a corresponding value, you should use a lookup table. For example, if you were replacing a country abbreviation with the full name of  the country, you could have a simple 2 column table with the abbreviation in one column and the full name in the other. By using a lookup table it becomes very easy to update values, enter new ones, or possibly delete obsolete ones. This also gives you the added benefit of being able to reuse your lookup table if you need to in other places. But what if you have a small group of items (say a dozen or less) that you need to replace? In that case you might want to look at using Pentaho’s Value Mapper component.</p>","more":"<p>For this example, I am using a simple MySQL database I have, containing books I read, and the month and year when I read them (yes I am a geek). The source table can be created with this code:</p>\n<p>CREATE TABLE `booksread` (<br> `Title` varchar(74) DEFAULT NULL,<br> `Author` varchar(33) DEFAULT NULL,<br> `Rating` varchar(6) DEFAULT NULL,<br> `MonthRead` varchar(9) DEFAULT NULL,<br> `YearRead` varchar(15) DEFAULT NULL<br>) ENGINE=InnoDB DEFAULT CHARSET=latin1;</p>\n<p>With the table created, I start a new Pentaho transformation with a connection defined to this database. I drag a Table Input step and a Text file output component to the work area. Between them, I drag a Value Mapper component from the Transform node in the Design panel, and connect them together like this: <a href=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper1-300x88.png\" alt=\"value_mapper1\"></a>   The Table Input is fairly generic, with a Select on the various fields.  If you preview the table input, you would see the month name in the source (January, February,etc).The text file output component accepts all of the defaults except the output is written to the desktop. Opening the Value Mapper, in the header area, I identified the source field I want to replace by selecting it from the drop down list.  The target field name can remain empty if you are replacing the existing field, or a new field name can be supplied. Then a default value can be specified in the event that none of the values you define are found. Think of it as an ELSE value.  After that I have entered in the values that I would expect to see along with the value I want to replace them with. For January, replace it with 1, for February replace with 2, and so on. The finished result looks like this:<a href=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper3.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/03/value_mapper3-300x209.png\" alt=\"value_mapper3\"></a>Save the transformation and run it. As long as everything is defined correctly a new text file will be created with a number replacing the month name in the output.</p>"},{"title":"Power BI R Packages - An RVEST example","id":"4212","comments":0,"date":"2018-12-04T23:29:19.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/12/rvest-copy-259x300.png)](http://edpflager.com/wp-content/uploads/2018/12/rvest-copy.png)Activity in my day job often provides inspiration for content here, and this post is an example of that.  In my work, I use Microsoft's SQL Server and Power BI. Both applications now offer some level of support for R:\n\n*   Microsoft now includes an R server with SQL Server\n*   Power BI allows you to use R visualizations to provide functionality that is not included natively.\n*   If you have an R script that cleans and preps your data, Power BI use it as a data source for visualizations and analytics.\n\nPower BI doesn't support all R packages. Of the 10,000 plus packages available in the CRAN repository only 850 or so packages are currently supported. This [webpage](https://docs.microsoft.com/en-us/power-bi/service-r-packages-support) includes an explanation for what they do support and why. All of that is a preface to this post. At the above link is a table of the supported Power BI packages with version number and a URL for the package on CRAN. I wanted to capture that information for incorporation into some (non-work) documentation. I could easily copy and paste it into a spreadsheet or a text file, but I wanted to set it up as a repeatable process in case the website is updated. I could then capture the updated information quickly and easily. Enter [RVEST](https://cran.r-project.org/web/packages/rvest/index.html) - a screen scrapping package for R.\n<!-- more -->\nRVest is one of many R packages authored by Hadley Wickham, famous for GGPLOT2 and the wider Tidyverse set of packages. Its purpose is to easily harvest or scrape webpages. To get the data from the Microsoft webpage about supported R packages, we can use the RVest package to do about 90% of the work and with a little help from Tibble, we can shape the data in to usable format as a data frame. I will present this process first in a long format showing each step that calls different RVest functions, and at the end, I will wrap it up with the Magrittr pipe operator to make the code a bit cleaner. Before we start we need to use some kind of tool to find the element name on the page that we want to harvest. Hadley Wickham on the GitHub page for RVest recommends SelectorGadget an extension for Google Chrome. In RStudio, at a command line enter the following to get more information:\n\nvignette(\"selectorgadget\")\n\nUsing SelectorGadget on the Microsoft webpage, we see that the table we are interested in is not named, but it is the first one on the page. That is enough information to get started.\n\n#### INITIAL VERSION\n\nSwitching over to R Studio after loading our needed packages (rvest and tibble) we call the read\\_html function and pass it the webpage address we want to harvest.\n\n\\# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\nUsing the harvested webpage information stored in our page list, we want to retrieve the table elements sections.\n\n # Element type to grab from page\nnodes <- html\\_nodes(page,\"table\")\n\nSince there are multiple tables on that webpage, and the individual tables are not given unique names we specify the table number as the argument for the html\\_table function and pass it to a new variable. In this case its the first table:\n\n\\# we want the first table\nmsrpackages <- html\\_table(nodes\\[1\\])\n\nFinally, we pass the table variable (msrpackages) to the as.data.frame function, nested within an as\\_tibble function, and store the data frame in a new variable called RPackages.\n\nRPackages <- as\\_tibble(as.data.frame(msrpackages))\n\nTo see the data stored in the tibble, we call the View function with the RPackages element as the argument to generate the results are below.\n\nView(RPackages)\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView-300x143.png)](http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView.png) The long code for this script then looks like below (see my previous post about [pacman](http://edpflager.com/2018/11/12/r-pacman-package-manager-with-require/) for an explanation of that function):\n\n\\# clear environment\nrm(list=ls())\n\nif( !require(pacman)) { install.packages('pacman')}\nlibrary(pacman)\n\np\\_load(rvest, tibble)\n\n# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\n# Element type to grab from page\nnodes <- html\\_nodes(page,\"table\")\n\n# Since there are multiple tables on that webpage, we need to specify\n# that we want the first table\nmsrpackages <- html\\_table(nodes\\[1\\])\n\n# Convert the list into a tibble/data frame.\nRPackages <- as\\_tibble(as.data.frame(msrpackages))\n\n# Display the data\nView(RPackages)\n\n#### CLEANER VERSION\n\nThe code is fairly straightforward, but it can be cleaner if we incorporate the Magrittr pipe function, which Hadley Wickham [indicates](https://github.com/hadley/rvest) that rvest was designed to do.\n\n\\# clear environment\nrm(list=ls())\n\n# Load pacman to make sure all other packages are available\nif( !require(pacman)) { install.packages('pacman')}\nlibrary(pacman)\n\n# Pacman function to load needed packages\np\\_load(rvest, tibble, magrittr)\n\n# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\n# Perform rvest tasks to get data into a usable format\nRPackages <- page %>%\nhtml\\_nodes(\"table\") %>%\n.\\[1\\] %>%\nhtml\\_table %>%\nas.data.frame() %>%\nas\\_tibble\n\n# Display the data\nView(RPackages)\n\nSo there you have it - a simple script to harvest the table from Microsoft's webpage showing Power BI support for CRAN packages.","source":"_posts/power-bi-r-packages-an-rvest-example.md","raw":"---\ntitle: Power BI R Packages - An RVEST example\ntags:\n  - howto\n  - rvest\n  - technical\nid: '4212'\ncategories:\n  - - Business Intelligence\n  - - Misc\n  - - R\ncomments: false\ndate: 2018-12-04 18:29:19\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/rvest-copy-259x300.png)](http://edpflager.com/wp-content/uploads/2018/12/rvest-copy.png)Activity in my day job often provides inspiration for content here, and this post is an example of that.  In my work, I use Microsoft's SQL Server and Power BI. Both applications now offer some level of support for R:\n\n*   Microsoft now includes an R server with SQL Server\n*   Power BI allows you to use R visualizations to provide functionality that is not included natively.\n*   If you have an R script that cleans and preps your data, Power BI use it as a data source for visualizations and analytics.\n\nPower BI doesn't support all R packages. Of the 10,000 plus packages available in the CRAN repository only 850 or so packages are currently supported. This [webpage](https://docs.microsoft.com/en-us/power-bi/service-r-packages-support) includes an explanation for what they do support and why. All of that is a preface to this post. At the above link is a table of the supported Power BI packages with version number and a URL for the package on CRAN. I wanted to capture that information for incorporation into some (non-work) documentation. I could easily copy and paste it into a spreadsheet or a text file, but I wanted to set it up as a repeatable process in case the website is updated. I could then capture the updated information quickly and easily. Enter [RVEST](https://cran.r-project.org/web/packages/rvest/index.html) - a screen scrapping package for R.\n<!-- more -->\nRVest is one of many R packages authored by Hadley Wickham, famous for GGPLOT2 and the wider Tidyverse set of packages. Its purpose is to easily harvest or scrape webpages. To get the data from the Microsoft webpage about supported R packages, we can use the RVest package to do about 90% of the work and with a little help from Tibble, we can shape the data in to usable format as a data frame. I will present this process first in a long format showing each step that calls different RVest functions, and at the end, I will wrap it up with the Magrittr pipe operator to make the code a bit cleaner. Before we start we need to use some kind of tool to find the element name on the page that we want to harvest. Hadley Wickham on the GitHub page for RVest recommends SelectorGadget an extension for Google Chrome. In RStudio, at a command line enter the following to get more information:\n\nvignette(\"selectorgadget\")\n\nUsing SelectorGadget on the Microsoft webpage, we see that the table we are interested in is not named, but it is the first one on the page. That is enough information to get started.\n\n#### INITIAL VERSION\n\nSwitching over to R Studio after loading our needed packages (rvest and tibble) we call the read\\_html function and pass it the webpage address we want to harvest.\n\n\\# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\nUsing the harvested webpage information stored in our page list, we want to retrieve the table elements sections.\n\n # Element type to grab from page\nnodes <- html\\_nodes(page,\"table\")\n\nSince there are multiple tables on that webpage, and the individual tables are not given unique names we specify the table number as the argument for the html\\_table function and pass it to a new variable. In this case its the first table:\n\n\\# we want the first table\nmsrpackages <- html\\_table(nodes\\[1\\])\n\nFinally, we pass the table variable (msrpackages) to the as.data.frame function, nested within an as\\_tibble function, and store the data frame in a new variable called RPackages.\n\nRPackages <- as\\_tibble(as.data.frame(msrpackages))\n\nTo see the data stored in the tibble, we call the View function with the RPackages element as the argument to generate the results are below.\n\nView(RPackages)\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView-300x143.png)](http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView.png) The long code for this script then looks like below (see my previous post about [pacman](http://edpflager.com/2018/11/12/r-pacman-package-manager-with-require/) for an explanation of that function):\n\n\\# clear environment\nrm(list=ls())\n\nif( !require(pacman)) { install.packages('pacman')}\nlibrary(pacman)\n\np\\_load(rvest, tibble)\n\n# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\n# Element type to grab from page\nnodes <- html\\_nodes(page,\"table\")\n\n# Since there are multiple tables on that webpage, we need to specify\n# that we want the first table\nmsrpackages <- html\\_table(nodes\\[1\\])\n\n# Convert the list into a tibble/data frame.\nRPackages <- as\\_tibble(as.data.frame(msrpackages))\n\n# Display the data\nView(RPackages)\n\n#### CLEANER VERSION\n\nThe code is fairly straightforward, but it can be cleaner if we incorporate the Magrittr pipe function, which Hadley Wickham [indicates](https://github.com/hadley/rvest) that rvest was designed to do.\n\n\\# clear environment\nrm(list=ls())\n\n# Load pacman to make sure all other packages are available\nif( !require(pacman)) { install.packages('pacman')}\nlibrary(pacman)\n\n# Pacman function to load needed packages\np\\_load(rvest, tibble, magrittr)\n\n# URL to scrape\npage <- read\\_html(\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\")\n\n# Perform rvest tasks to get data into a usable format\nRPackages <- page %>%\nhtml\\_nodes(\"table\") %>%\n.\\[1\\] %>%\nhtml\\_table %>%\nas.data.frame() %>%\nas\\_tibble\n\n# Display the data\nView(RPackages)\n\nSo there you have it - a simple script to harvest the table from Microsoft's webpage showing Power BI support for CRAN packages.","slug":"power-bi-r-packages-an-rvest-example","published":1,"updated":"2020-08-23T20:54:35.202Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a77007rsdjx2yxfcols","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/rvest-copy.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/rvest-copy-259x300.png\"></a>Activity in my day job often provides inspiration for content here, and this post is an example of that.  In my work, I use Microsoft’s SQL Server and Power BI. Both applications now offer some level of support for R:</p>\n<ul>\n<li>Microsoft now includes an R server with SQL Server</li>\n<li>Power BI allows you to use R visualizations to provide functionality that is not included natively.</li>\n<li>If you have an R script that cleans and preps your data, Power BI use it as a data source for visualizations and analytics.</li>\n</ul>\n<p>Power BI doesn’t support all R packages. Of the 10,000 plus packages available in the CRAN repository only 850 or so packages are currently supported. This <a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\">webpage</a> includes an explanation for what they do support and why. All of that is a preface to this post. At the above link is a table of the supported Power BI packages with version number and a URL for the package on CRAN. I wanted to capture that information for incorporation into some (non-work) documentation. I could easily copy and paste it into a spreadsheet or a text file, but I wanted to set it up as a repeatable process in case the website is updated. I could then capture the updated information quickly and easily. Enter <a href=\"https://cran.r-project.org/web/packages/rvest/index.html\">RVEST</a> - a screen scrapping package for R.</p>\n<a id=\"more\"></a>\n<p>RVest is one of many R packages authored by Hadley Wickham, famous for GGPLOT2 and the wider Tidyverse set of packages. Its purpose is to easily harvest or scrape webpages. To get the data from the Microsoft webpage about supported R packages, we can use the RVest package to do about 90% of the work and with a little help from Tibble, we can shape the data in to usable format as a data frame. I will present this process first in a long format showing each step that calls different RVest functions, and at the end, I will wrap it up with the Magrittr pipe operator to make the code a bit cleaner. Before we start we need to use some kind of tool to find the element name on the page that we want to harvest. Hadley Wickham on the GitHub page for RVest recommends SelectorGadget an extension for Google Chrome. In RStudio, at a command line enter the following to get more information:</p>\n<p>vignette(“selectorgadget”)</p>\n<p>Using SelectorGadget on the Microsoft webpage, we see that the table we are interested in is not named, but it is the first one on the page. That is enough information to get started.</p>\n<h4 id=\"INITIAL-VERSION\"><a href=\"#INITIAL-VERSION\" class=\"headerlink\" title=\"INITIAL VERSION\"></a>INITIAL VERSION</h4><p>Switching over to R Studio after loading our needed packages (rvest and tibble) we call the read_html function and pass it the webpage address we want to harvest.</p>\n<p># URL to scrape<br>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<p>Using the harvested webpage information stored in our page list, we want to retrieve the table elements sections.</p>\n<h1 id=\"Element-type-to-grab-from-page\"><a href=\"#Element-type-to-grab-from-page\" class=\"headerlink\" title=\"Element type to grab from page\"></a>Element type to grab from page</h1><p>nodes &lt;- html_nodes(page,”table”)</p>\n<p>Since there are multiple tables on that webpage, and the individual tables are not given unique names we specify the table number as the argument for the html_table function and pass it to a new variable. In this case its the first table:</p>\n<p># we want the first table<br>msrpackages &lt;- html_table(nodes[1])</p>\n<p>Finally, we pass the table variable (msrpackages) to the as.data.frame function, nested within an as_tibble function, and store the data frame in a new variable called RPackages.</p>\n<p>RPackages &lt;- as_tibble(as.data.frame(msrpackages))</p>\n<p>To see the data stored in the tibble, we call the View function with the RPackages element as the argument to generate the results are below.</p>\n<p>View(RPackages)</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView-300x143.png\"></a> The long code for this script then looks like below (see my previous post about <a href=\"http://edpflager.com/2018/11/12/r-pacman-package-manager-with-require/\">pacman</a> for an explanation of that function):</p>\n<p># clear environment<br>rm(list=ls())</p>\n<p>if( !require(pacman)) { install.packages(‘pacman’)}<br>library(pacman)</p>\n<p>p_load(rvest, tibble)</p>\n<h1 id=\"URL-to-scrape\"><a href=\"#URL-to-scrape\" class=\"headerlink\" title=\"URL to scrape\"></a>URL to scrape</h1><p>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<h1 id=\"Element-type-to-grab-from-page-1\"><a href=\"#Element-type-to-grab-from-page-1\" class=\"headerlink\" title=\"Element type to grab from page\"></a>Element type to grab from page</h1><p>nodes &lt;- html_nodes(page,”table”)</p>\n<h1 id=\"Since-there-are-multiple-tables-on-that-webpage-we-need-to-specify\"><a href=\"#Since-there-are-multiple-tables-on-that-webpage-we-need-to-specify\" class=\"headerlink\" title=\"Since there are multiple tables on that webpage, we need to specify\"></a>Since there are multiple tables on that webpage, we need to specify</h1><h1 id=\"that-we-want-the-first-table\"><a href=\"#that-we-want-the-first-table\" class=\"headerlink\" title=\"that we want the first table\"></a>that we want the first table</h1><p>msrpackages &lt;- html_table(nodes[1])</p>\n<h1 id=\"Convert-the-list-into-a-tibble-data-frame\"><a href=\"#Convert-the-list-into-a-tibble-data-frame\" class=\"headerlink\" title=\"Convert the list into a tibble/data frame.\"></a>Convert the list into a tibble/data frame.</h1><p>RPackages &lt;- as_tibble(as.data.frame(msrpackages))</p>\n<h1 id=\"Display-the-data\"><a href=\"#Display-the-data\" class=\"headerlink\" title=\"Display the data\"></a>Display the data</h1><p>View(RPackages)</p>\n<h4 id=\"CLEANER-VERSION\"><a href=\"#CLEANER-VERSION\" class=\"headerlink\" title=\"CLEANER VERSION\"></a>CLEANER VERSION</h4><p>The code is fairly straightforward, but it can be cleaner if we incorporate the Magrittr pipe function, which Hadley Wickham <a href=\"https://github.com/hadley/rvest\">indicates</a> that rvest was designed to do.</p>\n<p># clear environment<br>rm(list=ls())</p>\n<h1 id=\"Load-pacman-to-make-sure-all-other-packages-are-available\"><a href=\"#Load-pacman-to-make-sure-all-other-packages-are-available\" class=\"headerlink\" title=\"Load pacman to make sure all other packages are available\"></a>Load pacman to make sure all other packages are available</h1><p>if( !require(pacman)) { install.packages(‘pacman’)}<br>library(pacman)</p>\n<h1 id=\"Pacman-function-to-load-needed-packages\"><a href=\"#Pacman-function-to-load-needed-packages\" class=\"headerlink\" title=\"Pacman function to load needed packages\"></a>Pacman function to load needed packages</h1><p>p_load(rvest, tibble, magrittr)</p>\n<h1 id=\"URL-to-scrape-1\"><a href=\"#URL-to-scrape-1\" class=\"headerlink\" title=\"URL to scrape\"></a>URL to scrape</h1><p>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<h1 id=\"Perform-rvest-tasks-to-get-data-into-a-usable-format\"><a href=\"#Perform-rvest-tasks-to-get-data-into-a-usable-format\" class=\"headerlink\" title=\"Perform rvest tasks to get data into a usable format\"></a>Perform rvest tasks to get data into a usable format</h1><p>RPackages &lt;- page %&gt;%<br>html_nodes(“table”) %&gt;%<br>.[1] %&gt;%<br>html_table %&gt;%<br>as.data.frame() %&gt;%<br>as_tibble</p>\n<h1 id=\"Display-the-data-1\"><a href=\"#Display-the-data-1\" class=\"headerlink\" title=\"Display the data\"></a>Display the data</h1><p>View(RPackages)</p>\n<p>So there you have it - a simple script to harvest the table from Microsoft’s webpage showing Power BI support for CRAN packages.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/rvest-copy.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/rvest-copy-259x300.png\"></a>Activity in my day job often provides inspiration for content here, and this post is an example of that.  In my work, I use Microsoft’s SQL Server and Power BI. Both applications now offer some level of support for R:</p>\n<ul>\n<li>Microsoft now includes an R server with SQL Server</li>\n<li>Power BI allows you to use R visualizations to provide functionality that is not included natively.</li>\n<li>If you have an R script that cleans and preps your data, Power BI use it as a data source for visualizations and analytics.</li>\n</ul>\n<p>Power BI doesn’t support all R packages. Of the 10,000 plus packages available in the CRAN repository only 850 or so packages are currently supported. This <a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support\">webpage</a> includes an explanation for what they do support and why. All of that is a preface to this post. At the above link is a table of the supported Power BI packages with version number and a URL for the package on CRAN. I wanted to capture that information for incorporation into some (non-work) documentation. I could easily copy and paste it into a spreadsheet or a text file, but I wanted to set it up as a repeatable process in case the website is updated. I could then capture the updated information quickly and easily. Enter <a href=\"https://cran.r-project.org/web/packages/rvest/index.html\">RVEST</a> - a screen scrapping package for R.</p>","more":"<p>RVest is one of many R packages authored by Hadley Wickham, famous for GGPLOT2 and the wider Tidyverse set of packages. Its purpose is to easily harvest or scrape webpages. To get the data from the Microsoft webpage about supported R packages, we can use the RVest package to do about 90% of the work and with a little help from Tibble, we can shape the data in to usable format as a data frame. I will present this process first in a long format showing each step that calls different RVest functions, and at the end, I will wrap it up with the Magrittr pipe operator to make the code a bit cleaner. Before we start we need to use some kind of tool to find the element name on the page that we want to harvest. Hadley Wickham on the GitHub page for RVest recommends SelectorGadget an extension for Google Chrome. In RStudio, at a command line enter the following to get more information:</p>\n<p>vignette(“selectorgadget”)</p>\n<p>Using SelectorGadget on the Microsoft webpage, we see that the table we are interested in is not named, but it is the first one on the page. That is enough information to get started.</p>\n<h4 id=\"INITIAL-VERSION\"><a href=\"#INITIAL-VERSION\" class=\"headerlink\" title=\"INITIAL VERSION\"></a>INITIAL VERSION</h4><p>Switching over to R Studio after loading our needed packages (rvest and tibble) we call the read_html function and pass it the webpage address we want to harvest.</p>\n<p># URL to scrape<br>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<p>Using the harvested webpage information stored in our page list, we want to retrieve the table elements sections.</p>\n<h1 id=\"Element-type-to-grab-from-page\"><a href=\"#Element-type-to-grab-from-page\" class=\"headerlink\" title=\"Element type to grab from page\"></a>Element type to grab from page</h1><p>nodes &lt;- html_nodes(page,”table”)</p>\n<p>Since there are multiple tables on that webpage, and the individual tables are not given unique names we specify the table number as the argument for the html_table function and pass it to a new variable. In this case its the first table:</p>\n<p># we want the first table<br>msrpackages &lt;- html_table(nodes[1])</p>\n<p>Finally, we pass the table variable (msrpackages) to the as.data.frame function, nested within an as_tibble function, and store the data frame in a new variable called RPackages.</p>\n<p>RPackages &lt;- as_tibble(as.data.frame(msrpackages))</p>\n<p>To see the data stored in the tibble, we call the View function with the RPackages element as the argument to generate the results are below.</p>\n<p>View(RPackages)</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/MSRPackagesView-300x143.png\"></a> The long code for this script then looks like below (see my previous post about <a href=\"http://edpflager.com/2018/11/12/r-pacman-package-manager-with-require/\">pacman</a> for an explanation of that function):</p>\n<p># clear environment<br>rm(list=ls())</p>\n<p>if( !require(pacman)) { install.packages(‘pacman’)}<br>library(pacman)</p>\n<p>p_load(rvest, tibble)</p>\n<h1 id=\"URL-to-scrape\"><a href=\"#URL-to-scrape\" class=\"headerlink\" title=\"URL to scrape\"></a>URL to scrape</h1><p>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<h1 id=\"Element-type-to-grab-from-page-1\"><a href=\"#Element-type-to-grab-from-page-1\" class=\"headerlink\" title=\"Element type to grab from page\"></a>Element type to grab from page</h1><p>nodes &lt;- html_nodes(page,”table”)</p>\n<h1 id=\"Since-there-are-multiple-tables-on-that-webpage-we-need-to-specify\"><a href=\"#Since-there-are-multiple-tables-on-that-webpage-we-need-to-specify\" class=\"headerlink\" title=\"Since there are multiple tables on that webpage, we need to specify\"></a>Since there are multiple tables on that webpage, we need to specify</h1><h1 id=\"that-we-want-the-first-table\"><a href=\"#that-we-want-the-first-table\" class=\"headerlink\" title=\"that we want the first table\"></a>that we want the first table</h1><p>msrpackages &lt;- html_table(nodes[1])</p>\n<h1 id=\"Convert-the-list-into-a-tibble-data-frame\"><a href=\"#Convert-the-list-into-a-tibble-data-frame\" class=\"headerlink\" title=\"Convert the list into a tibble/data frame.\"></a>Convert the list into a tibble/data frame.</h1><p>RPackages &lt;- as_tibble(as.data.frame(msrpackages))</p>\n<h1 id=\"Display-the-data\"><a href=\"#Display-the-data\" class=\"headerlink\" title=\"Display the data\"></a>Display the data</h1><p>View(RPackages)</p>\n<h4 id=\"CLEANER-VERSION\"><a href=\"#CLEANER-VERSION\" class=\"headerlink\" title=\"CLEANER VERSION\"></a>CLEANER VERSION</h4><p>The code is fairly straightforward, but it can be cleaner if we incorporate the Magrittr pipe function, which Hadley Wickham <a href=\"https://github.com/hadley/rvest\">indicates</a> that rvest was designed to do.</p>\n<p># clear environment<br>rm(list=ls())</p>\n<h1 id=\"Load-pacman-to-make-sure-all-other-packages-are-available\"><a href=\"#Load-pacman-to-make-sure-all-other-packages-are-available\" class=\"headerlink\" title=\"Load pacman to make sure all other packages are available\"></a>Load pacman to make sure all other packages are available</h1><p>if( !require(pacman)) { install.packages(‘pacman’)}<br>library(pacman)</p>\n<h1 id=\"Pacman-function-to-load-needed-packages\"><a href=\"#Pacman-function-to-load-needed-packages\" class=\"headerlink\" title=\"Pacman function to load needed packages\"></a>Pacman function to load needed packages</h1><p>p_load(rvest, tibble, magrittr)</p>\n<h1 id=\"URL-to-scrape-1\"><a href=\"#URL-to-scrape-1\" class=\"headerlink\" title=\"URL to scrape\"></a>URL to scrape</h1><p>page &lt;- read_html(“<a href=\"https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;\">https://docs.microsoft.com/en-us/power-bi/service-r-packages-support&quot;</a>)</p>\n<h1 id=\"Perform-rvest-tasks-to-get-data-into-a-usable-format\"><a href=\"#Perform-rvest-tasks-to-get-data-into-a-usable-format\" class=\"headerlink\" title=\"Perform rvest tasks to get data into a usable format\"></a>Perform rvest tasks to get data into a usable format</h1><p>RPackages &lt;- page %&gt;%<br>html_nodes(“table”) %&gt;%<br>.[1] %&gt;%<br>html_table %&gt;%<br>as.data.frame() %&gt;%<br>as_tibble</p>\n<h1 id=\"Display-the-data-1\"><a href=\"#Display-the-data-1\" class=\"headerlink\" title=\"Display the data\"></a>Display the data</h1><p>View(RPackages)</p>\n<p>So there you have it - a simple script to harvest the table from Microsoft’s webpage showing Power BI support for CRAN packages.</p>"},{"title":"Public Data Sets","id":"2107","comments":0,"date":"2014-05-23T23:26:32.000Z","_content":"\n[![3d database structure](http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg)](http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg)If you are just starting working with Hadoop and Big Data, you may be at a loss for data to experiment with. Luckily, there is an abundant supply of freely available data sets on the Internet. Here I will highlight a few of the sources I have found out about, and I'll add more as I find them. InfoChimps is a company of data scientists, cloud computing and open source experts who provide solutions for their customers to make Big Data platforms. They provide over 11,000 freely available data sets for you to download. Everything from an Excel readable list of crossword puzzle words to UFO sighting data sets are [here](http://www.infochimps.com/marketplace).\n<!-- more -->\nInterested in movie information and movie review datasets? GroupLens (a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities) has compiled data sets of varying sizes of movie reviews from a large number of reviews. Also available are other recommender data sets on different topics. Check it out [here](http://grouplens.org/datasets/movielens/).  An interesting website in [theinfo.org](http://theinfo.org), where you can download large numbers of public records. Organization is almost non-existent, with no search function. You click on a couple of dots and are presented with a court documents. Interesting if  you are in need of random data for a project. Finally, one of the largest repositories of freely available data sets is provided by the [US Government](http://www.data.gov). Encompassing over 100,000 sets including subjects like Real-time 911 Fire Calls in Seattle to a cross reference of domestic and foreign companies doing business with the US Government, its a treasure trove of haystacks with numerous needles ready for you to discover. Updated 8/22/2014 - Just found out about a website offering over 15,000 data sets of public information to help people learn how (UK) government works. Available at [data.gov.uk](http://data.gov.uk/) there are data sets across several broad categories and in various formats. A pretty cool feature is they also offer links to apps built using these data sets. If you have any other sites you've found, drop me a line on the contact page, and  I'll include them in a future post!","source":"_posts/public-data-sets.md","raw":"---\ntitle: Public Data Sets\ntags:\n  - external article\n  - goofy\n  - Hadoop\n  - inspiration\n  - technical\nid: '2107'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2014-05-23 19:26:32\n---\n\n[![3d database structure](http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg)](http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg)If you are just starting working with Hadoop and Big Data, you may be at a loss for data to experiment with. Luckily, there is an abundant supply of freely available data sets on the Internet. Here I will highlight a few of the sources I have found out about, and I'll add more as I find them. InfoChimps is a company of data scientists, cloud computing and open source experts who provide solutions for their customers to make Big Data platforms. They provide over 11,000 freely available data sets for you to download. Everything from an Excel readable list of crossword puzzle words to UFO sighting data sets are [here](http://www.infochimps.com/marketplace).\n<!-- more -->\nInterested in movie information and movie review datasets? GroupLens (a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities) has compiled data sets of varying sizes of movie reviews from a large number of reviews. Also available are other recommender data sets on different topics. Check it out [here](http://grouplens.org/datasets/movielens/).  An interesting website in [theinfo.org](http://theinfo.org), where you can download large numbers of public records. Organization is almost non-existent, with no search function. You click on a couple of dots and are presented with a court documents. Interesting if  you are in need of random data for a project. Finally, one of the largest repositories of freely available data sets is provided by the [US Government](http://www.data.gov). Encompassing over 100,000 sets including subjects like Real-time 911 Fire Calls in Seattle to a cross reference of domestic and foreign companies doing business with the US Government, its a treasure trove of haystacks with numerous needles ready for you to discover. Updated 8/22/2014 - Just found out about a website offering over 15,000 data sets of public information to help people learn how (UK) government works. Available at [data.gov.uk](http://data.gov.uk/) there are data sets across several broad categories and in various formats. A pretty cool feature is they also offer links to apps built using these data sets. If you have any other sites you've found, drop me a line on the contact page, and  I'll include them in a future post!","slug":"public-data-sets","published":1,"updated":"2020-08-23T20:54:34.850Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7a007vsdjx8ym69hi1","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg\" alt=\"3d database structure\"></a>If you are just starting working with Hadoop and Big Data, you may be at a loss for data to experiment with. Luckily, there is an abundant supply of freely available data sets on the Internet. Here I will highlight a few of the sources I have found out about, and I’ll add more as I find them. InfoChimps is a company of data scientists, cloud computing and open source experts who provide solutions for their customers to make Big Data platforms. They provide over 11,000 freely available data sets for you to download. Everything from an Excel readable list of crossword puzzle words to UFO sighting data sets are <a href=\"http://www.infochimps.com/marketplace\">here</a>.</p>\n<a id=\"more\"></a>\n<p>Interested in movie information and movie review datasets? GroupLens (a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities) has compiled data sets of varying sizes of movie reviews from a large number of reviews. Also available are other recommender data sets on different topics. Check it out <a href=\"http://grouplens.org/datasets/movielens/\">here</a>.  An interesting website in <a href=\"http://theinfo.org/\">theinfo.org</a>, where you can download large numbers of public records. Organization is almost non-existent, with no search function. You click on a couple of dots and are presented with a court documents. Interesting if  you are in need of random data for a project. Finally, one of the largest repositories of freely available data sets is provided by the <a href=\"http://www.data.gov/\">US Government</a>. Encompassing over 100,000 sets including subjects like Real-time 911 Fire Calls in Seattle to a cross reference of domestic and foreign companies doing business with the US Government, its a treasure trove of haystacks with numerous needles ready for you to discover. Updated 8/22/2014 - Just found out about a website offering over 15,000 data sets of public information to help people learn how (UK) government works. Available at <a href=\"http://data.gov.uk/\">data.gov.uk</a> there are data sets across several broad categories and in various formats. A pretty cool feature is they also offer links to apps built using these data sets. If you have any other sites you’ve found, drop me a line on the contact page, and  I’ll include them in a future post!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/dataset.jpg\" alt=\"3d database structure\"></a>If you are just starting working with Hadoop and Big Data, you may be at a loss for data to experiment with. Luckily, there is an abundant supply of freely available data sets on the Internet. Here I will highlight a few of the sources I have found out about, and I’ll add more as I find them. InfoChimps is a company of data scientists, cloud computing and open source experts who provide solutions for their customers to make Big Data platforms. They provide over 11,000 freely available data sets for you to download. Everything from an Excel readable list of crossword puzzle words to UFO sighting data sets are <a href=\"http://www.infochimps.com/marketplace\">here</a>.</p>","more":"<p>Interested in movie information and movie review datasets? GroupLens (a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities) has compiled data sets of varying sizes of movie reviews from a large number of reviews. Also available are other recommender data sets on different topics. Check it out <a href=\"http://grouplens.org/datasets/movielens/\">here</a>.  An interesting website in <a href=\"http://theinfo.org/\">theinfo.org</a>, where you can download large numbers of public records. Organization is almost non-existent, with no search function. You click on a couple of dots and are presented with a court documents. Interesting if  you are in need of random data for a project. Finally, one of the largest repositories of freely available data sets is provided by the <a href=\"http://www.data.gov/\">US Government</a>. Encompassing over 100,000 sets including subjects like Real-time 911 Fire Calls in Seattle to a cross reference of domestic and foreign companies doing business with the US Government, its a treasure trove of haystacks with numerous needles ready for you to discover. Updated 8/22/2014 - Just found out about a website offering over 15,000 data sets of public information to help people learn how (UK) government works. Available at <a href=\"http://data.gov.uk/\">data.gov.uk</a> there are data sets across several broad categories and in various formats. A pretty cool feature is they also offer links to apps built using these data sets. If you have any other sites you’ve found, drop me a line on the contact page, and  I’ll include them in a future post!</p>"},{"title":"Quick Tips - Spacing and Bulleted Lists","id":"4290","comments":0,"date":"2018-12-27T17:55:03.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Wrapping up the year with a few simple tips for R Markdown that work with PDF, Word or HTML output. **VSPACE** When you insert a series of blank lines in an R Markdown document and knit the document, the parsing of your document strips out those blank lines. As an example I entered in my file the lines on the left in the image below, and when I Knitted the document, the results were those on the right: [![](http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example-300x79.png)](http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example.png)\n<!-- more -->\nIf the blank lines are significant in the output, the parsing engine can be forced to insert them for you with a simple command. Here are two examples:\n\nvspace{20pt}  \n\nvspace{1.5in}\n\nThe **vspace** command will allow you to specify vertical space in your document. Just include a specific measurement in braces after the command. You can use standard measurement scales like cm (centimeters), mm(millimeters), or in(inches), or web based measurements like pt (points), em (current font size), or px(pixels). **HSPACE** If you want to enter horizontal space, you can use the **hspace** command, again with the various scales above. It can be used for a custom indentation on a paragraph or if you want to position something in a specific location in your document. An example of the second option might be similar to entering this command:\n\nHere there be hspace{2in}DRAGONS\n\nTo produce:   [![](http://edpflager.com/wp-content/uploads/2018/12/hspace-example-300x23.png)](http://edpflager.com/wp-content/uploads/2018/12/hspace-example.png) **Bulleted Lists Tips** Its fairly well documented that to include a bulleted list if items in R Markdown, proceed each entry with an asterisk(\\*). If after knitting, your bullets display as asterisks then you probably need to precede your list with a blank lime to get them to display correctly. Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example:\n\nThere are two types of output formats in the rmarkdown package: \nPresentations:\n\n\\* beamer (this line has extra space) \n\\* ioslides\n\\* powerpoint\n\\* slidy\n\nvspace{.5cm}\nDocuments:\n\n\\* github\n\\* html\n\\* latex\n\\* md\n\\* odt\n\\* pdf\n\\* rtf\n\\* word\n\nThis code produces this output: [![](http://edpflager.com/wp-content/uploads/2018/12/Bullets-300x220.png)](http://edpflager.com/wp-content/uploads/2018/12/Bullets.png) To fix it, remove the extra space at the end of the \"beamer\" line. Here is the code for the above examples, in one document:\n\n\\---\ntitle: \"SimpleTips\"\noutput:\npdf\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\nline 1 - followed by 5 blank lines\n\nline 2\n\n###VSpace\n\nvspace{2em}\nThe vspace command will allow you to specify vertical space in your document. Typically RMarkdown will ignore blank lines in your document. If you want to explicitly specify space, then this is a good way to do it. Just include a specific measurement in braces after the command.\n\nvspace{2em}\n\n###HSpace\n\nHere there be hspace{2in}DRAGONS\n\n\n###Bulleted lists\nWhen doing bulleted lists, make sure to have a blank line before the list to ensure that the bullet shows properly. If it shows as an asterick (\\*) then you didn't precede your list correctly.\n\nAlso if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example\n\nThere are two types of output formats in the rmarkdown package: \nPresentations:\n\n\\* beamer (this line has extra space) \n\\* ioslides\n\\* powerpoint\n\\* slidy\n\nvspace{.5cm}\nDocuments:\n\n\\* github\n\\* html\n\\* latex\n\\* md\n\\* odt\n\\* pdf\n\\* rtf\n\\* word","source":"_posts/quick-tips-spacing-and-bulleted-lists.md","raw":"---\ntitle: Quick Tips - Spacing and Bulleted Lists\ntags:\n  - cookbook\n  - howto\n  - R Markdown\n  - technical\nid: '4290'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-12-27 12:55:03\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Wrapping up the year with a few simple tips for R Markdown that work with PDF, Word or HTML output. **VSPACE** When you insert a series of blank lines in an R Markdown document and knit the document, the parsing of your document strips out those blank lines. As an example I entered in my file the lines on the left in the image below, and when I Knitted the document, the results were those on the right: [![](http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example-300x79.png)](http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example.png)\n<!-- more -->\nIf the blank lines are significant in the output, the parsing engine can be forced to insert them for you with a simple command. Here are two examples:\n\nvspace{20pt}  \n\nvspace{1.5in}\n\nThe **vspace** command will allow you to specify vertical space in your document. Just include a specific measurement in braces after the command. You can use standard measurement scales like cm (centimeters), mm(millimeters), or in(inches), or web based measurements like pt (points), em (current font size), or px(pixels). **HSPACE** If you want to enter horizontal space, you can use the **hspace** command, again with the various scales above. It can be used for a custom indentation on a paragraph or if you want to position something in a specific location in your document. An example of the second option might be similar to entering this command:\n\nHere there be hspace{2in}DRAGONS\n\nTo produce:   [![](http://edpflager.com/wp-content/uploads/2018/12/hspace-example-300x23.png)](http://edpflager.com/wp-content/uploads/2018/12/hspace-example.png) **Bulleted Lists Tips** Its fairly well documented that to include a bulleted list if items in R Markdown, proceed each entry with an asterisk(\\*). If after knitting, your bullets display as asterisks then you probably need to precede your list with a blank lime to get them to display correctly. Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example:\n\nThere are two types of output formats in the rmarkdown package: \nPresentations:\n\n\\* beamer (this line has extra space) \n\\* ioslides\n\\* powerpoint\n\\* slidy\n\nvspace{.5cm}\nDocuments:\n\n\\* github\n\\* html\n\\* latex\n\\* md\n\\* odt\n\\* pdf\n\\* rtf\n\\* word\n\nThis code produces this output: [![](http://edpflager.com/wp-content/uploads/2018/12/Bullets-300x220.png)](http://edpflager.com/wp-content/uploads/2018/12/Bullets.png) To fix it, remove the extra space at the end of the \"beamer\" line. Here is the code for the above examples, in one document:\n\n\\---\ntitle: \"SimpleTips\"\noutput:\npdf\\_document\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\nline 1 - followed by 5 blank lines\n\nline 2\n\n###VSpace\n\nvspace{2em}\nThe vspace command will allow you to specify vertical space in your document. Typically RMarkdown will ignore blank lines in your document. If you want to explicitly specify space, then this is a good way to do it. Just include a specific measurement in braces after the command.\n\nvspace{2em}\n\n###HSpace\n\nHere there be hspace{2in}DRAGONS\n\n\n###Bulleted lists\nWhen doing bulleted lists, make sure to have a blank line before the list to ensure that the bullet shows properly. If it shows as an asterick (\\*) then you didn't precede your list correctly.\n\nAlso if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example\n\nThere are two types of output formats in the rmarkdown package: \nPresentations:\n\n\\* beamer (this line has extra space) \n\\* ioslides\n\\* powerpoint\n\\* slidy\n\nvspace{.5cm}\nDocuments:\n\n\\* github\n\\* html\n\\* latex\n\\* md\n\\* odt\n\\* pdf\n\\* rtf\n\\* word","slug":"quick-tips-spacing-and-bulleted-lists","published":1,"updated":"2020-08-23T20:54:35.210Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7c007ysdjxe9zx4ses","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Wrapping up the year with a few simple tips for R Markdown that work with PDF, Word or HTML output. <strong>VSPACE</strong> When you insert a series of blank lines in an R Markdown document and knit the document, the parsing of your document strips out those blank lines. As an example I entered in my file the lines on the left in the image below, and when I Knitted the document, the results were those on the right: <a href=\"http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example-300x79.png\"></a></p>\n<a id=\"more\"></a>\n<p>If the blank lines are significant in the output, the parsing engine can be forced to insert them for you with a simple command. Here are two examples:</p>\n<p>vspace{20pt}  </p>\n<p>vspace{1.5in}</p>\n<p>The <strong>vspace</strong> command will allow you to specify vertical space in your document. Just include a specific measurement in braces after the command. You can use standard measurement scales like cm (centimeters), mm(millimeters), or in(inches), or web based measurements like pt (points), em (current font size), or px(pixels). <strong>HSPACE</strong> If you want to enter horizontal space, you can use the <strong>hspace</strong> command, again with the various scales above. It can be used for a custom indentation on a paragraph or if you want to position something in a specific location in your document. An example of the second option might be similar to entering this command:</p>\n<p>Here there be hspace{2in}DRAGONS</p>\n<p>To produce:   <a href=\"http://edpflager.com/wp-content/uploads/2018/12/hspace-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/hspace-example-300x23.png\"></a> <strong>Bulleted Lists Tips</strong> Its fairly well documented that to include a bulleted list if items in R Markdown, proceed each entry with an asterisk(*). If after knitting, your bullets display as asterisks then you probably need to precede your list with a blank lime to get them to display correctly. Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example:</p>\n<p>There are two types of output formats in the rmarkdown package:<br>Presentations:</p>\n<p>* beamer (this line has extra space)<br>* ioslides<br>* powerpoint<br>* slidy</p>\n<p>vspace{.5cm}<br>Documents:</p>\n<p>* github<br>* html<br>* latex<br>* md<br>* odt<br>* pdf<br>* rtf<br>* word</p>\n<p>This code produces this output: <a href=\"http://edpflager.com/wp-content/uploads/2018/12/Bullets.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/Bullets-300x220.png\"></a> To fix it, remove the extra space at the end of the “beamer” line. Here is the code for the above examples, in one document:</p>\n<p>-–<br>title: “SimpleTips”<br>output:<br>pdf_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>line 1 - followed by 5 blank lines</p>\n<p>line 2</p>\n<p>###VSpace</p>\n<p>vspace{2em}<br>The vspace command will allow you to specify vertical space in your document. Typically RMarkdown will ignore blank lines in your document. If you want to explicitly specify space, then this is a good way to do it. Just include a specific measurement in braces after the command.</p>\n<p>vspace{2em}</p>\n<p>###HSpace</p>\n<p>Here there be hspace{2in}DRAGONS</p>\n<p>###Bulleted lists<br>When doing bulleted lists, make sure to have a blank line before the list to ensure that the bullet shows properly. If it shows as an asterick (*) then you didn’t precede your list correctly.</p>\n<p>Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example</p>\n<p>There are two types of output formats in the rmarkdown package:<br>Presentations:</p>\n<p>* beamer (this line has extra space)<br>* ioslides<br>* powerpoint<br>* slidy</p>\n<p>vspace{.5cm}<br>Documents:</p>\n<p>* github<br>* html<br>* latex<br>* md<br>* odt<br>* pdf<br>* rtf<br>* word</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Wrapping up the year with a few simple tips for R Markdown that work with PDF, Word or HTML output. <strong>VSPACE</strong> When you insert a series of blank lines in an R Markdown document and knit the document, the parsing of your document strips out those blank lines. As an example I entered in my file the lines on the left in the image below, and when I Knitted the document, the results were those on the right: <a href=\"http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/R-Markdown-spacing-example-300x79.png\"></a></p>","more":"<p>If the blank lines are significant in the output, the parsing engine can be forced to insert them for you with a simple command. Here are two examples:</p>\n<p>vspace{20pt}  </p>\n<p>vspace{1.5in}</p>\n<p>The <strong>vspace</strong> command will allow you to specify vertical space in your document. Just include a specific measurement in braces after the command. You can use standard measurement scales like cm (centimeters), mm(millimeters), or in(inches), or web based measurements like pt (points), em (current font size), or px(pixels). <strong>HSPACE</strong> If you want to enter horizontal space, you can use the <strong>hspace</strong> command, again with the various scales above. It can be used for a custom indentation on a paragraph or if you want to position something in a specific location in your document. An example of the second option might be similar to entering this command:</p>\n<p>Here there be hspace{2in}DRAGONS</p>\n<p>To produce:   <a href=\"http://edpflager.com/wp-content/uploads/2018/12/hspace-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/hspace-example-300x23.png\"></a> <strong>Bulleted Lists Tips</strong> Its fairly well documented that to include a bulleted list if items in R Markdown, proceed each entry with an asterisk(*). If after knitting, your bullets display as asterisks then you probably need to precede your list with a blank lime to get them to display correctly. Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example:</p>\n<p>There are two types of output formats in the rmarkdown package:<br>Presentations:</p>\n<p>* beamer (this line has extra space)<br>* ioslides<br>* powerpoint<br>* slidy</p>\n<p>vspace{.5cm}<br>Documents:</p>\n<p>* github<br>* html<br>* latex<br>* md<br>* odt<br>* pdf<br>* rtf<br>* word</p>\n<p>This code produces this output: <a href=\"http://edpflager.com/wp-content/uploads/2018/12/Bullets.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/Bullets-300x220.png\"></a> To fix it, remove the extra space at the end of the “beamer” line. Here is the code for the above examples, in one document:</p>\n<p>-–<br>title: “SimpleTips”<br>output:<br>pdf_document</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>line 1 - followed by 5 blank lines</p>\n<p>line 2</p>\n<p>###VSpace</p>\n<p>vspace{2em}<br>The vspace command will allow you to specify vertical space in your document. Typically RMarkdown will ignore blank lines in your document. If you want to explicitly specify space, then this is a good way to do it. Just include a specific measurement in braces after the command.</p>\n<p>vspace{2em}</p>\n<p>###HSpace</p>\n<p>Here there be hspace{2in}DRAGONS</p>\n<p>###Bulleted lists<br>When doing bulleted lists, make sure to have a blank line before the list to ensure that the bullet shows properly. If it shows as an asterick (*) then you didn’t precede your list correctly.</p>\n<p>Also if there appear to be blank lines between your items, you may have extraneous characters at the end of your items. Try removing it and Knit your document again to clean it up. Here is an example</p>\n<p>There are two types of output formats in the rmarkdown package:<br>Presentations:</p>\n<p>* beamer (this line has extra space)<br>* ioslides<br>* powerpoint<br>* slidy</p>\n<p>vspace{.5cm}<br>Documents:</p>\n<p>* github<br>* html<br>* latex<br>* md<br>* odt<br>* pdf<br>* rtf<br>* word</p>"},{"title":"Font Formatting - Typefaces","id":"3871","comments":0,"date":"2018-09-11T23:15:45.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) First a little background - I've been working with R - the free open source statistical computing and graphics software for about a year, exploring various libraries and add-ons, trying to get a better understanding of it. This was spurred on by a couple of things. First, in SQL Server 2016, Microsoft incorporated R into their database system, allowing you to use R with the SQL Server Management Studio for a variety of purposes. And second, Power BI from Microsoft also incorporates R as a data source, allowing you to massage and manipulate data before passing it to Power BI, and you can use it to develop custom visualizations within Power BI. Because I take notes while I learn, I wanted some way to organize handy tips and tricks and show the results. I first experimented with Jupyter, but then I discovered R Markdown. For a full description of what R Markdown is and what its capable of, check out [this article](https://rmarkdown.rstudio.com/articles_intro.html) from RStudio, the developers of R Markdown.  With this library you can create documents (Word, PDF or HTML) within the R Studio IDE to show your code, and the results of the code in a single document. As a way to save my notes while working with R, I have been working with R Markdown, and I will pass on a few of those tips as part of a series: R Markdown Cookbook.\n<!-- more -->\nBy default, the RMarkdown PDF option uses a Times New Roman font. Its very old-school and I believe its used because a lot of R users are academics and it looks like the preferred format for a lot of R documentation. Or it could be that a fixed width font is preferable for academic journals. Regardless, IMHO, the output looks dated and I wanted to use a different font overall in my RMarkdown PDFs. Keep in mind that from a UX perspective, its best to limit your document to two fonts. This gives it a more professional appearance. To change the default font, in a new RMarkdown document add these lines to your YAML header, under the pdf\\_document line (watch the spacing!):\n\noutput:\n   pdf\\_document:\n     latex\\_engine: xelatex\nmainfont: font name (like Calibri Light)\n\nTo have just the R code use a different font, add this line:\n\nmonofont: font name (Arial)\n\nYou can also change the font size to one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To set this option, add this line to your YAML header:\n\nfontsize: 12 pt\n\nBTW - There is away around this that I will cover in a future post. If you like, you can combine all of these, resulting in this YAML header:\n\n\\---\ntitle: \"Font test\"\noutput:\n   pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 10 pt\n---\n\n  The finished result once you run KNITR will look like this: [![](http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample-1024x510.png)](http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample.png)","source":"_posts/r-markdown-cookbook-font-formatting.md","raw":"---\ntitle: Font Formatting - Typefaces\ntags:\n  - Big Data\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '3871'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-11 19:15:45\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png) First a little background - I've been working with R - the free open source statistical computing and graphics software for about a year, exploring various libraries and add-ons, trying to get a better understanding of it. This was spurred on by a couple of things. First, in SQL Server 2016, Microsoft incorporated R into their database system, allowing you to use R with the SQL Server Management Studio for a variety of purposes. And second, Power BI from Microsoft also incorporates R as a data source, allowing you to massage and manipulate data before passing it to Power BI, and you can use it to develop custom visualizations within Power BI. Because I take notes while I learn, I wanted some way to organize handy tips and tricks and show the results. I first experimented with Jupyter, but then I discovered R Markdown. For a full description of what R Markdown is and what its capable of, check out [this article](https://rmarkdown.rstudio.com/articles_intro.html) from RStudio, the developers of R Markdown.  With this library you can create documents (Word, PDF or HTML) within the R Studio IDE to show your code, and the results of the code in a single document. As a way to save my notes while working with R, I have been working with R Markdown, and I will pass on a few of those tips as part of a series: R Markdown Cookbook.\n<!-- more -->\nBy default, the RMarkdown PDF option uses a Times New Roman font. Its very old-school and I believe its used because a lot of R users are academics and it looks like the preferred format for a lot of R documentation. Or it could be that a fixed width font is preferable for academic journals. Regardless, IMHO, the output looks dated and I wanted to use a different font overall in my RMarkdown PDFs. Keep in mind that from a UX perspective, its best to limit your document to two fonts. This gives it a more professional appearance. To change the default font, in a new RMarkdown document add these lines to your YAML header, under the pdf\\_document line (watch the spacing!):\n\noutput:\n   pdf\\_document:\n     latex\\_engine: xelatex\nmainfont: font name (like Calibri Light)\n\nTo have just the R code use a different font, add this line:\n\nmonofont: font name (Arial)\n\nYou can also change the font size to one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To set this option, add this line to your YAML header:\n\nfontsize: 12 pt\n\nBTW - There is away around this that I will cover in a future post. If you like, you can combine all of these, resulting in this YAML header:\n\n\\---\ntitle: \"Font test\"\noutput:\n   pdf\\_document:\n      latex\\_engine: xelatex\nmainfont: Calibri Light\nmonofont: Arial\nfontsize: 10 pt\n---\n\n  The finished result once you run KNITR will look like this: [![](http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample-1024x510.png)](http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample.png)","slug":"r-markdown-cookbook-font-formatting","published":1,"updated":"2020-08-23T20:54:35.130Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7h0082sdjxcixv41qo","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> First a little background - I’ve been working with R - the free open source statistical computing and graphics software for about a year, exploring various libraries and add-ons, trying to get a better understanding of it. This was spurred on by a couple of things. First, in SQL Server 2016, Microsoft incorporated R into their database system, allowing you to use R with the SQL Server Management Studio for a variety of purposes. And second, Power BI from Microsoft also incorporates R as a data source, allowing you to massage and manipulate data before passing it to Power BI, and you can use it to develop custom visualizations within Power BI. Because I take notes while I learn, I wanted some way to organize handy tips and tricks and show the results. I first experimented with Jupyter, but then I discovered R Markdown. For a full description of what R Markdown is and what its capable of, check out <a href=\"https://rmarkdown.rstudio.com/articles_intro.html\">this article</a> from RStudio, the developers of R Markdown.  With this library you can create documents (Word, PDF or HTML) within the R Studio IDE to show your code, and the results of the code in a single document. As a way to save my notes while working with R, I have been working with R Markdown, and I will pass on a few of those tips as part of a series: R Markdown Cookbook.</p>\n<a id=\"more\"></a>\n<p>By default, the RMarkdown PDF option uses a Times New Roman font. Its very old-school and I believe its used because a lot of R users are academics and it looks like the preferred format for a lot of R documentation. Or it could be that a fixed width font is preferable for academic journals. Regardless, IMHO, the output looks dated and I wanted to use a different font overall in my RMarkdown PDFs. Keep in mind that from a UX perspective, its best to limit your document to two fonts. This gives it a more professional appearance. To change the default font, in a new RMarkdown document add these lines to your YAML header, under the pdf_document line (watch the spacing!):</p>\n<p>output:<br>   pdf_document:<br>     latex_engine: xelatex<br>mainfont: font name (like Calibri Light)</p>\n<p>To have just the R code use a different font, add this line:</p>\n<p>monofont: font name (Arial)</p>\n<p>You can also change the font size to one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To set this option, add this line to your YAML header:</p>\n<p>fontsize: 12 pt</p>\n<p>BTW - There is away around this that I will cover in a future post. If you like, you can combine all of these, resulting in this YAML header:</p>\n<p>-–<br>title: “Font test”<br>output:<br>   pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 10 pt</p>\n<hr>\n<p>  The finished result once you run KNITR will look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample-1024x510.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a> First a little background - I’ve been working with R - the free open source statistical computing and graphics software for about a year, exploring various libraries and add-ons, trying to get a better understanding of it. This was spurred on by a couple of things. First, in SQL Server 2016, Microsoft incorporated R into their database system, allowing you to use R with the SQL Server Management Studio for a variety of purposes. And second, Power BI from Microsoft also incorporates R as a data source, allowing you to massage and manipulate data before passing it to Power BI, and you can use it to develop custom visualizations within Power BI. Because I take notes while I learn, I wanted some way to organize handy tips and tricks and show the results. I first experimented with Jupyter, but then I discovered R Markdown. For a full description of what R Markdown is and what its capable of, check out <a href=\"https://rmarkdown.rstudio.com/articles_intro.html\">this article</a> from RStudio, the developers of R Markdown.  With this library you can create documents (Word, PDF or HTML) within the R Studio IDE to show your code, and the results of the code in a single document. As a way to save my notes while working with R, I have been working with R Markdown, and I will pass on a few of those tips as part of a series: R Markdown Cookbook.</p>","more":"<p>By default, the RMarkdown PDF option uses a Times New Roman font. Its very old-school and I believe its used because a lot of R users are academics and it looks like the preferred format for a lot of R documentation. Or it could be that a fixed width font is preferable for academic journals. Regardless, IMHO, the output looks dated and I wanted to use a different font overall in my RMarkdown PDFs. Keep in mind that from a UX perspective, its best to limit your document to two fonts. This gives it a more professional appearance. To change the default font, in a new RMarkdown document add these lines to your YAML header, under the pdf_document line (watch the spacing!):</p>\n<p>output:<br>   pdf_document:<br>     latex_engine: xelatex<br>mainfont: font name (like Calibri Light)</p>\n<p>To have just the R code use a different font, add this line:</p>\n<p>monofont: font name (Arial)</p>\n<p>You can also change the font size to one of three options (10pt, 11pt or 12pt) in PDF output. This limitation is due to standard latex classes only accepting those three. To set this option, add this line to your YAML header:</p>\n<p>fontsize: 12 pt</p>\n<p>BTW - There is away around this that I will cover in a future post. If you like, you can combine all of these, resulting in this YAML header:</p>\n<p>-–<br>title: “Font test”<br>output:<br>   pdf_document:<br>      latex_engine: xelatex<br>mainfont: Calibri Light<br>monofont: Arial<br>fontsize: 10 pt</p>\n<hr>\n<p>  The finished result once you run KNITR will look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/FontOutputSample-1024x510.png\"></a></p>"},{"title":"Lorem Ipsum text","id":"3993","comments":0,"date":"2018-09-27T15:35:05.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)A quick tip this time out for R Markdown. For many reasons you may need to add blocks of text to your R Markdown documents. From [Infogalactic.com](https://infogalactic.com/info/Lorem_ipsum), \"in publishing and graphic design ... filler text is commonly used to demonstrate the graphic elements of a document or visual presentation. Replacing meaningful content with placeholder text allows viewers to focus on graphic aspects such as font, typography, and page layout without being distracted by the content. It also reduces the need for the designer to come up with meaningful text, as they can instead use quickly-generated lorem ipsum.\" If you need to generate filler text for you R Markdown document, you can do so very quickly.\n<!-- more -->\nIn the XAML header section of your file, add this:\n\nheader-includes:\n    usepackage{blindtext}\n\nThen in the body of your document locate where you want your filler text to be. The command you enter in your document will depend on the amount of filler text you want to generate. For a short section of filler text use the command:\n\nblindtext\n\nFor a longer section of filler text, use:\n\nBlindtext\n\nThe only difference between the two is a capital or lowercase B. The shell code for the R Markdown file would them look like this: [![](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-300x211.png)](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum.png) The resulting output is: [![](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results-1024x890.png)](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results.png)","source":"_posts/r-markdown-cookbook-lorem-ipsum-text.md","raw":"---\ntitle: Lorem Ipsum text\ntags:\n  - cookbook\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '3993'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-27 11:35:05\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png)A quick tip this time out for R Markdown. For many reasons you may need to add blocks of text to your R Markdown documents. From [Infogalactic.com](https://infogalactic.com/info/Lorem_ipsum), \"in publishing and graphic design ... filler text is commonly used to demonstrate the graphic elements of a document or visual presentation. Replacing meaningful content with placeholder text allows viewers to focus on graphic aspects such as font, typography, and page layout without being distracted by the content. It also reduces the need for the designer to come up with meaningful text, as they can instead use quickly-generated lorem ipsum.\" If you need to generate filler text for you R Markdown document, you can do so very quickly.\n<!-- more -->\nIn the XAML header section of your file, add this:\n\nheader-includes:\n    usepackage{blindtext}\n\nThen in the body of your document locate where you want your filler text to be. The command you enter in your document will depend on the amount of filler text you want to generate. For a short section of filler text use the command:\n\nblindtext\n\nFor a longer section of filler text, use:\n\nBlindtext\n\nThe only difference between the two is a capital or lowercase B. The shell code for the R Markdown file would them look like this: [![](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-300x211.png)](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum.png) The resulting output is: [![](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results-1024x890.png)](http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results.png)","slug":"r-markdown-cookbook-lorem-ipsum-text","published":1,"updated":"2020-08-23T20:54:35.146Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7l0085sdjxgeeo0um9","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"></a>A quick tip this time out for R Markdown. For many reasons you may need to add blocks of text to your R Markdown documents. From <a href=\"https://infogalactic.com/info/Lorem_ipsum\">Infogalactic.com</a>, “in publishing and graphic design … filler text is commonly used to demonstrate the graphic elements of a document or visual presentation. Replacing meaningful content with placeholder text allows viewers to focus on graphic aspects such as font, typography, and page layout without being distracted by the content. It also reduces the need for the designer to come up with meaningful text, as they can instead use quickly-generated lorem ipsum.” If you need to generate filler text for you R Markdown document, you can do so very quickly.</p>\n<a id=\"more\"></a>\n<p>In the XAML header section of your file, add this:</p>\n<p>header-includes:<br>    usepackage{blindtext}</p>\n<p>Then in the body of your document locate where you want your filler text to be. The command you enter in your document will depend on the amount of filler text you want to generate. For a short section of filler text use the command:</p>\n<p>blindtext</p>\n<p>For a longer section of filler text, use:</p>\n<p>Blindtext</p>\n<p>The only difference between the two is a capital or lowercase B. The shell code for the R Markdown file would them look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-300x211.png\"></a> The resulting output is: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results-1024x890.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown.png\"></a>A quick tip this time out for R Markdown. For many reasons you may need to add blocks of text to your R Markdown documents. From <a href=\"https://infogalactic.com/info/Lorem_ipsum\">Infogalactic.com</a>, “in publishing and graphic design … filler text is commonly used to demonstrate the graphic elements of a document or visual presentation. Replacing meaningful content with placeholder text allows viewers to focus on graphic aspects such as font, typography, and page layout without being distracted by the content. It also reduces the need for the designer to come up with meaningful text, as they can instead use quickly-generated lorem ipsum.” If you need to generate filler text for you R Markdown document, you can do so very quickly.</p>","more":"<p>In the XAML header section of your file, add this:</p>\n<p>header-includes:<br>    usepackage{blindtext}</p>\n<p>Then in the body of your document locate where you want your filler text to be. The command you enter in your document will depend on the amount of filler text you want to generate. For a short section of filler text use the command:</p>\n<p>blindtext</p>\n<p>For a longer section of filler text, use:</p>\n<p>Blindtext</p>\n<p>The only difference between the two is a capital or lowercase B. The shell code for the R Markdown file would them look like this: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-300x211.png\"></a> The resulting output is: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Lorem-Ipsum-results-1024x890.png\"></a></p>"},{"title":"PDF Page options","id":"3976","comments":0,"date":"2018-09-25T18:50:21.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)This time around I'm deep diving into tips for formatting PDF output with R Markdown. One advantage I have found with PDFs over EPUB files is that when you create a PDF it looks the way you intended to, almost a snapshot of a printed page. While EPUB has some advantages over PDFs (including much smaller file sizes), a PDF to me is still preferable for technical documentation where there are screen shots, figures and charts. When generating a PDF with R Markdown there are a few options you can define to determine how your pages look, including the document size (letter, legal, A4, etc), page orientation, and even the set different margins for each side of the page. All of these options are specified within the YAML metadata header portion of your R Markdown document, and make use of many LaTex functions. By default, R Markdown uses the pdflatex engine, but if you plan to use fonts from your system, or need support for Unicode, you should use one of the alternatives: xelatex or lualatex.\n<!-- more -->\nFirst lets look at how to set up your header section to include LaTex support. You define your document title, then specify your output format. In this case a pdf\\_document. If you are going to be printing tables of data, [include the kable package](http://edpflager.com/2018/09/14/r-markdown-cookbook-table-formatting-for-pdf/) for better formatting control and then specify the latex\\_engine to use. The result should resemble this:\n\ntitle: \"PDFTest\"\n  output: \n    pdf\\_document:\n      df\\_print: kable\n      latex\\_engine: xelatex\n\nIn a previous post I covered [how to use system fonts](http://edpflager.com/2018/09/12/r-markdown-cookbook-font-formatting/) in your PDF output from R Markdown, so I will not cover that here.\n\n#### PAPER SIZE\n\nStarting with the dimensions of our document, we can define the size of our PDF output \"page\" using the papersize option. R Markdown and LaText support both the ISO 216 standard and the North American standard, so you have numerous options to use, although the default is the North American letter size. The basic format for the papersize option is:\n\npapersize: size  -- such as letter/legal/ledger/tabloid/a4/b3\n\n[BelightSoft has a good article](https://www.belightsoft.com/products/resources/paper-sizes-and-formats-explained) on the various paper sizes used throughout the world.\n\n#### PAGE ORIENTATION\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape-300x213.jpg)](http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape.jpg)Besides the size of the output \"page\", you can also set the orientation of the page. Typically page orientation is set to portrait as the default. Portrait mode means the width of the page is shorter than the length. However, if you want to switch it to make your output print horizontally across the wider portion of the documentation  you would use landscape orientation. To use portrait mode, you don't need to add anything to your YAML header, although you can specify portrait as an option if you want it to be explicit. If you'd prefer to set your documentation to landscape, add this line to your header section:\n\nclassoption:\n   landscape\n\n#### MARGINS\n\nOur final page setup option for R Markdown is margins for your PDF document. Generally margins are consistent on all four sides of a document page, with a default of 1 inch in the North America system or 2.5 centimeters in the ISO 216 standard. To override those defaults, you can use the geometry option in your YAML header, and specify your margins. If you want to have the same margin on all four sides of the document, use the margin option, like this:\n\ngeometry:\n    margin= .75in\n\nIf you'd like to specify different margin widths on various sides, you can specify them individually or use a default for all of them, and then override the default for a specific side. As an example, to have a 2.5cm margin on the top, bottom and right, with a 5cm margin on the left, enter this in your YAML header :\n\ngeometry:\n    margin= 2.5cm,\n    left=5cm\n\nYou can also specify each side individually using several different options, seperated by commas, followed by your preferred measurement:\n\n*   left | lmargin | inner\n*   right | rmargin | outer\n*   top | tmargin\n*   bottom | bmargin\n\nTo wrapup, here is the YAML header using the options discussed above: [![](http://edpflager.com/wp-content/uploads/2018/09/YAML-example-263x300.png)](http://edpflager.com/wp-content/uploads/2018/09/YAML-example.png)   The R logo is © 2016 [The R Foundation](https://www.r-project.org/logo/). Used under terms of the CC-BY-SA 4.0 license.","source":"_posts/r-markdown-cookbook-pdf-page-options.md","raw":"---\ntitle: PDF Page options\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - R Markdown\n  - technical\nid: '3976'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-25 14:50:21\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)This time around I'm deep diving into tips for formatting PDF output with R Markdown. One advantage I have found with PDFs over EPUB files is that when you create a PDF it looks the way you intended to, almost a snapshot of a printed page. While EPUB has some advantages over PDFs (including much smaller file sizes), a PDF to me is still preferable for technical documentation where there are screen shots, figures and charts. When generating a PDF with R Markdown there are a few options you can define to determine how your pages look, including the document size (letter, legal, A4, etc), page orientation, and even the set different margins for each side of the page. All of these options are specified within the YAML metadata header portion of your R Markdown document, and make use of many LaTex functions. By default, R Markdown uses the pdflatex engine, but if you plan to use fonts from your system, or need support for Unicode, you should use one of the alternatives: xelatex or lualatex.\n<!-- more -->\nFirst lets look at how to set up your header section to include LaTex support. You define your document title, then specify your output format. In this case a pdf\\_document. If you are going to be printing tables of data, [include the kable package](http://edpflager.com/2018/09/14/r-markdown-cookbook-table-formatting-for-pdf/) for better formatting control and then specify the latex\\_engine to use. The result should resemble this:\n\ntitle: \"PDFTest\"\n  output: \n    pdf\\_document:\n      df\\_print: kable\n      latex\\_engine: xelatex\n\nIn a previous post I covered [how to use system fonts](http://edpflager.com/2018/09/12/r-markdown-cookbook-font-formatting/) in your PDF output from R Markdown, so I will not cover that here.\n\n#### PAPER SIZE\n\nStarting with the dimensions of our document, we can define the size of our PDF output \"page\" using the papersize option. R Markdown and LaText support both the ISO 216 standard and the North American standard, so you have numerous options to use, although the default is the North American letter size. The basic format for the papersize option is:\n\npapersize: size  -- such as letter/legal/ledger/tabloid/a4/b3\n\n[BelightSoft has a good article](https://www.belightsoft.com/products/resources/paper-sizes-and-formats-explained) on the various paper sizes used throughout the world.\n\n#### PAGE ORIENTATION\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape-300x213.jpg)](http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape.jpg)Besides the size of the output \"page\", you can also set the orientation of the page. Typically page orientation is set to portrait as the default. Portrait mode means the width of the page is shorter than the length. However, if you want to switch it to make your output print horizontally across the wider portion of the documentation  you would use landscape orientation. To use portrait mode, you don't need to add anything to your YAML header, although you can specify portrait as an option if you want it to be explicit. If you'd prefer to set your documentation to landscape, add this line to your header section:\n\nclassoption:\n   landscape\n\n#### MARGINS\n\nOur final page setup option for R Markdown is margins for your PDF document. Generally margins are consistent on all four sides of a document page, with a default of 1 inch in the North America system or 2.5 centimeters in the ISO 216 standard. To override those defaults, you can use the geometry option in your YAML header, and specify your margins. If you want to have the same margin on all four sides of the document, use the margin option, like this:\n\ngeometry:\n    margin= .75in\n\nIf you'd like to specify different margin widths on various sides, you can specify them individually or use a default for all of them, and then override the default for a specific side. As an example, to have a 2.5cm margin on the top, bottom and right, with a 5cm margin on the left, enter this in your YAML header :\n\ngeometry:\n    margin= 2.5cm,\n    left=5cm\n\nYou can also specify each side individually using several different options, seperated by commas, followed by your preferred measurement:\n\n*   left | lmargin | inner\n*   right | rmargin | outer\n*   top | tmargin\n*   bottom | bmargin\n\nTo wrapup, here is the YAML header using the options discussed above: [![](http://edpflager.com/wp-content/uploads/2018/09/YAML-example-263x300.png)](http://edpflager.com/wp-content/uploads/2018/09/YAML-example.png)   The R logo is © 2016 [The R Foundation](https://www.r-project.org/logo/). Used under terms of the CC-BY-SA 4.0 license.","slug":"r-markdown-cookbook-pdf-page-options","published":1,"updated":"2020-08-23T20:54:35.142Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7q0089sdjx8v197vfm","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>This time around I’m deep diving into tips for formatting PDF output with R Markdown. One advantage I have found with PDFs over EPUB files is that when you create a PDF it looks the way you intended to, almost a snapshot of a printed page. While EPUB has some advantages over PDFs (including much smaller file sizes), a PDF to me is still preferable for technical documentation where there are screen shots, figures and charts. When generating a PDF with R Markdown there are a few options you can define to determine how your pages look, including the document size (letter, legal, A4, etc), page orientation, and even the set different margins for each side of the page. All of these options are specified within the YAML metadata header portion of your R Markdown document, and make use of many LaTex functions. By default, R Markdown uses the pdflatex engine, but if you plan to use fonts from your system, or need support for Unicode, you should use one of the alternatives: xelatex or lualatex.</p>\n<a id=\"more\"></a>\n<p>First lets look at how to set up your header section to include LaTex support. You define your document title, then specify your output format. In this case a pdf_document. If you are going to be printing tables of data, <a href=\"http://edpflager.com/2018/09/14/r-markdown-cookbook-table-formatting-for-pdf/\">include the kable package</a> for better formatting control and then specify the latex_engine to use. The result should resemble this:</p>\n<p>title: “PDFTest”<br>  output:<br>    pdf_document:<br>      df_print: kable<br>      latex_engine: xelatex</p>\n<p>In a previous post I covered <a href=\"http://edpflager.com/2018/09/12/r-markdown-cookbook-font-formatting/\">how to use system fonts</a> in your PDF output from R Markdown, so I will not cover that here.</p>\n<h4 id=\"PAPER-SIZE\"><a href=\"#PAPER-SIZE\" class=\"headerlink\" title=\"PAPER SIZE\"></a>PAPER SIZE</h4><p>Starting with the dimensions of our document, we can define the size of our PDF output “page” using the papersize option. R Markdown and LaText support both the ISO 216 standard and the North American standard, so you have numerous options to use, although the default is the North American letter size. The basic format for the papersize option is:</p>\n<p>papersize: size  – such as letter/legal/ledger/tabloid/a4/b3</p>\n<p><a href=\"https://www.belightsoft.com/products/resources/paper-sizes-and-formats-explained\">BelightSoft has a good article</a> on the various paper sizes used throughout the world.</p>\n<h4 id=\"PAGE-ORIENTATION\"><a href=\"#PAGE-ORIENTATION\" class=\"headerlink\" title=\"PAGE ORIENTATION\"></a>PAGE ORIENTATION</h4><p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape-300x213.jpg\"></a>Besides the size of the output “page”, you can also set the orientation of the page. Typically page orientation is set to portrait as the default. Portrait mode means the width of the page is shorter than the length. However, if you want to switch it to make your output print horizontally across the wider portion of the documentation  you would use landscape orientation. To use portrait mode, you don’t need to add anything to your YAML header, although you can specify portrait as an option if you want it to be explicit. If you’d prefer to set your documentation to landscape, add this line to your header section:</p>\n<p>classoption:<br>   landscape</p>\n<h4 id=\"MARGINS\"><a href=\"#MARGINS\" class=\"headerlink\" title=\"MARGINS\"></a>MARGINS</h4><p>Our final page setup option for R Markdown is margins for your PDF document. Generally margins are consistent on all four sides of a document page, with a default of 1 inch in the North America system or 2.5 centimeters in the ISO 216 standard. To override those defaults, you can use the geometry option in your YAML header, and specify your margins. If you want to have the same margin on all four sides of the document, use the margin option, like this:</p>\n<p>geometry:<br>    margin= .75in</p>\n<p>If you’d like to specify different margin widths on various sides, you can specify them individually or use a default for all of them, and then override the default for a specific side. As an example, to have a 2.5cm margin on the top, bottom and right, with a 5cm margin on the left, enter this in your YAML header :</p>\n<p>geometry:<br>    margin= 2.5cm,<br>    left=5cm</p>\n<p>You can also specify each side individually using several different options, seperated by commas, followed by your preferred measurement:</p>\n<ul>\n<li>left | lmargin | inner</li>\n<li>right | rmargin | outer</li>\n<li>top | tmargin</li>\n<li>bottom | bmargin</li>\n</ul>\n<p>To wrapup, here is the YAML header using the options discussed above: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/YAML-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/YAML-example-263x300.png\"></a>   The R logo is © 2016 <a href=\"https://www.r-project.org/logo/\">The R Foundation</a>. Used under terms of the CC-BY-SA 4.0 license.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>This time around I’m deep diving into tips for formatting PDF output with R Markdown. One advantage I have found with PDFs over EPUB files is that when you create a PDF it looks the way you intended to, almost a snapshot of a printed page. While EPUB has some advantages over PDFs (including much smaller file sizes), a PDF to me is still preferable for technical documentation where there are screen shots, figures and charts. When generating a PDF with R Markdown there are a few options you can define to determine how your pages look, including the document size (letter, legal, A4, etc), page orientation, and even the set different margins for each side of the page. All of these options are specified within the YAML metadata header portion of your R Markdown document, and make use of many LaTex functions. By default, R Markdown uses the pdflatex engine, but if you plan to use fonts from your system, or need support for Unicode, you should use one of the alternatives: xelatex or lualatex.</p>","more":"<p>First lets look at how to set up your header section to include LaTex support. You define your document title, then specify your output format. In this case a pdf_document. If you are going to be printing tables of data, <a href=\"http://edpflager.com/2018/09/14/r-markdown-cookbook-table-formatting-for-pdf/\">include the kable package</a> for better formatting control and then specify the latex_engine to use. The result should resemble this:</p>\n<p>title: “PDFTest”<br>  output:<br>    pdf_document:<br>      df_print: kable<br>      latex_engine: xelatex</p>\n<p>In a previous post I covered <a href=\"http://edpflager.com/2018/09/12/r-markdown-cookbook-font-formatting/\">how to use system fonts</a> in your PDF output from R Markdown, so I will not cover that here.</p>\n<h4 id=\"PAPER-SIZE\"><a href=\"#PAPER-SIZE\" class=\"headerlink\" title=\"PAPER SIZE\"></a>PAPER SIZE</h4><p>Starting with the dimensions of our document, we can define the size of our PDF output “page” using the papersize option. R Markdown and LaText support both the ISO 216 standard and the North American standard, so you have numerous options to use, although the default is the North American letter size. The basic format for the papersize option is:</p>\n<p>papersize: size  – such as letter/legal/ledger/tabloid/a4/b3</p>\n<p><a href=\"https://www.belightsoft.com/products/resources/paper-sizes-and-formats-explained\">BelightSoft has a good article</a> on the various paper sizes used throughout the world.</p>\n<h4 id=\"PAGE-ORIENTATION\"><a href=\"#PAGE-ORIENTATION\" class=\"headerlink\" title=\"PAGE ORIENTATION\"></a>PAGE ORIENTATION</h4><p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/portrait-landscape-300x213.jpg\"></a>Besides the size of the output “page”, you can also set the orientation of the page. Typically page orientation is set to portrait as the default. Portrait mode means the width of the page is shorter than the length. However, if you want to switch it to make your output print horizontally across the wider portion of the documentation  you would use landscape orientation. To use portrait mode, you don’t need to add anything to your YAML header, although you can specify portrait as an option if you want it to be explicit. If you’d prefer to set your documentation to landscape, add this line to your header section:</p>\n<p>classoption:<br>   landscape</p>\n<h4 id=\"MARGINS\"><a href=\"#MARGINS\" class=\"headerlink\" title=\"MARGINS\"></a>MARGINS</h4><p>Our final page setup option for R Markdown is margins for your PDF document. Generally margins are consistent on all four sides of a document page, with a default of 1 inch in the North America system or 2.5 centimeters in the ISO 216 standard. To override those defaults, you can use the geometry option in your YAML header, and specify your margins. If you want to have the same margin on all four sides of the document, use the margin option, like this:</p>\n<p>geometry:<br>    margin= .75in</p>\n<p>If you’d like to specify different margin widths on various sides, you can specify them individually or use a default for all of them, and then override the default for a specific side. As an example, to have a 2.5cm margin on the top, bottom and right, with a 5cm margin on the left, enter this in your YAML header :</p>\n<p>geometry:<br>    margin= 2.5cm,<br>    left=5cm</p>\n<p>You can also specify each side individually using several different options, seperated by commas, followed by your preferred measurement:</p>\n<ul>\n<li>left | lmargin | inner</li>\n<li>right | rmargin | outer</li>\n<li>top | tmargin</li>\n<li>bottom | bmargin</li>\n</ul>\n<p>To wrapup, here is the YAML header using the options discussed above: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/YAML-example.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/YAML-example-263x300.png\"></a>   The R logo is © 2016 <a href=\"https://www.r-project.org/logo/\">The R Foundation</a>. Used under terms of the CC-BY-SA 4.0 license.</p>"},{"title":"Table formatting for PDF","id":"3884","comments":0,"date":"2018-09-13T23:40:52.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Using R Markdown to create PDF files for sharing, you can chose several different options for printing tables of data: **default, kable** and **tibble**. If you are using HTML, you have another option called **paged**, which allows you to specify the number of rows and columns to display, and the viewer can page between pages without having to scroll up and down a large table. I'll cover that option in a future post. To use the default style for PDF output, don't include a df\\_print option in your R-Markdown YAML header section. To use kable or tibble add the df\\_print option like below and follow it with the option you want to use.\n<!-- more -->\nBe sure to watch the indentation of the df\\_print line.\n\n\\---\ntitle: \"Table example\"\noutput:\n   pdf\\_document:\n      df\\_print: kable/tibble\n---\n\nHere are examples of the three outputs: [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-300x253.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-e1536890311448.png) Fig 1 . Traditional table output [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-244x300.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-e1536890265304.png) Fig 2. Kable table output [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample-297x300.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample.png) Fig 3. Tibble table output If you do use **kable**, be aware that on wider tables the results may be cut off due to page sizes. To fix that, you will need to include the **kableExtra** and **magrittr** libraries. Then wrap the table function like this:\n\nkable(head(bike)) %>% kable\\_styling(latex\\_options = c(\"striped\",\"scale\\_down\"))\n\n  Finally, although the latex\\_option is called \"scale\\_down\", what it does is make your table fit within the margins of your document. On tables that already fit comfortably, using this function will enlarge them to fit from left to right margin. Below is what the output looks like using **kable** for a wide data set, first using the default **kable** settings, and then using the **kable\\_styling:** [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-1024x408.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-e1536892335405.png) Fig 4 . Table without kable\\_styling and with kable\\_styling I included the option **striped** as well, which subtly highlights every other line for easier viewing.   The R logo is © 2016 [The R Foundation](https://www.r-project.org/logo/). Used under terms of the CC-BY-SA 4.0 license.","source":"_posts/r-markdown-cookbook-table-formatting-for-pdf.md","raw":"---\ntitle: Table formatting for PDF\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - technical\nid: '3884'\ncategories:\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-13 19:40:52\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Using R Markdown to create PDF files for sharing, you can chose several different options for printing tables of data: **default, kable** and **tibble**. If you are using HTML, you have another option called **paged**, which allows you to specify the number of rows and columns to display, and the viewer can page between pages without having to scroll up and down a large table. I'll cover that option in a future post. To use the default style for PDF output, don't include a df\\_print option in your R-Markdown YAML header section. To use kable or tibble add the df\\_print option like below and follow it with the option you want to use.\n<!-- more -->\nBe sure to watch the indentation of the df\\_print line.\n\n\\---\ntitle: \"Table example\"\noutput:\n   pdf\\_document:\n      df\\_print: kable/tibble\n---\n\nHere are examples of the three outputs: [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-300x253.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-e1536890311448.png) Fig 1 . Traditional table output [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-244x300.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-e1536890265304.png) Fig 2. Kable table output [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample-297x300.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample.png) Fig 3. Tibble table output If you do use **kable**, be aware that on wider tables the results may be cut off due to page sizes. To fix that, you will need to include the **kableExtra** and **magrittr** libraries. Then wrap the table function like this:\n\nkable(head(bike)) %>% kable\\_styling(latex\\_options = c(\"striped\",\"scale\\_down\"))\n\n  Finally, although the latex\\_option is called \"scale\\_down\", what it does is make your table fit within the margins of your document. On tables that already fit comfortably, using this function will enlarge them to fit from left to right margin. Below is what the output looks like using **kable** for a wide data set, first using the default **kable** settings, and then using the **kable\\_styling:** [![](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-1024x408.png)](http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-e1536892335405.png) Fig 4 . Table without kable\\_styling and with kable\\_styling I included the option **striped** as well, which subtly highlights every other line for easier viewing.   The R logo is © 2016 [The R Foundation](https://www.r-project.org/logo/). Used under terms of the CC-BY-SA 4.0 license.","slug":"r-markdown-cookbook-table-formatting-for-pdf","published":1,"updated":"2020-08-23T20:54:35.134Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7t008csdjx6ir3haiv","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Using R Markdown to create PDF files for sharing, you can chose several different options for printing tables of data: <strong>default, kable</strong> and <strong>tibble</strong>. If you are using HTML, you have another option called <strong>paged</strong>, which allows you to specify the number of rows and columns to display, and the viewer can page between pages without having to scroll up and down a large table. I’ll cover that option in a future post. To use the default style for PDF output, don’t include a df_print option in your R-Markdown YAML header section. To use kable or tibble add the df_print option like below and follow it with the option you want to use.</p>\n<a id=\"more\"></a>\n<p>Be sure to watch the indentation of the df_print line.</p>\n<p>-–<br>title: “Table example”<br>output:<br>   pdf_document:<br>      df_print: kable/tibble</p>\n<hr>\n<p>Here are examples of the three outputs: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-e1536890311448.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-300x253.png\"></a> Fig 1 . Traditional table output <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-e1536890265304.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-244x300.png\"></a> Fig 2. Kable table output <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample-297x300.png\"></a> Fig 3. Tibble table output If you do use <strong>kable</strong>, be aware that on wider tables the results may be cut off due to page sizes. To fix that, you will need to include the <strong>kableExtra</strong> and <strong>magrittr</strong> libraries. Then wrap the table function like this:</p>\n<p>kable(head(bike)) %&gt;% kable_styling(latex_options = c(“striped”,”scale_down”))</p>\n<p>  Finally, although the latex_option is called “scale_down”, what it does is make your table fit within the margins of your document. On tables that already fit comfortably, using this function will enlarge them to fit from left to right margin. Below is what the output looks like using <strong>kable</strong> for a wide data set, first using the default <strong>kable</strong> settings, and then using the <strong>kable_styling:</strong> <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-e1536892335405.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-1024x408.png\"></a> Fig 4 . Table without kable_styling and with kable_styling I included the option <strong>striped</strong> as well, which subtly highlights every other line for easier viewing.   The R logo is © 2016 <a href=\"https://www.r-project.org/logo/\">The R Foundation</a>. Used under terms of the CC-BY-SA 4.0 license.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Using R Markdown to create PDF files for sharing, you can chose several different options for printing tables of data: <strong>default, kable</strong> and <strong>tibble</strong>. If you are using HTML, you have another option called <strong>paged</strong>, which allows you to specify the number of rows and columns to display, and the viewer can page between pages without having to scroll up and down a large table. I’ll cover that option in a future post. To use the default style for PDF output, don’t include a df_print option in your R-Markdown YAML header section. To use kable or tibble add the df_print option like below and follow it with the option you want to use.</p>","more":"<p>Be sure to watch the indentation of the df_print line.</p>\n<p>-–<br>title: “Table example”<br>output:<br>   pdf_document:<br>      df_print: kable/tibble</p>\n<hr>\n<p>Here are examples of the three outputs: <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-e1536890311448.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-default-sample-300x253.png\"></a> Fig 1 . Traditional table output <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-e1536890265304.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kable-sample-244x300.png\"></a> Fig 2. Kable table output <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-tibble-sample-297x300.png\"></a> Fig 3. Tibble table output If you do use <strong>kable</strong>, be aware that on wider tables the results may be cut off due to page sizes. To fix that, you will need to include the <strong>kableExtra</strong> and <strong>magrittr</strong> libraries. Then wrap the table function like this:</p>\n<p>kable(head(bike)) %&gt;% kable_styling(latex_options = c(“striped”,”scale_down”))</p>\n<p>  Finally, although the latex_option is called “scale_down”, what it does is make your table fit within the margins of your document. On tables that already fit comfortably, using this function will enlarge them to fit from left to right margin. Below is what the output looks like using <strong>kable</strong> for a wide data set, first using the default <strong>kable</strong> settings, and then using the <strong>kable_styling:</strong> <a href=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-e1536892335405.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/dataframe-kamble-wide-1024x408.png\"></a> Fig 4 . Table without kable_styling and with kable_styling I included the option <strong>striped</strong> as well, which subtly highlights every other line for easier viewing.   The R logo is © 2016 <a href=\"https://www.r-project.org/logo/\">The R Foundation</a>. Used under terms of the CC-BY-SA 4.0 license.</p>"},{"title":"Using Inline Code","id":"3961","comments":0,"date":"2018-09-17T16:46:51.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)When working with R Markdown, there may be times where rather than having a chart or figure set off from the rest of your document, you may want to have the results of an R statement embedded directly within a line or paragraph of text. So, for example, if you were working on an HTML document using a dataset that changes over time, you could write your text and include R Markdown inline code. The syntax is similar to a normal Code chunk but stripped down a bit. You use a tick mark around your code, and an r indicator followed by your code. This example displays the current time in your output document:\n\n'r Sys.time()\\`.\n<!-- more -->\nIf you want to wrap that in text, you could have a line like this:\n\nAs of now \\`r Sys.time()\\`, the mean of the Car Speed data was: \\`r mean(cars$speed)\\`\n\nWhich generates results like this:[![](http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png)](http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png) As I said earlier this is very handy when you are generating an HTML document, because both the system time and the result of the mean statement will be retrieved when the webpage is refreshed. It does have some use in PDF output, displaying when the document was generated and the mean value at that time.","source":"_posts/r-markdown-cookbook-using-inline-code.md","raw":"---\ntitle: Using Inline Code\ntags:\n  - cookbook\n  - How-to\n  - howto\n  - technical\nid: '3961'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-09-17 12:46:51\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)When working with R Markdown, there may be times where rather than having a chart or figure set off from the rest of your document, you may want to have the results of an R statement embedded directly within a line or paragraph of text. So, for example, if you were working on an HTML document using a dataset that changes over time, you could write your text and include R Markdown inline code. The syntax is similar to a normal Code chunk but stripped down a bit. You use a tick mark around your code, and an r indicator followed by your code. This example displays the current time in your output document:\n\n'r Sys.time()\\`.\n<!-- more -->\nIf you want to wrap that in text, you could have a line like this:\n\nAs of now \\`r Sys.time()\\`, the mean of the Car Speed data was: \\`r mean(cars$speed)\\`\n\nWhich generates results like this:[![](http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png)](http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png) As I said earlier this is very handy when you are generating an HTML document, because both the system time and the result of the mean statement will be retrieved when the webpage is refreshed. It does have some use in PDF output, displaying when the document was generated and the mean value at that time.","slug":"r-markdown-cookbook-using-inline-code","published":1,"updated":"2020-08-23T20:54:35.138Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7v008gsdjxc7ema2ye","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>When working with R Markdown, there may be times where rather than having a chart or figure set off from the rest of your document, you may want to have the results of an R statement embedded directly within a line or paragraph of text. So, for example, if you were working on an HTML document using a dataset that changes over time, you could write your text and include R Markdown inline code. The syntax is similar to a normal Code chunk but stripped down a bit. You use a tick mark around your code, and an r indicator followed by your code. This example displays the current time in your output document:</p>\n<p>‘r Sys.time()`.</p>\n<a id=\"more\"></a>\n<p>If you want to wrap that in text, you could have a line like this:</p>\n<p>As of now `r Sys.time()`, the mean of the Car Speed data was: `r mean(cars$speed)`</p>\n<p>Which generates results like this:<a href=\"http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png\"></a> As I said earlier this is very handy when you are generating an HTML document, because both the system time and the result of the mean statement will be retrieved when the webpage is refreshed. It does have some use in PDF output, displaying when the document was generated and the mean value at that time.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>When working with R Markdown, there may be times where rather than having a chart or figure set off from the rest of your document, you may want to have the results of an R statement embedded directly within a line or paragraph of text. So, for example, if you were working on an HTML document using a dataset that changes over time, you could write your text and include R Markdown inline code. The syntax is similar to a normal Code chunk but stripped down a bit. You use a tick mark around your code, and an r indicator followed by your code. This example displays the current time in your output document:</p>\n<p>‘r Sys.time()`.</p>","more":"<p>If you want to wrap that in text, you could have a line like this:</p>\n<p>As of now `r Sys.time()`, the mean of the Car Speed data was: `r mean(cars$speed)`</p>\n<p>Which generates results like this:<a href=\"http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/inline-code-with-text.png\"></a> As I said earlier this is very handy when you are generating an HTML document, because both the system time and the result of the mean statement will be retrieved when the webpage is refreshed. It does have some use in PDF output, displaying when the document was generated and the mean value at that time.</p>"},{"title":"R - Pacman package manager with !require","id":"4154","comments":0,"date":"2018-11-11T23:09:40.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png)](http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png)A few weeks back, I wrote a [quick post](http://edpflager.com/2018/10/26/r-quick-code-to-install-needed-packages/) about a one-line piece of R code that will check for required libraries, and if they are not installed will attempt to install them. Nice tip and helpful to use. This past week, I attended an R Users Group meeting in Ann Arbor, MI where Sam Firke was presenting about his package [Janitor](https://github.com/sfirke/janitor). When he showed some code, he mentioned a package he uses called [pacman](https://cran.r-project.org/web/packages/pacman/index.html).  Its a utility type package, designed to manage R packages, hence the name pacman. From the description on the CRAN site, it sounds dry, and not too interesting, but for locating information about your R installation, it is extremely helpful.  But one pacman function that Sam was using in his code caught my attention: p\\_load. From the package documentation:\n<!-- more -->\n\\[p\\_load\\] checks to see if a package is installed, if not it attempts\nto install the package from CRAN and/or any other repository in the\npacman repository list.\n\nThe other feature included in this function is the ability to use a vector of packages as part of the call. So rather than have multiple lines to load several packages using my previous tip, like this:\n\nif( !require(janitor){ install.packages('janitor')}\nlibrary(janitor)\n\nif( !require(tidyverse)){ install.packages('tidyverse')}\nlibrary(tidyverse)\n\nif( !require(odbc)){ install.packages('odbc')}\nlibrary(odbc)\n\nYou could instead use this:\n\nif( !require(pacman)){ install.packages('pacman')}\nlibrary(pacman) \\# for loading packages\n\np\\_load(janitor, tidyverse, odbc)\n\nThe first two lines are a repeat of my earlier tip to check to see if pacman is installed. If its not, then install it. Then using the pacman p\\_load function, load the required packages for the R script being run. By default, packages are installed if they are not present when you try to load them with p\\_load! If you are trying to use packages from GitHub instead of the CRAN repository, you can use p\\_load\\_gh instead. The rest of the functionality is the same, although with github repositories you do need to enclose each repository/package in quotes.  So for example to load the lvplot package from Hadley Wickham's repository, you would use:\n\np\\_load\\_gh(\"hadley/lvplot\")\n\nThat's it!","source":"_posts/r-pacman-package-manager-with-require.md","raw":"---\ntitle: R - Pacman package manager with !require\ntags:\n  - howto\nid: '4154'\ncategories:\n  - - Blog\n  - - R\ncomments: false\ndate: 2018-11-11 18:09:40\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png)](http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png)A few weeks back, I wrote a [quick post](http://edpflager.com/2018/10/26/r-quick-code-to-install-needed-packages/) about a one-line piece of R code that will check for required libraries, and if they are not installed will attempt to install them. Nice tip and helpful to use. This past week, I attended an R Users Group meeting in Ann Arbor, MI where Sam Firke was presenting about his package [Janitor](https://github.com/sfirke/janitor). When he showed some code, he mentioned a package he uses called [pacman](https://cran.r-project.org/web/packages/pacman/index.html).  Its a utility type package, designed to manage R packages, hence the name pacman. From the description on the CRAN site, it sounds dry, and not too interesting, but for locating information about your R installation, it is extremely helpful.  But one pacman function that Sam was using in his code caught my attention: p\\_load. From the package documentation:\n<!-- more -->\n\\[p\\_load\\] checks to see if a package is installed, if not it attempts\nto install the package from CRAN and/or any other repository in the\npacman repository list.\n\nThe other feature included in this function is the ability to use a vector of packages as part of the call. So rather than have multiple lines to load several packages using my previous tip, like this:\n\nif( !require(janitor){ install.packages('janitor')}\nlibrary(janitor)\n\nif( !require(tidyverse)){ install.packages('tidyverse')}\nlibrary(tidyverse)\n\nif( !require(odbc)){ install.packages('odbc')}\nlibrary(odbc)\n\nYou could instead use this:\n\nif( !require(pacman)){ install.packages('pacman')}\nlibrary(pacman) \\# for loading packages\n\np\\_load(janitor, tidyverse, odbc)\n\nThe first two lines are a repeat of my earlier tip to check to see if pacman is installed. If its not, then install it. Then using the pacman p\\_load function, load the required packages for the R script being run. By default, packages are installed if they are not present when you try to load them with p\\_load! If you are trying to use packages from GitHub instead of the CRAN repository, you can use p\\_load\\_gh instead. The rest of the functionality is the same, although with github repositories you do need to enclose each repository/package in quotes.  So for example to load the lvplot package from Hadley Wickham's repository, you would use:\n\np\\_load\\_gh(\"hadley/lvplot\")\n\nThat's it!","slug":"r-pacman-package-manager-with-require","published":1,"updated":"2020-08-23T20:54:35.186Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a7y008jsdjx8ocq01gl","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png\"></a>A few weeks back, I wrote a <a href=\"http://edpflager.com/2018/10/26/r-quick-code-to-install-needed-packages/\">quick post</a> about a one-line piece of R code that will check for required libraries, and if they are not installed will attempt to install them. Nice tip and helpful to use. This past week, I attended an R Users Group meeting in Ann Arbor, MI where Sam Firke was presenting about his package <a href=\"https://github.com/sfirke/janitor\">Janitor</a>. When he showed some code, he mentioned a package he uses called <a href=\"https://cran.r-project.org/web/packages/pacman/index.html\">pacman</a>.  Its a utility type package, designed to manage R packages, hence the name pacman. From the description on the CRAN site, it sounds dry, and not too interesting, but for locating information about your R installation, it is extremely helpful.  But one pacman function that Sam was using in his code caught my attention: p_load. From the package documentation:</p>\n<a id=\"more\"></a>\n<p>[p_load] checks to see if a package is installed, if not it attempts<br>to install the package from CRAN and/or any other repository in the<br>pacman repository list.</p>\n<p>The other feature included in this function is the ability to use a vector of packages as part of the call. So rather than have multiple lines to load several packages using my previous tip, like this:</p>\n<p>if( !require(janitor){ install.packages(‘janitor’)}<br>library(janitor)</p>\n<p>if( !require(tidyverse)){ install.packages(‘tidyverse’)}<br>library(tidyverse)</p>\n<p>if( !require(odbc)){ install.packages(‘odbc’)}<br>library(odbc)</p>\n<p>You could instead use this:</p>\n<p>if( !require(pacman)){ install.packages(‘pacman’)}<br>library(pacman) # for loading packages</p>\n<p>p_load(janitor, tidyverse, odbc)</p>\n<p>The first two lines are a repeat of my earlier tip to check to see if pacman is installed. If its not, then install it. Then using the pacman p_load function, load the required packages for the R script being run. By default, packages are installed if they are not present when you try to load them with p_load! If you are trying to use packages from GitHub instead of the CRAN repository, you can use p_load_gh instead. The rest of the functionality is the same, although with github repositories you do need to enclose each repository/package in quotes.  So for example to load the lvplot package from Hadley Wickham’s repository, you would use:</p>\n<p>p_load_gh(“hadley/lvplot”)</p>\n<p>That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png\"></a>A few weeks back, I wrote a <a href=\"http://edpflager.com/2018/10/26/r-quick-code-to-install-needed-packages/\">quick post</a> about a one-line piece of R code that will check for required libraries, and if they are not installed will attempt to install them. Nice tip and helpful to use. This past week, I attended an R Users Group meeting in Ann Arbor, MI where Sam Firke was presenting about his package <a href=\"https://github.com/sfirke/janitor\">Janitor</a>. When he showed some code, he mentioned a package he uses called <a href=\"https://cran.r-project.org/web/packages/pacman/index.html\">pacman</a>.  Its a utility type package, designed to manage R packages, hence the name pacman. From the description on the CRAN site, it sounds dry, and not too interesting, but for locating information about your R installation, it is extremely helpful.  But one pacman function that Sam was using in his code caught my attention: p_load. From the package documentation:</p>","more":"<p>[p_load] checks to see if a package is installed, if not it attempts<br>to install the package from CRAN and/or any other repository in the<br>pacman repository list.</p>\n<p>The other feature included in this function is the ability to use a vector of packages as part of the call. So rather than have multiple lines to load several packages using my previous tip, like this:</p>\n<p>if( !require(janitor){ install.packages(‘janitor’)}<br>library(janitor)</p>\n<p>if( !require(tidyverse)){ install.packages(‘tidyverse’)}<br>library(tidyverse)</p>\n<p>if( !require(odbc)){ install.packages(‘odbc’)}<br>library(odbc)</p>\n<p>You could instead use this:</p>\n<p>if( !require(pacman)){ install.packages(‘pacman’)}<br>library(pacman) # for loading packages</p>\n<p>p_load(janitor, tidyverse, odbc)</p>\n<p>The first two lines are a repeat of my earlier tip to check to see if pacman is installed. If its not, then install it. Then using the pacman p_load function, load the required packages for the R script being run. By default, packages are installed if they are not present when you try to load them with p_load! If you are trying to use packages from GitHub instead of the CRAN repository, you can use p_load_gh instead. The rest of the functionality is the same, although with github repositories you do need to enclose each repository/package in quotes.  So for example to load the lvplot package from Hadley Wickham’s repository, you would use:</p>\n<p>p_load_gh(“hadley/lvplot”)</p>\n<p>That’s it!</p>"},{"title":"R - Quick code to install needed packages","id":"4111","comments":0,"date":"2018-10-25T22:37:46.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png)](http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png)A quick tip this time but a very useful one. If you share your R code with someone, or need to come back to an older piece of code, there are times when a needed package will not be installed on the machine running it. The person will attempt to run the code, and will get an error because of the missing library. To circumvent this, here is a one-line piece of code you can use, that will check for required libraries, and if they are not installed will attempt to install them.  Just substitute the name of the package in the two spots and follow it with a call to load the library:\n\nif( !require(Package\\_Name)){ install.packages('Package\\_Name')}\nlibrary(Package\\_Name)\n\nI have tried this with non-CRAN repositories, and it does work. If you do attempt that, be careful when using experimental libraries, because you will need to load any dependencies from the console. It also works with dev\\_tools::install\\_github inside the braces, to replace the install.packages call, but again, you may need to agree to install dependencies from the console.","source":"_posts/r-quick-code-to-install-needed-packages.md","raw":"---\ntitle: R - Quick code to install needed packages\ntags:\n  - cookbook\n  - How-to\n  - howto\n  - install\n  - technical\nid: '4111'\ncategories:\n  - - Misc\n  - - R\ncomments: false\ndate: 2018-10-25 18:37:46\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png)](http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png)A quick tip this time but a very useful one. If you share your R code with someone, or need to come back to an older piece of code, there are times when a needed package will not be installed on the machine running it. The person will attempt to run the code, and will get an error because of the missing library. To circumvent this, here is a one-line piece of code you can use, that will check for required libraries, and if they are not installed will attempt to install them.  Just substitute the name of the package in the two spots and follow it with a call to load the library:\n\nif( !require(Package\\_Name)){ install.packages('Package\\_Name')}\nlibrary(Package\\_Name)\n\nI have tried this with non-CRAN repositories, and it does work. If you do attempt that, be careful when using experimental libraries, because you will need to load any dependencies from the console. It also works with dev\\_tools::install\\_github inside the braces, to replace the install.packages call, but again, you may need to agree to install dependencies from the console.","slug":"r-quick-code-to-install-needed-packages","published":1,"updated":"2020-08-23T20:54:35.174Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a81008nsdjx6qwy3sjh","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png\"></a>A quick tip this time but a very useful one. If you share your R code with someone, or need to come back to an older piece of code, there are times when a needed package will not be installed on the machine running it. The person will attempt to run the code, and will get an error because of the missing library. To circumvent this, here is a one-line piece of code you can use, that will check for required libraries, and if they are not installed will attempt to install them.  Just substitute the name of the package in the two spots and follow it with a call to load the library:</p>\n<p>if( !require(Package_Name)){ install.packages(‘Package_Name’)}<br>library(Package_Name)</p>\n<p>I have tried this with non-CRAN repositories, and it does work. If you do attempt that, be careful when using experimental libraries, because you will need to load any dependencies from the console. It also works with dev_tools::install_github inside the braces, to replace the install.packages call, but again, you may need to agree to install dependencies from the console.</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/Rlogo-300x232.png\"></a>A quick tip this time but a very useful one. If you share your R code with someone, or need to come back to an older piece of code, there are times when a needed package will not be installed on the machine running it. The person will attempt to run the code, and will get an error because of the missing library. To circumvent this, here is a one-line piece of code you can use, that will check for required libraries, and if they are not installed will attempt to install them.  Just substitute the name of the package in the two spots and follow it with a call to load the library:</p>\n<p>if( !require(Package_Name)){ install.packages(‘Package_Name’)}<br>library(Package_Name)</p>\n<p>I have tried this with non-CRAN repositories, and it does work. If you do attempt that, be careful when using experimental libraries, because you will need to load any dependencies from the console. It also works with dev_tools::install_github inside the braces, to replace the install.packages call, but again, you may need to agree to install dependencies from the console.</p>\n"},{"title":"Remove evaluator login from Pentaho BI server","id":"3020","comments":0,"date":"2015-11-11T18:13:45.000Z","_content":"\n[![evaluate](http://edpflager.com/wp-content/uploads/2015/11/evaluate-173x300.jpg)](http://edpflager.com/wp-content/uploads/2015/11/evaluate.jpg)When you first install the Pentaho BI server, the login screen includes an option to **Login as an Evaluator**, either as an Administrator (Admin) or a Power User (Suzy). While this is handy if you just want to check the software out, its a huge security hole if you plan to move to production mode. The good news is that removing that functionality involves editing one configuration file to change a couple of settings. Open a terminal and navigate to where the BI-Server was installed. On my system that is **/opt/pentaho/biserver-ce**. Drill down into  the pentaho-solutions folder, and then to system folder. Using a text editor, open the pentaho.xml file.\n<!-- more -->\nLook for the option **login-show-user-list**. It should look like this\n\n\\-->\n<login-show-users-list>true</login-show-users-list>\n<!--\n\nChange the value \"true\"  to \"false\". Now look for the option node <login-show-sample-users-hint> which should follow the first one.\n\n\\-->\n <login-show-sample-users-hint>true</login-show-sample-users-hint>\n<!--\n\nAgain the value should by default be \"true\". Change it to \"false\". Save the file and exit. Restart the BI-Service daemon, and the option to **Login as an Evaluator** should no longer appear. Be sure to change the default Admin password, and remove the Suzy account to make your system more secure.","source":"_posts/remove-evaluator-login-from-pentaho-bi-server.md","raw":"---\ntitle: Remove evaluator login from Pentaho BI server\ntags:\n  - guides\n  - How-to\n  - howto\nid: '3020'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2015-11-11 13:13:45\n---\n\n[![evaluate](http://edpflager.com/wp-content/uploads/2015/11/evaluate-173x300.jpg)](http://edpflager.com/wp-content/uploads/2015/11/evaluate.jpg)When you first install the Pentaho BI server, the login screen includes an option to **Login as an Evaluator**, either as an Administrator (Admin) or a Power User (Suzy). While this is handy if you just want to check the software out, its a huge security hole if you plan to move to production mode. The good news is that removing that functionality involves editing one configuration file to change a couple of settings. Open a terminal and navigate to where the BI-Server was installed. On my system that is **/opt/pentaho/biserver-ce**. Drill down into  the pentaho-solutions folder, and then to system folder. Using a text editor, open the pentaho.xml file.\n<!-- more -->\nLook for the option **login-show-user-list**. It should look like this\n\n\\-->\n<login-show-users-list>true</login-show-users-list>\n<!--\n\nChange the value \"true\"  to \"false\". Now look for the option node <login-show-sample-users-hint> which should follow the first one.\n\n\\-->\n <login-show-sample-users-hint>true</login-show-sample-users-hint>\n<!--\n\nAgain the value should by default be \"true\". Change it to \"false\". Save the file and exit. Restart the BI-Service daemon, and the option to **Login as an Evaluator** should no longer appear. Be sure to change the default Admin password, and remove the Suzy account to make your system more secure.","slug":"remove-evaluator-login-from-pentaho-bi-server","published":1,"updated":"2020-08-23T20:54:34.986Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a84008qsdjx2uid9asx","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/evaluate.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/evaluate-173x300.jpg\" alt=\"evaluate\"></a>When you first install the Pentaho BI server, the login screen includes an option to <strong>Login as an Evaluator</strong>, either as an Administrator (Admin) or a Power User (Suzy). While this is handy if you just want to check the software out, its a huge security hole if you plan to move to production mode. The good news is that removing that functionality involves editing one configuration file to change a couple of settings. Open a terminal and navigate to where the BI-Server was installed. On my system that is <strong>/opt/pentaho/biserver-ce</strong>. Drill down into  the pentaho-solutions folder, and then to system folder. Using a text editor, open the pentaho.xml file.</p>\n<a id=\"more\"></a>\n<p>Look for the option <strong>login-show-user-list</strong>. It should look like this</p>\n<p>--&gt;<br><login-show-users-list>true</login-show-users-list></p>\n<!--\n\nChange the value \"true\"  to \"false\". Now look for the option node <login-show-sample-users-hint> which should follow the first one.\n\n\\-->\n<p> <login-show-sample-users-hint>true</login-show-sample-users-hint></p>\n<p>&lt;!–</p>\n<p>Again the value should by default be “true”. Change it to “false”. Save the file and exit. Restart the BI-Service daemon, and the option to <strong>Login as an Evaluator</strong> should no longer appear. Be sure to change the default Admin password, and remove the Suzy account to make your system more secure.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/evaluate.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/evaluate-173x300.jpg\" alt=\"evaluate\"></a>When you first install the Pentaho BI server, the login screen includes an option to <strong>Login as an Evaluator</strong>, either as an Administrator (Admin) or a Power User (Suzy). While this is handy if you just want to check the software out, its a huge security hole if you plan to move to production mode. The good news is that removing that functionality involves editing one configuration file to change a couple of settings. Open a terminal and navigate to where the BI-Server was installed. On my system that is <strong>/opt/pentaho/biserver-ce</strong>. Drill down into  the pentaho-solutions folder, and then to system folder. Using a text editor, open the pentaho.xml file.</p>","more":"<p>Look for the option <strong>login-show-user-list</strong>. It should look like this</p>\n<p>--&gt;<br><login-show-users-list>true</login-show-users-list></p>\n<!--\n\nChange the value \"true\"  to \"false\". Now look for the option node <login-show-sample-users-hint> which should follow the first one.\n\n\\-->\n<p> <login-show-sample-users-hint>true</login-show-sample-users-hint></p>\n<p>&lt;!–</p>\n<p>Again the value should by default be “true”. Change it to “false”. Save the file and exit. Restart the BI-Service daemon, and the option to <strong>Login as an Evaluator</strong> should no longer appear. Be sure to change the default Admin password, and remove the Suzy account to make your system more secure.</p>"},{"title":"Review: Big Data Glossary","id":"1550","comments":0,"date":"2013-05-18T13:17:29.000Z","_content":"\n[![bigdataglossary](http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif)](http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif)A few weeks back I posted about a new trend in IT called Big Data, and because I like to stay marketable in my chosen field, I’ve started playing around with some Big Data tools. For those looking for a primer on Big Data, the various applications and technologies , and a way to make sense of it all, I can’t recommend enough the [Big Data Glossary](http://shop.oreilly.com/product/0636920022466.do) by Pete Warden from O’Reilly Media.\n\nIt’s a small 62 page book, that starts with a terms chapter providing some plain English (or as close as possible) explanations of the lingo that is used throughout the rest of the book.\n<!-- more -->\nFrom there it parses the Big Data world into logical groupings, such as NoSQL databases, Storage Systems, Processing, and Visualization with a couple of paragraphs devoted to each of the many applications/technologies in that category.\n\nAs a relative newcomer to the Big Data arena, this book provided just enough detail to let me examine the Big Data landscape from a 10,000 foot view and learn what technology complemented others, which were trying to achieve the same end results, and the best usage for each. From there I was able to decide where I wanted to dive in. More about that coming soon!","source":"_posts/review-big-data-glossary.md","raw":"---\ntitle: 'Review: Big Data Glossary'\ntags: []\nid: '1550'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2013-05-18 09:17:29\n---\n\n[![bigdataglossary](http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif)](http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif)A few weeks back I posted about a new trend in IT called Big Data, and because I like to stay marketable in my chosen field, I’ve started playing around with some Big Data tools. For those looking for a primer on Big Data, the various applications and technologies , and a way to make sense of it all, I can’t recommend enough the [Big Data Glossary](http://shop.oreilly.com/product/0636920022466.do) by Pete Warden from O’Reilly Media.\n\nIt’s a small 62 page book, that starts with a terms chapter providing some plain English (or as close as possible) explanations of the lingo that is used throughout the rest of the book.\n<!-- more -->\nFrom there it parses the Big Data world into logical groupings, such as NoSQL databases, Storage Systems, Processing, and Visualization with a couple of paragraphs devoted to each of the many applications/technologies in that category.\n\nAs a relative newcomer to the Big Data arena, this book provided just enough detail to let me examine the Big Data landscape from a 10,000 foot view and learn what technology complemented others, which were trying to achieve the same end results, and the best usage for each. From there I was able to decide where I wanted to dive in. More about that coming soon!","slug":"review-big-data-glossary","published":1,"updated":"2020-08-23T20:54:34.738Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a86008usdjx458i40ly","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif\"><img src=\"http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif\" alt=\"bigdataglossary\"></a>A few weeks back I posted about a new trend in IT called Big Data, and because I like to stay marketable in my chosen field, I’ve started playing around with some Big Data tools. For those looking for a primer on Big Data, the various applications and technologies , and a way to make sense of it all, I can’t recommend enough the <a href=\"http://shop.oreilly.com/product/0636920022466.do\">Big Data Glossary</a> by Pete Warden from O’Reilly Media.</p>\n<p>It’s a small 62 page book, that starts with a terms chapter providing some plain English (or as close as possible) explanations of the lingo that is used throughout the rest of the book.</p>\n<a id=\"more\"></a>\n<p>From there it parses the Big Data world into logical groupings, such as NoSQL databases, Storage Systems, Processing, and Visualization with a couple of paragraphs devoted to each of the many applications/technologies in that category.</p>\n<p>As a relative newcomer to the Big Data arena, this book provided just enough detail to let me examine the Big Data landscape from a 10,000 foot view and learn what technology complemented others, which were trying to achieve the same end results, and the best usage for each. From there I was able to decide where I wanted to dive in. More about that coming soon!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif\"><img src=\"http://edpflager.com/wp-content/uploads/2013/05/bigdataglossary.gif\" alt=\"bigdataglossary\"></a>A few weeks back I posted about a new trend in IT called Big Data, and because I like to stay marketable in my chosen field, I’ve started playing around with some Big Data tools. For those looking for a primer on Big Data, the various applications and technologies , and a way to make sense of it all, I can’t recommend enough the <a href=\"http://shop.oreilly.com/product/0636920022466.do\">Big Data Glossary</a> by Pete Warden from O’Reilly Media.</p>\n<p>It’s a small 62 page book, that starts with a terms chapter providing some plain English (or as close as possible) explanations of the lingo that is used throughout the rest of the book.</p>","more":"<p>From there it parses the Big Data world into logical groupings, such as NoSQL databases, Storage Systems, Processing, and Visualization with a couple of paragraphs devoted to each of the many applications/technologies in that category.</p>\n<p>As a relative newcomer to the Big Data arena, this book provided just enough detail to let me examine the Big Data landscape from a 10,000 foot view and learn what technology complemented others, which were trying to achieve the same end results, and the best usage for each. From there I was able to decide where I wanted to dive in. More about that coming soon!</p>"},{"title":"Review: Clean Data by Megan Squire","id":"2817","comments":0,"date":"2015-06-24T11:00:01.000Z","_content":"\n## [![CleanData](http://edpflager.com/wp-content/uploads/2015/06/CleanData-243x300.png)](http://edpflager.com/wp-content/uploads/2015/06/CleanData.png)CLEAN DATA by Megan Squire Grade: A\n\nOne of the most time consuming, but also most important aspects of a data analysis project is cleaning the source data you are using. In a [New York Times](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0) piece last year, this process was called data wrangling or data janitor work, and it was stated that 50 to 80 percent of a data scientist's time will be spent on it.  The challenges can involve anything from removing extraneous characters to misspelled words to incorrect dates to any of hundreds of other issues. But the thing they all have in common is that before fixing the issue you have to understand what the issues are.\n\nMegan Squire, professor of Computing Sciences at Elon University in North Carolina, attempts to break through some of these problems with a series of techniques to make the process more manageable in her book [**Clean Data** from Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/clean-data).  Aimed at data scientists, this book also can benefit those who work in the ETL end of the process, since there is considerable cross over in functions.\n<!-- more -->\nStarting with basic issues like file types, encoding, null and empty fields, it progresses to using widely available tools like Microsoft Excel, Google Spreadsheets, and Sublime Editor to winding up with  writing custom code in PHP and Python. Specific examples of how to overcome some aspect of data cleansing that you may be faced with are presented and some good rules to follow are emphasized, such as document the steps you took.\n\nOne of the more interesting aspects for me in this book was the chapter on cleaning data from PDF files. Designed by Adobe to allow sharing of documents across multiple platforms, anyone who has tried to copy data out of PDF files can tell you its often its not an easy process. Luckily, the bulk of the time I have had to do it has involved a page or two at most, so cleaning up the data hasn't been too time consuming. Dr. Squire presents several techniques for getting this information out of a PDF depending on how the information was originally formatted, and for longer extractions I can definitely see a benefit.\n\nThe biggest issue for me with this book is the reliance on writing your own code for many of the projects. While I would expect custom SQL code, since by its very nature Cleaning Data involves databases, I tend to steer away from customized coding whenever possible. By relying on external programming languages,  it becomes more difficult to share with others, since they may be unfamiliar with the programming language. But, while I am not a huge fan of using custom programming to perform ETL functionality, I understand it has its place. And in the real world the quickest technique to overcome the existing problem is usually the one that will be used.\n\nWhile not perfect, this is definitely a worthwhile addition to the fledging data wrangler's bookcase. Real world data sets (Facebook, Twitter, and Stack Exchange to name only a few) are used rather than contrived text book examples, and the techniques can be reused or expanded upon by the reader.","source":"_posts/review-clean-data-by-megan-squire.md","raw":"---\ntitle: 'Review: Clean Data by Megan Squire'\ntags: []\nid: '2817'\ncategories:\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2015-06-24 07:00:01\n---\n\n## [![CleanData](http://edpflager.com/wp-content/uploads/2015/06/CleanData-243x300.png)](http://edpflager.com/wp-content/uploads/2015/06/CleanData.png)CLEAN DATA by Megan Squire Grade: A\n\nOne of the most time consuming, but also most important aspects of a data analysis project is cleaning the source data you are using. In a [New York Times](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0) piece last year, this process was called data wrangling or data janitor work, and it was stated that 50 to 80 percent of a data scientist's time will be spent on it.  The challenges can involve anything from removing extraneous characters to misspelled words to incorrect dates to any of hundreds of other issues. But the thing they all have in common is that before fixing the issue you have to understand what the issues are.\n\nMegan Squire, professor of Computing Sciences at Elon University in North Carolina, attempts to break through some of these problems with a series of techniques to make the process more manageable in her book [**Clean Data** from Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/clean-data).  Aimed at data scientists, this book also can benefit those who work in the ETL end of the process, since there is considerable cross over in functions.\n<!-- more -->\nStarting with basic issues like file types, encoding, null and empty fields, it progresses to using widely available tools like Microsoft Excel, Google Spreadsheets, and Sublime Editor to winding up with  writing custom code in PHP and Python. Specific examples of how to overcome some aspect of data cleansing that you may be faced with are presented and some good rules to follow are emphasized, such as document the steps you took.\n\nOne of the more interesting aspects for me in this book was the chapter on cleaning data from PDF files. Designed by Adobe to allow sharing of documents across multiple platforms, anyone who has tried to copy data out of PDF files can tell you its often its not an easy process. Luckily, the bulk of the time I have had to do it has involved a page or two at most, so cleaning up the data hasn't been too time consuming. Dr. Squire presents several techniques for getting this information out of a PDF depending on how the information was originally formatted, and for longer extractions I can definitely see a benefit.\n\nThe biggest issue for me with this book is the reliance on writing your own code for many of the projects. While I would expect custom SQL code, since by its very nature Cleaning Data involves databases, I tend to steer away from customized coding whenever possible. By relying on external programming languages,  it becomes more difficult to share with others, since they may be unfamiliar with the programming language. But, while I am not a huge fan of using custom programming to perform ETL functionality, I understand it has its place. And in the real world the quickest technique to overcome the existing problem is usually the one that will be used.\n\nWhile not perfect, this is definitely a worthwhile addition to the fledging data wrangler's bookcase. Real world data sets (Facebook, Twitter, and Stack Exchange to name only a few) are used rather than contrived text book examples, and the techniques can be reused or expanded upon by the reader.","slug":"review-clean-data-by-megan-squire","published":1,"updated":"2020-08-23T20:54:34.954Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a88008xsdjx1d1nf654","content":"<h2 id=\"CLEAN-DATA-by-Megan-Squire-Grade-A\"><a href=\"#CLEAN-DATA-by-Megan-Squire-Grade-A\" class=\"headerlink\" title=\"CLEAN DATA by Megan Squire Grade: A\"></a><a href=\"http://edpflager.com/wp-content/uploads/2015/06/CleanData.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/CleanData-243x300.png\" alt=\"CleanData\"></a>CLEAN DATA by Megan Squire Grade: A</h2><p>One of the most time consuming, but also most important aspects of a data analysis project is cleaning the source data you are using. In a <a href=\"http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0\">New York Times</a> piece last year, this process was called data wrangling or data janitor work, and it was stated that 50 to 80 percent of a data scientist’s time will be spent on it.  The challenges can involve anything from removing extraneous characters to misspelled words to incorrect dates to any of hundreds of other issues. But the thing they all have in common is that before fixing the issue you have to understand what the issues are.</p>\n<p>Megan Squire, professor of Computing Sciences at Elon University in North Carolina, attempts to break through some of these problems with a series of techniques to make the process more manageable in her book <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/clean-data\"><strong>Clean Data</strong> from Packt Publishing</a>.  Aimed at data scientists, this book also can benefit those who work in the ETL end of the process, since there is considerable cross over in functions.</p>\n<a id=\"more\"></a>\n<p>Starting with basic issues like file types, encoding, null and empty fields, it progresses to using widely available tools like Microsoft Excel, Google Spreadsheets, and Sublime Editor to winding up with  writing custom code in PHP and Python. Specific examples of how to overcome some aspect of data cleansing that you may be faced with are presented and some good rules to follow are emphasized, such as document the steps you took.</p>\n<p>One of the more interesting aspects for me in this book was the chapter on cleaning data from PDF files. Designed by Adobe to allow sharing of documents across multiple platforms, anyone who has tried to copy data out of PDF files can tell you its often its not an easy process. Luckily, the bulk of the time I have had to do it has involved a page or two at most, so cleaning up the data hasn’t been too time consuming. Dr. Squire presents several techniques for getting this information out of a PDF depending on how the information was originally formatted, and for longer extractions I can definitely see a benefit.</p>\n<p>The biggest issue for me with this book is the reliance on writing your own code for many of the projects. While I would expect custom SQL code, since by its very nature Cleaning Data involves databases, I tend to steer away from customized coding whenever possible. By relying on external programming languages,  it becomes more difficult to share with others, since they may be unfamiliar with the programming language. But, while I am not a huge fan of using custom programming to perform ETL functionality, I understand it has its place. And in the real world the quickest technique to overcome the existing problem is usually the one that will be used.</p>\n<p>While not perfect, this is definitely a worthwhile addition to the fledging data wrangler’s bookcase. Real world data sets (Facebook, Twitter, and Stack Exchange to name only a few) are used rather than contrived text book examples, and the techniques can be reused or expanded upon by the reader.</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"CLEAN-DATA-by-Megan-Squire-Grade-A\"><a href=\"#CLEAN-DATA-by-Megan-Squire-Grade-A\" class=\"headerlink\" title=\"CLEAN DATA by Megan Squire Grade: A\"></a><a href=\"http://edpflager.com/wp-content/uploads/2015/06/CleanData.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/CleanData-243x300.png\" alt=\"CleanData\"></a>CLEAN DATA by Megan Squire Grade: A</h2><p>One of the most time consuming, but also most important aspects of a data analysis project is cleaning the source data you are using. In a <a href=\"http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0\">New York Times</a> piece last year, this process was called data wrangling or data janitor work, and it was stated that 50 to 80 percent of a data scientist’s time will be spent on it.  The challenges can involve anything from removing extraneous characters to misspelled words to incorrect dates to any of hundreds of other issues. But the thing they all have in common is that before fixing the issue you have to understand what the issues are.</p>\n<p>Megan Squire, professor of Computing Sciences at Elon University in North Carolina, attempts to break through some of these problems with a series of techniques to make the process more manageable in her book <a href=\"https://www.packtpub.com/big-data-and-business-intelligence/clean-data\"><strong>Clean Data</strong> from Packt Publishing</a>.  Aimed at data scientists, this book also can benefit those who work in the ETL end of the process, since there is considerable cross over in functions.</p>","more":"<p>Starting with basic issues like file types, encoding, null and empty fields, it progresses to using widely available tools like Microsoft Excel, Google Spreadsheets, and Sublime Editor to winding up with  writing custom code in PHP and Python. Specific examples of how to overcome some aspect of data cleansing that you may be faced with are presented and some good rules to follow are emphasized, such as document the steps you took.</p>\n<p>One of the more interesting aspects for me in this book was the chapter on cleaning data from PDF files. Designed by Adobe to allow sharing of documents across multiple platforms, anyone who has tried to copy data out of PDF files can tell you its often its not an easy process. Luckily, the bulk of the time I have had to do it has involved a page or two at most, so cleaning up the data hasn’t been too time consuming. Dr. Squire presents several techniques for getting this information out of a PDF depending on how the information was originally formatted, and for longer extractions I can definitely see a benefit.</p>\n<p>The biggest issue for me with this book is the reliance on writing your own code for many of the projects. While I would expect custom SQL code, since by its very nature Cleaning Data involves databases, I tend to steer away from customized coding whenever possible. By relying on external programming languages,  it becomes more difficult to share with others, since they may be unfamiliar with the programming language. But, while I am not a huge fan of using custom programming to perform ETL functionality, I understand it has its place. And in the real world the quickest technique to overcome the existing problem is usually the one that will be used.</p>\n<p>While not perfect, this is definitely a worthwhile addition to the fledging data wrangler’s bookcase. Real world data sets (Facebook, Twitter, and Stack Exchange to name only a few) are used rather than contrived text book examples, and the techniques can be reused or expanded upon by the reader.</p>"},{"title":"RStudio on Linux Mint 19","id":"4269","comments":0,"date":"2018-12-18T00:34:02.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png)](http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png)R Studio is a popular IDE to use with R and runs on Windows (7/8/10), Mac OS X (10.6+) and the most widely used Linux distributions: Debian/Ubuntu and Fedora/Red Hat/OpenSuSE. For Linux, that generally means installation on any other distributions that are derivatives of these should also work with with minimal effort. For my personal Linux efforts, I have bounced between CentOS (a derivative of Red Hat), and Linux Mint (version 19 built from Ubuntu 18.04). Recently I picked up a refurbished Acer Swift 1 (cheap!) so I would have a Linux laptop to work with. The Comprehensive R Archive Network (CRAN) has Ubuntu installation packages of R-base up through Ubuntu Cosmic 18.10. Before installing RStudio, you need to install base-R the minimum installation of R. Here is instructions for that based on the [CRAN website](https://cran.r-project.org/bin/linux/ubuntu/).\n<!-- more -->\n1.  Add the following line to your /etc/apt/sources.list file by editing it as sudo.\n    \n    deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/\n    \n2.  Import the ubuntu apt secure key for the CRAN repository by executing this statement:\n    \n    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n    \n3.  Update your repository list:\n    \n    sudo apt-get update\n    \n4.  Install the base version of R\n    \n    sudo apt-get install r-base\n    \n5.  Start R by typing a capital letter R and pressing Enter, and you should be greeted with the startup message for R telling you the version number.\n6.  Follow the onscreen instructions to exit R and answer n to the save workspace command to exit back to the command line.\n7.  Congratulations you have a working installation of R!\n\nNow we want to install RStudio so we have a graphical IDE for working with R. Ubuntu packages for RStudio are only available through 16.04 Xenial but you can install them in Linux Mint 19 if you install a dependency first.\n\n1.  Back at the command line, make sure to update your repository list again:\n    \n    sudo apt-get update\n    \n2.  Install any new versions:\n    \n    sudo apt-get upgrade\n    \n3.  Now install the GStreamer library:\n    \n    sudo apt-get install libgstreamer1.0\n    \n4.  Open a web browser, and download the latest Debian/Ubuntu version of RStudio from [rstudio.com](http://rstudio.com/).\n5.  You can install the package by opening the file manager in Mint, and double clicking the package. This will open the GDebi Package Installer application, which will inform you that one dependency is needed.[![](http://edpflager.com/wp-content/uploads/2018/12/GDebi-300x290.png)](http://edpflager.com/wp-content/uploads/2018/12/GDebi.png)\n6.  Click the DETAILS button and then click on the Details window to proceed, and when you return to the main GDebi window, click Install Package.![](http://edpflager.com/wp-content/uploads/2018/12/RStudioDependency-300x198.png)\n7.  When the installation completes, click on the Mint Menu icon in the lower left corner of the screen. Navigate to the Programming folder, and you should now see RStudio listed. Click on it, and the application will open.\n\nTa-da!","source":"_posts/rstudio-on-linux-mint-19.md","raw":"---\ntitle: RStudio on Linux Mint 19\ntags:\n  - guides\n  - How-to\n  - howto\n  - install\n  - Mint\n  - RStudio\nid: '4269'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Misc\n  - - R\ncomments: false\ndate: 2018-12-17 19:34:02\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png)](http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png)R Studio is a popular IDE to use with R and runs on Windows (7/8/10), Mac OS X (10.6+) and the most widely used Linux distributions: Debian/Ubuntu and Fedora/Red Hat/OpenSuSE. For Linux, that generally means installation on any other distributions that are derivatives of these should also work with with minimal effort. For my personal Linux efforts, I have bounced between CentOS (a derivative of Red Hat), and Linux Mint (version 19 built from Ubuntu 18.04). Recently I picked up a refurbished Acer Swift 1 (cheap!) so I would have a Linux laptop to work with. The Comprehensive R Archive Network (CRAN) has Ubuntu installation packages of R-base up through Ubuntu Cosmic 18.10. Before installing RStudio, you need to install base-R the minimum installation of R. Here is instructions for that based on the [CRAN website](https://cran.r-project.org/bin/linux/ubuntu/).\n<!-- more -->\n1.  Add the following line to your /etc/apt/sources.list file by editing it as sudo.\n    \n    deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/\n    \n2.  Import the ubuntu apt secure key for the CRAN repository by executing this statement:\n    \n    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n    \n3.  Update your repository list:\n    \n    sudo apt-get update\n    \n4.  Install the base version of R\n    \n    sudo apt-get install r-base\n    \n5.  Start R by typing a capital letter R and pressing Enter, and you should be greeted with the startup message for R telling you the version number.\n6.  Follow the onscreen instructions to exit R and answer n to the save workspace command to exit back to the command line.\n7.  Congratulations you have a working installation of R!\n\nNow we want to install RStudio so we have a graphical IDE for working with R. Ubuntu packages for RStudio are only available through 16.04 Xenial but you can install them in Linux Mint 19 if you install a dependency first.\n\n1.  Back at the command line, make sure to update your repository list again:\n    \n    sudo apt-get update\n    \n2.  Install any new versions:\n    \n    sudo apt-get upgrade\n    \n3.  Now install the GStreamer library:\n    \n    sudo apt-get install libgstreamer1.0\n    \n4.  Open a web browser, and download the latest Debian/Ubuntu version of RStudio from [rstudio.com](http://rstudio.com/).\n5.  You can install the package by opening the file manager in Mint, and double clicking the package. This will open the GDebi Package Installer application, which will inform you that one dependency is needed.[![](http://edpflager.com/wp-content/uploads/2018/12/GDebi-300x290.png)](http://edpflager.com/wp-content/uploads/2018/12/GDebi.png)\n6.  Click the DETAILS button and then click on the Details window to proceed, and when you return to the main GDebi window, click Install Package.![](http://edpflager.com/wp-content/uploads/2018/12/RStudioDependency-300x198.png)\n7.  When the installation completes, click on the Mint Menu icon in the lower left corner of the screen. Navigate to the Programming folder, and you should now see RStudio listed. Click on it, and the application will open.\n\nTa-da!","slug":"rstudio-on-linux-mint-19","published":1,"updated":"2020-08-23T20:54:35.206Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8a0091sdjxep3v8ugw","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png\"></a>R Studio is a popular IDE to use with R and runs on Windows (7/8/10), Mac OS X (10.6+) and the most widely used Linux distributions: Debian/Ubuntu and Fedora/Red Hat/OpenSuSE. For Linux, that generally means installation on any other distributions that are derivatives of these should also work with with minimal effort. For my personal Linux efforts, I have bounced between CentOS (a derivative of Red Hat), and Linux Mint (version 19 built from Ubuntu 18.04). Recently I picked up a refurbished Acer Swift 1 (cheap!) so I would have a Linux laptop to work with. The Comprehensive R Archive Network (CRAN) has Ubuntu installation packages of R-base up through Ubuntu Cosmic 18.10. Before installing RStudio, you need to install base-R the minimum installation of R. Here is instructions for that based on the <a href=\"https://cran.r-project.org/bin/linux/ubuntu/\">CRAN website</a>.</p>\n<a id=\"more\"></a>\n<ol>\n<li><p>Add the following line to your /etc/apt/sources.list file by editing it as sudo.</p>\n<p>deb <a href=\"https://cloud.r-project.org/bin/linux/ubuntu\">https://cloud.r-project.org/bin/linux/ubuntu</a> bionic-cran35/</p>\n</li>\n<li><p>Import the ubuntu apt secure key for the CRAN repository by executing this statement:</p>\n<p>sudo apt-key adv –keyserver keyserver.ubuntu.com –recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9</p>\n</li>\n<li><p>Update your repository list:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Install the base version of R</p>\n<p>sudo apt-get install r-base</p>\n</li>\n<li><p>Start R by typing a capital letter R and pressing Enter, and you should be greeted with the startup message for R telling you the version number.</p>\n</li>\n<li><p>Follow the onscreen instructions to exit R and answer n to the save workspace command to exit back to the command line.</p>\n</li>\n<li><p>Congratulations you have a working installation of R!</p>\n</li>\n</ol>\n<p>Now we want to install RStudio so we have a graphical IDE for working with R. Ubuntu packages for RStudio are only available through 16.04 Xenial but you can install them in Linux Mint 19 if you install a dependency first.</p>\n<ol>\n<li><p>Back at the command line, make sure to update your repository list again:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Install any new versions:</p>\n<p>sudo apt-get upgrade</p>\n</li>\n<li><p>Now install the GStreamer library:</p>\n<p>sudo apt-get install libgstreamer1.0</p>\n</li>\n<li><p>Open a web browser, and download the latest Debian/Ubuntu version of RStudio from <a href=\"http://rstudio.com/\">rstudio.com</a>.</p>\n</li>\n<li><p>You can install the package by opening the file manager in Mint, and double clicking the package. This will open the GDebi Package Installer application, which will inform you that one dependency is needed.<a href=\"http://edpflager.com/wp-content/uploads/2018/12/GDebi.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/GDebi-300x290.png\"></a></p>\n</li>\n<li><p>Click the DETAILS button and then click on the Details window to proceed, and when you return to the main GDebi window, click Install Package.<img src=\"http://edpflager.com/wp-content/uploads/2018/12/RStudioDependency-300x198.png\"></p>\n</li>\n<li><p>When the installation completes, click on the Mint Menu icon in the lower left corner of the screen. Navigate to the Programming folder, and you should now see RStudio listed. Click on it, and the application will open.</p>\n</li>\n</ol>\n<p>Ta-da!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/RStudio-e1545081980313.png\"></a>R Studio is a popular IDE to use with R and runs on Windows (7/8/10), Mac OS X (10.6+) and the most widely used Linux distributions: Debian/Ubuntu and Fedora/Red Hat/OpenSuSE. For Linux, that generally means installation on any other distributions that are derivatives of these should also work with with minimal effort. For my personal Linux efforts, I have bounced between CentOS (a derivative of Red Hat), and Linux Mint (version 19 built from Ubuntu 18.04). Recently I picked up a refurbished Acer Swift 1 (cheap!) so I would have a Linux laptop to work with. The Comprehensive R Archive Network (CRAN) has Ubuntu installation packages of R-base up through Ubuntu Cosmic 18.10. Before installing RStudio, you need to install base-R the minimum installation of R. Here is instructions for that based on the <a href=\"https://cran.r-project.org/bin/linux/ubuntu/\">CRAN website</a>.</p>","more":"<ol>\n<li><p>Add the following line to your /etc/apt/sources.list file by editing it as sudo.</p>\n<p>deb <a href=\"https://cloud.r-project.org/bin/linux/ubuntu\">https://cloud.r-project.org/bin/linux/ubuntu</a> bionic-cran35/</p>\n</li>\n<li><p>Import the ubuntu apt secure key for the CRAN repository by executing this statement:</p>\n<p>sudo apt-key adv –keyserver keyserver.ubuntu.com –recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9</p>\n</li>\n<li><p>Update your repository list:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Install the base version of R</p>\n<p>sudo apt-get install r-base</p>\n</li>\n<li><p>Start R by typing a capital letter R and pressing Enter, and you should be greeted with the startup message for R telling you the version number.</p>\n</li>\n<li><p>Follow the onscreen instructions to exit R and answer n to the save workspace command to exit back to the command line.</p>\n</li>\n<li><p>Congratulations you have a working installation of R!</p>\n</li>\n</ol>\n<p>Now we want to install RStudio so we have a graphical IDE for working with R. Ubuntu packages for RStudio are only available through 16.04 Xenial but you can install them in Linux Mint 19 if you install a dependency first.</p>\n<ol>\n<li><p>Back at the command line, make sure to update your repository list again:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>Install any new versions:</p>\n<p>sudo apt-get upgrade</p>\n</li>\n<li><p>Now install the GStreamer library:</p>\n<p>sudo apt-get install libgstreamer1.0</p>\n</li>\n<li><p>Open a web browser, and download the latest Debian/Ubuntu version of RStudio from <a href=\"http://rstudio.com/\">rstudio.com</a>.</p>\n</li>\n<li><p>You can install the package by opening the file manager in Mint, and double clicking the package. This will open the GDebi Package Installer application, which will inform you that one dependency is needed.<a href=\"http://edpflager.com/wp-content/uploads/2018/12/GDebi.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/GDebi-300x290.png\"></a></p>\n</li>\n<li><p>Click the DETAILS button and then click on the Details window to proceed, and when you return to the main GDebi window, click Install Package.<img src=\"http://edpflager.com/wp-content/uploads/2018/12/RStudioDependency-300x198.png\"></p>\n</li>\n<li><p>When the installation completes, click on the Mint Menu icon in the lower left corner of the screen. Navigate to the Programming folder, and you should now see RStudio listed. Click on it, and the application will open.</p>\n</li>\n</ol>\n<p>Ta-da!</p>"},{"title":"Run Pentaho BIServer on Linux - Demystified!","id":"2066","comments":0,"date":"2014-05-08T07:44:32.000Z","_content":"\n[![frustration](http://edpflager.com/wp-content/uploads/2014/05/frustration-300x224.jpg)](http://edpflager.com/wp-content/uploads/2014/05/frustration.jpg)I'm posting this just because there seems to be a lot of confusion and misinformation on the web about getting the open source version of [Pentaho's BIServer](http://community.pentaho.com/) up and running on Linux. (I know I spent several hours over the course of a week running through them.) The current version is 5.01 and you download a ZIP file that contains almost everything you need to get started. You do NOT need to install MySQL or PostgreSQL or Oracle on your server to get Pentaho BIServer to work, nor do you need to set the PENTAHO\\_JAVA\\_HOME or JAVA\\_HOME system variables or create a Pentaho user account. If you want to change the repository database to something besides the embedded HSQLDB you can find directions on that on the Internet (Good luck to you). If you just want to setup a simple system for testing and low volume usage, read on...\n<!-- more -->\nHere is a step by step account of how I got Pentaho BIServer to run on my system:\n\n1.  Install a Linux server ( I am using CentOS 64-bit). A minimum of 2GB of RAM is necessary, although 4GB or more is better.\n2.  Remove any unneeded applications and install any system updates.\n3.  Disable iptables and iptablesv6 (be cautious of this if you plan to use this in a production environment!)\n4.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") (RedHat based Linux distros) and set it to disabled. Restart your server for all the config changes to take effect.\n5.  If its present, remove OpenJDK software (**java -version** will tell you).\n6.  Download Oracle's JAVA 1.7\\_55 RPM package from their [website](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html) and install it.\n7.  All of this above was the difficult part. :)\n8.  Download the Pentaho biserver-ce from [Pentaho's website](http://community.pentaho.com) and click the Download Pentaho CE link. You be redirected to a page on SourceForge.net and the file will begin downloading. It takes a while.\n9.  Once the download completes, create a pentaho folder in your home folder and extract the biserver-ce-5.0.1-stable.zip to it.\n10.  At the command line, switch back to your user account if you are still running as root.\n11.  Navigate to your /home/username/pentaho/biserver-ce folder.\n12.  To start the server, enter this command: ./start-pentaho.sh\n13.  The shell script will produce several lines of output, and then return to a command prompt.\n14.  If you are running a GUI environment on your server, open a browser, and navigate to http://localhost:8080/pentaho. Otherwise open a browser on a machine on the same network and access the IPAddress for your server with the ending notation (:8080/pentaho).\n15.  It may take a few minutes for the webpage to load up, but once it does, you'll see the Pentaho User Console login screen. Login using the default user account. User name: admin     Password: password\n16.  You're in!\n\nIf you would like to access the server from a different machine, set a static IP address and a fully qualified domain name, but if all of your work will be local, you are all set.","source":"_posts/run-pentaho-biserver-on-linux-demystified.md","raw":"---\ntitle: Run Pentaho BIServer on Linux - Demystified!\ntags:\n  - Big Data\n  - howto\n  - install\n  - technical\nid: '2066'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-05-08 03:44:32\n---\n\n[![frustration](http://edpflager.com/wp-content/uploads/2014/05/frustration-300x224.jpg)](http://edpflager.com/wp-content/uploads/2014/05/frustration.jpg)I'm posting this just because there seems to be a lot of confusion and misinformation on the web about getting the open source version of [Pentaho's BIServer](http://community.pentaho.com/) up and running on Linux. (I know I spent several hours over the course of a week running through them.) The current version is 5.01 and you download a ZIP file that contains almost everything you need to get started. You do NOT need to install MySQL or PostgreSQL or Oracle on your server to get Pentaho BIServer to work, nor do you need to set the PENTAHO\\_JAVA\\_HOME or JAVA\\_HOME system variables or create a Pentaho user account. If you want to change the repository database to something besides the embedded HSQLDB you can find directions on that on the Internet (Good luck to you). If you just want to setup a simple system for testing and low volume usage, read on...\n<!-- more -->\nHere is a step by step account of how I got Pentaho BIServer to run on my system:\n\n1.  Install a Linux server ( I am using CentOS 64-bit). A minimum of 2GB of RAM is necessary, although 4GB or more is better.\n2.  Remove any unneeded applications and install any system updates.\n3.  Disable iptables and iptablesv6 (be cautious of this if you plan to use this in a production environment!)\n4.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") (RedHat based Linux distros) and set it to disabled. Restart your server for all the config changes to take effect.\n5.  If its present, remove OpenJDK software (**java -version** will tell you).\n6.  Download Oracle's JAVA 1.7\\_55 RPM package from their [website](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html) and install it.\n7.  All of this above was the difficult part. :)\n8.  Download the Pentaho biserver-ce from [Pentaho's website](http://community.pentaho.com) and click the Download Pentaho CE link. You be redirected to a page on SourceForge.net and the file will begin downloading. It takes a while.\n9.  Once the download completes, create a pentaho folder in your home folder and extract the biserver-ce-5.0.1-stable.zip to it.\n10.  At the command line, switch back to your user account if you are still running as root.\n11.  Navigate to your /home/username/pentaho/biserver-ce folder.\n12.  To start the server, enter this command: ./start-pentaho.sh\n13.  The shell script will produce several lines of output, and then return to a command prompt.\n14.  If you are running a GUI environment on your server, open a browser, and navigate to http://localhost:8080/pentaho. Otherwise open a browser on a machine on the same network and access the IPAddress for your server with the ending notation (:8080/pentaho).\n15.  It may take a few minutes for the webpage to load up, but once it does, you'll see the Pentaho User Console login screen. Login using the default user account. User name: admin     Password: password\n16.  You're in!\n\nIf you would like to access the server from a different machine, set a static IP address and a fully qualified domain name, but if all of your work will be local, you are all set.","slug":"run-pentaho-biserver-on-linux-demystified","published":1,"updated":"2020-08-23T20:54:34.838Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8d0094sdjxeikg9f2j","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/frustration.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/frustration-300x224.jpg\" alt=\"frustration\"></a>I’m posting this just because there seems to be a lot of confusion and misinformation on the web about getting the open source version of <a href=\"http://community.pentaho.com/\">Pentaho’s BIServer</a> up and running on Linux. (I know I spent several hours over the course of a week running through them.) The current version is 5.01 and you download a ZIP file that contains almost everything you need to get started. You do NOT need to install MySQL or PostgreSQL or Oracle on your server to get Pentaho BIServer to work, nor do you need to set the PENTAHO_JAVA_HOME or JAVA_HOME system variables or create a Pentaho user account. If you want to change the repository database to something besides the embedded HSQLDB you can find directions on that on the Internet (Good luck to you). If you just want to setup a simple system for testing and low volume usage, read on…</p>\n<a id=\"more\"></a>\n<p>Here is a step by step account of how I got Pentaho BIServer to run on my system:</p>\n<ol>\n<li>Install a Linux server ( I am using CentOS 64-bit). A minimum of 2GB of RAM is necessary, although 4GB or more is better.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Disable iptables and iptablesv6 (be cautious of this if you plan to use this in a production environment!)</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> (RedHat based Linux distros) and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>If its present, remove OpenJDK software (<strong>java -version</strong> will tell you).</li>\n<li>Download Oracle’s JAVA 1.7_55 RPM package from their <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html\">website</a> and install it.</li>\n<li>All of this above was the difficult part. :)</li>\n<li>Download the Pentaho biserver-ce from <a href=\"http://community.pentaho.com/\">Pentaho’s website</a> and click the Download Pentaho CE link. You be redirected to a page on SourceForge.net and the file will begin downloading. It takes a while.</li>\n<li>Once the download completes, create a pentaho folder in your home folder and extract the biserver-ce-5.0.1-stable.zip to it.</li>\n<li>At the command line, switch back to your user account if you are still running as root.</li>\n<li>Navigate to your /home/username/pentaho/biserver-ce folder.</li>\n<li>To start the server, enter this command: ./start-pentaho.sh</li>\n<li>The shell script will produce several lines of output, and then return to a command prompt.</li>\n<li>If you are running a GUI environment on your server, open a browser, and navigate to <a href=\"http://localhost:8080/pentaho\">http://localhost:8080/pentaho</a>. Otherwise open a browser on a machine on the same network and access the IPAddress for your server with the ending notation (:8080/pentaho).</li>\n<li>It may take a few minutes for the webpage to load up, but once it does, you’ll see the Pentaho User Console login screen. Login using the default user account. User name: admin     Password: password</li>\n<li>You’re in!</li>\n</ol>\n<p>If you would like to access the server from a different machine, set a static IP address and a fully qualified domain name, but if all of your work will be local, you are all set.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/frustration.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/frustration-300x224.jpg\" alt=\"frustration\"></a>I’m posting this just because there seems to be a lot of confusion and misinformation on the web about getting the open source version of <a href=\"http://community.pentaho.com/\">Pentaho’s BIServer</a> up and running on Linux. (I know I spent several hours over the course of a week running through them.) The current version is 5.01 and you download a ZIP file that contains almost everything you need to get started. You do NOT need to install MySQL or PostgreSQL or Oracle on your server to get Pentaho BIServer to work, nor do you need to set the PENTAHO_JAVA_HOME or JAVA_HOME system variables or create a Pentaho user account. If you want to change the repository database to something besides the embedded HSQLDB you can find directions on that on the Internet (Good luck to you). If you just want to setup a simple system for testing and low volume usage, read on…</p>","more":"<p>Here is a step by step account of how I got Pentaho BIServer to run on my system:</p>\n<ol>\n<li>Install a Linux server ( I am using CentOS 64-bit). A minimum of 2GB of RAM is necessary, although 4GB or more is better.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Disable iptables and iptablesv6 (be cautious of this if you plan to use this in a production environment!)</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> (RedHat based Linux distros) and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>If its present, remove OpenJDK software (<strong>java -version</strong> will tell you).</li>\n<li>Download Oracle’s JAVA 1.7_55 RPM package from their <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html\">website</a> and install it.</li>\n<li>All of this above was the difficult part. :)</li>\n<li>Download the Pentaho biserver-ce from <a href=\"http://community.pentaho.com/\">Pentaho’s website</a> and click the Download Pentaho CE link. You be redirected to a page on SourceForge.net and the file will begin downloading. It takes a while.</li>\n<li>Once the download completes, create a pentaho folder in your home folder and extract the biserver-ce-5.0.1-stable.zip to it.</li>\n<li>At the command line, switch back to your user account if you are still running as root.</li>\n<li>Navigate to your /home/username/pentaho/biserver-ce folder.</li>\n<li>To start the server, enter this command: ./start-pentaho.sh</li>\n<li>The shell script will produce several lines of output, and then return to a command prompt.</li>\n<li>If you are running a GUI environment on your server, open a browser, and navigate to <a href=\"http://localhost:8080/pentaho\">http://localhost:8080/pentaho</a>. Otherwise open a browser on a machine on the same network and access the IPAddress for your server with the ending notation (:8080/pentaho).</li>\n<li>It may take a few minutes for the webpage to load up, but once it does, you’ll see the Pentaho User Console login screen. Login using the default user account. User name: admin     Password: password</li>\n<li>You’re in!</li>\n</ol>\n<p>If you would like to access the server from a different machine, set a static IP address and a fully qualified domain name, but if all of your work will be local, you are all set.</p>"},{"title":"Run services (like MySQL) on demand with Ubuntu 15.04","id":"2892","comments":0,"date":"2015-07-08T21:23:14.000Z","_content":"\n[![stopstart](http://edpflager.com/wp-content/uploads/2015/07/stopstart-300x285.jpg)](http://edpflager.com/wp-content/uploads/2015/07/stopstart.jpg)This is a followup to an [article](http://edpflager.com/?p=1618) I wrote a couple of years ago where I covered how to start the MySQL server daemon on demand in Ubuntu. With version 15.04, the controller for services in Ubuntu has changed to systemd from upstart. Getting services to start when you want them is still fairly simple, though and I'll illustrate the process by using MySQL as an example. Be careful when disabling services, because you could cause your system to become unstable if you disable the wrong one. For this tutoriaI I assume you have MySQL version 5.6 or higher installed and the server daemon starts up when you boot your system.\n<!-- more -->\n##### Disable the MySQL Service\n\nLets walk through disabling that, and how to start MySQL when you want it to run.\n\n1.  Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.\n2.  When the terminal window appears, enter this command to see what services are currently running on your PC:\n    \n    ###### systemctl -t service\n    \n3.  You should see a fairly lengthy list of services, and more than likely there are more that don't show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.[![services](http://edpflager.com/wp-content/uploads/2015/07/services-300x185.png)](http://edpflager.com/wp-content/uploads/2015/07/services.png)\n4.  Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mysql.service (see the image above).\n5.  Once you have verified that the MySQL service is installed and running, press Ctrl-C to exit back to a terminal prompt.\n6.  Enter the following commands to stop and disable the MySQL service:\n    \n    ###### sudo systemctl stop mysql sudo systemctl disable mysql\n    \n7.  Repeat step 2 from above, and this time the mysql.service shouldn't be listed.\n\n##### RUN THE MYSQL SERVICE\n\nOnce you have disabled the MySQL service, you can start it up by entering one of these two commands:\n\n######    sudo systemctl start mysql sudo service mysql start\n\n##### STOP the MYSQL Service\n\nAS shown above, to stop the service, you can either of these two commands: **sudo systemctl stop mysql    ** **sudo service mysql stop**","source":"_posts/run-services-like-mysql-on-demand-with-ubuntu-15-04.md","raw":"---\ntitle: Run services (like MySQL) on demand with Ubuntu 15.04\ntags:\n  - guides\n  - How-to\n  - howto\n  - MySQL\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '2892'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-07-08 17:23:14\n---\n\n[![stopstart](http://edpflager.com/wp-content/uploads/2015/07/stopstart-300x285.jpg)](http://edpflager.com/wp-content/uploads/2015/07/stopstart.jpg)This is a followup to an [article](http://edpflager.com/?p=1618) I wrote a couple of years ago where I covered how to start the MySQL server daemon on demand in Ubuntu. With version 15.04, the controller for services in Ubuntu has changed to systemd from upstart. Getting services to start when you want them is still fairly simple, though and I'll illustrate the process by using MySQL as an example. Be careful when disabling services, because you could cause your system to become unstable if you disable the wrong one. For this tutoriaI I assume you have MySQL version 5.6 or higher installed and the server daemon starts up when you boot your system.\n<!-- more -->\n##### Disable the MySQL Service\n\nLets walk through disabling that, and how to start MySQL when you want it to run.\n\n1.  Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.\n2.  When the terminal window appears, enter this command to see what services are currently running on your PC:\n    \n    ###### systemctl -t service\n    \n3.  You should see a fairly lengthy list of services, and more than likely there are more that don't show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.[![services](http://edpflager.com/wp-content/uploads/2015/07/services-300x185.png)](http://edpflager.com/wp-content/uploads/2015/07/services.png)\n4.  Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mysql.service (see the image above).\n5.  Once you have verified that the MySQL service is installed and running, press Ctrl-C to exit back to a terminal prompt.\n6.  Enter the following commands to stop and disable the MySQL service:\n    \n    ###### sudo systemctl stop mysql sudo systemctl disable mysql\n    \n7.  Repeat step 2 from above, and this time the mysql.service shouldn't be listed.\n\n##### RUN THE MYSQL SERVICE\n\nOnce you have disabled the MySQL service, you can start it up by entering one of these two commands:\n\n######    sudo systemctl start mysql sudo service mysql start\n\n##### STOP the MYSQL Service\n\nAS shown above, to stop the service, you can either of these two commands: **sudo systemctl stop mysql    ** **sudo service mysql stop**","slug":"run-services-like-mysql-on-demand-with-ubuntu-15-04","published":1,"updated":"2020-08-23T20:54:34.962Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8f0098sdjxe9go7ui7","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/07/stopstart.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/stopstart-300x285.jpg\" alt=\"stopstart\"></a>This is a followup to an <a href=\"http://edpflager.com/?p=1618\">article</a> I wrote a couple of years ago where I covered how to start the MySQL server daemon on demand in Ubuntu. With version 15.04, the controller for services in Ubuntu has changed to systemd from upstart. Getting services to start when you want them is still fairly simple, though and I’ll illustrate the process by using MySQL as an example. Be careful when disabling services, because you could cause your system to become unstable if you disable the wrong one. For this tutoriaI I assume you have MySQL version 5.6 or higher installed and the server daemon starts up when you boot your system.</p>\n<a id=\"more\"></a>\n<h5 id=\"Disable-the-MySQL-Service\"><a href=\"#Disable-the-MySQL-Service\" class=\"headerlink\" title=\"Disable the MySQL Service\"></a>Disable the MySQL Service</h5><p>Lets walk through disabling that, and how to start MySQL when you want it to run.</p>\n<ol>\n<li><p>Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.</p>\n</li>\n<li><p>When the terminal window appears, enter this command to see what services are currently running on your PC:</p>\n<h6 id=\"systemctl-t-service\"><a href=\"#systemctl-t-service\" class=\"headerlink\" title=\"systemctl -t service\"></a>systemctl -t service</h6></li>\n<li><p>You should see a fairly lengthy list of services, and more than likely there are more that don’t show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.<a href=\"http://edpflager.com/wp-content/uploads/2015/07/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/services-300x185.png\" alt=\"services\"></a></p>\n</li>\n<li><p>Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mysql.service (see the image above).</p>\n</li>\n<li><p>Once you have verified that the MySQL service is installed and running, press Ctrl-C to exit back to a terminal prompt.</p>\n</li>\n<li><p>Enter the following commands to stop and disable the MySQL service:</p>\n<h6 id=\"sudo-systemctl-stop-mysql-sudo-systemctl-disable-mysql\"><a href=\"#sudo-systemctl-stop-mysql-sudo-systemctl-disable-mysql\" class=\"headerlink\" title=\"sudo systemctl stop mysql sudo systemctl disable mysql\"></a>sudo systemctl stop mysql sudo systemctl disable mysql</h6></li>\n<li><p>Repeat step 2 from above, and this time the mysql.service shouldn’t be listed.</p>\n</li>\n</ol>\n<h5 id=\"RUN-THE-MYSQL-SERVICE\"><a href=\"#RUN-THE-MYSQL-SERVICE\" class=\"headerlink\" title=\"RUN THE MYSQL SERVICE\"></a>RUN THE MYSQL SERVICE</h5><p>Once you have disabled the MySQL service, you can start it up by entering one of these two commands:</p>\n<h6 id=\"sudo-systemctl-start-mysql-sudo-service-mysql-start\"><a href=\"#sudo-systemctl-start-mysql-sudo-service-mysql-start\" class=\"headerlink\" title=\"   sudo systemctl start mysql sudo service mysql start\"></a>   sudo systemctl start mysql sudo service mysql start</h6><h5 id=\"STOP-the-MYSQL-Service\"><a href=\"#STOP-the-MYSQL-Service\" class=\"headerlink\" title=\"STOP the MYSQL Service\"></a>STOP the MYSQL Service</h5><p>AS shown above, to stop the service, you can either of these two commands: <strong>sudo systemctl stop mysql    ** **sudo service mysql stop</strong></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/07/stopstart.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/stopstart-300x285.jpg\" alt=\"stopstart\"></a>This is a followup to an <a href=\"http://edpflager.com/?p=1618\">article</a> I wrote a couple of years ago where I covered how to start the MySQL server daemon on demand in Ubuntu. With version 15.04, the controller for services in Ubuntu has changed to systemd from upstart. Getting services to start when you want them is still fairly simple, though and I’ll illustrate the process by using MySQL as an example. Be careful when disabling services, because you could cause your system to become unstable if you disable the wrong one. For this tutoriaI I assume you have MySQL version 5.6 or higher installed and the server daemon starts up when you boot your system.</p>","more":"<h5 id=\"Disable-the-MySQL-Service\"><a href=\"#Disable-the-MySQL-Service\" class=\"headerlink\" title=\"Disable the MySQL Service\"></a>Disable the MySQL Service</h5><p>Lets walk through disabling that, and how to start MySQL when you want it to run.</p>\n<ol>\n<li><p>Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.</p>\n</li>\n<li><p>When the terminal window appears, enter this command to see what services are currently running on your PC:</p>\n<h6 id=\"systemctl-t-service\"><a href=\"#systemctl-t-service\" class=\"headerlink\" title=\"systemctl -t service\"></a>systemctl -t service</h6></li>\n<li><p>You should see a fairly lengthy list of services, and more than likely there are more that don’t show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.<a href=\"http://edpflager.com/wp-content/uploads/2015/07/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/07/services-300x185.png\" alt=\"services\"></a></p>\n</li>\n<li><p>Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mysql.service (see the image above).</p>\n</li>\n<li><p>Once you have verified that the MySQL service is installed and running, press Ctrl-C to exit back to a terminal prompt.</p>\n</li>\n<li><p>Enter the following commands to stop and disable the MySQL service:</p>\n<h6 id=\"sudo-systemctl-stop-mysql-sudo-systemctl-disable-mysql\"><a href=\"#sudo-systemctl-stop-mysql-sudo-systemctl-disable-mysql\" class=\"headerlink\" title=\"sudo systemctl stop mysql sudo systemctl disable mysql\"></a>sudo systemctl stop mysql sudo systemctl disable mysql</h6></li>\n<li><p>Repeat step 2 from above, and this time the mysql.service shouldn’t be listed.</p>\n</li>\n</ol>\n<h5 id=\"RUN-THE-MYSQL-SERVICE\"><a href=\"#RUN-THE-MYSQL-SERVICE\" class=\"headerlink\" title=\"RUN THE MYSQL SERVICE\"></a>RUN THE MYSQL SERVICE</h5><p>Once you have disabled the MySQL service, you can start it up by entering one of these two commands:</p>\n<h6 id=\"sudo-systemctl-start-mysql-sudo-service-mysql-start\"><a href=\"#sudo-systemctl-start-mysql-sudo-service-mysql-start\" class=\"headerlink\" title=\"   sudo systemctl start mysql sudo service mysql start\"></a>   sudo systemctl start mysql sudo service mysql start</h6><h5 id=\"STOP-the-MYSQL-Service\"><a href=\"#STOP-the-MYSQL-Service\" class=\"headerlink\" title=\"STOP the MYSQL Service\"></a>STOP the MYSQL Service</h5><p>AS shown above, to stop the service, you can either of these two commands: <strong>sudo systemctl stop mysql    ** **sudo service mysql stop</strong></p>"},{"title":"Run SQLServer service on Ubuntu 16.04 on demand","id":"3530","comments":0,"date":"2017-04-25T20:01:18.000Z","_content":"\nGreetings all! I have been a bit absent from the blog for the past few months due to a variety of external factors, but I **am** endeavoring to be more regular on here. This time around you may be aware that Microsoft recently released a Linux version of SQL Server.  Because I was testing something else, I thought this would be useful to install as well as part of that process.  I won't bother with instructions on how to install it, as Microsoft has provided a decent [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). in my case, I run a variety of different applications on my development system so I don't always want those apps to start when the PC starts up.  I blogged previously on how to [start services on demand in Ubuntu 15.04](http://edpflager.com/?p=2892), so I went back to that post to test it out with the new SQL Server.  Happily, my previous instructions work with this new application! I'll present them here in modified form for SQL Server after the break...\n<!-- more -->\n##### DISABLE THE MSSQL-SERVER SERVICE\n\nWhen you install Microsoft SQL Server on Ubuntu, the service is set to start automatically when the system restarts. For a dedicated server, that makes sense, for a variety of reasons. If you are working on a DEV machine like I am, you may not want that. So let's walk through disabling the service, and how to start it when you want it to run.\n\n1.  Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.\n2.  When the terminal window appears, enter this command to see what services are currently running on your PC:\n    \n    ###### systemctl -t service\n    \n3.  You should see a fairly lengthy list of services, and more than likely there are more that don't show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.[![](http://edpflager.com/wp-content/uploads/2017/04/mssql-300x172.png)](http://edpflager.com/?attachment_id=3534#main)\n4.  Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mssql-server.service (see the image above).\n5.  Once you have verified that the SQL Server service is installed and running, press Ctrl-C to exit back to a terminal prompt.\n6.  Enter the following commands to stop and disable the MySQL service:\n    \n    ###### sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service\n    \n7.  Repeat step 2 from above, and this time the mssql-server.service shouldn't be listed.\n\n##### RUN THE MSSQL-SERVER SERVICE\n\nOnce you have disabled the SQL Server service, you can start it up by entering one of these two commands:\n\n######  sudo systemctl start mssql-server.service\n\n \n\n##### STOP THE SQL SERVER SERVICE\n\nAs shown above, to stop the service, enter this command:\n\n**sudo systemctl stop mssql-server.service**","source":"_posts/run-sqlserver-service-on-ubuntu-16-04-on-demand.md","raw":"---\ntitle: Run SQLServer service on Ubuntu 16.04 on demand\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - SQL Server\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3530'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2017-04-25 16:01:18\n---\n\nGreetings all! I have been a bit absent from the blog for the past few months due to a variety of external factors, but I **am** endeavoring to be more regular on here. This time around you may be aware that Microsoft recently released a Linux version of SQL Server.  Because I was testing something else, I thought this would be useful to install as well as part of that process.  I won't bother with instructions on how to install it, as Microsoft has provided a decent [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). in my case, I run a variety of different applications on my development system so I don't always want those apps to start when the PC starts up.  I blogged previously on how to [start services on demand in Ubuntu 15.04](http://edpflager.com/?p=2892), so I went back to that post to test it out with the new SQL Server.  Happily, my previous instructions work with this new application! I'll present them here in modified form for SQL Server after the break...\n<!-- more -->\n##### DISABLE THE MSSQL-SERVER SERVICE\n\nWhen you install Microsoft SQL Server on Ubuntu, the service is set to start automatically when the system restarts. For a dedicated server, that makes sense, for a variety of reasons. If you are working on a DEV machine like I am, you may not want that. So let's walk through disabling the service, and how to start it when you want it to run.\n\n1.  Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.\n2.  When the terminal window appears, enter this command to see what services are currently running on your PC:\n    \n    ###### systemctl -t service\n    \n3.  You should see a fairly lengthy list of services, and more than likely there are more that don't show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.[![](http://edpflager.com/wp-content/uploads/2017/04/mssql-300x172.png)](http://edpflager.com/?attachment_id=3534#main)\n4.  Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mssql-server.service (see the image above).\n5.  Once you have verified that the SQL Server service is installed and running, press Ctrl-C to exit back to a terminal prompt.\n6.  Enter the following commands to stop and disable the MySQL service:\n    \n    ###### sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service\n    \n7.  Repeat step 2 from above, and this time the mssql-server.service shouldn't be listed.\n\n##### RUN THE MSSQL-SERVER SERVICE\n\nOnce you have disabled the SQL Server service, you can start it up by entering one of these two commands:\n\n######  sudo systemctl start mssql-server.service\n\n \n\n##### STOP THE SQL SERVER SERVICE\n\nAs shown above, to stop the service, enter this command:\n\n**sudo systemctl stop mssql-server.service**","slug":"run-sqlserver-service-on-ubuntu-16-04-on-demand","published":1,"updated":"2020-08-23T20:54:35.094Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8j009bsdjx4sw84go9","content":"<p>Greetings all! I have been a bit absent from the blog for the past few months due to a variety of external factors, but I <strong>am</strong> endeavoring to be more regular on here. This time around you may be aware that Microsoft recently released a Linux version of SQL Server.  Because I was testing something else, I thought this would be useful to install as well as part of that process.  I won’t bother with instructions on how to install it, as Microsoft has provided a decent <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. in my case, I run a variety of different applications on my development system so I don’t always want those apps to start when the PC starts up.  I blogged previously on how to <a href=\"http://edpflager.com/?p=2892\">start services on demand in Ubuntu 15.04</a>, so I went back to that post to test it out with the new SQL Server.  Happily, my previous instructions work with this new application! I’ll present them here in modified form for SQL Server after the break…</p>\n<a id=\"more\"></a>\n<h5 id=\"DISABLE-THE-MSSQL-SERVER-SERVICE\"><a href=\"#DISABLE-THE-MSSQL-SERVER-SERVICE\" class=\"headerlink\" title=\"DISABLE THE MSSQL-SERVER SERVICE\"></a>DISABLE THE MSSQL-SERVER SERVICE</h5><p>When you install Microsoft SQL Server on Ubuntu, the service is set to start automatically when the system restarts. For a dedicated server, that makes sense, for a variety of reasons. If you are working on a DEV machine like I am, you may not want that. So let’s walk through disabling the service, and how to start it when you want it to run.</p>\n<ol>\n<li><p>Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.</p>\n</li>\n<li><p>When the terminal window appears, enter this command to see what services are currently running on your PC:</p>\n<h6 id=\"systemctl-t-service\"><a href=\"#systemctl-t-service\" class=\"headerlink\" title=\"systemctl -t service\"></a>systemctl -t service</h6></li>\n<li><p>You should see a fairly lengthy list of services, and more than likely there are more that don’t show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.<a href=\"http://edpflager.com/?attachment_id=3534#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/mssql-300x172.png\"></a></p>\n</li>\n<li><p>Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mssql-server.service (see the image above).</p>\n</li>\n<li><p>Once you have verified that the SQL Server service is installed and running, press Ctrl-C to exit back to a terminal prompt.</p>\n</li>\n<li><p>Enter the following commands to stop and disable the MySQL service:</p>\n<h6 id=\"sudo-systemctl-stop-mssql-server-service-sudo-systemctl-disable-mssql-server-service\"><a href=\"#sudo-systemctl-stop-mssql-server-service-sudo-systemctl-disable-mssql-server-service\" class=\"headerlink\" title=\"sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service\"></a>sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service</h6></li>\n<li><p>Repeat step 2 from above, and this time the mssql-server.service shouldn’t be listed.</p>\n</li>\n</ol>\n<h5 id=\"RUN-THE-MSSQL-SERVER-SERVICE\"><a href=\"#RUN-THE-MSSQL-SERVER-SERVICE\" class=\"headerlink\" title=\"RUN THE MSSQL-SERVER SERVICE\"></a>RUN THE MSSQL-SERVER SERVICE</h5><p>Once you have disabled the SQL Server service, you can start it up by entering one of these two commands:</p>\n<h6 id=\"sudo-systemctl-start-mssql-server-service\"><a href=\"#sudo-systemctl-start-mssql-server-service\" class=\"headerlink\" title=\" sudo systemctl start mssql-server.service\"></a> sudo systemctl start mssql-server.service</h6><p> </p>\n<h5 id=\"STOP-THE-SQL-SERVER-SERVICE\"><a href=\"#STOP-THE-SQL-SERVER-SERVICE\" class=\"headerlink\" title=\"STOP THE SQL SERVER SERVICE\"></a>STOP THE SQL SERVER SERVICE</h5><p>As shown above, to stop the service, enter this command:</p>\n<p><strong>sudo systemctl stop mssql-server.service</strong></p>\n","site":{"data":{}},"excerpt":"<p>Greetings all! I have been a bit absent from the blog for the past few months due to a variety of external factors, but I <strong>am</strong> endeavoring to be more regular on here. This time around you may be aware that Microsoft recently released a Linux version of SQL Server.  Because I was testing something else, I thought this would be useful to install as well as part of that process.  I won’t bother with instructions on how to install it, as Microsoft has provided a decent <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. in my case, I run a variety of different applications on my development system so I don’t always want those apps to start when the PC starts up.  I blogged previously on how to <a href=\"http://edpflager.com/?p=2892\">start services on demand in Ubuntu 15.04</a>, so I went back to that post to test it out with the new SQL Server.  Happily, my previous instructions work with this new application! I’ll present them here in modified form for SQL Server after the break…</p>","more":"<h5 id=\"DISABLE-THE-MSSQL-SERVER-SERVICE\"><a href=\"#DISABLE-THE-MSSQL-SERVER-SERVICE\" class=\"headerlink\" title=\"DISABLE THE MSSQL-SERVER SERVICE\"></a>DISABLE THE MSSQL-SERVER SERVICE</h5><p>When you install Microsoft SQL Server on Ubuntu, the service is set to start automatically when the system restarts. For a dedicated server, that makes sense, for a variety of reasons. If you are working on a DEV machine like I am, you may not want that. So let’s walk through disabling the service, and how to start it when you want it to run.</p>\n<ol>\n<li><p>Open a terminal window, either by pressing CTRL-ALT-T on your keyboard or searching for Terminal in the Unity search interface.</p>\n</li>\n<li><p>When the terminal window appears, enter this command to see what services are currently running on your PC:</p>\n<h6 id=\"systemctl-t-service\"><a href=\"#systemctl-t-service\" class=\"headerlink\" title=\"systemctl -t service\"></a>systemctl -t service</h6></li>\n<li><p>You should see a fairly lengthy list of services, and more than likely there are more that don’t show yet. On the left side is the name of the service (annotated as .service). The three part status follows, and finally a brief description of the service is on the right.<a href=\"http://edpflager.com/?attachment_id=3534#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/mssql-300x172.png\"></a></p>\n</li>\n<li><p>Press the Enter key to go down one line at a time, or press the space bar to scroll down one screen full at a time. Somewhere in the list you should see mssql-server.service (see the image above).</p>\n</li>\n<li><p>Once you have verified that the SQL Server service is installed and running, press Ctrl-C to exit back to a terminal prompt.</p>\n</li>\n<li><p>Enter the following commands to stop and disable the MySQL service:</p>\n<h6 id=\"sudo-systemctl-stop-mssql-server-service-sudo-systemctl-disable-mssql-server-service\"><a href=\"#sudo-systemctl-stop-mssql-server-service-sudo-systemctl-disable-mssql-server-service\" class=\"headerlink\" title=\"sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service\"></a>sudo systemctl stop mssql-server.service sudo systemctl disable mssql-server.service</h6></li>\n<li><p>Repeat step 2 from above, and this time the mssql-server.service shouldn’t be listed.</p>\n</li>\n</ol>\n<h5 id=\"RUN-THE-MSSQL-SERVER-SERVICE\"><a href=\"#RUN-THE-MSSQL-SERVER-SERVICE\" class=\"headerlink\" title=\"RUN THE MSSQL-SERVER SERVICE\"></a>RUN THE MSSQL-SERVER SERVICE</h5><p>Once you have disabled the SQL Server service, you can start it up by entering one of these two commands:</p>\n<h6 id=\"sudo-systemctl-start-mssql-server-service\"><a href=\"#sudo-systemctl-start-mssql-server-service\" class=\"headerlink\" title=\" sudo systemctl start mssql-server.service\"></a> sudo systemctl start mssql-server.service</h6><p> </p>\n<h5 id=\"STOP-THE-SQL-SERVER-SERVICE\"><a href=\"#STOP-THE-SQL-SERVER-SERVICE\" class=\"headerlink\" title=\"STOP THE SQL SERVER SERVICE\"></a>STOP THE SQL SERVER SERVICE</h5><p>As shown above, to stop the service, enter this command:</p>\n<p><strong>sudo systemctl stop mssql-server.service</strong></p>"},{"title":"Running Kettle (Pentaho Data Integration) on Mac OSX 10.12 Sierra","id":"3571","comments":0,"date":"2017-05-05T20:05:02.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2017/05/eye-roll-300x229.jpeg)](http://edpflager.com/?attachment_id=3572#main)A new version of Mac OSX and a new version of Pentaho Data Integration (aka Kettle) but the same old problem getting Kettle to run. Apple tries to keep their operating system locked down and secure, so if you download applications from the Internet that aren't from the Apple App Store, the files are quarantined. With the update to Sierra, the quarantine process has been \"improved\". Keep reading to see how to do it!\n<!-- more -->\nOpen a Finder window to where you extracted the PDI zip file. Then open a terminal prompt, next to it. In the terminal window, enter the following command:\n\nsudo xattr -dr com.apple.quarantine\n\nFrom the Finder window, drag the Data Integration.app file to the terminal window. The path to the Data Integration.app will be added to the end of the command you just entered. The result should look similar to this:\n\nsudo xattr -dr com.apple.quarantine /Applications/data-integration/Data Integration.app\n\nThat should do it! Close the terminal window and then double click the Data Integration.app file, and shortly you'll see the Kettle splash screen.   Eye roll graphic from [clipartfest.com](https://clipartfest.com/)","source":"_posts/running-kettle-pentaho-data-integration-on-mac-osx-10-12-sierra.md","raw":"---\ntitle: Running Kettle (Pentaho Data Integration) on Mac OSX 10.12 Sierra\ntags:\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - install\n  - kettle\n  - Mac\n  - PDI\n  - technical\nid: '3571'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2017-05-05 16:05:02\n---\n\n[![](http://edpflager.com/wp-content/uploads/2017/05/eye-roll-300x229.jpeg)](http://edpflager.com/?attachment_id=3572#main)A new version of Mac OSX and a new version of Pentaho Data Integration (aka Kettle) but the same old problem getting Kettle to run. Apple tries to keep their operating system locked down and secure, so if you download applications from the Internet that aren't from the Apple App Store, the files are quarantined. With the update to Sierra, the quarantine process has been \"improved\". Keep reading to see how to do it!\n<!-- more -->\nOpen a Finder window to where you extracted the PDI zip file. Then open a terminal prompt, next to it. In the terminal window, enter the following command:\n\nsudo xattr -dr com.apple.quarantine\n\nFrom the Finder window, drag the Data Integration.app file to the terminal window. The path to the Data Integration.app will be added to the end of the command you just entered. The result should look similar to this:\n\nsudo xattr -dr com.apple.quarantine /Applications/data-integration/Data Integration.app\n\nThat should do it! Close the terminal window and then double click the Data Integration.app file, and shortly you'll see the Kettle splash screen.   Eye roll graphic from [clipartfest.com](https://clipartfest.com/)","slug":"running-kettle-pentaho-data-integration-on-mac-osx-10-12-sierra","published":1,"updated":"2020-08-23T20:54:35.110Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8o009fsdjx6w9s1bj2","content":"<p><a href=\"http://edpflager.com/?attachment_id=3572#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/05/eye-roll-300x229.jpeg\"></a>A new version of Mac OSX and a new version of Pentaho Data Integration (aka Kettle) but the same old problem getting Kettle to run. Apple tries to keep their operating system locked down and secure, so if you download applications from the Internet that aren’t from the Apple App Store, the files are quarantined. With the update to Sierra, the quarantine process has been “improved”. Keep reading to see how to do it!</p>\n<a id=\"more\"></a>\n<p>Open a Finder window to where you extracted the PDI zip file. Then open a terminal prompt, next to it. In the terminal window, enter the following command:</p>\n<p>sudo xattr -dr com.apple.quarantine</p>\n<p>From the Finder window, drag the Data Integration.app file to the terminal window. The path to the Data Integration.app will be added to the end of the command you just entered. The result should look similar to this:</p>\n<p>sudo xattr -dr com.apple.quarantine /Applications/data-integration/Data Integration.app</p>\n<p>That should do it! Close the terminal window and then double click the Data Integration.app file, and shortly you’ll see the Kettle splash screen.   Eye roll graphic from <a href=\"https://clipartfest.com/\">clipartfest.com</a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3572#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/05/eye-roll-300x229.jpeg\"></a>A new version of Mac OSX and a new version of Pentaho Data Integration (aka Kettle) but the same old problem getting Kettle to run. Apple tries to keep their operating system locked down and secure, so if you download applications from the Internet that aren’t from the Apple App Store, the files are quarantined. With the update to Sierra, the quarantine process has been “improved”. Keep reading to see how to do it!</p>","more":"<p>Open a Finder window to where you extracted the PDI zip file. Then open a terminal prompt, next to it. In the terminal window, enter the following command:</p>\n<p>sudo xattr -dr com.apple.quarantine</p>\n<p>From the Finder window, drag the Data Integration.app file to the terminal window. The path to the Data Integration.app will be added to the end of the command you just entered. The result should look similar to this:</p>\n<p>sudo xattr -dr com.apple.quarantine /Applications/data-integration/Data Integration.app</p>\n<p>That should do it! Close the terminal window and then double click the Data Integration.app file, and shortly you’ll see the Kettle splash screen.   Eye roll graphic from <a href=\"https://clipartfest.com/\">clipartfest.com</a></p>"},{"title":"Running Pentaho Kettle 7 on Windows 10","id":"3475","comments":0,"date":"2017-01-21T19:12:38.000Z","_content":"\nRecently I switched PCs to a newer Windows 10 based laptop for some of my work, and I wanted to get Pentaho Data Integration up and running on it. I downloaded the pdi-ce-7.0.0.0.25.zip file from the Community website, and extracted the contents to a folder in my Program Files directory. I tried running the SPOON.BAT to start it up but a window flashed on screen quickly and disappeared, but nothing else happened. I opened a command prompt and executed the SPOON.BAT file, but got a message that the JAVAW.EXE file could not be found. So I needed to perform a few other things to get it working. A quick search engine query showed me that many people had the same issue, but there didn't seem to be a consensus on how to resolve it. Below is how I managed to get it running.\n<!-- more -->\nFirst be sure to download the JAVA 64 bit version. If you just go to JAVA.com and click the Free Java Download, you are likely to get the 32 bit version which you don't want. Instead go to [this page](https://www.java.com/en/download/manual.jsp) on the JAVA.com website and download the Windows Offline (64-bit) file. Install it with the instructions that are linked next to the file download.\n\nHow to tell if you have 32 bit installed:\n\nOpen Control Panel. Switch to Icon view. Look for the Java icon. If it says 32 bit you have the wrong one installed.[![](http://edpflager.com/wp-content/uploads/2017/01/java32.png)](http://edpflager.com/?attachment_id=3476#main)\n\nOr, if you still have the downloaded JAVA installation file, the name should have X64 in it for 64-bit. If it doesn't then its the 32 bit version.\n\nAfter installing the 64 bit version of JAVA, you need to add a USER VARIABLE called JAVA\\_HOME with the path to the Java install folder. Right click on the Windows icon in the lower left side of your task bar, and from the menu, choose SYSTEM to open the Control Panel - System window. Click the Advanced system setting option in the left panel. [![](http://edpflager.com/wp-content/uploads/2017/01/advancedsystem-300x117.png)](http://edpflager.com/?attachment_id=3479#main)The SYSTEM PROPERTIES window will open. Click the Environment Variables button at the bottom. [![](http://edpflager.com/wp-content/uploads/2017/01/sysproperties-262x300.png)](http://edpflager.com/?attachment_id=3480#main) The Environment Variables window will open. Click the NEW button in the middle of the window to add a new User variable. The New User Variable window will appear. In the top box type JAVA\\_HOME and in the bottom box, enter the path to the JAVA installation. On my system that is: C:Program FilesJavajre1.8.0\\_121. Validate the path on your system. Click OK on each window to close them. [![](http://edpflager.com/wp-content/uploads/2017/01/variable-300x253.png)](http://edpflager.com/?attachment_id=3484#main) Open a command prompt and type (without the quotes) \"echo %JAVA\\_HOME% \" and hit ENTER. The system should respond with the path you entered previously in the Environment variables window. As a test, in the command prompt window navigate to where you extracted the KETTLE files. On my system, I would enter: cd C:Program FilesPentahodata-integration. Once there I enter: spoon.bat to run the Kettle batch file. After a few seconds the Pentaho splash screen appears and a bit later the KETTLE IDE will appear! [![](http://edpflager.com/wp-content/uploads/2017/01/kettle-1024x627.png)](http://edpflager.com/?attachment_id=3486#main)","source":"_posts/running-pentaho-kettle-7-on-windows-10.md","raw":"---\ntitle: Running Pentaho Kettle 7 on Windows 10\ntags:\n  - cookbook\n  - ETL\n  - How-to\n  - howto\n  - install\n  - kettle\n  - technical\n  - Windows\nid: '3475'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2017-01-21 14:12:38\n---\n\nRecently I switched PCs to a newer Windows 10 based laptop for some of my work, and I wanted to get Pentaho Data Integration up and running on it. I downloaded the pdi-ce-7.0.0.0.25.zip file from the Community website, and extracted the contents to a folder in my Program Files directory. I tried running the SPOON.BAT to start it up but a window flashed on screen quickly and disappeared, but nothing else happened. I opened a command prompt and executed the SPOON.BAT file, but got a message that the JAVAW.EXE file could not be found. So I needed to perform a few other things to get it working. A quick search engine query showed me that many people had the same issue, but there didn't seem to be a consensus on how to resolve it. Below is how I managed to get it running.\n<!-- more -->\nFirst be sure to download the JAVA 64 bit version. If you just go to JAVA.com and click the Free Java Download, you are likely to get the 32 bit version which you don't want. Instead go to [this page](https://www.java.com/en/download/manual.jsp) on the JAVA.com website and download the Windows Offline (64-bit) file. Install it with the instructions that are linked next to the file download.\n\nHow to tell if you have 32 bit installed:\n\nOpen Control Panel. Switch to Icon view. Look for the Java icon. If it says 32 bit you have the wrong one installed.[![](http://edpflager.com/wp-content/uploads/2017/01/java32.png)](http://edpflager.com/?attachment_id=3476#main)\n\nOr, if you still have the downloaded JAVA installation file, the name should have X64 in it for 64-bit. If it doesn't then its the 32 bit version.\n\nAfter installing the 64 bit version of JAVA, you need to add a USER VARIABLE called JAVA\\_HOME with the path to the Java install folder. Right click on the Windows icon in the lower left side of your task bar, and from the menu, choose SYSTEM to open the Control Panel - System window. Click the Advanced system setting option in the left panel. [![](http://edpflager.com/wp-content/uploads/2017/01/advancedsystem-300x117.png)](http://edpflager.com/?attachment_id=3479#main)The SYSTEM PROPERTIES window will open. Click the Environment Variables button at the bottom. [![](http://edpflager.com/wp-content/uploads/2017/01/sysproperties-262x300.png)](http://edpflager.com/?attachment_id=3480#main) The Environment Variables window will open. Click the NEW button in the middle of the window to add a new User variable. The New User Variable window will appear. In the top box type JAVA\\_HOME and in the bottom box, enter the path to the JAVA installation. On my system that is: C:Program FilesJavajre1.8.0\\_121. Validate the path on your system. Click OK on each window to close them. [![](http://edpflager.com/wp-content/uploads/2017/01/variable-300x253.png)](http://edpflager.com/?attachment_id=3484#main) Open a command prompt and type (without the quotes) \"echo %JAVA\\_HOME% \" and hit ENTER. The system should respond with the path you entered previously in the Environment variables window. As a test, in the command prompt window navigate to where you extracted the KETTLE files. On my system, I would enter: cd C:Program FilesPentahodata-integration. Once there I enter: spoon.bat to run the Kettle batch file. After a few seconds the Pentaho splash screen appears and a bit later the KETTLE IDE will appear! [![](http://edpflager.com/wp-content/uploads/2017/01/kettle-1024x627.png)](http://edpflager.com/?attachment_id=3486#main)","slug":"running-pentaho-kettle-7-on-windows-10","published":1,"updated":"2020-08-23T20:54:35.082Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8q009isdjxd9657doc","content":"<p>Recently I switched PCs to a newer Windows 10 based laptop for some of my work, and I wanted to get Pentaho Data Integration up and running on it. I downloaded the pdi-ce-7.0.0.0.25.zip file from the Community website, and extracted the contents to a folder in my Program Files directory. I tried running the SPOON.BAT to start it up but a window flashed on screen quickly and disappeared, but nothing else happened. I opened a command prompt and executed the SPOON.BAT file, but got a message that the JAVAW.EXE file could not be found. So I needed to perform a few other things to get it working. A quick search engine query showed me that many people had the same issue, but there didn’t seem to be a consensus on how to resolve it. Below is how I managed to get it running.</p>\n<a id=\"more\"></a>\n<p>First be sure to download the JAVA 64 bit version. If you just go to JAVA.com and click the Free Java Download, you are likely to get the 32 bit version which you don’t want. Instead go to <a href=\"https://www.java.com/en/download/manual.jsp\">this page</a> on the JAVA.com website and download the Windows Offline (64-bit) file. Install it with the instructions that are linked next to the file download.</p>\n<p>How to tell if you have 32 bit installed:</p>\n<p>Open Control Panel. Switch to Icon view. Look for the Java icon. If it says 32 bit you have the wrong one installed.<a href=\"http://edpflager.com/?attachment_id=3476#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/java32.png\"></a></p>\n<p>Or, if you still have the downloaded JAVA installation file, the name should have X64 in it for 64-bit. If it doesn’t then its the 32 bit version.</p>\n<p>After installing the 64 bit version of JAVA, you need to add a USER VARIABLE called JAVA_HOME with the path to the Java install folder. Right click on the Windows icon in the lower left side of your task bar, and from the menu, choose SYSTEM to open the Control Panel - System window. Click the Advanced system setting option in the left panel. <a href=\"http://edpflager.com/?attachment_id=3479#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/advancedsystem-300x117.png\"></a>The SYSTEM PROPERTIES window will open. Click the Environment Variables button at the bottom. <a href=\"http://edpflager.com/?attachment_id=3480#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/sysproperties-262x300.png\"></a> The Environment Variables window will open. Click the NEW button in the middle of the window to add a new User variable. The New User Variable window will appear. In the top box type JAVA_HOME and in the bottom box, enter the path to the JAVA installation. On my system that is: C:Program FilesJavajre1.8.0_121. Validate the path on your system. Click OK on each window to close them. <a href=\"http://edpflager.com/?attachment_id=3484#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/variable-300x253.png\"></a> Open a command prompt and type (without the quotes) “echo %JAVA_HOME% “ and hit ENTER. The system should respond with the path you entered previously in the Environment variables window. As a test, in the command prompt window navigate to where you extracted the KETTLE files. On my system, I would enter: cd C:Program FilesPentahodata-integration. Once there I enter: spoon.bat to run the Kettle batch file. After a few seconds the Pentaho splash screen appears and a bit later the KETTLE IDE will appear! <a href=\"http://edpflager.com/?attachment_id=3486#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/kettle-1024x627.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p>Recently I switched PCs to a newer Windows 10 based laptop for some of my work, and I wanted to get Pentaho Data Integration up and running on it. I downloaded the pdi-ce-7.0.0.0.25.zip file from the Community website, and extracted the contents to a folder in my Program Files directory. I tried running the SPOON.BAT to start it up but a window flashed on screen quickly and disappeared, but nothing else happened. I opened a command prompt and executed the SPOON.BAT file, but got a message that the JAVAW.EXE file could not be found. So I needed to perform a few other things to get it working. A quick search engine query showed me that many people had the same issue, but there didn’t seem to be a consensus on how to resolve it. Below is how I managed to get it running.</p>","more":"<p>First be sure to download the JAVA 64 bit version. If you just go to JAVA.com and click the Free Java Download, you are likely to get the 32 bit version which you don’t want. Instead go to <a href=\"https://www.java.com/en/download/manual.jsp\">this page</a> on the JAVA.com website and download the Windows Offline (64-bit) file. Install it with the instructions that are linked next to the file download.</p>\n<p>How to tell if you have 32 bit installed:</p>\n<p>Open Control Panel. Switch to Icon view. Look for the Java icon. If it says 32 bit you have the wrong one installed.<a href=\"http://edpflager.com/?attachment_id=3476#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/java32.png\"></a></p>\n<p>Or, if you still have the downloaded JAVA installation file, the name should have X64 in it for 64-bit. If it doesn’t then its the 32 bit version.</p>\n<p>After installing the 64 bit version of JAVA, you need to add a USER VARIABLE called JAVA_HOME with the path to the Java install folder. Right click on the Windows icon in the lower left side of your task bar, and from the menu, choose SYSTEM to open the Control Panel - System window. Click the Advanced system setting option in the left panel. <a href=\"http://edpflager.com/?attachment_id=3479#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/advancedsystem-300x117.png\"></a>The SYSTEM PROPERTIES window will open. Click the Environment Variables button at the bottom. <a href=\"http://edpflager.com/?attachment_id=3480#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/sysproperties-262x300.png\"></a> The Environment Variables window will open. Click the NEW button in the middle of the window to add a new User variable. The New User Variable window will appear. In the top box type JAVA_HOME and in the bottom box, enter the path to the JAVA installation. On my system that is: C:Program FilesJavajre1.8.0_121. Validate the path on your system. Click OK on each window to close them. <a href=\"http://edpflager.com/?attachment_id=3484#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/variable-300x253.png\"></a> Open a command prompt and type (without the quotes) “echo %JAVA_HOME% “ and hit ENTER. The system should respond with the path you entered previously in the Environment variables window. As a test, in the command prompt window navigate to where you extracted the KETTLE files. On my system, I would enter: cd C:Program FilesPentahodata-integration. Once there I enter: spoon.bat to run the Kettle batch file. After a few seconds the Pentaho splash screen appears and a bit later the KETTLE IDE will appear! <a href=\"http://edpflager.com/?attachment_id=3486#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/01/kettle-1024x627.png\"></a></p>"},{"title":"Send Push Notifications from your Kettle jobs - Android","id":"2417","comments":0,"date":"2014-09-10T22:52:59.000Z","_content":"\n[![notification](http://edpflager.com/wp-content/uploads/2014/09/notification-300x199.png)](http://edpflager.com/wp-content/uploads/2014/09/notification.png)When supporting ETL processes, one of the things you have to be conscious of is job failures. To get timely notification of those failures you can include email messages within your workflows that are generated when a failure occurs. Another way that is available, that many Pentaho Data Integration users may not be aware of, is the ability to send Push notifications out from your workflows. A developer named [Joel Latino](http://about.me/latinojoel) has supplied plugins to the Pentaho Marketplace that can be used to send notifications to Android and Apple iOS devices. In this post, I'll look at sending them to Android devices, and in a followup article I hope to cover sending to iOS using Joel's plugins.\n<!-- more -->\n###### Install the Marketplace plugin\n\nThere are a couple of setup steps that need to be performed to enable Push Notifications in PDI. First up is adding the plugin to the PDI environment. It will need to be added to all systems that PDI is installed on.\n\n1.  Start the PDI graphical user interfacer (Spoon), click on Help in the menu bar, and then Marketplace in the drop down list.\n2.  [![marketplace](http://edpflager.com/wp-content/uploads/2014/09/marketplace-300x239.png)](http://edpflager.com/wp-content/uploads/2014/09/marketplace.png)A new window will appear showing the available plugins in the Marketplace.\n3.  Locate the Android Push Notifications plugin in the list and toggle the entry to see the detail. At the bottom of the information there should be a Install this Plugin button. Click on it and PDI will download the necessary files and add them to the environment. Restart PDI to make it active.\n\n###### Install the Android app\n\n1.  On the Android device that will receive the notifications, access the Google Play store, and download and install the PDI Manager app. Once its installed start it up.\n2.  Access the menu for the application using the menu button on the device.\n3.  From the menu popup, choose Registration ID, and a new screen will appear with your registration ID. Click the SEND button to choose a method to transmit the ID to the PDI system.\n\n###### Create a transformation\n\nFor this portion I will be using a modified version of Joe Latino's sample transformation to illustrate the .\n\n1.  Create a new transformation in PDI.\n2.  On the Design tab in the left panel, locate the Input node, toggle it to open it and drag a Generate Rows object to the work space.\n3.  From the Scripting node, drag a Modified Java Script object to the work space.\n4.  Finally from the Utility node, drag the Android Push Notification object to the work space.\n5.  Create two hops to link the three objects together and it should resemble this :[![transformflow](http://edpflager.com/wp-content/uploads/2014/09/transformflow-300x35.png)](http://edpflager.com/wp-content/uploads/2014/09/transformflow.png)\n6.  Open the Generate Rows object, and at the top of the window set the Limit value to 1.\n7.  In the grid at the bottom, add four string fields: registrationid, status, data and project.\n8.  In the Value column for the registration field, enter the  registration ID that was generated from the Android app.\n9.  For the status field, enter a value of Error or Success. Those are the only two accepted options.\n10.  For data, enter a status message that corresponds to the notification, providing more detail.\n11.  Finally for the project field, enter an appropriate value, such as the workflow name or the ETL job that called the transform. The results should be similar to this:[![GenerateRows](http://edpflager.com/wp-content/uploads/2014/09/GenerateRows-300x153.png)](http://edpflager.com/wp-content/uploads/2014/09/GenerateRows.png)\n12.  Click OK to exit, and double click on the Script object to edit it.\n13.  In the Script 1 area of the screen, paste this code in to get the current date and time: var simpleDateFormat = new java.text.SimpleDateFormat(\"yyyy/MM/dd HH:mm:ss.SSS\"); var date = simpleDateFormat.format(java.util.CalendarTodaysDateTime.getInstance().getTime());\n14.  At the bottom of the window, enter a field name - \"date\" with a data type of string. The results should look like this: [![javascript](http://edpflager.com/wp-content/uploads/2014/09/javascript-300x175.png)](http://edpflager.com/wp-content/uploads/2014/09/javascript.png)\n15.  Click OK to exit. Double click the Push Notification object to configure it.\n16.  In the Main Options tab, drop the Registration ID list down and choose registrationid from the values.\n17.  At the bottom of the window, click the Get Fields button, and five lines will be pulled in. Delete the line called registrationid leaving the four other values.![maintab](http://edpflager.com/wp-content/uploads/2014/09/maintab-300x270.png)\n18.  Switch to the Properties tab and enter Joel Latino's API key (AIzaSyAh7Nf-N7bE4xIwsVb7nk4mmls\\_yEQwZQA) in the appropriate spot. (To use a different Google Cloud Messaging key, it should be possible to alter Joe's source code and change it to point to a new server key. I have not tried it though).![propertiestab](http://edpflager.com/wp-content/uploads/2014/09/propertiestab-300x269.png)\n19.  Leave the other values as they are. Click OK to return to your workflow.\n\n######  Test the transformation\n\n1.  Execute the transformation and check the Android device and a notification should appear almost immediately.\n2.  Open the PDI Manager to get more detail on the notification, and to clear it.\n\n###### Things to try out\n\nAt this point the push notification works. There are some additional functionality I would like to experiment with as I have time:\n\n*   For success notifications, in the PDI manager there is only an option to delete the notification. When an error notification is sent, there are two options: Delete or Resolved.  I am interested in working on implementing code to acknowledge the Resolved option.\n*   The Generate Rows step is used normally for testing. I would like to implement code to define the fields using a different object.\n*   I am interested in creating a transformation that is reusable, which would require passing a variable to the transformation indicating the job name.\n*   Finally, I'm not fond of using JavaScript, and would like to replace the code that generates the time/date field with using a System Information step to pull that information.\n\nThanks to Joel Latino for his plugin! Pentaho is a trademark of Pentaho LLC.","source":"_posts/send-push-notifications-from-your-kettle-jobs-android.md","raw":"---\ntitle: Send Push Notifications from your Kettle jobs - Android\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2417'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-09-10 18:52:59\n---\n\n[![notification](http://edpflager.com/wp-content/uploads/2014/09/notification-300x199.png)](http://edpflager.com/wp-content/uploads/2014/09/notification.png)When supporting ETL processes, one of the things you have to be conscious of is job failures. To get timely notification of those failures you can include email messages within your workflows that are generated when a failure occurs. Another way that is available, that many Pentaho Data Integration users may not be aware of, is the ability to send Push notifications out from your workflows. A developer named [Joel Latino](http://about.me/latinojoel) has supplied plugins to the Pentaho Marketplace that can be used to send notifications to Android and Apple iOS devices. In this post, I'll look at sending them to Android devices, and in a followup article I hope to cover sending to iOS using Joel's plugins.\n<!-- more -->\n###### Install the Marketplace plugin\n\nThere are a couple of setup steps that need to be performed to enable Push Notifications in PDI. First up is adding the plugin to the PDI environment. It will need to be added to all systems that PDI is installed on.\n\n1.  Start the PDI graphical user interfacer (Spoon), click on Help in the menu bar, and then Marketplace in the drop down list.\n2.  [![marketplace](http://edpflager.com/wp-content/uploads/2014/09/marketplace-300x239.png)](http://edpflager.com/wp-content/uploads/2014/09/marketplace.png)A new window will appear showing the available plugins in the Marketplace.\n3.  Locate the Android Push Notifications plugin in the list and toggle the entry to see the detail. At the bottom of the information there should be a Install this Plugin button. Click on it and PDI will download the necessary files and add them to the environment. Restart PDI to make it active.\n\n###### Install the Android app\n\n1.  On the Android device that will receive the notifications, access the Google Play store, and download and install the PDI Manager app. Once its installed start it up.\n2.  Access the menu for the application using the menu button on the device.\n3.  From the menu popup, choose Registration ID, and a new screen will appear with your registration ID. Click the SEND button to choose a method to transmit the ID to the PDI system.\n\n###### Create a transformation\n\nFor this portion I will be using a modified version of Joe Latino's sample transformation to illustrate the .\n\n1.  Create a new transformation in PDI.\n2.  On the Design tab in the left panel, locate the Input node, toggle it to open it and drag a Generate Rows object to the work space.\n3.  From the Scripting node, drag a Modified Java Script object to the work space.\n4.  Finally from the Utility node, drag the Android Push Notification object to the work space.\n5.  Create two hops to link the three objects together and it should resemble this :[![transformflow](http://edpflager.com/wp-content/uploads/2014/09/transformflow-300x35.png)](http://edpflager.com/wp-content/uploads/2014/09/transformflow.png)\n6.  Open the Generate Rows object, and at the top of the window set the Limit value to 1.\n7.  In the grid at the bottom, add four string fields: registrationid, status, data and project.\n8.  In the Value column for the registration field, enter the  registration ID that was generated from the Android app.\n9.  For the status field, enter a value of Error or Success. Those are the only two accepted options.\n10.  For data, enter a status message that corresponds to the notification, providing more detail.\n11.  Finally for the project field, enter an appropriate value, such as the workflow name or the ETL job that called the transform. The results should be similar to this:[![GenerateRows](http://edpflager.com/wp-content/uploads/2014/09/GenerateRows-300x153.png)](http://edpflager.com/wp-content/uploads/2014/09/GenerateRows.png)\n12.  Click OK to exit, and double click on the Script object to edit it.\n13.  In the Script 1 area of the screen, paste this code in to get the current date and time: var simpleDateFormat = new java.text.SimpleDateFormat(\"yyyy/MM/dd HH:mm:ss.SSS\"); var date = simpleDateFormat.format(java.util.CalendarTodaysDateTime.getInstance().getTime());\n14.  At the bottom of the window, enter a field name - \"date\" with a data type of string. The results should look like this: [![javascript](http://edpflager.com/wp-content/uploads/2014/09/javascript-300x175.png)](http://edpflager.com/wp-content/uploads/2014/09/javascript.png)\n15.  Click OK to exit. Double click the Push Notification object to configure it.\n16.  In the Main Options tab, drop the Registration ID list down and choose registrationid from the values.\n17.  At the bottom of the window, click the Get Fields button, and five lines will be pulled in. Delete the line called registrationid leaving the four other values.![maintab](http://edpflager.com/wp-content/uploads/2014/09/maintab-300x270.png)\n18.  Switch to the Properties tab and enter Joel Latino's API key (AIzaSyAh7Nf-N7bE4xIwsVb7nk4mmls\\_yEQwZQA) in the appropriate spot. (To use a different Google Cloud Messaging key, it should be possible to alter Joe's source code and change it to point to a new server key. I have not tried it though).![propertiestab](http://edpflager.com/wp-content/uploads/2014/09/propertiestab-300x269.png)\n19.  Leave the other values as they are. Click OK to return to your workflow.\n\n######  Test the transformation\n\n1.  Execute the transformation and check the Android device and a notification should appear almost immediately.\n2.  Open the PDI Manager to get more detail on the notification, and to clear it.\n\n###### Things to try out\n\nAt this point the push notification works. There are some additional functionality I would like to experiment with as I have time:\n\n*   For success notifications, in the PDI manager there is only an option to delete the notification. When an error notification is sent, there are two options: Delete or Resolved.  I am interested in working on implementing code to acknowledge the Resolved option.\n*   The Generate Rows step is used normally for testing. I would like to implement code to define the fields using a different object.\n*   I am interested in creating a transformation that is reusable, which would require passing a variable to the transformation indicating the job name.\n*   Finally, I'm not fond of using JavaScript, and would like to replace the code that generates the time/date field with using a System Information step to pull that information.\n\nThanks to Joel Latino for his plugin! Pentaho is a trademark of Pentaho LLC.","slug":"send-push-notifications-from-your-kettle-jobs-android","published":1,"updated":"2020-08-23T20:54:34.894Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8t009msdjx02m62a6t","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/09/notification.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/notification-300x199.png\" alt=\"notification\"></a>When supporting ETL processes, one of the things you have to be conscious of is job failures. To get timely notification of those failures you can include email messages within your workflows that are generated when a failure occurs. Another way that is available, that many Pentaho Data Integration users may not be aware of, is the ability to send Push notifications out from your workflows. A developer named <a href=\"http://about.me/latinojoel\">Joel Latino</a> has supplied plugins to the Pentaho Marketplace that can be used to send notifications to Android and Apple iOS devices. In this post, I’ll look at sending them to Android devices, and in a followup article I hope to cover sending to iOS using Joel’s plugins.</p>\n<a id=\"more\"></a>\n<h6 id=\"Install-the-Marketplace-plugin\"><a href=\"#Install-the-Marketplace-plugin\" class=\"headerlink\" title=\"Install the Marketplace plugin\"></a>Install the Marketplace plugin</h6><p>There are a couple of setup steps that need to be performed to enable Push Notifications in PDI. First up is adding the plugin to the PDI environment. It will need to be added to all systems that PDI is installed on.</p>\n<ol>\n<li>Start the PDI graphical user interfacer (Spoon), click on Help in the menu bar, and then Marketplace in the drop down list.</li>\n<li><a href=\"http://edpflager.com/wp-content/uploads/2014/09/marketplace.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/marketplace-300x239.png\" alt=\"marketplace\"></a>A new window will appear showing the available plugins in the Marketplace.</li>\n<li>Locate the Android Push Notifications plugin in the list and toggle the entry to see the detail. At the bottom of the information there should be a Install this Plugin button. Click on it and PDI will download the necessary files and add them to the environment. Restart PDI to make it active.</li>\n</ol>\n<h6 id=\"Install-the-Android-app\"><a href=\"#Install-the-Android-app\" class=\"headerlink\" title=\"Install the Android app\"></a>Install the Android app</h6><ol>\n<li>On the Android device that will receive the notifications, access the Google Play store, and download and install the PDI Manager app. Once its installed start it up.</li>\n<li>Access the menu for the application using the menu button on the device.</li>\n<li>From the menu popup, choose Registration ID, and a new screen will appear with your registration ID. Click the SEND button to choose a method to transmit the ID to the PDI system.</li>\n</ol>\n<h6 id=\"Create-a-transformation\"><a href=\"#Create-a-transformation\" class=\"headerlink\" title=\"Create a transformation\"></a>Create a transformation</h6><p>For this portion I will be using a modified version of Joe Latino’s sample transformation to illustrate the .</p>\n<ol>\n<li>Create a new transformation in PDI.</li>\n<li>On the Design tab in the left panel, locate the Input node, toggle it to open it and drag a Generate Rows object to the work space.</li>\n<li>From the Scripting node, drag a Modified Java Script object to the work space.</li>\n<li>Finally from the Utility node, drag the Android Push Notification object to the work space.</li>\n<li>Create two hops to link the three objects together and it should resemble this :<a href=\"http://edpflager.com/wp-content/uploads/2014/09/transformflow.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/transformflow-300x35.png\" alt=\"transformflow\"></a></li>\n<li>Open the Generate Rows object, and at the top of the window set the Limit value to 1.</li>\n<li>In the grid at the bottom, add four string fields: registrationid, status, data and project.</li>\n<li>In the Value column for the registration field, enter the  registration ID that was generated from the Android app.</li>\n<li>For the status field, enter a value of Error or Success. Those are the only two accepted options.</li>\n<li>For data, enter a status message that corresponds to the notification, providing more detail.</li>\n<li>Finally for the project field, enter an appropriate value, such as the workflow name or the ETL job that called the transform. The results should be similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2014/09/GenerateRows.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/GenerateRows-300x153.png\" alt=\"GenerateRows\"></a></li>\n<li>Click OK to exit, and double click on the Script object to edit it.</li>\n<li>In the Script 1 area of the screen, paste this code in to get the current date and time: var simpleDateFormat = new java.text.SimpleDateFormat(“yyyy/MM/dd HH:mm:ss.SSS”); var date = simpleDateFormat.format(java.util.CalendarTodaysDateTime.getInstance().getTime());</li>\n<li>At the bottom of the window, enter a field name - “date” with a data type of string. The results should look like this: <a href=\"http://edpflager.com/wp-content/uploads/2014/09/javascript.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/javascript-300x175.png\" alt=\"javascript\"></a></li>\n<li>Click OK to exit. Double click the Push Notification object to configure it.</li>\n<li>In the Main Options tab, drop the Registration ID list down and choose registrationid from the values.</li>\n<li>At the bottom of the window, click the Get Fields button, and five lines will be pulled in. Delete the line called registrationid leaving the four other values.<img src=\"http://edpflager.com/wp-content/uploads/2014/09/maintab-300x270.png\" alt=\"maintab\"></li>\n<li>Switch to the Properties tab and enter Joel Latino’s API key (AIzaSyAh7Nf-N7bE4xIwsVb7nk4mmls_yEQwZQA) in the appropriate spot. (To use a different Google Cloud Messaging key, it should be possible to alter Joe’s source code and change it to point to a new server key. I have not tried it though).<img src=\"http://edpflager.com/wp-content/uploads/2014/09/propertiestab-300x269.png\" alt=\"propertiestab\"></li>\n<li>Leave the other values as they are. Click OK to return to your workflow.</li>\n</ol>\n<h6 id=\"Test-the-transformation\"><a href=\"#Test-the-transformation\" class=\"headerlink\" title=\" Test the transformation\"></a> Test the transformation</h6><ol>\n<li>Execute the transformation and check the Android device and a notification should appear almost immediately.</li>\n<li>Open the PDI Manager to get more detail on the notification, and to clear it.</li>\n</ol>\n<h6 id=\"Things-to-try-out\"><a href=\"#Things-to-try-out\" class=\"headerlink\" title=\"Things to try out\"></a>Things to try out</h6><p>At this point the push notification works. There are some additional functionality I would like to experiment with as I have time:</p>\n<ul>\n<li>For success notifications, in the PDI manager there is only an option to delete the notification. When an error notification is sent, there are two options: Delete or Resolved.  I am interested in working on implementing code to acknowledge the Resolved option.</li>\n<li>The Generate Rows step is used normally for testing. I would like to implement code to define the fields using a different object.</li>\n<li>I am interested in creating a transformation that is reusable, which would require passing a variable to the transformation indicating the job name.</li>\n<li>Finally, I’m not fond of using JavaScript, and would like to replace the code that generates the time/date field with using a System Information step to pull that information.</li>\n</ul>\n<p>Thanks to Joel Latino for his plugin! Pentaho is a trademark of Pentaho LLC.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/09/notification.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/notification-300x199.png\" alt=\"notification\"></a>When supporting ETL processes, one of the things you have to be conscious of is job failures. To get timely notification of those failures you can include email messages within your workflows that are generated when a failure occurs. Another way that is available, that many Pentaho Data Integration users may not be aware of, is the ability to send Push notifications out from your workflows. A developer named <a href=\"http://about.me/latinojoel\">Joel Latino</a> has supplied plugins to the Pentaho Marketplace that can be used to send notifications to Android and Apple iOS devices. In this post, I’ll look at sending them to Android devices, and in a followup article I hope to cover sending to iOS using Joel’s plugins.</p>","more":"<h6 id=\"Install-the-Marketplace-plugin\"><a href=\"#Install-the-Marketplace-plugin\" class=\"headerlink\" title=\"Install the Marketplace plugin\"></a>Install the Marketplace plugin</h6><p>There are a couple of setup steps that need to be performed to enable Push Notifications in PDI. First up is adding the plugin to the PDI environment. It will need to be added to all systems that PDI is installed on.</p>\n<ol>\n<li>Start the PDI graphical user interfacer (Spoon), click on Help in the menu bar, and then Marketplace in the drop down list.</li>\n<li><a href=\"http://edpflager.com/wp-content/uploads/2014/09/marketplace.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/marketplace-300x239.png\" alt=\"marketplace\"></a>A new window will appear showing the available plugins in the Marketplace.</li>\n<li>Locate the Android Push Notifications plugin in the list and toggle the entry to see the detail. At the bottom of the information there should be a Install this Plugin button. Click on it and PDI will download the necessary files and add them to the environment. Restart PDI to make it active.</li>\n</ol>\n<h6 id=\"Install-the-Android-app\"><a href=\"#Install-the-Android-app\" class=\"headerlink\" title=\"Install the Android app\"></a>Install the Android app</h6><ol>\n<li>On the Android device that will receive the notifications, access the Google Play store, and download and install the PDI Manager app. Once its installed start it up.</li>\n<li>Access the menu for the application using the menu button on the device.</li>\n<li>From the menu popup, choose Registration ID, and a new screen will appear with your registration ID. Click the SEND button to choose a method to transmit the ID to the PDI system.</li>\n</ol>\n<h6 id=\"Create-a-transformation\"><a href=\"#Create-a-transformation\" class=\"headerlink\" title=\"Create a transformation\"></a>Create a transformation</h6><p>For this portion I will be using a modified version of Joe Latino’s sample transformation to illustrate the .</p>\n<ol>\n<li>Create a new transformation in PDI.</li>\n<li>On the Design tab in the left panel, locate the Input node, toggle it to open it and drag a Generate Rows object to the work space.</li>\n<li>From the Scripting node, drag a Modified Java Script object to the work space.</li>\n<li>Finally from the Utility node, drag the Android Push Notification object to the work space.</li>\n<li>Create two hops to link the three objects together and it should resemble this :<a href=\"http://edpflager.com/wp-content/uploads/2014/09/transformflow.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/transformflow-300x35.png\" alt=\"transformflow\"></a></li>\n<li>Open the Generate Rows object, and at the top of the window set the Limit value to 1.</li>\n<li>In the grid at the bottom, add four string fields: registrationid, status, data and project.</li>\n<li>In the Value column for the registration field, enter the  registration ID that was generated from the Android app.</li>\n<li>For the status field, enter a value of Error or Success. Those are the only two accepted options.</li>\n<li>For data, enter a status message that corresponds to the notification, providing more detail.</li>\n<li>Finally for the project field, enter an appropriate value, such as the workflow name or the ETL job that called the transform. The results should be similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2014/09/GenerateRows.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/GenerateRows-300x153.png\" alt=\"GenerateRows\"></a></li>\n<li>Click OK to exit, and double click on the Script object to edit it.</li>\n<li>In the Script 1 area of the screen, paste this code in to get the current date and time: var simpleDateFormat = new java.text.SimpleDateFormat(“yyyy/MM/dd HH:mm:ss.SSS”); var date = simpleDateFormat.format(java.util.CalendarTodaysDateTime.getInstance().getTime());</li>\n<li>At the bottom of the window, enter a field name - “date” with a data type of string. The results should look like this: <a href=\"http://edpflager.com/wp-content/uploads/2014/09/javascript.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/javascript-300x175.png\" alt=\"javascript\"></a></li>\n<li>Click OK to exit. Double click the Push Notification object to configure it.</li>\n<li>In the Main Options tab, drop the Registration ID list down and choose registrationid from the values.</li>\n<li>At the bottom of the window, click the Get Fields button, and five lines will be pulled in. Delete the line called registrationid leaving the four other values.<img src=\"http://edpflager.com/wp-content/uploads/2014/09/maintab-300x270.png\" alt=\"maintab\"></li>\n<li>Switch to the Properties tab and enter Joel Latino’s API key (AIzaSyAh7Nf-N7bE4xIwsVb7nk4mmls_yEQwZQA) in the appropriate spot. (To use a different Google Cloud Messaging key, it should be possible to alter Joe’s source code and change it to point to a new server key. I have not tried it though).<img src=\"http://edpflager.com/wp-content/uploads/2014/09/propertiestab-300x269.png\" alt=\"propertiestab\"></li>\n<li>Leave the other values as they are. Click OK to return to your workflow.</li>\n</ol>\n<h6 id=\"Test-the-transformation\"><a href=\"#Test-the-transformation\" class=\"headerlink\" title=\" Test the transformation\"></a> Test the transformation</h6><ol>\n<li>Execute the transformation and check the Android device and a notification should appear almost immediately.</li>\n<li>Open the PDI Manager to get more detail on the notification, and to clear it.</li>\n</ol>\n<h6 id=\"Things-to-try-out\"><a href=\"#Things-to-try-out\" class=\"headerlink\" title=\"Things to try out\"></a>Things to try out</h6><p>At this point the push notification works. There are some additional functionality I would like to experiment with as I have time:</p>\n<ul>\n<li>For success notifications, in the PDI manager there is only an option to delete the notification. When an error notification is sent, there are two options: Delete or Resolved.  I am interested in working on implementing code to acknowledge the Resolved option.</li>\n<li>The Generate Rows step is used normally for testing. I would like to implement code to define the fields using a different object.</li>\n<li>I am interested in creating a transformation that is reusable, which would require passing a variable to the transformation indicating the job name.</li>\n<li>Finally, I’m not fond of using JavaScript, and would like to replace the code that generates the time/date field with using a System Information step to pull that information.</li>\n</ul>\n<p>Thanks to Joel Latino for his plugin! Pentaho is a trademark of Pentaho LLC.</p>"},{"title":"Services GUI tool on CentOS","id":"1871","comments":0,"date":"2014-03-12T21:29:22.000Z","_content":"\n[![services](http://edpflager.com/wp-content/uploads/2014/03/services-300x221.png)](http://edpflager.com/wp-content/uploads/2014/03/services.png)SysAdm purists often look down on people who use a GUI to handle tasks on  their servers, but having worked for several years on Novell Netware at the beginning of my career (shudder), give me a GUI over a command line every time! On my home CentOS servers, I have the GNOME desktop environment loaded, and it makes me a lot more productive, because I don't have to remember the locations of many scripts, or the various command line switches to run various applications. Recently, I was installing a replacement server for my Hadoop cluster, and I found that the Services GUI option was not present under the System - Administration menu. A little hunting turned up that the system-config-services package wasn't installed. If this happens to you, here's a quick way to get it back. Open up a terminal - kidding!!!\n\n1.  Start the Add/Remove Software application from the Administration menu under System.\n2.  Search for system-config-services and check the two options that should appear. One is the application, the other the documentation.\n3.  Click Apply down in the lower right corner, and authenticate as Root.\n4.  Wait a few second, then check under System - Administration. Service should be back right above Software Update.","source":"_posts/services-gui-tool-on-centos.md","raw":"---\ntitle: Services GUI tool on CentOS\ntags:\n  - SysAdmin\nid: '1871'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2014-03-12 17:29:22\n---\n\n[![services](http://edpflager.com/wp-content/uploads/2014/03/services-300x221.png)](http://edpflager.com/wp-content/uploads/2014/03/services.png)SysAdm purists often look down on people who use a GUI to handle tasks on  their servers, but having worked for several years on Novell Netware at the beginning of my career (shudder), give me a GUI over a command line every time! On my home CentOS servers, I have the GNOME desktop environment loaded, and it makes me a lot more productive, because I don't have to remember the locations of many scripts, or the various command line switches to run various applications. Recently, I was installing a replacement server for my Hadoop cluster, and I found that the Services GUI option was not present under the System - Administration menu. A little hunting turned up that the system-config-services package wasn't installed. If this happens to you, here's a quick way to get it back. Open up a terminal - kidding!!!\n\n1.  Start the Add/Remove Software application from the Administration menu under System.\n2.  Search for system-config-services and check the two options that should appear. One is the application, the other the documentation.\n3.  Click Apply down in the lower right corner, and authenticate as Root.\n4.  Wait a few second, then check under System - Administration. Service should be back right above Software Update.","slug":"services-gui-tool-on-centos","published":1,"updated":"2020-08-23T20:54:34.798Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a8w009psdjxazxghrs5","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/services-300x221.png\" alt=\"services\"></a>SysAdm purists often look down on people who use a GUI to handle tasks on  their servers, but having worked for several years on Novell Netware at the beginning of my career (shudder), give me a GUI over a command line every time! On my home CentOS servers, I have the GNOME desktop environment loaded, and it makes me a lot more productive, because I don’t have to remember the locations of many scripts, or the various command line switches to run various applications. Recently, I was installing a replacement server for my Hadoop cluster, and I found that the Services GUI option was not present under the System - Administration menu. A little hunting turned up that the system-config-services package wasn’t installed. If this happens to you, here’s a quick way to get it back. Open up a terminal - kidding!!!</p>\n<ol>\n<li>Start the Add/Remove Software application from the Administration menu under System.</li>\n<li>Search for system-config-services and check the two options that should appear. One is the application, the other the documentation.</li>\n<li>Click Apply down in the lower right corner, and authenticate as Root.</li>\n<li>Wait a few second, then check under System - Administration. Service should be back right above Software Update.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/03/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/03/services-300x221.png\" alt=\"services\"></a>SysAdm purists often look down on people who use a GUI to handle tasks on  their servers, but having worked for several years on Novell Netware at the beginning of my career (shudder), give me a GUI over a command line every time! On my home CentOS servers, I have the GNOME desktop environment loaded, and it makes me a lot more productive, because I don’t have to remember the locations of many scripts, or the various command line switches to run various applications. Recently, I was installing a replacement server for my Hadoop cluster, and I found that the Services GUI option was not present under the System - Administration menu. A little hunting turned up that the system-config-services package wasn’t installed. If this happens to you, here’s a quick way to get it back. Open up a terminal - kidding!!!</p>\n<ol>\n<li>Start the Add/Remove Software application from the Administration menu under System.</li>\n<li>Search for system-config-services and check the two options that should appear. One is the application, the other the documentation.</li>\n<li>Click Apply down in the lower right corner, and authenticate as Root.</li>\n<li>Wait a few second, then check under System - Administration. Service should be back right above Software Update.</li>\n</ol>\n"},{"title":"Set Up a Hadoop Cluster - Part 2","id":"1721","comments":0,"date":"2014-01-25T14:40:49.000Z","_content":"\n[![StaticIP](http://edpflager.com/wp-content/uploads/2014/01/StaticIP-237x300.png)](http://edpflager.com/wp-content/uploads/2014/01/StaticIP.png)In [part 1 of this series](http://edpflager.com/?p=1714 \"Set up a Hadoop Cluster – Part 1\"), I talked about starting a project to set up a cluster of inexpensive refurbished PCs (cheap) to act as a test Hadoop cluster, and some of the issues I encountered in getting the operating system installed. With this part, I'll talk a little bit about configuring these workstations to get them setup for Hadoop. (And I did want to add, I introduced a fourth PC into the mix, to act as the main Hadoop server - a Zotac Zbox PC with 4GB of memory.)\n\n### **Static IPs**\n\nOn all of the PCs, I added a static IP, using the DHCP assigned address. Before you start changing your connection info, I recommend opening a terminal window and entering the \"ifconfig\" command. This will show you the existing address for your PC, the gateway, the net mask and a lot of other info you probably won't care about.\n<!-- more -->\nThen in the CentOS desktop, right click on the network icon (looks like two computers - one on top of the other), and choose Edit Connections. The Network Connections window opens, and you click on your Wired Auto eth0 entry, and then the edit button. Make sure the Connect Automatically and Available to all users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter your Address, Net mask and Gateway (usually the address of your router that connects to your ISP). Finally be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP. All done? Click over to the IPv6 tab and set it to \"Ignore\" in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made. Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. (Google.com or Yahoo.com work pretty well.) If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping, and exit the terminal.\n\n### **Hostname setup**\n\nNow that I had a static IP setup for each PC, I needed to add a hostname to each PC as well. During the CentOS installation you are asked for a hostname, but I like to make sure its all working now. First open a terminal window again. Enter \"su -\" to switch to superuser mode (enter your password when prompted). Now, using your favorite text editor, open up /etc/sysconfig/network. Change the Hostname line to your fully qualified domain name (FQDN). It should read:\n\n#### HOSTNAME=<<computername>>.<<domain>>.<<domain extension>>\n\nSo for example:\n\n#### HOSTNAME=bob.aaaaaa.com\n\nSave the file and exit back to the terminal. Type the command \"hostname\" and you should be rewarded with the entry you just put into the /etc/sysconfig/network file. Repeat on each system, using a unique computer name for each. Generally its a good idea to use the same domain and domain extension for all of the systems on your local network, but the computer name has to be different.\n\n### **Host file**\n\nIf you have a DNS server setup on your network, you will need to enter the various server names into your DNS cache. If however, you have a smaller network like me, you can get around this by editing the HOSTS file on your boxes. The HOSTS file is like a shortcut DNS. Its a pain to keep up to date on a large network, but for the small four node cluster I am setting up, its not a big deal. Using your favorite text editor again, add lines to the end of the HOSTS file for each box. The syntax is: IP Address    FQDN    Hostname So for example:\n\n#### 192.168.0.1     bob.aaaaa.com bob\n\nOnce you have all of the systems entered, save the file and exit to the terminal. Try pinging one or more of the systems you just added to the Hosts file. If you get responses back, you can move on to the next one. If you don't, examine the file and make sure you have the correct IP address and host. **TIP: I advise getting a small label maker and printing all labels with the PC name and the IP address and affixing them to the front of each computer. It makes it much easier to do this setup.**","source":"_posts/set-up-a-hadoop-cluster-part-2.md","raw":"---\ntitle: Set Up a Hadoop Cluster - Part 2\ntags:\n  - CDH\n  - Hadoop\n  - HortonWorks\nid: '1721'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-01-25 09:40:49\n---\n\n[![StaticIP](http://edpflager.com/wp-content/uploads/2014/01/StaticIP-237x300.png)](http://edpflager.com/wp-content/uploads/2014/01/StaticIP.png)In [part 1 of this series](http://edpflager.com/?p=1714 \"Set up a Hadoop Cluster – Part 1\"), I talked about starting a project to set up a cluster of inexpensive refurbished PCs (cheap) to act as a test Hadoop cluster, and some of the issues I encountered in getting the operating system installed. With this part, I'll talk a little bit about configuring these workstations to get them setup for Hadoop. (And I did want to add, I introduced a fourth PC into the mix, to act as the main Hadoop server - a Zotac Zbox PC with 4GB of memory.)\n\n### **Static IPs**\n\nOn all of the PCs, I added a static IP, using the DHCP assigned address. Before you start changing your connection info, I recommend opening a terminal window and entering the \"ifconfig\" command. This will show you the existing address for your PC, the gateway, the net mask and a lot of other info you probably won't care about.\n<!-- more -->\nThen in the CentOS desktop, right click on the network icon (looks like two computers - one on top of the other), and choose Edit Connections. The Network Connections window opens, and you click on your Wired Auto eth0 entry, and then the edit button. Make sure the Connect Automatically and Available to all users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter your Address, Net mask and Gateway (usually the address of your router that connects to your ISP). Finally be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP. All done? Click over to the IPv6 tab and set it to \"Ignore\" in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made. Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. (Google.com or Yahoo.com work pretty well.) If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping, and exit the terminal.\n\n### **Hostname setup**\n\nNow that I had a static IP setup for each PC, I needed to add a hostname to each PC as well. During the CentOS installation you are asked for a hostname, but I like to make sure its all working now. First open a terminal window again. Enter \"su -\" to switch to superuser mode (enter your password when prompted). Now, using your favorite text editor, open up /etc/sysconfig/network. Change the Hostname line to your fully qualified domain name (FQDN). It should read:\n\n#### HOSTNAME=<<computername>>.<<domain>>.<<domain extension>>\n\nSo for example:\n\n#### HOSTNAME=bob.aaaaaa.com\n\nSave the file and exit back to the terminal. Type the command \"hostname\" and you should be rewarded with the entry you just put into the /etc/sysconfig/network file. Repeat on each system, using a unique computer name for each. Generally its a good idea to use the same domain and domain extension for all of the systems on your local network, but the computer name has to be different.\n\n### **Host file**\n\nIf you have a DNS server setup on your network, you will need to enter the various server names into your DNS cache. If however, you have a smaller network like me, you can get around this by editing the HOSTS file on your boxes. The HOSTS file is like a shortcut DNS. Its a pain to keep up to date on a large network, but for the small four node cluster I am setting up, its not a big deal. Using your favorite text editor again, add lines to the end of the HOSTS file for each box. The syntax is: IP Address    FQDN    Hostname So for example:\n\n#### 192.168.0.1     bob.aaaaa.com bob\n\nOnce you have all of the systems entered, save the file and exit to the terminal. Try pinging one or more of the systems you just added to the Hosts file. If you get responses back, you can move on to the next one. If you don't, examine the file and make sure you have the correct IP address and host. **TIP: I advise getting a small label maker and printing all labels with the PC name and the IP address and affixing them to the front of each computer. It makes it much easier to do this setup.**","slug":"set-up-a-hadoop-cluster-part-2","published":1,"updated":"2020-08-23T20:54:34.766Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a90009tsdjx5cc6hzzt","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/StaticIP.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/StaticIP-237x300.png\" alt=\"StaticIP\"></a>In <a href=\"http://edpflager.com/?p=1714\" title=\"Set up a Hadoop Cluster – Part 1\">part 1 of this series</a>, I talked about starting a project to set up a cluster of inexpensive refurbished PCs (cheap) to act as a test Hadoop cluster, and some of the issues I encountered in getting the operating system installed. With this part, I’ll talk a little bit about configuring these workstations to get them setup for Hadoop. (And I did want to add, I introduced a fourth PC into the mix, to act as the main Hadoop server - a Zotac Zbox PC with 4GB of memory.)</p>\n<h3 id=\"Static-IPs\"><a href=\"#Static-IPs\" class=\"headerlink\" title=\"Static IPs\"></a><strong>Static IPs</strong></h3><p>On all of the PCs, I added a static IP, using the DHCP assigned address. Before you start changing your connection info, I recommend opening a terminal window and entering the “ifconfig” command. This will show you the existing address for your PC, the gateway, the net mask and a lot of other info you probably won’t care about.</p>\n<a id=\"more\"></a>\n<p>Then in the CentOS desktop, right click on the network icon (looks like two computers - one on top of the other), and choose Edit Connections. The Network Connections window opens, and you click on your Wired Auto eth0 entry, and then the edit button. Make sure the Connect Automatically and Available to all users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter your Address, Net mask and Gateway (usually the address of your router that connects to your ISP). Finally be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP. All done? Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made. Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. (Google.com or Yahoo.com work pretty well.) If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping, and exit the terminal.</p>\n<h3 id=\"Hostname-setup\"><a href=\"#Hostname-setup\" class=\"headerlink\" title=\"Hostname setup\"></a><strong>Hostname setup</strong></h3><p>Now that I had a static IP setup for each PC, I needed to add a hostname to each PC as well. During the CentOS installation you are asked for a hostname, but I like to make sure its all working now. First open a terminal window again. Enter “su -“ to switch to superuser mode (enter your password when prompted). Now, using your favorite text editor, open up /etc/sysconfig/network. Change the Hostname line to your fully qualified domain name (FQDN). It should read:</p>\n<h4 id=\"HOSTNAME-lt-gt-lt-gt-lt-gt\"><a href=\"#HOSTNAME-lt-gt-lt-gt-lt-gt\" class=\"headerlink\" title=\"HOSTNAME=&lt;&gt;.&lt;&gt;.&lt;&gt;\"></a>HOSTNAME=&lt;<computername>&gt;.&lt;<domain>&gt;.&lt;<domain extension>&gt;</h4><p>So for example:</p>\n<h4 id=\"HOSTNAME-bob-aaaaaa-com\"><a href=\"#HOSTNAME-bob-aaaaaa-com\" class=\"headerlink\" title=\"HOSTNAME=bob.aaaaaa.com\"></a>HOSTNAME=bob.aaaaaa.com</h4><p>Save the file and exit back to the terminal. Type the command “hostname” and you should be rewarded with the entry you just put into the /etc/sysconfig/network file. Repeat on each system, using a unique computer name for each. Generally its a good idea to use the same domain and domain extension for all of the systems on your local network, but the computer name has to be different.</p>\n<h3 id=\"Host-file\"><a href=\"#Host-file\" class=\"headerlink\" title=\"Host file\"></a><strong>Host file</strong></h3><p>If you have a DNS server setup on your network, you will need to enter the various server names into your DNS cache. If however, you have a smaller network like me, you can get around this by editing the HOSTS file on your boxes. The HOSTS file is like a shortcut DNS. Its a pain to keep up to date on a large network, but for the small four node cluster I am setting up, its not a big deal. Using your favorite text editor again, add lines to the end of the HOSTS file for each box. The syntax is: IP Address    FQDN    Hostname So for example:</p>\n<h4 id=\"192-168-0-1-bob-aaaaa-com-bob\"><a href=\"#192-168-0-1-bob-aaaaa-com-bob\" class=\"headerlink\" title=\"192.168.0.1     bob.aaaaa.com bob\"></a>192.168.0.1     bob.aaaaa.com bob</h4><p>Once you have all of the systems entered, save the file and exit to the terminal. Try pinging one or more of the systems you just added to the Hosts file. If you get responses back, you can move on to the next one. If you don’t, examine the file and make sure you have the correct IP address and host. <strong>TIP: I advise getting a small label maker and printing all labels with the PC name and the IP address and affixing them to the front of each computer. It makes it much easier to do this setup.</strong></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/StaticIP.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/StaticIP-237x300.png\" alt=\"StaticIP\"></a>In <a href=\"http://edpflager.com/?p=1714\" title=\"Set up a Hadoop Cluster – Part 1\">part 1 of this series</a>, I talked about starting a project to set up a cluster of inexpensive refurbished PCs (cheap) to act as a test Hadoop cluster, and some of the issues I encountered in getting the operating system installed. With this part, I’ll talk a little bit about configuring these workstations to get them setup for Hadoop. (And I did want to add, I introduced a fourth PC into the mix, to act as the main Hadoop server - a Zotac Zbox PC with 4GB of memory.)</p>\n<h3 id=\"Static-IPs\"><a href=\"#Static-IPs\" class=\"headerlink\" title=\"Static IPs\"></a><strong>Static IPs</strong></h3><p>On all of the PCs, I added a static IP, using the DHCP assigned address. Before you start changing your connection info, I recommend opening a terminal window and entering the “ifconfig” command. This will show you the existing address for your PC, the gateway, the net mask and a lot of other info you probably won’t care about.</p>","more":"<p>Then in the CentOS desktop, right click on the network icon (looks like two computers - one on top of the other), and choose Edit Connections. The Network Connections window opens, and you click on your Wired Auto eth0 entry, and then the edit button. Make sure the Connect Automatically and Available to all users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter your Address, Net mask and Gateway (usually the address of your router that connects to your ISP). Finally be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP. All done? Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made. Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. (Google.com or Yahoo.com work pretty well.) If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping, and exit the terminal.</p>\n<h3 id=\"Hostname-setup\"><a href=\"#Hostname-setup\" class=\"headerlink\" title=\"Hostname setup\"></a><strong>Hostname setup</strong></h3><p>Now that I had a static IP setup for each PC, I needed to add a hostname to each PC as well. During the CentOS installation you are asked for a hostname, but I like to make sure its all working now. First open a terminal window again. Enter “su -“ to switch to superuser mode (enter your password when prompted). Now, using your favorite text editor, open up /etc/sysconfig/network. Change the Hostname line to your fully qualified domain name (FQDN). It should read:</p>\n<h4 id=\"HOSTNAME-lt-gt-lt-gt-lt-gt\"><a href=\"#HOSTNAME-lt-gt-lt-gt-lt-gt\" class=\"headerlink\" title=\"HOSTNAME=&lt;&gt;.&lt;&gt;.&lt;&gt;\"></a>HOSTNAME=&lt;<computername>&gt;.&lt;<domain>&gt;.&lt;<domain extension>&gt;</h4><p>So for example:</p>\n<h4 id=\"HOSTNAME-bob-aaaaaa-com\"><a href=\"#HOSTNAME-bob-aaaaaa-com\" class=\"headerlink\" title=\"HOSTNAME=bob.aaaaaa.com\"></a>HOSTNAME=bob.aaaaaa.com</h4><p>Save the file and exit back to the terminal. Type the command “hostname” and you should be rewarded with the entry you just put into the /etc/sysconfig/network file. Repeat on each system, using a unique computer name for each. Generally its a good idea to use the same domain and domain extension for all of the systems on your local network, but the computer name has to be different.</p>\n<h3 id=\"Host-file\"><a href=\"#Host-file\" class=\"headerlink\" title=\"Host file\"></a><strong>Host file</strong></h3><p>If you have a DNS server setup on your network, you will need to enter the various server names into your DNS cache. If however, you have a smaller network like me, you can get around this by editing the HOSTS file on your boxes. The HOSTS file is like a shortcut DNS. Its a pain to keep up to date on a large network, but for the small four node cluster I am setting up, its not a big deal. Using your favorite text editor again, add lines to the end of the HOSTS file for each box. The syntax is: IP Address    FQDN    Hostname So for example:</p>\n<h4 id=\"192-168-0-1-bob-aaaaa-com-bob\"><a href=\"#192-168-0-1-bob-aaaaa-com-bob\" class=\"headerlink\" title=\"192.168.0.1     bob.aaaaa.com bob\"></a>192.168.0.1     bob.aaaaa.com bob</h4><p>Once you have all of the systems entered, save the file and exit to the terminal. Try pinging one or more of the systems you just added to the Hosts file. If you get responses back, you can move on to the next one. If you don’t, examine the file and make sure you have the correct IP address and host. <strong>TIP: I advise getting a small label maker and printing all labels with the PC name and the IP address and affixing them to the front of each computer. It makes it much easier to do this setup.</strong></p>"},{"title":"Set up a Hadoop Cluster - Part 3","id":"1725","comments":0,"date":"2014-01-28T14:23:24.000Z","_content":"\n[![services](http://edpflager.com/wp-content/uploads/2014/01/services-300x160.png)](http://edpflager.com/wp-content/uploads/2014/01/services.png)[Third part of this series](http://edpflager.com/?p=1721 \"Set Up a Hadoop Cluster – Part 2\"), and we haven't even gotten to installing Hadoop yet! This is a quick one though. One of the things you need to make Hadoop work is to be able to use SSH across all of the servers in your cluster. To enable it, log into each box, and go to System in the menu, Administration from the menu and over to Services. Click on it.\n<!-- more -->\nThe Services window will open. In the left pane, scroll down until you find \"sshd\". If it has a red dot next to it, it means the ssh daemon is not running and you won't be able to connect to the box from your main Hadoop server. Click on the line, and then click on the Enable button at the top of the window. After a few seconds, the dot next to \"sshd\" will change to green. You're almost there! Now up in the top again, click on the button labeled Start. Underneather the Stop and Restart buttons is a small panel that will show you the status of the SSH daemon. It should now say the service is enabled and the service is running. If it does, you are good to go! Now you just need to enable password less SSH access to each of the boxes from your main one. For a good walkthrough of setting this up, [look here](http://www.thegeekstuff.com/2008/11/3-steps-to-perform-ssh-login-without-password-using-ssh-keygen-ssh-copy-id/)","source":"_posts/set-up-a-hadoop-cluster-part-3.md","raw":"---\ntitle: Set up a Hadoop Cluster - Part 3\ntags:\n  - CDH\n  - Hadoop\n  - HortonWorks\nid: '1725'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-01-28 09:23:24\n---\n\n[![services](http://edpflager.com/wp-content/uploads/2014/01/services-300x160.png)](http://edpflager.com/wp-content/uploads/2014/01/services.png)[Third part of this series](http://edpflager.com/?p=1721 \"Set Up a Hadoop Cluster – Part 2\"), and we haven't even gotten to installing Hadoop yet! This is a quick one though. One of the things you need to make Hadoop work is to be able to use SSH across all of the servers in your cluster. To enable it, log into each box, and go to System in the menu, Administration from the menu and over to Services. Click on it.\n<!-- more -->\nThe Services window will open. In the left pane, scroll down until you find \"sshd\". If it has a red dot next to it, it means the ssh daemon is not running and you won't be able to connect to the box from your main Hadoop server. Click on the line, and then click on the Enable button at the top of the window. After a few seconds, the dot next to \"sshd\" will change to green. You're almost there! Now up in the top again, click on the button labeled Start. Underneather the Stop and Restart buttons is a small panel that will show you the status of the SSH daemon. It should now say the service is enabled and the service is running. If it does, you are good to go! Now you just need to enable password less SSH access to each of the boxes from your main one. For a good walkthrough of setting this up, [look here](http://www.thegeekstuff.com/2008/11/3-steps-to-perform-ssh-login-without-password-using-ssh-keygen-ssh-copy-id/)","slug":"set-up-a-hadoop-cluster-part-3","published":1,"updated":"2020-08-23T20:54:34.766Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a92009wsdjxbohpa8na","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/services-300x160.png\" alt=\"services\"></a><a href=\"http://edpflager.com/?p=1721\" title=\"Set Up a Hadoop Cluster – Part 2\">Third part of this series</a>, and we haven’t even gotten to installing Hadoop yet! This is a quick one though. One of the things you need to make Hadoop work is to be able to use SSH across all of the servers in your cluster. To enable it, log into each box, and go to System in the menu, Administration from the menu and over to Services. Click on it.</p>\n<a id=\"more\"></a>\n<p>The Services window will open. In the left pane, scroll down until you find “sshd”. If it has a red dot next to it, it means the ssh daemon is not running and you won’t be able to connect to the box from your main Hadoop server. Click on the line, and then click on the Enable button at the top of the window. After a few seconds, the dot next to “sshd” will change to green. You’re almost there! Now up in the top again, click on the button labeled Start. Underneather the Stop and Restart buttons is a small panel that will show you the status of the SSH daemon. It should now say the service is enabled and the service is running. If it does, you are good to go! Now you just need to enable password less SSH access to each of the boxes from your main one. For a good walkthrough of setting this up, <a href=\"http://www.thegeekstuff.com/2008/11/3-steps-to-perform-ssh-login-without-password-using-ssh-keygen-ssh-copy-id/\">look here</a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/services.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/services-300x160.png\" alt=\"services\"></a><a href=\"http://edpflager.com/?p=1721\" title=\"Set Up a Hadoop Cluster – Part 2\">Third part of this series</a>, and we haven’t even gotten to installing Hadoop yet! This is a quick one though. One of the things you need to make Hadoop work is to be able to use SSH across all of the servers in your cluster. To enable it, log into each box, and go to System in the menu, Administration from the menu and over to Services. Click on it.</p>","more":"<p>The Services window will open. In the left pane, scroll down until you find “sshd”. If it has a red dot next to it, it means the ssh daemon is not running and you won’t be able to connect to the box from your main Hadoop server. Click on the line, and then click on the Enable button at the top of the window. After a few seconds, the dot next to “sshd” will change to green. You’re almost there! Now up in the top again, click on the button labeled Start. Underneather the Stop and Restart buttons is a small panel that will show you the status of the SSH daemon. It should now say the service is enabled and the service is running. If it does, you are good to go! Now you just need to enable password less SSH access to each of the boxes from your main one. For a good walkthrough of setting this up, <a href=\"http://www.thegeekstuff.com/2008/11/3-steps-to-perform-ssh-login-without-password-using-ssh-keygen-ssh-copy-id/\">look here</a></p>"},{"title":"Set up a Hadoop Cluster - Part 4","id":"1746","comments":0,"date":"2014-02-01T17:21:54.000Z","_content":"\n[![apache-ambari-project](http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg)](http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg)This is part 4 of my series about setting up an inexpensive Hadoop cluster using refurbished desktop PCs as Linux servers. (see here for parts [1](http://edpflager.com/?p=1714 \"Set up a Hadoop Cluster – Part 1\"), [2](http://edpflager.com/?p=1721 \"Set Up a Hadoop Cluster – Part 2\") and [3](http://edpflager.com/?p=1725 \"Set up a Hadoop Cluster – Part 3\")). I installed HortonWorks HDP 2.0. As part of the installation, the main server also installs [Ambari](http://ambari.apache.org/) - an open source server application that is used to monitor and manage Hadoop clusters (hence my reason for adding a fourth server with more memory than the other three) and Nagios - a server monitoring application.\n<!-- more -->\nThe installation is pretty easy. Download the management application package and start it. You provide the server names or the IP addresses of the various servers, and the private SSH key that you set up earlier. Ambari attempts to connect to each of the servers, and installs the various components from the Hadoop ecosystem across your setup. It attempts to balance which components go on each server, providing redundancy and allows you to override the default configuration.\n\n#### **Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don't have this service enabled, you'll likely get an error in your Cloudera Manager console.** \n\nOnce it was complete, I attempted to start the services via Ambari. This part is not so intuitive, and I struggled here. Finally I found the Start All/Stop All buttons and used them. Unfortunately, some services would start OK and after a few minutes would shut down. Very frustrating! After struggling with it for a couple of hours, the only thing I could determine was possibly the servers needed more memory. The first server in the cluster had the bulk of the components installed on it (twenty), but only 2GB of RAM. Rather than go down that path, I have decided to remove HortonWorks and try Cloudera.","source":"_posts/set-up-a-hadoop-cluster-part-4.md","raw":"---\ntitle: Set up a Hadoop Cluster - Part 4\ntags:\n  - CDH\n  - Hadoop\n  - HortonWorks\nid: '1746'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-02-01 12:21:54\n---\n\n[![apache-ambari-project](http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg)](http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg)This is part 4 of my series about setting up an inexpensive Hadoop cluster using refurbished desktop PCs as Linux servers. (see here for parts [1](http://edpflager.com/?p=1714 \"Set up a Hadoop Cluster – Part 1\"), [2](http://edpflager.com/?p=1721 \"Set Up a Hadoop Cluster – Part 2\") and [3](http://edpflager.com/?p=1725 \"Set up a Hadoop Cluster – Part 3\")). I installed HortonWorks HDP 2.0. As part of the installation, the main server also installs [Ambari](http://ambari.apache.org/) - an open source server application that is used to monitor and manage Hadoop clusters (hence my reason for adding a fourth server with more memory than the other three) and Nagios - a server monitoring application.\n<!-- more -->\nThe installation is pretty easy. Download the management application package and start it. You provide the server names or the IP addresses of the various servers, and the private SSH key that you set up earlier. Ambari attempts to connect to each of the servers, and installs the various components from the Hadoop ecosystem across your setup. It attempts to balance which components go on each server, providing redundancy and allows you to override the default configuration.\n\n#### **Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don't have this service enabled, you'll likely get an error in your Cloudera Manager console.** \n\nOnce it was complete, I attempted to start the services via Ambari. This part is not so intuitive, and I struggled here. Finally I found the Start All/Stop All buttons and used them. Unfortunately, some services would start OK and after a few minutes would shut down. Very frustrating! After struggling with it for a couple of hours, the only thing I could determine was possibly the servers needed more memory. The first server in the cluster had the bulk of the components installed on it (twenty), but only 2GB of RAM. Rather than go down that path, I have decided to remove HortonWorks and try Cloudera.","slug":"set-up-a-hadoop-cluster-part-4","published":1,"updated":"2020-08-23T20:54:34.770Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9300a0sdjx8jzq6mav","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg\" alt=\"apache-ambari-project\"></a>This is part 4 of my series about setting up an inexpensive Hadoop cluster using refurbished desktop PCs as Linux servers. (see here for parts <a href=\"http://edpflager.com/?p=1714\" title=\"Set up a Hadoop Cluster – Part 1\">1</a>, <a href=\"http://edpflager.com/?p=1721\" title=\"Set Up a Hadoop Cluster – Part 2\">2</a> and <a href=\"http://edpflager.com/?p=1725\" title=\"Set up a Hadoop Cluster – Part 3\">3</a>). I installed HortonWorks HDP 2.0. As part of the installation, the main server also installs <a href=\"http://ambari.apache.org/\">Ambari</a> - an open source server application that is used to monitor and manage Hadoop clusters (hence my reason for adding a fourth server with more memory than the other three) and Nagios - a server monitoring application.</p>\n<a id=\"more\"></a>\n<p>The installation is pretty easy. Download the management application package and start it. You provide the server names or the IP addresses of the various servers, and the private SSH key that you set up earlier. Ambari attempts to connect to each of the servers, and installs the various components from the Hadoop ecosystem across your setup. It attempts to balance which components go on each server, providing redundancy and allows you to override the default configuration.</p>\n<h4 id=\"Note-As-part-of-the-installation-be-sure-to-turn-OFF-the-iptables-service-on-each-of-the-servers-before-you-start-and-turn-ON-the-NTP-service-If-your-cluster-will-be-for-test-purposes-you-are-probably-OK-with-leaving-iptables-disabled-The-NTP-service-on-each-of-your-nodes-allows-them-to-sync-date-and-time-across-your-cluster-which-is-very-important-If-you-don’t-have-this-service-enabled-you’ll-likely-get-an-error-in-your-Cloudera-Manager-console\"><a href=\"#Note-As-part-of-the-installation-be-sure-to-turn-OFF-the-iptables-service-on-each-of-the-servers-before-you-start-and-turn-ON-the-NTP-service-If-your-cluster-will-be-for-test-purposes-you-are-probably-OK-with-leaving-iptables-disabled-The-NTP-service-on-each-of-your-nodes-allows-them-to-sync-date-and-time-across-your-cluster-which-is-very-important-If-you-don’t-have-this-service-enabled-you’ll-likely-get-an-error-in-your-Cloudera-Manager-console\" class=\"headerlink\" title=\"Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don’t have this service enabled, you’ll likely get an error in your Cloudera Manager console.\"></a><strong>Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don’t have this service enabled, you’ll likely get an error in your Cloudera Manager console.</strong></h4><p>Once it was complete, I attempted to start the services via Ambari. This part is not so intuitive, and I struggled here. Finally I found the Start All/Stop All buttons and used them. Unfortunately, some services would start OK and after a few minutes would shut down. Very frustrating! After struggling with it for a couple of hours, the only thing I could determine was possibly the servers needed more memory. The first server in the cluster had the bulk of the components installed on it (twenty), but only 2GB of RAM. Rather than go down that path, I have decided to remove HortonWorks and try Cloudera.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/apache-ambari-project.jpg\" alt=\"apache-ambari-project\"></a>This is part 4 of my series about setting up an inexpensive Hadoop cluster using refurbished desktop PCs as Linux servers. (see here for parts <a href=\"http://edpflager.com/?p=1714\" title=\"Set up a Hadoop Cluster – Part 1\">1</a>, <a href=\"http://edpflager.com/?p=1721\" title=\"Set Up a Hadoop Cluster – Part 2\">2</a> and <a href=\"http://edpflager.com/?p=1725\" title=\"Set up a Hadoop Cluster – Part 3\">3</a>). I installed HortonWorks HDP 2.0. As part of the installation, the main server also installs <a href=\"http://ambari.apache.org/\">Ambari</a> - an open source server application that is used to monitor and manage Hadoop clusters (hence my reason for adding a fourth server with more memory than the other three) and Nagios - a server monitoring application.</p>","more":"<p>The installation is pretty easy. Download the management application package and start it. You provide the server names or the IP addresses of the various servers, and the private SSH key that you set up earlier. Ambari attempts to connect to each of the servers, and installs the various components from the Hadoop ecosystem across your setup. It attempts to balance which components go on each server, providing redundancy and allows you to override the default configuration.</p>\n<h4 id=\"Note-As-part-of-the-installation-be-sure-to-turn-OFF-the-iptables-service-on-each-of-the-servers-before-you-start-and-turn-ON-the-NTP-service-If-your-cluster-will-be-for-test-purposes-you-are-probably-OK-with-leaving-iptables-disabled-The-NTP-service-on-each-of-your-nodes-allows-them-to-sync-date-and-time-across-your-cluster-which-is-very-important-If-you-don’t-have-this-service-enabled-you’ll-likely-get-an-error-in-your-Cloudera-Manager-console\"><a href=\"#Note-As-part-of-the-installation-be-sure-to-turn-OFF-the-iptables-service-on-each-of-the-servers-before-you-start-and-turn-ON-the-NTP-service-If-your-cluster-will-be-for-test-purposes-you-are-probably-OK-with-leaving-iptables-disabled-The-NTP-service-on-each-of-your-nodes-allows-them-to-sync-date-and-time-across-your-cluster-which-is-very-important-If-you-don’t-have-this-service-enabled-you’ll-likely-get-an-error-in-your-Cloudera-Manager-console\" class=\"headerlink\" title=\"Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don’t have this service enabled, you’ll likely get an error in your Cloudera Manager console.\"></a><strong>Note: As part of the installation be sure to turn OFF the iptables service on each of the servers before you start and turn ON the NTP service. If your cluster will be for test purposes, you are probably OK with leaving iptables disabled. The NTP service on each of your nodes allows them to sync date and time across your cluster, which is very important. If you don’t have this service enabled, you’ll likely get an error in your Cloudera Manager console.</strong></h4><p>Once it was complete, I attempted to start the services via Ambari. This part is not so intuitive, and I struggled here. Finally I found the Start All/Stop All buttons and used them. Unfortunately, some services would start OK and after a few minutes would shut down. Very frustrating! After struggling with it for a couple of hours, the only thing I could determine was possibly the servers needed more memory. The first server in the cluster had the bulk of the components installed on it (twenty), but only 2GB of RAM. Rather than go down that path, I have decided to remove HortonWorks and try Cloudera.</p>"},{"title":"Set up a Hadoop Cluster – Part 5","id":"1769","comments":0,"date":"2014-02-07T20:15:15.000Z","_content":"\n[![ClouderaManager](http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager-300x183.png)](http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager.png)In [Part 4](http://edpflager.com/?p=1746 \"Set up a Hadoop Cluster – Part 4\") of this series, I talked about how I installed HortonWorks' Hadoop distribution on my small test cluster, and how I had some problems with the various services shutting down repeatedly. My initial feelings were that the server with Ambari and Nagios (along with 18 other services) possibly needed more memory, but because the Zotac ZBox I was using would only support 4GB, I was kind of stuck. I could have added more memory to the other boxes in the cluster (which have 2GB each), and reallocated the services, but I wanted to get a vanilla installation up and running reliably before I started tweaking it.\n<!-- more -->\nAfter some consideration I rebuilt the cluster, reinstalling the OS and patches, removing unwanted software and services. I could have [uninstalled](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/bk_installing_manually_book/content/rpm-chap15.html) HortonWorks using the directions from their website, but to ensure I had a clean setup to begin with, I opted for the rebuild. Once that was complete, I followed [Cloudera's instructions](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/Cloudera-Manager-Installation-Guide.html) for installing their CDH4 management tool, and then proceeded to install the software components for the cluster on the various boxes. As with Hortonworks distribution, the install was pretty straightforward, and pretty painless, just make sure you have iptables turned off, and also make sure SELinux is disabled on your boxes too. One nice feature was the ability to select different installation scenarios that included bundles of various Hadoop components. Unlike Hortonworks, which bundles Nagios in for network monitoring, Cloudera integrates their own monitoring tools into the management package. It seems to me to be a little more intuitive than HortonWorks, but that is a judgement call. If you are familiar with Nagios, then its probably a minimal difference. Cloudera uses two web interfaces - one for administration of your cluster (CDH Manager) and one for using the various components (HUE). The CDH Manager web interface starts up when your cluster starts up, but you still have to login to it and start the management service, and then start the various components. You can start them individually, or all of them as a whole (recommended). Either way, you'll see the progress of the start up graphically, and if any issues occur, just click on the service to find out more details. Everything was working right out of the box, except HBASE and Cloudera. I received error messages about the Thrift service not running. How that was resolved will be discussed in an upcoming entry. Other than that, my cluster was up and running!","source":"_posts/set-up-a-hadoop-cluster-part-5.md","raw":"---\ntitle: Set up a Hadoop Cluster – Part 5\ntags:\n  - CDH\n  - Hadoop\nid: '1769'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-02-07 15:15:15\n---\n\n[![ClouderaManager](http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager-300x183.png)](http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager.png)In [Part 4](http://edpflager.com/?p=1746 \"Set up a Hadoop Cluster – Part 4\") of this series, I talked about how I installed HortonWorks' Hadoop distribution on my small test cluster, and how I had some problems with the various services shutting down repeatedly. My initial feelings were that the server with Ambari and Nagios (along with 18 other services) possibly needed more memory, but because the Zotac ZBox I was using would only support 4GB, I was kind of stuck. I could have added more memory to the other boxes in the cluster (which have 2GB each), and reallocated the services, but I wanted to get a vanilla installation up and running reliably before I started tweaking it.\n<!-- more -->\nAfter some consideration I rebuilt the cluster, reinstalling the OS and patches, removing unwanted software and services. I could have [uninstalled](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/bk_installing_manually_book/content/rpm-chap15.html) HortonWorks using the directions from their website, but to ensure I had a clean setup to begin with, I opted for the rebuild. Once that was complete, I followed [Cloudera's instructions](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/Cloudera-Manager-Installation-Guide.html) for installing their CDH4 management tool, and then proceeded to install the software components for the cluster on the various boxes. As with Hortonworks distribution, the install was pretty straightforward, and pretty painless, just make sure you have iptables turned off, and also make sure SELinux is disabled on your boxes too. One nice feature was the ability to select different installation scenarios that included bundles of various Hadoop components. Unlike Hortonworks, which bundles Nagios in for network monitoring, Cloudera integrates their own monitoring tools into the management package. It seems to me to be a little more intuitive than HortonWorks, but that is a judgement call. If you are familiar with Nagios, then its probably a minimal difference. Cloudera uses two web interfaces - one for administration of your cluster (CDH Manager) and one for using the various components (HUE). The CDH Manager web interface starts up when your cluster starts up, but you still have to login to it and start the management service, and then start the various components. You can start them individually, or all of them as a whole (recommended). Either way, you'll see the progress of the start up graphically, and if any issues occur, just click on the service to find out more details. Everything was working right out of the box, except HBASE and Cloudera. I received error messages about the Thrift service not running. How that was resolved will be discussed in an upcoming entry. Other than that, my cluster was up and running!","slug":"set-up-a-hadoop-cluster-part-5","published":1,"updated":"2020-08-23T20:54:34.782Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9500a3sdjxh9s508o8","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager-300x183.png\" alt=\"ClouderaManager\"></a>In <a href=\"http://edpflager.com/?p=1746\" title=\"Set up a Hadoop Cluster – Part 4\">Part 4</a> of this series, I talked about how I installed HortonWorks’ Hadoop distribution on my small test cluster, and how I had some problems with the various services shutting down repeatedly. My initial feelings were that the server with Ambari and Nagios (along with 18 other services) possibly needed more memory, but because the Zotac ZBox I was using would only support 4GB, I was kind of stuck. I could have added more memory to the other boxes in the cluster (which have 2GB each), and reallocated the services, but I wanted to get a vanilla installation up and running reliably before I started tweaking it.</p>\n<a id=\"more\"></a>\n<p>After some consideration I rebuilt the cluster, reinstalling the OS and patches, removing unwanted software and services. I could have <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/bk_installing_manually_book/content/rpm-chap15.html\">uninstalled</a> HortonWorks using the directions from their website, but to ensure I had a clean setup to begin with, I opted for the rebuild. Once that was complete, I followed <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/Cloudera-Manager-Installation-Guide.html\">Cloudera’s instructions</a> for installing their CDH4 management tool, and then proceeded to install the software components for the cluster on the various boxes. As with Hortonworks distribution, the install was pretty straightforward, and pretty painless, just make sure you have iptables turned off, and also make sure SELinux is disabled on your boxes too. One nice feature was the ability to select different installation scenarios that included bundles of various Hadoop components. Unlike Hortonworks, which bundles Nagios in for network monitoring, Cloudera integrates their own monitoring tools into the management package. It seems to me to be a little more intuitive than HortonWorks, but that is a judgement call. If you are familiar with Nagios, then its probably a minimal difference. Cloudera uses two web interfaces - one for administration of your cluster (CDH Manager) and one for using the various components (HUE). The CDH Manager web interface starts up when your cluster starts up, but you still have to login to it and start the management service, and then start the various components. You can start them individually, or all of them as a whole (recommended). Either way, you’ll see the progress of the start up graphically, and if any issues occur, just click on the service to find out more details. Everything was working right out of the box, except HBASE and Cloudera. I received error messages about the Thrift service not running. How that was resolved will be discussed in an upcoming entry. Other than that, my cluster was up and running!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/ClouderaManager-300x183.png\" alt=\"ClouderaManager\"></a>In <a href=\"http://edpflager.com/?p=1746\" title=\"Set up a Hadoop Cluster – Part 4\">Part 4</a> of this series, I talked about how I installed HortonWorks’ Hadoop distribution on my small test cluster, and how I had some problems with the various services shutting down repeatedly. My initial feelings were that the server with Ambari and Nagios (along with 18 other services) possibly needed more memory, but because the Zotac ZBox I was using would only support 4GB, I was kind of stuck. I could have added more memory to the other boxes in the cluster (which have 2GB each), and reallocated the services, but I wanted to get a vanilla installation up and running reliably before I started tweaking it.</p>","more":"<p>After some consideration I rebuilt the cluster, reinstalling the OS and patches, removing unwanted software and services. I could have <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/bk_installing_manually_book/content/rpm-chap15.html\">uninstalled</a> HortonWorks using the directions from their website, but to ensure I had a clean setup to begin with, I opted for the rebuild. Once that was complete, I followed <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CM4Ent/latest/Cloudera-Manager-Installation-Guide/Cloudera-Manager-Installation-Guide.html\">Cloudera’s instructions</a> for installing their CDH4 management tool, and then proceeded to install the software components for the cluster on the various boxes. As with Hortonworks distribution, the install was pretty straightforward, and pretty painless, just make sure you have iptables turned off, and also make sure SELinux is disabled on your boxes too. One nice feature was the ability to select different installation scenarios that included bundles of various Hadoop components. Unlike Hortonworks, which bundles Nagios in for network monitoring, Cloudera integrates their own monitoring tools into the management package. It seems to me to be a little more intuitive than HortonWorks, but that is a judgement call. If you are familiar with Nagios, then its probably a minimal difference. Cloudera uses two web interfaces - one for administration of your cluster (CDH Manager) and one for using the various components (HUE). The CDH Manager web interface starts up when your cluster starts up, but you still have to login to it and start the management service, and then start the various components. You can start them individually, or all of them as a whole (recommended). Either way, you’ll see the progress of the start up graphically, and if any issues occur, just click on the service to find out more details. Everything was working right out of the box, except HBASE and Cloudera. I received error messages about the Thrift service not running. How that was resolved will be discussed in an upcoming entry. Other than that, my cluster was up and running!</p>"},{"title":"Updated: Set up a Kettle repository using MySQL","id":"1622","comments":0,"date":"2013-08-17T16:32:12.000Z","_content":"\n[![kettle](http://edpflager.com/wp-content/uploads/2013/08/kettle-260x300.jpg)](http://edpflager.com/wp-content/uploads/2013/08/kettle.jpg)When creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. Pentaho Data Integration (aka Kettle) is an open source ETL tool that has a repository feature, which allows you to store your transformations and jobs in local files, or in a central repository database. The file option is pretty easy to implement, so I won't cover it here. Because of my work experience, I prefer to use a database server based repository. Unfortunately, the documentation for setting up a DB repository is sorely lacking (a common problem with a lot of open source projects). After some experimenting, I did figure out how to create a MySQL based repository, and how to connect to it from a Linux based installation of PDI. Here is a walk through of the process:\n<!-- more -->\nPrerequisites:\n\n*   Make sure you have MySQL installed and you can access it from your workstation.\n*   Create (or have your sysadmin create) an empty database, and create a user account with permissions on the database to add/delete/etc.\n*   On the Linux machine where you have Pentaho installed, download the MySQL Connector/J driver. ([http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)). Extract it and locate the mysql-connector-java-XXXX-bin.jar file. Copy that file to the pentahodata-integrationlibextJDBC subfolder.\n*   **Update: In Pentaho 5, the location for the JAR file has moved. Its should now be copied into pentahodata-integrationlib.**\n\nCreate a Kettle(PDI) repository Start PDI on your system. When it first loads up, you will see a Repository Connection screen. Locate the small green circle with the plus sign, and click on it to add a new repository. [![add_repository](http://edpflager.com/wp-content/uploads/2013/08/add_repository-300x188.png)](http://edpflager.com/wp-content/uploads/2013/08/add_repository.png) In the Select the repository type window, select the first option: Kettle database repository, and then click OK. ![repository_type2](http://edpflager.com/wp-content/uploads/2013/08/repository_type2-300x93.png) For the Repository information window, click the NEW button, to create a new connection. ![repository_dbconnection](http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection-300x87.png) You'll see the Database Connection window now. This is a pretty busy window, but we are almost done: On the General tab, enter a Connection Name. I use \"pentaho\". Scroll through the Connection Type panel and click on MySQL (if you are using MariaDB, use MySQL as well). In the Access panel, choose Native(JDBC). In the settings panel, enter the host name or IP address of your database server. If you are running it on the same machine you have PDI installed on, just enter localhost. Enter the database name that was created as part of the requirements section ( I used pentaho). Leave the port set to 3306, unless your sysadmin has changed the default MySQL port. Enter the User Name and Password set up as part of the requirements section. Click OK. ![repository_dbconnection2](http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection2-300x253.png) You'll be back to the Repository Information window now. Enter an ID and a Name for your connection in the appropriate box. Finally, click on the Create or Upgrade button. At the ARE YOU SURE? prompt, click on the YES button. At the Dry Run prompt, you can click the NO prompt unless you are curious about the SQL that PDI will generate to create the repository tables. ![DryRun](http://edpflager.com/wp-content/uploads/2013/08/DryRun-300x143.png) A progress window will appear for a few seconds, and when it finishes a message will appear on the screen letting you know. ![created repository](http://edpflager.com/wp-content/uploads/2013/08/created-repository-300x105.png) Click OK to return to the Repository Information window again. Click OK to go back to the Repository Connection window. You new connection should be here. Click the name of your repository, and enter the password for the admin account. (It's 'admin'). PDI will finish loading, and you'll be connected to your repository where your transformations and jobs will be saved.","source":"_posts/set-up-a-kettle-repository-using-mysql.md","raw":"---\ntitle: 'Updated: Set up a Kettle repository using MySQL'\ntags:\n  - ETL\n  - kettle\n  - PDI\nid: '1622'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2013-08-17 12:32:12\n---\n\n[![kettle](http://edpflager.com/wp-content/uploads/2013/08/kettle-260x300.jpg)](http://edpflager.com/wp-content/uploads/2013/08/kettle.jpg)When creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. Pentaho Data Integration (aka Kettle) is an open source ETL tool that has a repository feature, which allows you to store your transformations and jobs in local files, or in a central repository database. The file option is pretty easy to implement, so I won't cover it here. Because of my work experience, I prefer to use a database server based repository. Unfortunately, the documentation for setting up a DB repository is sorely lacking (a common problem with a lot of open source projects). After some experimenting, I did figure out how to create a MySQL based repository, and how to connect to it from a Linux based installation of PDI. Here is a walk through of the process:\n<!-- more -->\nPrerequisites:\n\n*   Make sure you have MySQL installed and you can access it from your workstation.\n*   Create (or have your sysadmin create) an empty database, and create a user account with permissions on the database to add/delete/etc.\n*   On the Linux machine where you have Pentaho installed, download the MySQL Connector/J driver. ([http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)). Extract it and locate the mysql-connector-java-XXXX-bin.jar file. Copy that file to the pentahodata-integrationlibextJDBC subfolder.\n*   **Update: In Pentaho 5, the location for the JAR file has moved. Its should now be copied into pentahodata-integrationlib.**\n\nCreate a Kettle(PDI) repository Start PDI on your system. When it first loads up, you will see a Repository Connection screen. Locate the small green circle with the plus sign, and click on it to add a new repository. [![add_repository](http://edpflager.com/wp-content/uploads/2013/08/add_repository-300x188.png)](http://edpflager.com/wp-content/uploads/2013/08/add_repository.png) In the Select the repository type window, select the first option: Kettle database repository, and then click OK. ![repository_type2](http://edpflager.com/wp-content/uploads/2013/08/repository_type2-300x93.png) For the Repository information window, click the NEW button, to create a new connection. ![repository_dbconnection](http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection-300x87.png) You'll see the Database Connection window now. This is a pretty busy window, but we are almost done: On the General tab, enter a Connection Name. I use \"pentaho\". Scroll through the Connection Type panel and click on MySQL (if you are using MariaDB, use MySQL as well). In the Access panel, choose Native(JDBC). In the settings panel, enter the host name or IP address of your database server. If you are running it on the same machine you have PDI installed on, just enter localhost. Enter the database name that was created as part of the requirements section ( I used pentaho). Leave the port set to 3306, unless your sysadmin has changed the default MySQL port. Enter the User Name and Password set up as part of the requirements section. Click OK. ![repository_dbconnection2](http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection2-300x253.png) You'll be back to the Repository Information window now. Enter an ID and a Name for your connection in the appropriate box. Finally, click on the Create or Upgrade button. At the ARE YOU SURE? prompt, click on the YES button. At the Dry Run prompt, you can click the NO prompt unless you are curious about the SQL that PDI will generate to create the repository tables. ![DryRun](http://edpflager.com/wp-content/uploads/2013/08/DryRun-300x143.png) A progress window will appear for a few seconds, and when it finishes a message will appear on the screen letting you know. ![created repository](http://edpflager.com/wp-content/uploads/2013/08/created-repository-300x105.png) Click OK to return to the Repository Information window again. Click OK to go back to the Repository Connection window. You new connection should be here. Click the name of your repository, and enter the password for the admin account. (It's 'admin'). PDI will finish loading, and you'll be connected to your repository where your transformations and jobs will be saved.","slug":"set-up-a-kettle-repository-using-mysql","published":1,"updated":"2020-08-23T20:54:34.746Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9700a7sdjxgfjregzi","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/kettle.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/kettle-260x300.jpg\" alt=\"kettle\"></a>When creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. Pentaho Data Integration (aka Kettle) is an open source ETL tool that has a repository feature, which allows you to store your transformations and jobs in local files, or in a central repository database. The file option is pretty easy to implement, so I won’t cover it here. Because of my work experience, I prefer to use a database server based repository. Unfortunately, the documentation for setting up a DB repository is sorely lacking (a common problem with a lot of open source projects). After some experimenting, I did figure out how to create a MySQL based repository, and how to connect to it from a Linux based installation of PDI. Here is a walk through of the process:</p>\n<a id=\"more\"></a>\n<p>Prerequisites:</p>\n<ul>\n<li>Make sure you have MySQL installed and you can access it from your workstation.</li>\n<li>Create (or have your sysadmin create) an empty database, and create a user account with permissions on the database to add/delete/etc.</li>\n<li>On the Linux machine where you have Pentaho installed, download the MySQL Connector/J driver. (<a href=\"http://dev.mysql.com/downloads/connector/j/\">http://dev.mysql.com/downloads/connector/j/</a>). Extract it and locate the mysql-connector-java-XXXX-bin.jar file. Copy that file to the pentahodata-integrationlibextJDBC subfolder.</li>\n<li><strong>Update: In Pentaho 5, the location for the JAR file has moved. Its should now be copied into pentahodata-integrationlib.</strong></li>\n</ul>\n<p>Create a Kettle(PDI) repository Start PDI on your system. When it first loads up, you will see a Repository Connection screen. Locate the small green circle with the plus sign, and click on it to add a new repository. <a href=\"http://edpflager.com/wp-content/uploads/2013/08/add_repository.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/add_repository-300x188.png\" alt=\"add_repository\"></a> In the Select the repository type window, select the first option: Kettle database repository, and then click OK. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_type2-300x93.png\" alt=\"repository_type2\"> For the Repository information window, click the NEW button, to create a new connection. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection-300x87.png\" alt=\"repository_dbconnection\"> You’ll see the Database Connection window now. This is a pretty busy window, but we are almost done: On the General tab, enter a Connection Name. I use “pentaho”. Scroll through the Connection Type panel and click on MySQL (if you are using MariaDB, use MySQL as well). In the Access panel, choose Native(JDBC). In the settings panel, enter the host name or IP address of your database server. If you are running it on the same machine you have PDI installed on, just enter localhost. Enter the database name that was created as part of the requirements section ( I used pentaho). Leave the port set to 3306, unless your sysadmin has changed the default MySQL port. Enter the User Name and Password set up as part of the requirements section. Click OK. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection2-300x253.png\" alt=\"repository_dbconnection2\"> You’ll be back to the Repository Information window now. Enter an ID and a Name for your connection in the appropriate box. Finally, click on the Create or Upgrade button. At the ARE YOU SURE? prompt, click on the YES button. At the Dry Run prompt, you can click the NO prompt unless you are curious about the SQL that PDI will generate to create the repository tables. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/DryRun-300x143.png\" alt=\"DryRun\"> A progress window will appear for a few seconds, and when it finishes a message will appear on the screen letting you know. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/created-repository-300x105.png\" alt=\"created repository\"> Click OK to return to the Repository Information window again. Click OK to go back to the Repository Connection window. You new connection should be here. Click the name of your repository, and enter the password for the admin account. (It’s ‘admin’). PDI will finish loading, and you’ll be connected to your repository where your transformations and jobs will be saved.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/kettle.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/kettle-260x300.jpg\" alt=\"kettle\"></a>When creating ETL workflows, its useful to store the information in a database repository, rather than as individual files on your workstation. This allows multiple users to have access to the information (why recreate the wheel?),  it allows you to pull it into your jobs quickly and easily, and you can back it up quickly and restore it if necessary. Pentaho Data Integration (aka Kettle) is an open source ETL tool that has a repository feature, which allows you to store your transformations and jobs in local files, or in a central repository database. The file option is pretty easy to implement, so I won’t cover it here. Because of my work experience, I prefer to use a database server based repository. Unfortunately, the documentation for setting up a DB repository is sorely lacking (a common problem with a lot of open source projects). After some experimenting, I did figure out how to create a MySQL based repository, and how to connect to it from a Linux based installation of PDI. Here is a walk through of the process:</p>","more":"<p>Prerequisites:</p>\n<ul>\n<li>Make sure you have MySQL installed and you can access it from your workstation.</li>\n<li>Create (or have your sysadmin create) an empty database, and create a user account with permissions on the database to add/delete/etc.</li>\n<li>On the Linux machine where you have Pentaho installed, download the MySQL Connector/J driver. (<a href=\"http://dev.mysql.com/downloads/connector/j/\">http://dev.mysql.com/downloads/connector/j/</a>). Extract it and locate the mysql-connector-java-XXXX-bin.jar file. Copy that file to the pentahodata-integrationlibextJDBC subfolder.</li>\n<li><strong>Update: In Pentaho 5, the location for the JAR file has moved. Its should now be copied into pentahodata-integrationlib.</strong></li>\n</ul>\n<p>Create a Kettle(PDI) repository Start PDI on your system. When it first loads up, you will see a Repository Connection screen. Locate the small green circle with the plus sign, and click on it to add a new repository. <a href=\"http://edpflager.com/wp-content/uploads/2013/08/add_repository.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/add_repository-300x188.png\" alt=\"add_repository\"></a> In the Select the repository type window, select the first option: Kettle database repository, and then click OK. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_type2-300x93.png\" alt=\"repository_type2\"> For the Repository information window, click the NEW button, to create a new connection. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection-300x87.png\" alt=\"repository_dbconnection\"> You’ll see the Database Connection window now. This is a pretty busy window, but we are almost done: On the General tab, enter a Connection Name. I use “pentaho”. Scroll through the Connection Type panel and click on MySQL (if you are using MariaDB, use MySQL as well). In the Access panel, choose Native(JDBC). In the settings panel, enter the host name or IP address of your database server. If you are running it on the same machine you have PDI installed on, just enter localhost. Enter the database name that was created as part of the requirements section ( I used pentaho). Leave the port set to 3306, unless your sysadmin has changed the default MySQL port. Enter the User Name and Password set up as part of the requirements section. Click OK. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/repository_dbconnection2-300x253.png\" alt=\"repository_dbconnection2\"> You’ll be back to the Repository Information window now. Enter an ID and a Name for your connection in the appropriate box. Finally, click on the Create or Upgrade button. At the ARE YOU SURE? prompt, click on the YES button. At the Dry Run prompt, you can click the NO prompt unless you are curious about the SQL that PDI will generate to create the repository tables. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/DryRun-300x143.png\" alt=\"DryRun\"> A progress window will appear for a few seconds, and when it finishes a message will appear on the screen letting you know. <img src=\"http://edpflager.com/wp-content/uploads/2013/08/created-repository-300x105.png\" alt=\"created repository\"> Click OK to return to the Repository Information window again. Click OK to go back to the Repository Connection window. You new connection should be here. Click the name of your repository, and enter the password for the admin account. (It’s ‘admin’). PDI will finish loading, and you’ll be connected to your repository where your transformations and jobs will be saved.</p>"},{"title":"Set up a Hadoop Cluster - Part 1","id":"1714","comments":0,"date":"2014-01-22T20:10:05.000Z","_content":"\n[![cluster1](http://edpflager.com/wp-content/uploads/2014/01/cluster1.png)](http://edpflager.com/wp-content/uploads/2014/01/cluster1.png)I have posted a few times about various aspects of my experimenting with Hadoop and other Big Data platforms. Between Thanksgiving and Christmas, I even maxed out the memory of my desktop PC (a Mac Mini) so I could run multiple virtual machines to act as a Hadoop cluster. It worked, but performance was poor. Since then, I've been looking into getting a small number of cheap PCs to setup as a physical cluster.\n<!-- more -->\nAbout two weeks ago, I found a [good deal](http://www.newegg.com/Product/Product.aspx?Item=N82E16883250715) on refurbished HP desktops with a 2.0 GHz dual core 64 bit processor.  The memory wasn't stellar at 2 GB each (up-gradable to  8 GB, if necessary), and an 80 GB hard drive (again up-gradable if necessary) but for a small cluster it should be doable. Preloaded with Windows 7 Home Premium 64,  I planned on using CentOS 6.5 as my operating system. (With Ubuntu as a fallback). They showed up last week, and in my free time I've been working to get the initial configuration loaded. I have had a few issues but nothing earth shattering:\n\n*   Only one wanted to boot from CD/DVD. It may have been a BIOS setting, but I got around that by [making a bootable USB drive](http://unix.stackexchange.com/questions/81858/run-centos-6-from-a-usb-flash-drive) from the CentOS LIVE DVD.\n*   Using the live DVD installs both KDE and GNOME desktop as well as a lot of other apps that you aren't likely to need. You can remove them pretty quickly using the system tools and it isn't difficult.\n*   I found removing Evolution and Postfix caused the system to hand at boot-up, so I've left those installed, just disabled. Something in one of those packages seems to cause issues.\n\nYesterday, I finished the OS install and got the boxes hooked up to a [cheap 4 port KVM](http://www.amazon.com/gp/product/B001S2PJO6/ref=oh_details_o01_s00_i00?ie=UTF8&psc=1), a wireless keyboard (Target was clearing them out for $6 - cash) and mouse. I picked up a small wire shelving unit from Home Depot for less than $20, and using a bunch of cable ties, got the cabling under control. For a monitor, I had a small TV that also have a VGA input so no need to buy a monitor. Since most work with Hadoop is done via a web interface, I can access the system from my regular desktop once I'm finished. So that's where I'm at. Next steps are to get the software prerequisites in place, and then start installing Hadoop. Stay tuned for more details!","source":"_posts/setting-up-a-hadoop-cluster-part-1.md","raw":"---\ntitle: Set up a Hadoop Cluster - Part 1\ntags:\n  - CDH\n  - Hadoop\n  - HortonWorks\nid: '1714'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-01-22 15:10:05\n---\n\n[![cluster1](http://edpflager.com/wp-content/uploads/2014/01/cluster1.png)](http://edpflager.com/wp-content/uploads/2014/01/cluster1.png)I have posted a few times about various aspects of my experimenting with Hadoop and other Big Data platforms. Between Thanksgiving and Christmas, I even maxed out the memory of my desktop PC (a Mac Mini) so I could run multiple virtual machines to act as a Hadoop cluster. It worked, but performance was poor. Since then, I've been looking into getting a small number of cheap PCs to setup as a physical cluster.\n<!-- more -->\nAbout two weeks ago, I found a [good deal](http://www.newegg.com/Product/Product.aspx?Item=N82E16883250715) on refurbished HP desktops with a 2.0 GHz dual core 64 bit processor.  The memory wasn't stellar at 2 GB each (up-gradable to  8 GB, if necessary), and an 80 GB hard drive (again up-gradable if necessary) but for a small cluster it should be doable. Preloaded with Windows 7 Home Premium 64,  I planned on using CentOS 6.5 as my operating system. (With Ubuntu as a fallback). They showed up last week, and in my free time I've been working to get the initial configuration loaded. I have had a few issues but nothing earth shattering:\n\n*   Only one wanted to boot from CD/DVD. It may have been a BIOS setting, but I got around that by [making a bootable USB drive](http://unix.stackexchange.com/questions/81858/run-centos-6-from-a-usb-flash-drive) from the CentOS LIVE DVD.\n*   Using the live DVD installs both KDE and GNOME desktop as well as a lot of other apps that you aren't likely to need. You can remove them pretty quickly using the system tools and it isn't difficult.\n*   I found removing Evolution and Postfix caused the system to hand at boot-up, so I've left those installed, just disabled. Something in one of those packages seems to cause issues.\n\nYesterday, I finished the OS install and got the boxes hooked up to a [cheap 4 port KVM](http://www.amazon.com/gp/product/B001S2PJO6/ref=oh_details_o01_s00_i00?ie=UTF8&psc=1), a wireless keyboard (Target was clearing them out for $6 - cash) and mouse. I picked up a small wire shelving unit from Home Depot for less than $20, and using a bunch of cable ties, got the cabling under control. For a monitor, I had a small TV that also have a VGA input so no need to buy a monitor. Since most work with Hadoop is done via a web interface, I can access the system from my regular desktop once I'm finished. So that's where I'm at. Next steps are to get the software prerequisites in place, and then start installing Hadoop. Stay tuned for more details!","slug":"setting-up-a-hadoop-cluster-part-1","published":1,"updated":"2020-08-23T20:54:34.762Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9900aasdjxfj9v52cs","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/cluster1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/cluster1.png\" alt=\"cluster1\"></a>I have posted a few times about various aspects of my experimenting with Hadoop and other Big Data platforms. Between Thanksgiving and Christmas, I even maxed out the memory of my desktop PC (a Mac Mini) so I could run multiple virtual machines to act as a Hadoop cluster. It worked, but performance was poor. Since then, I’ve been looking into getting a small number of cheap PCs to setup as a physical cluster.</p>\n<a id=\"more\"></a>\n<p>About two weeks ago, I found a <a href=\"http://www.newegg.com/Product/Product.aspx?Item=N82E16883250715\">good deal</a> on refurbished HP desktops with a 2.0 GHz dual core 64 bit processor.  The memory wasn’t stellar at 2 GB each (up-gradable to  8 GB, if necessary), and an 80 GB hard drive (again up-gradable if necessary) but for a small cluster it should be doable. Preloaded with Windows 7 Home Premium 64,  I planned on using CentOS 6.5 as my operating system. (With Ubuntu as a fallback). They showed up last week, and in my free time I’ve been working to get the initial configuration loaded. I have had a few issues but nothing earth shattering:</p>\n<ul>\n<li>Only one wanted to boot from CD/DVD. It may have been a BIOS setting, but I got around that by <a href=\"http://unix.stackexchange.com/questions/81858/run-centos-6-from-a-usb-flash-drive\">making a bootable USB drive</a> from the CentOS LIVE DVD.</li>\n<li>Using the live DVD installs both KDE and GNOME desktop as well as a lot of other apps that you aren’t likely to need. You can remove them pretty quickly using the system tools and it isn’t difficult.</li>\n<li>I found removing Evolution and Postfix caused the system to hand at boot-up, so I’ve left those installed, just disabled. Something in one of those packages seems to cause issues.</li>\n</ul>\n<p>Yesterday, I finished the OS install and got the boxes hooked up to a <a href=\"http://www.amazon.com/gp/product/B001S2PJO6/ref=oh_details_o01_s00_i00?ie=UTF8&psc=1\">cheap 4 port KVM</a>, a wireless keyboard (Target was clearing them out for $6 - cash) and mouse. I picked up a small wire shelving unit from Home Depot for less than $20, and using a bunch of cable ties, got the cabling under control. For a monitor, I had a small TV that also have a VGA input so no need to buy a monitor. Since most work with Hadoop is done via a web interface, I can access the system from my regular desktop once I’m finished. So that’s where I’m at. Next steps are to get the software prerequisites in place, and then start installing Hadoop. Stay tuned for more details!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/01/cluster1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/01/cluster1.png\" alt=\"cluster1\"></a>I have posted a few times about various aspects of my experimenting with Hadoop and other Big Data platforms. Between Thanksgiving and Christmas, I even maxed out the memory of my desktop PC (a Mac Mini) so I could run multiple virtual machines to act as a Hadoop cluster. It worked, but performance was poor. Since then, I’ve been looking into getting a small number of cheap PCs to setup as a physical cluster.</p>","more":"<p>About two weeks ago, I found a <a href=\"http://www.newegg.com/Product/Product.aspx?Item=N82E16883250715\">good deal</a> on refurbished HP desktops with a 2.0 GHz dual core 64 bit processor.  The memory wasn’t stellar at 2 GB each (up-gradable to  8 GB, if necessary), and an 80 GB hard drive (again up-gradable if necessary) but for a small cluster it should be doable. Preloaded with Windows 7 Home Premium 64,  I planned on using CentOS 6.5 as my operating system. (With Ubuntu as a fallback). They showed up last week, and in my free time I’ve been working to get the initial configuration loaded. I have had a few issues but nothing earth shattering:</p>\n<ul>\n<li>Only one wanted to boot from CD/DVD. It may have been a BIOS setting, but I got around that by <a href=\"http://unix.stackexchange.com/questions/81858/run-centos-6-from-a-usb-flash-drive\">making a bootable USB drive</a> from the CentOS LIVE DVD.</li>\n<li>Using the live DVD installs both KDE and GNOME desktop as well as a lot of other apps that you aren’t likely to need. You can remove them pretty quickly using the system tools and it isn’t difficult.</li>\n<li>I found removing Evolution and Postfix caused the system to hand at boot-up, so I’ve left those installed, just disabled. Something in one of those packages seems to cause issues.</li>\n</ul>\n<p>Yesterday, I finished the OS install and got the boxes hooked up to a <a href=\"http://www.amazon.com/gp/product/B001S2PJO6/ref=oh_details_o01_s00_i00?ie=UTF8&psc=1\">cheap 4 port KVM</a>, a wireless keyboard (Target was clearing them out for $6 - cash) and mouse. I picked up a small wire shelving unit from Home Depot for less than $20, and using a bunch of cable ties, got the cabling under control. For a monitor, I had a small TV that also have a VGA input so no need to buy a monitor. Since most work with Hadoop is done via a web interface, I can access the system from my regular desktop once I’m finished. So that’s where I’m at. Next steps are to get the software prerequisites in place, and then start installing Hadoop. Stay tuned for more details!</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE - Part 1","id":"1945","comments":0,"date":"2014-04-14T19:15:31.000Z","_content":"\n[![Hadoop](http://edpflager.com/wp-content/uploads/2014/04/Hadoop-300x191.png)](http://edpflager.com/wp-content/uploads/2014/04/Hadoop.png) I've been awaiting the open source introduction of Cloudera's Hadoop distribution, CDH 5,  to try installing a pseudo-distributed cluster using CDH, with the HUE GUI interface. (If you are not familiar with the terminology, pseudo-distributed mode allows you to run Hadoop on one machine, with the various daemons each running in a separate JVM.) By setting up a pseudo-distributed cluster, I could free up two other machines for other projects I'm working on. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\nI've managed to install a vanilla version of Hadoop with Hive previously, but I couldn't get Cloudera's distribution to work with the extra bells and whistles of HUE, HBase and Impala. With the introduction of a new version, it seems like a good time to try it. The good news is, I have it working successfully, on two different machines! Yeh! The bad news, its not a simple process, but it is doable if you have a few hours to spare. The biggest drawback for me was trying to navigate Cloudera's website to get the information I needed to make this work. (Having worked in IT for more than a dozen years, I'm used to challenging documentation.) Hopefully the instructions I will be providing here will make it easier for others to setup as well. This series is based on the Cloudera documentation, but I've modified it to be easier to follow for setting up a pseudo cluster. Some may ask, \"What's the point?\" Cloudera offers a Sandbox VM of their distribution that you can download and play with (although at this writing it has not been updated to CDH5) so why install it yourself. My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I am providing no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures! With that said, here is part one.\n\n### **Prepare your single node for Hadoop**\n\n1.  Install a CentOS 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.\n2.  Remove any unneeded applications and install any system updates.\n3.  Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.\n4.  Disable iptables and iptablesv6 from the Service Configuration window.\n5.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") and set it to disabled. Restart your server for all the config changes to take effect.\n6.  Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for \"OpenJDK\". Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.\n7.  Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   [http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html)\n8.  Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**\n9.  Once the installation completes repeat step 6, and it should report that java version 1.7.0\\_45 is installed.\n10.  You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**\n11.  On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.\n12.  Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.\n13.  Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.\n14.  Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.\n15.  Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.\n16.  While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**\n17.  Edit the **/etc/sysconfig/network** file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.\n18.  Still as root in the terminal window, edit the root .profile file (~/.bash\\_profile) and add these two lines to the end of the file, after \"export PATH\" line: **         export JAVA\\_HOME=/usr/java/latest** **         export PATH=$JAVA\\_HOME/bin:$PATH**\n19.  After you save the file, exit the root user account by typing Exit in the terminal.\n20.  With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java\\_home value is set correctly. Type: **        echo $JAVA\\_HOME**\n21.  The system should respond with: **/usr/java/latest**\n\nStop and take a breath. Get a drink. At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.\n\n### **Install CDH5**\n\n1.  Open a web browser and download the CDH 5 package for Red Hat from this link: [http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86\\_64/cloudera-cdh-5-0.x86\\_64.rpm](http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm)\n2.  In the terminal window navigate to where the RPM file was downloaded. and enter this command (that's two dashes after YUM): **        yum --nogpgcheck localinstall cloudera-cdh-5-0.x86\\_64.rpm** The CDH5 quick start package will be installed.\n3.  Now add the Cloudera repository GPG key (that's two dashes after RPM): **       rpm --import  [http://archive.cloudera.com/cdh5/redhat/6/x86\\_64/cdh/RPM-GPG-KEY-cloudera](http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera)**\n4.  And finally install Hadoop with MRv1 with this command: **       yum install hadoop-0.20-conf-pseudo** (At present 16 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)\n5.  Verify that Hadoop is installed correctly. At the command prompt enter                       **rpm -ql hadoop-0.20-conf-pseudo** and you should see a list of hadoop folders. No errors? All good.\n6.  Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.\n7.  With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **     for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x start ; done**  (Watch the angle of the quotes - I have found that using normal single quotes errors out).\n8.  Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).\n9.  Now you need to create a temp folder in HDFS and set its permissions. This is extremely important, as a number of components really on this folder to work properly. With the terminal window open, enter: **sudo -u hdfs hadoop fs -mkdir -p /tmp** **sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.\n10.  Next, before you start MapReduce, you need to create its system directories. Enter these three commands one after another in the terminal: ** **sudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging**** ****sudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging****        **sudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred**\n11.  Once that is complete, you can start MapReduce with this command (again be careful of the quotes): **    for x in \\`cd /etc/init.d ; ls hadoop-0.20-mapreduce-\\*\\` ; do sudo service $x start ; done** If all goes well, you'll see messages indicating Hadoop jobtracker and Hadoop tasktracker have started OK.\n12.  Open a web browser and point it to your your system name and port 50030. So for example, if your system is called HadoopTest,  you would enter http://HadoopTest:50030 in your browser. You should see a website that shows the condition of your MapReduce system.\n13.  Finally, create a home directory for yourself in the HDFS file system, and change the owner to yourself as well: **sudo -u hdfs hadoop fs -mkdir -p /user/<your user name>** **sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name>** \n\nIf you've reached this point, you should now have a running Hadoop installation in pseudo-cluster mode. From a command line, you can access the Hadoop interface, and submit mapreduce jobs. (Limited by your RAM and hard drive space). Next time, we'll start the installation of the various GUI tools to allow you to interact with your system via a web browser.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-1.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE - Part 1\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - HUE\nid: '1945'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-04-14 15:15:31\n---\n\n[![Hadoop](http://edpflager.com/wp-content/uploads/2014/04/Hadoop-300x191.png)](http://edpflager.com/wp-content/uploads/2014/04/Hadoop.png) I've been awaiting the open source introduction of Cloudera's Hadoop distribution, CDH 5,  to try installing a pseudo-distributed cluster using CDH, with the HUE GUI interface. (If you are not familiar with the terminology, pseudo-distributed mode allows you to run Hadoop on one machine, with the various daemons each running in a separate JVM.) By setting up a pseudo-distributed cluster, I could free up two other machines for other projects I'm working on. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\nI've managed to install a vanilla version of Hadoop with Hive previously, but I couldn't get Cloudera's distribution to work with the extra bells and whistles of HUE, HBase and Impala. With the introduction of a new version, it seems like a good time to try it. The good news is, I have it working successfully, on two different machines! Yeh! The bad news, its not a simple process, but it is doable if you have a few hours to spare. The biggest drawback for me was trying to navigate Cloudera's website to get the information I needed to make this work. (Having worked in IT for more than a dozen years, I'm used to challenging documentation.) Hopefully the instructions I will be providing here will make it easier for others to setup as well. This series is based on the Cloudera documentation, but I've modified it to be easier to follow for setting up a pseudo cluster. Some may ask, \"What's the point?\" Cloudera offers a Sandbox VM of their distribution that you can download and play with (although at this writing it has not been updated to CDH5) so why install it yourself. My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I am providing no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures! With that said, here is part one.\n\n### **Prepare your single node for Hadoop**\n\n1.  Install a CentOS 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.\n2.  Remove any unneeded applications and install any system updates.\n3.  Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.\n4.  Disable iptables and iptablesv6 from the Service Configuration window.\n5.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") and set it to disabled. Restart your server for all the config changes to take effect.\n6.  Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for \"OpenJDK\". Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.\n7.  Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   [http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html)\n8.  Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**\n9.  Once the installation completes repeat step 6, and it should report that java version 1.7.0\\_45 is installed.\n10.  You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**\n11.  On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.\n12.  Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.\n13.  Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.\n14.  Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.\n15.  Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.\n16.  While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**\n17.  Edit the **/etc/sysconfig/network** file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.\n18.  Still as root in the terminal window, edit the root .profile file (~/.bash\\_profile) and add these two lines to the end of the file, after \"export PATH\" line: **         export JAVA\\_HOME=/usr/java/latest** **         export PATH=$JAVA\\_HOME/bin:$PATH**\n19.  After you save the file, exit the root user account by typing Exit in the terminal.\n20.  With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java\\_home value is set correctly. Type: **        echo $JAVA\\_HOME**\n21.  The system should respond with: **/usr/java/latest**\n\nStop and take a breath. Get a drink. At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.\n\n### **Install CDH5**\n\n1.  Open a web browser and download the CDH 5 package for Red Hat from this link: [http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86\\_64/cloudera-cdh-5-0.x86\\_64.rpm](http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm)\n2.  In the terminal window navigate to where the RPM file was downloaded. and enter this command (that's two dashes after YUM): **        yum --nogpgcheck localinstall cloudera-cdh-5-0.x86\\_64.rpm** The CDH5 quick start package will be installed.\n3.  Now add the Cloudera repository GPG key (that's two dashes after RPM): **       rpm --import  [http://archive.cloudera.com/cdh5/redhat/6/x86\\_64/cdh/RPM-GPG-KEY-cloudera](http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera)**\n4.  And finally install Hadoop with MRv1 with this command: **       yum install hadoop-0.20-conf-pseudo** (At present 16 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)\n5.  Verify that Hadoop is installed correctly. At the command prompt enter                       **rpm -ql hadoop-0.20-conf-pseudo** and you should see a list of hadoop folders. No errors? All good.\n6.  Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.\n7.  With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **     for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x start ; done**  (Watch the angle of the quotes - I have found that using normal single quotes errors out).\n8.  Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).\n9.  Now you need to create a temp folder in HDFS and set its permissions. This is extremely important, as a number of components really on this folder to work properly. With the terminal window open, enter: **sudo -u hdfs hadoop fs -mkdir -p /tmp** **sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.\n10.  Next, before you start MapReduce, you need to create its system directories. Enter these three commands one after another in the terminal: ** **sudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging**** ****sudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging****        **sudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred**\n11.  Once that is complete, you can start MapReduce with this command (again be careful of the quotes): **    for x in \\`cd /etc/init.d ; ls hadoop-0.20-mapreduce-\\*\\` ; do sudo service $x start ; done** If all goes well, you'll see messages indicating Hadoop jobtracker and Hadoop tasktracker have started OK.\n12.  Open a web browser and point it to your your system name and port 50030. So for example, if your system is called HadoopTest,  you would enter http://HadoopTest:50030 in your browser. You should see a website that shows the condition of your MapReduce system.\n13.  Finally, create a home directory for yourself in the HDFS file system, and change the owner to yourself as well: **sudo -u hdfs hadoop fs -mkdir -p /user/<your user name>** **sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name>** \n\nIf you've reached this point, you should now have a running Hadoop installation in pseudo-cluster mode. From a command line, you can access the Hadoop interface, and submit mapreduce jobs. (Limited by your RAM and hard drive space). Next time, we'll start the installation of the various GUI tools to allow you to interact with your system via a web browser.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-1","published":1,"updated":"2020-08-23T20:54:34.814Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9b00aesdjx0p0kcxsf","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/Hadoop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/Hadoop-300x191.png\" alt=\"Hadoop\"></a> I’ve been awaiting the open source introduction of Cloudera’s Hadoop distribution, CDH 5,  to try installing a pseudo-distributed cluster using CDH, with the HUE GUI interface. (If you are not familiar with the terminology, pseudo-distributed mode allows you to run Hadoop on one machine, with the various daemons each running in a separate JVM.) By setting up a pseudo-distributed cluster, I could free up two other machines for other projects I’m working on. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<p>I’ve managed to install a vanilla version of Hadoop with Hive previously, but I couldn’t get Cloudera’s distribution to work with the extra bells and whistles of HUE, HBase and Impala. With the introduction of a new version, it seems like a good time to try it. The good news is, I have it working successfully, on two different machines! Yeh! The bad news, its not a simple process, but it is doable if you have a few hours to spare. The biggest drawback for me was trying to navigate Cloudera’s website to get the information I needed to make this work. (Having worked in IT for more than a dozen years, I’m used to challenging documentation.) Hopefully the instructions I will be providing here will make it easier for others to setup as well. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Some may ask, “What’s the point?” Cloudera offers a Sandbox VM of their distribution that you can download and play with (although at this writing it has not been updated to CDH5) so why install it yourself. My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I am providing no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures! With that said, here is part one.</p>\n<h3 id=\"Prepare-your-single-node-for-Hadoop\"><a href=\"#Prepare-your-single-node-for-Hadoop\" class=\"headerlink\" title=\"Prepare your single node for Hadoop\"></a><strong>Prepare your single node for Hadoop</strong></h3><ol>\n<li>Install a CentOS 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.</li>\n<li>Disable iptables and iptablesv6 from the Service Configuration window.</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for “OpenJDK”. Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.</li>\n<li>Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a></li>\n<li>Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**</li>\n<li>Once the installation completes repeat step 6, and it should report that java version 1.7.0_45 is installed.</li>\n<li>You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**</li>\n<li>On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.</li>\n<li>Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.</li>\n<li>Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.</li>\n<li>Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.</li>\n<li>Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.</li>\n<li>While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**</li>\n<li>Edit the <strong>/etc/sysconfig/network</strong> file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.</li>\n<li>Still as root in the terminal window, edit the root .profile file (~/.bash_profile) and add these two lines to the end of the file, after “export PATH” line: **         export JAVA_HOME=/usr/java/latest** **         export PATH=$JAVA_HOME/bin:$PATH**</li>\n<li>After you save the file, exit the root user account by typing Exit in the terminal.</li>\n<li>With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java_home value is set correctly. Type: **        echo $JAVA_HOME**</li>\n<li>The system should respond with: <strong>/usr/java/latest</strong></li>\n</ol>\n<p>Stop and take a breath. Get a drink. At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.</p>\n<h3 id=\"Install-CDH5\"><a href=\"#Install-CDH5\" class=\"headerlink\" title=\"Install CDH5\"></a><strong>Install CDH5</strong></h3><ol>\n<li>Open a web browser and download the CDH 5 package for Red Hat from this link: <a href=\"http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\">http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm</a></li>\n<li>In the terminal window navigate to where the RPM file was downloaded. and enter this command (that’s two dashes after YUM): **        yum –nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm** The CDH5 quick start package will be installed.</li>\n<li>Now add the Cloudera repository GPG key (that’s two dashes after RPM): **       rpm –import  <a href=\"http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera</a>**</li>\n<li>And finally install Hadoop with MRv1 with this command: **       yum install hadoop-0.20-conf-pseudo** (At present 16 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)</li>\n<li>Verify that Hadoop is installed correctly. At the command prompt enter                       <strong>rpm -ql hadoop-0.20-conf-pseudo</strong> and you should see a list of hadoop folders. No errors? All good.</li>\n<li>Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.</li>\n<li>With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **     for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done**  (Watch the angle of the quotes - I have found that using normal single quotes errors out).</li>\n<li>Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).</li>\n<li>Now you need to create a temp folder in HDFS and set its permissions. This is extremely important, as a number of components really on this folder to work properly. With the terminal window open, enter: <strong>sudo -u hdfs hadoop fs -mkdir -p /tmp</strong> <strong>sudo -u hdfs hadoop fs -chmod -R 1777 /tmp</strong> Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.</li>\n<li>Next, before you start MapReduce, you need to create its system directories. Enter these three commands one after another in the terminal: ** <strong>sudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging**</strong> <strong><strong>sudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging</strong></strong>        <strong>sudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred</strong></li>\n<li>Once that is complete, you can start MapReduce with this command (again be careful of the quotes): **    for x in `cd /etc/init.d ; ls hadoop-0.20-mapreduce-*` ; do sudo service $x start ; done** If all goes well, you’ll see messages indicating Hadoop jobtracker and Hadoop tasktracker have started OK.</li>\n<li>Open a web browser and point it to your your system name and port 50030. So for example, if your system is called HadoopTest,  you would enter <a href=\"http://hadooptest:50030/\">http://HadoopTest:50030</a> in your browser. You should see a website that shows the condition of your MapReduce system.</li>\n<li>Finally, create a home directory for yourself in the HDFS file system, and change the owner to yourself as well: <strong>sudo -u hdfs hadoop fs -mkdir -p /user/<your user name></strong> <strong>sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name></strong> </li>\n</ol>\n<p>If you’ve reached this point, you should now have a running Hadoop installation in pseudo-cluster mode. From a command line, you can access the Hadoop interface, and submit mapreduce jobs. (Limited by your RAM and hard drive space). Next time, we’ll start the installation of the various GUI tools to allow you to interact with your system via a web browser.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/Hadoop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/Hadoop-300x191.png\" alt=\"Hadoop\"></a> I’ve been awaiting the open source introduction of Cloudera’s Hadoop distribution, CDH 5,  to try installing a pseudo-distributed cluster using CDH, with the HUE GUI interface. (If you are not familiar with the terminology, pseudo-distributed mode allows you to run Hadoop on one machine, with the various daemons each running in a separate JVM.) By setting up a pseudo-distributed cluster, I could free up two other machines for other projects I’m working on. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<p>I’ve managed to install a vanilla version of Hadoop with Hive previously, but I couldn’t get Cloudera’s distribution to work with the extra bells and whistles of HUE, HBase and Impala. With the introduction of a new version, it seems like a good time to try it. The good news is, I have it working successfully, on two different machines! Yeh! The bad news, its not a simple process, but it is doable if you have a few hours to spare. The biggest drawback for me was trying to navigate Cloudera’s website to get the information I needed to make this work. (Having worked in IT for more than a dozen years, I’m used to challenging documentation.) Hopefully the instructions I will be providing here will make it easier for others to setup as well. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Some may ask, “What’s the point?” Cloudera offers a Sandbox VM of their distribution that you can download and play with (although at this writing it has not been updated to CDH5) so why install it yourself. My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I am providing no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures! With that said, here is part one.</p>\n<h3 id=\"Prepare-your-single-node-for-Hadoop\"><a href=\"#Prepare-your-single-node-for-Hadoop\" class=\"headerlink\" title=\"Prepare your single node for Hadoop\"></a><strong>Prepare your single node for Hadoop</strong></h3><ol>\n<li>Install a CentOS 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.</li>\n<li>Disable iptables and iptablesv6 from the Service Configuration window.</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for “OpenJDK”. Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.</li>\n<li>Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a></li>\n<li>Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**</li>\n<li>Once the installation completes repeat step 6, and it should report that java version 1.7.0_45 is installed.</li>\n<li>You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**</li>\n<li>On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.</li>\n<li>Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.</li>\n<li>Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.</li>\n<li>Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.</li>\n<li>Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.</li>\n<li>While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**</li>\n<li>Edit the <strong>/etc/sysconfig/network</strong> file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.</li>\n<li>Still as root in the terminal window, edit the root .profile file (~/.bash_profile) and add these two lines to the end of the file, after “export PATH” line: **         export JAVA_HOME=/usr/java/latest** **         export PATH=$JAVA_HOME/bin:$PATH**</li>\n<li>After you save the file, exit the root user account by typing Exit in the terminal.</li>\n<li>With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java_home value is set correctly. Type: **        echo $JAVA_HOME**</li>\n<li>The system should respond with: <strong>/usr/java/latest</strong></li>\n</ol>\n<p>Stop and take a breath. Get a drink. At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.</p>\n<h3 id=\"Install-CDH5\"><a href=\"#Install-CDH5\" class=\"headerlink\" title=\"Install CDH5\"></a><strong>Install CDH5</strong></h3><ol>\n<li>Open a web browser and download the CDH 5 package for Red Hat from this link: <a href=\"http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\">http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm</a></li>\n<li>In the terminal window navigate to where the RPM file was downloaded. and enter this command (that’s two dashes after YUM): **        yum –nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm** The CDH5 quick start package will be installed.</li>\n<li>Now add the Cloudera repository GPG key (that’s two dashes after RPM): **       rpm –import  <a href=\"http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera</a>**</li>\n<li>And finally install Hadoop with MRv1 with this command: **       yum install hadoop-0.20-conf-pseudo** (At present 16 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)</li>\n<li>Verify that Hadoop is installed correctly. At the command prompt enter                       <strong>rpm -ql hadoop-0.20-conf-pseudo</strong> and you should see a list of hadoop folders. No errors? All good.</li>\n<li>Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.</li>\n<li>With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **     for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done**  (Watch the angle of the quotes - I have found that using normal single quotes errors out).</li>\n<li>Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).</li>\n<li>Now you need to create a temp folder in HDFS and set its permissions. This is extremely important, as a number of components really on this folder to work properly. With the terminal window open, enter: <strong>sudo -u hdfs hadoop fs -mkdir -p /tmp</strong> <strong>sudo -u hdfs hadoop fs -chmod -R 1777 /tmp</strong> Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.</li>\n<li>Next, before you start MapReduce, you need to create its system directories. Enter these three commands one after another in the terminal: ** <strong>sudo -u hdfs hadoop fs -mkdir -p /var/lib/hadoop-hdfs/cache/mapred/mapred/staging**</strong> <strong><strong>sudo -u hdfs hadoop fs -chmod 1777 /var/lib/hadoop-hdfs/cache/mapred/mapred/staging</strong></strong>        <strong>sudo -u hdfs hadoop fs -chown -R mapred /var/lib/hadoop-hdfs/cache/mapred</strong></li>\n<li>Once that is complete, you can start MapReduce with this command (again be careful of the quotes): **    for x in `cd /etc/init.d ; ls hadoop-0.20-mapreduce-*` ; do sudo service $x start ; done** If all goes well, you’ll see messages indicating Hadoop jobtracker and Hadoop tasktracker have started OK.</li>\n<li>Open a web browser and point it to your your system name and port 50030. So for example, if your system is called HadoopTest,  you would enter <a href=\"http://hadooptest:50030/\">http://HadoopTest:50030</a> in your browser. You should see a website that shows the condition of your MapReduce system.</li>\n<li>Finally, create a home directory for yourself in the HDFS file system, and change the owner to yourself as well: <strong>sudo -u hdfs hadoop fs -mkdir -p /user/<your user name></strong> <strong>sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name></strong> </li>\n</ol>\n<p>If you’ve reached this point, you should now have a running Hadoop installation in pseudo-cluster mode. From a command line, you can access the Hadoop interface, and submit mapreduce jobs. (Limited by your RAM and hard drive space). Next time, we’ll start the installation of the various GUI tools to allow you to interact with your system via a web browser.</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE - Part 2","id":"1964","comments":0,"date":"2014-04-16T19:12:01.000Z","_content":"\n[![hbase_zoo](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png)](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png)In [part 1 of this series](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps for setting up a single-node Hadoop pseudo-cluster. In that article, I showed you how to configure CentOS 6.5 with the necessary prerequisites for installation, download Cloudera's Hadoop distribution  (CDH5) and install it. In this article, I'll explain how to install:\n\n*   HBase - an open source non-relational database that runs on top of the Hadoop File System (HDFS)\n*   Zookeeper - an engine that provides distribution, synchronization and naming services for Hadoop,\n*   and SNAPPY a fast data compression and decompression library that incorporates with many different components in the Hadoop ecosystem.\n\nThis series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### **Install HBase and Zookeeper**\n\n1.  Open a terminal window on your CentOS server and switch to the root user account with:  **su -**\n2.  Enter this command: **yum install hbase** \n3.  Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.\n4.  HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the **/etc/security/limits.conf** file.\n5.  At the bottom of the file, before the #End of file line, add these two lines:  \n               hdfs             -       nofile  32768  \n               hbase           -       nofile  32768\n6.  Restart your system to allow the changes to take effect.\n7.  Edit the **/etc/hadoop/conf/hdfs-site.xml** file to allow your data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum you should use and xcievers is spelled correctly)  \n            <property>  \n              <name>dfs.datanode.max.xcievers</name>  \n              <value>4096</value>  \n            </property>\n8.   Restart HDFS by either restarting your server, or you can use this command:  \n          **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done**\n9.  Now install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:  \n     **yum install hbase-master**\n10.  Once the install completes, you can start the service with this command:  \n          **service hbase-master start**\n11.  Open a web browser and point it to your your system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your HBase system.\n12.  Scroll down to the bottom of the page, and there should be one entry under the Region Servers section.\n13.  Now you need to install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **          yum install hbase-thrift**  \n    and then start it with: **        service hbase-thrift start**\n14.  Once the Thrift install completes, enter this command: **        yum install hbase-rest.**\n15.  Before you run the REST server, we need to make some additional configuration changes. At this point you have HBase configured in Standalone mode. It will work fine as it is, but if your are running on one server in a Pseudo-distributed mode, you may want to change HBase to operate in a similar fashion. This will allow each of the component processes to run in a separate Java Virtual Machine.\n16.  Edit the config file **/etc/hbase/conf/hbase-site.xml**\n17.  The first change is optional. HBase by default uses port 8080, which is a commonly used one and one you may want to change. To use port 60050 add this property section between the configuration tags:  \n     **<property>**  \n     **<name>hbase.rest.port</name>**  \n     **<value>60050</value>**  \n     **</property>**\n18.  Now add these two additional properties, still inside the configuration tags:  \n     **<property>**  \n     **<name>hbase.cluster.distributed</name>**  \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>hbase.rootdir</name>**  \n     **<value>hdfs://myhost:8020/hbase</value>**   \n     **</property>**\n19.  **One gotcha here is the value for the hbase.rootdir hostname needs to match the value in /etc/hadoop/conf/core-site.xml's fs.default.name or fs.defaultFS.**\n20.  Save the file. Now you need to create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo - u hdfs portion to make sure the command is run by the correct Hadoop user):  \n             **sudo -u hdfs hadoop fs -mkdir /hbase**  \n             **sudo -u hdfs hadoop fs -chown hbase /hbase**\n21.  You are almost down with HBase, but before we finish, you need to install Zookeeper and configure it to run on a single server. Install the package by entering at the command line as root: **yum install zookeeper-server**\n22.  Create the Zookeeper folders in the Linux file system and change permissions by entering the following:  \n             **mkdir -p /var/lib/zookeeper**  \n            ** chown -R zookeeper /var/lib/zookeeper/**\n23.  Now you can initialize and start Zookeeper, by entering these commands as root:  \n     **service zookeeper-server init** (You'll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).  \n     **service zookeeper-server start**\n24.  Because this is a pseudo-distributed mode installation primarily for testing and development, you are OK with using a single Zookeeper server. If this were a production setup, you would want to install an odd number of Zookeeper servers to ensure reliability.\n25.  Finally we can start the HBase Master! Enter the following command to start it up:  \n     **service hbase-master start**\n26.  At this point we are almost finished. We just need to install the RegionServer (that's the part of HBase that hosts the data and actually handles requests). To install your region server, type the following command as root:  \n     **yum install hbase-regionserver**\n27.  Once it completes installing, start it with this command:  \n     **service hbase-regionserver start**\n28.  And then start the HBase Rest service with this command:   \n    \n     **service hbase-rest start**\n    \n29.  Reopen a web browser and point it to your your system name and port 60010. Click through the various tabs to check out the status of your HBase system.\n\n### **SNAPPY configuration**\n\nSnappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.\n\n1.  To enable Snappy for MapReduce, from a command line as root, edit: **/etc/hadoop/conf/mapred-site.xml** and add these sections to the file before the final </configuration> tag:  \n            **<!-- Enable Snappy for MapReduce -->**  \n     **<property>**  \n     **<name>mapred.compress.map.output</name>**   \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>mapred.map.output.compression.codec</name>**   \n     **<value>org.apache.hadoop.io.compress.SnappyCodec</value>**  \n     **</property>**\n2.  Configurations for other components are enabled in the code to call the specific job.\n3.  Restart your server for Snappy to take effect.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-2.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE - Part 2\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - How-to\nid: '1964'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-04-16 15:12:01\n---\n\n[![hbase_zoo](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png)](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png)In [part 1 of this series](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps for setting up a single-node Hadoop pseudo-cluster. In that article, I showed you how to configure CentOS 6.5 with the necessary prerequisites for installation, download Cloudera's Hadoop distribution  (CDH5) and install it. In this article, I'll explain how to install:\n\n*   HBase - an open source non-relational database that runs on top of the Hadoop File System (HDFS)\n*   Zookeeper - an engine that provides distribution, synchronization and naming services for Hadoop,\n*   and SNAPPY a fast data compression and decompression library that incorporates with many different components in the Hadoop ecosystem.\n\nThis series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### **Install HBase and Zookeeper**\n\n1.  Open a terminal window on your CentOS server and switch to the root user account with:  **su -**\n2.  Enter this command: **yum install hbase** \n3.  Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.\n4.  HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the **/etc/security/limits.conf** file.\n5.  At the bottom of the file, before the #End of file line, add these two lines:  \n               hdfs             -       nofile  32768  \n               hbase           -       nofile  32768\n6.  Restart your system to allow the changes to take effect.\n7.  Edit the **/etc/hadoop/conf/hdfs-site.xml** file to allow your data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum you should use and xcievers is spelled correctly)  \n            <property>  \n              <name>dfs.datanode.max.xcievers</name>  \n              <value>4096</value>  \n            </property>\n8.   Restart HDFS by either restarting your server, or you can use this command:  \n          **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done**\n9.  Now install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:  \n     **yum install hbase-master**\n10.  Once the install completes, you can start the service with this command:  \n          **service hbase-master start**\n11.  Open a web browser and point it to your your system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your HBase system.\n12.  Scroll down to the bottom of the page, and there should be one entry under the Region Servers section.\n13.  Now you need to install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **          yum install hbase-thrift**  \n    and then start it with: **        service hbase-thrift start**\n14.  Once the Thrift install completes, enter this command: **        yum install hbase-rest.**\n15.  Before you run the REST server, we need to make some additional configuration changes. At this point you have HBase configured in Standalone mode. It will work fine as it is, but if your are running on one server in a Pseudo-distributed mode, you may want to change HBase to operate in a similar fashion. This will allow each of the component processes to run in a separate Java Virtual Machine.\n16.  Edit the config file **/etc/hbase/conf/hbase-site.xml**\n17.  The first change is optional. HBase by default uses port 8080, which is a commonly used one and one you may want to change. To use port 60050 add this property section between the configuration tags:  \n     **<property>**  \n     **<name>hbase.rest.port</name>**  \n     **<value>60050</value>**  \n     **</property>**\n18.  Now add these two additional properties, still inside the configuration tags:  \n     **<property>**  \n     **<name>hbase.cluster.distributed</name>**  \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>hbase.rootdir</name>**  \n     **<value>hdfs://myhost:8020/hbase</value>**   \n     **</property>**\n19.  **One gotcha here is the value for the hbase.rootdir hostname needs to match the value in /etc/hadoop/conf/core-site.xml's fs.default.name or fs.defaultFS.**\n20.  Save the file. Now you need to create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo - u hdfs portion to make sure the command is run by the correct Hadoop user):  \n             **sudo -u hdfs hadoop fs -mkdir /hbase**  \n             **sudo -u hdfs hadoop fs -chown hbase /hbase**\n21.  You are almost down with HBase, but before we finish, you need to install Zookeeper and configure it to run on a single server. Install the package by entering at the command line as root: **yum install zookeeper-server**\n22.  Create the Zookeeper folders in the Linux file system and change permissions by entering the following:  \n             **mkdir -p /var/lib/zookeeper**  \n            ** chown -R zookeeper /var/lib/zookeeper/**\n23.  Now you can initialize and start Zookeeper, by entering these commands as root:  \n     **service zookeeper-server init** (You'll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).  \n     **service zookeeper-server start**\n24.  Because this is a pseudo-distributed mode installation primarily for testing and development, you are OK with using a single Zookeeper server. If this were a production setup, you would want to install an odd number of Zookeeper servers to ensure reliability.\n25.  Finally we can start the HBase Master! Enter the following command to start it up:  \n     **service hbase-master start**\n26.  At this point we are almost finished. We just need to install the RegionServer (that's the part of HBase that hosts the data and actually handles requests). To install your region server, type the following command as root:  \n     **yum install hbase-regionserver**\n27.  Once it completes installing, start it with this command:  \n     **service hbase-regionserver start**\n28.  And then start the HBase Rest service with this command:   \n    \n     **service hbase-rest start**\n    \n29.  Reopen a web browser and point it to your your system name and port 60010. Click through the various tabs to check out the status of your HBase system.\n\n### **SNAPPY configuration**\n\nSnappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.\n\n1.  To enable Snappy for MapReduce, from a command line as root, edit: **/etc/hadoop/conf/mapred-site.xml** and add these sections to the file before the final </configuration> tag:  \n            **<!-- Enable Snappy for MapReduce -->**  \n     **<property>**  \n     **<name>mapred.compress.map.output</name>**   \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>mapred.map.output.compression.codec</name>**   \n     **<value>org.apache.hadoop.io.compress.SnappyCodec</value>**  \n     **</property>**\n2.  Configurations for other components are enabled in the code to call the specific job.\n3.  Restart your server for Snappy to take effect.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-2","published":1,"updated":"2020-08-23T20:54:34.818Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9f00ahsdjxgw4zfq7h","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png\" alt=\"hbase_zoo\"></a>In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1 of this series</a>, I walked through the beginning steps for setting up a single-node Hadoop pseudo-cluster. In that article, I showed you how to configure CentOS 6.5 with the necessary prerequisites for installation, download Cloudera’s Hadoop distribution  (CDH5) and install it. In this article, I’ll explain how to install:</p>\n<ul>\n<li>HBase - an open source non-relational database that runs on top of the Hadoop File System (HDFS)</li>\n<li>Zookeeper - an engine that provides distribution, synchronization and naming services for Hadoop,</li>\n<li>and SNAPPY a fast data compression and decompression library that incorporates with many different components in the Hadoop ecosystem.</li>\n</ul>\n<p>This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<h3 id=\"Install-HBase-and-Zookeeper\"><a href=\"#Install-HBase-and-Zookeeper\" class=\"headerlink\" title=\"Install HBase and Zookeeper\"></a><strong>Install HBase and Zookeeper</strong></h3><ol>\n<li><p>Open a terminal window on your CentOS server and switch to the root user account with:  <strong>su -</strong></p>\n</li>\n<li><p>Enter this command: <strong>yum install hbase</strong> </p>\n</li>\n<li><p>Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.</p>\n</li>\n<li><p>HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the <strong>/etc/security/limits.conf</strong> file.</p>\n</li>\n<li><p>At the bottom of the file, before the #End of file line, add these two lines:<br>           hdfs             -       nofile  32768<br>           hbase           -       nofile  32768</p>\n</li>\n<li><p>Restart your system to allow the changes to take effect.</p>\n</li>\n<li><p>Edit the <strong>/etc/hadoop/conf/hdfs-site.xml</strong> file to allow your data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum you should use and xcievers is spelled correctly)<br>        <property><br>          <name>dfs.datanode.max.xcievers</name><br>          <value>4096</value><br>        </property></p>\n</li>\n<li><p> Restart HDFS by either restarting your server, or you can use this command:<br>      <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong></p>\n</li>\n<li><p>Now install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:<br> <strong>yum install hbase-master</strong></p>\n</li>\n<li><p>Once the install completes, you can start the service with this command:<br>      <strong>service hbase-master start</strong></p>\n</li>\n<li><p>Open a web browser and point it to your your system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your HBase system.</p>\n</li>\n<li><p>Scroll down to the bottom of the page, and there should be one entry under the Region Servers section.</p>\n</li>\n<li><p>Now you need to install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **          yum install hbase-thrift**<br>and then start it with: **        service hbase-thrift start**</p>\n</li>\n<li><p>Once the Thrift install completes, enter this command: **        yum install hbase-rest.**</p>\n</li>\n<li><p>Before you run the REST server, we need to make some additional configuration changes. At this point you have HBase configured in Standalone mode. It will work fine as it is, but if your are running on one server in a Pseudo-distributed mode, you may want to change HBase to operate in a similar fashion. This will allow each of the component processes to run in a separate Java Virtual Machine.</p>\n</li>\n<li><p>Edit the config file <strong>/etc/hbase/conf/hbase-site.xml</strong></p>\n</li>\n<li><p>The first change is optional. HBase by default uses port 8080, which is a commonly used one and one you may want to change. To use port 60050 add this property section between the configuration tags:<br><strong><property></strong><br><strong><name>hbase.rest.port</name></strong><br><strong><value>60050</value></strong><br><strong></property></strong></p>\n</li>\n<li><p>Now add these two additional properties, still inside the configuration tags:<br><strong><property></strong><br><strong><name>hbase.cluster.distributed</name></strong><br><strong><value>true</value></strong><br><strong></property></strong><br><strong><property></strong><br><strong><name>hbase.rootdir</name></strong><br><strong><value>hdfs://myhost:8020/hbase</value></strong><br><strong></property></strong></p>\n</li>\n<li><p><strong>One gotcha here is the value for the hbase.rootdir hostname needs to match the value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS.</strong></p>\n</li>\n<li><p>Save the file. Now you need to create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo - u hdfs portion to make sure the command is run by the correct Hadoop user):<br>         <strong>sudo -u hdfs hadoop fs -mkdir /hbase</strong><br>         <strong>sudo -u hdfs hadoop fs -chown hbase /hbase</strong></p>\n</li>\n<li><p>You are almost down with HBase, but before we finish, you need to install Zookeeper and configure it to run on a single server. Install the package by entering at the command line as root: <strong>yum install zookeeper-server</strong></p>\n</li>\n<li><p>Create the Zookeeper folders in the Linux file system and change permissions by entering the following:<br>         <strong>mkdir -p /var/lib/zookeeper</strong><br>        ** chown -R zookeeper /var/lib/zookeeper/**</p>\n</li>\n<li><p>Now you can initialize and start Zookeeper, by entering these commands as root:<br><strong>service zookeeper-server init</strong> (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).<br><strong>service zookeeper-server start</strong></p>\n</li>\n<li><p>Because this is a pseudo-distributed mode installation primarily for testing and development, you are OK with using a single Zookeeper server. If this were a production setup, you would want to install an odd number of Zookeeper servers to ensure reliability.</p>\n</li>\n<li><p>Finally we can start the HBase Master! Enter the following command to start it up:<br><strong>service hbase-master start</strong></p>\n</li>\n<li><p>At this point we are almost finished. We just need to install the RegionServer (that’s the part of HBase that hosts the data and actually handles requests). To install your region server, type the following command as root:<br><strong>yum install hbase-regionserver</strong></p>\n</li>\n<li><p>Once it completes installing, start it with this command:<br><strong>service hbase-regionserver start</strong></p>\n</li>\n<li><p>And then start the HBase Rest service with this command:   </p>\n<p><strong>service hbase-rest start</strong></p>\n</li>\n<li><p>Reopen a web browser and point it to your your system name and port 60010. Click through the various tabs to check out the status of your HBase system.</p>\n</li>\n</ol>\n<h3 id=\"SNAPPY-configuration\"><a href=\"#SNAPPY-configuration\" class=\"headerlink\" title=\"SNAPPY configuration\"></a><strong>SNAPPY configuration</strong></h3><p>Snappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.</p>\n<ol>\n<li>To enable Snappy for MapReduce, from a command line as root, edit: <strong>/etc/hadoop/conf/mapred-site.xml</strong> and add these sections to the file before the final </configuration> tag:<br>        <strong><!-- Enable Snappy for MapReduce --></strong><br> <strong><property></strong><br> <strong><name>mapred.compress.map.output</name></strong><br> <strong><value>true</value></strong><br> <strong></property></strong><br> <strong><property></strong><br> <strong><name>mapred.map.output.compression.codec</name></strong><br> <strong><value>org.apache.hadoop.io.compress.SnappyCodec</value></strong><br> <strong></property></strong></li>\n<li>Configurations for other components are enabled in the code to call the specific job.</li>\n<li>Restart your server for Snappy to take effect.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png\" alt=\"hbase_zoo\"></a>In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1 of this series</a>, I walked through the beginning steps for setting up a single-node Hadoop pseudo-cluster. In that article, I showed you how to configure CentOS 6.5 with the necessary prerequisites for installation, download Cloudera’s Hadoop distribution  (CDH5) and install it. In this article, I’ll explain how to install:</p>\n<ul>\n<li>HBase - an open source non-relational database that runs on top of the Hadoop File System (HDFS)</li>\n<li>Zookeeper - an engine that provides distribution, synchronization and naming services for Hadoop,</li>\n<li>and SNAPPY a fast data compression and decompression library that incorporates with many different components in the Hadoop ecosystem.</li>\n</ul>\n<p>This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<h3 id=\"Install-HBase-and-Zookeeper\"><a href=\"#Install-HBase-and-Zookeeper\" class=\"headerlink\" title=\"Install HBase and Zookeeper\"></a><strong>Install HBase and Zookeeper</strong></h3><ol>\n<li><p>Open a terminal window on your CentOS server and switch to the root user account with:  <strong>su -</strong></p>\n</li>\n<li><p>Enter this command: <strong>yum install hbase</strong> </p>\n</li>\n<li><p>Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.</p>\n</li>\n<li><p>HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the <strong>/etc/security/limits.conf</strong> file.</p>\n</li>\n<li><p>At the bottom of the file, before the #End of file line, add these two lines:<br>           hdfs             -       nofile  32768<br>           hbase           -       nofile  32768</p>\n</li>\n<li><p>Restart your system to allow the changes to take effect.</p>\n</li>\n<li><p>Edit the <strong>/etc/hadoop/conf/hdfs-site.xml</strong> file to allow your data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum you should use and xcievers is spelled correctly)<br>        <property><br>          <name>dfs.datanode.max.xcievers</name><br>          <value>4096</value><br>        </property></p>\n</li>\n<li><p> Restart HDFS by either restarting your server, or you can use this command:<br>      <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong></p>\n</li>\n<li><p>Now install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:<br> <strong>yum install hbase-master</strong></p>\n</li>\n<li><p>Once the install completes, you can start the service with this command:<br>      <strong>service hbase-master start</strong></p>\n</li>\n<li><p>Open a web browser and point it to your your system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your HBase system.</p>\n</li>\n<li><p>Scroll down to the bottom of the page, and there should be one entry under the Region Servers section.</p>\n</li>\n<li><p>Now you need to install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **          yum install hbase-thrift**<br>and then start it with: **        service hbase-thrift start**</p>\n</li>\n<li><p>Once the Thrift install completes, enter this command: **        yum install hbase-rest.**</p>\n</li>\n<li><p>Before you run the REST server, we need to make some additional configuration changes. At this point you have HBase configured in Standalone mode. It will work fine as it is, but if your are running on one server in a Pseudo-distributed mode, you may want to change HBase to operate in a similar fashion. This will allow each of the component processes to run in a separate Java Virtual Machine.</p>\n</li>\n<li><p>Edit the config file <strong>/etc/hbase/conf/hbase-site.xml</strong></p>\n</li>\n<li><p>The first change is optional. HBase by default uses port 8080, which is a commonly used one and one you may want to change. To use port 60050 add this property section between the configuration tags:<br><strong><property></strong><br><strong><name>hbase.rest.port</name></strong><br><strong><value>60050</value></strong><br><strong></property></strong></p>\n</li>\n<li><p>Now add these two additional properties, still inside the configuration tags:<br><strong><property></strong><br><strong><name>hbase.cluster.distributed</name></strong><br><strong><value>true</value></strong><br><strong></property></strong><br><strong><property></strong><br><strong><name>hbase.rootdir</name></strong><br><strong><value>hdfs://myhost:8020/hbase</value></strong><br><strong></property></strong></p>\n</li>\n<li><p><strong>One gotcha here is the value for the hbase.rootdir hostname needs to match the value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS.</strong></p>\n</li>\n<li><p>Save the file. Now you need to create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo - u hdfs portion to make sure the command is run by the correct Hadoop user):<br>         <strong>sudo -u hdfs hadoop fs -mkdir /hbase</strong><br>         <strong>sudo -u hdfs hadoop fs -chown hbase /hbase</strong></p>\n</li>\n<li><p>You are almost down with HBase, but before we finish, you need to install Zookeeper and configure it to run on a single server. Install the package by entering at the command line as root: <strong>yum install zookeeper-server</strong></p>\n</li>\n<li><p>Create the Zookeeper folders in the Linux file system and change permissions by entering the following:<br>         <strong>mkdir -p /var/lib/zookeeper</strong><br>        ** chown -R zookeeper /var/lib/zookeeper/**</p>\n</li>\n<li><p>Now you can initialize and start Zookeeper, by entering these commands as root:<br><strong>service zookeeper-server init</strong> (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).<br><strong>service zookeeper-server start</strong></p>\n</li>\n<li><p>Because this is a pseudo-distributed mode installation primarily for testing and development, you are OK with using a single Zookeeper server. If this were a production setup, you would want to install an odd number of Zookeeper servers to ensure reliability.</p>\n</li>\n<li><p>Finally we can start the HBase Master! Enter the following command to start it up:<br><strong>service hbase-master start</strong></p>\n</li>\n<li><p>At this point we are almost finished. We just need to install the RegionServer (that’s the part of HBase that hosts the data and actually handles requests). To install your region server, type the following command as root:<br><strong>yum install hbase-regionserver</strong></p>\n</li>\n<li><p>Once it completes installing, start it with this command:<br><strong>service hbase-regionserver start</strong></p>\n</li>\n<li><p>And then start the HBase Rest service with this command:   </p>\n<p><strong>service hbase-rest start</strong></p>\n</li>\n<li><p>Reopen a web browser and point it to your your system name and port 60010. Click through the various tabs to check out the status of your HBase system.</p>\n</li>\n</ol>\n<h3 id=\"SNAPPY-configuration\"><a href=\"#SNAPPY-configuration\" class=\"headerlink\" title=\"SNAPPY configuration\"></a><strong>SNAPPY configuration</strong></h3><p>Snappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.</p>\n<ol>\n<li>To enable Snappy for MapReduce, from a command line as root, edit: <strong>/etc/hadoop/conf/mapred-site.xml</strong> and add these sections to the file before the final </configuration> tag:<br>        <strong><!-- Enable Snappy for MapReduce --></strong><br> <strong><property></strong><br> <strong><name>mapred.compress.map.output</name></strong><br> <strong><value>true</value></strong><br> <strong></property></strong><br> <strong><property></strong><br> <strong><name>mapred.map.output.compression.codec</name></strong><br> <strong><value>org.apache.hadoop.io.compress.SnappyCodec</value></strong><br> <strong></property></strong></li>\n<li>Configurations for other components are enabled in the code to call the specific job.</li>\n<li>Restart your server for Snappy to take effect.</li>\n</ol>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3","id":"1973","comments":0,"date":"2014-04-19T17:45:53.000Z","_content":"\n[![hue_logo_300dpi_huge](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png)](http://gethue.com)Welcome to part 3 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera's CDH distribution and HUE. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. Below I'll present the steps necessary to install the web-based management tool HUE for using with your cluster. (Hue is a trademark of Cloudera but is released under the Apache license version 2). This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n**Prerequisites** Before you can install HUE on your pseudo server, you need to see if Python 2.6 or Python 2.7 is installed. Open a terminal and enter: python <ENTER>. If Python is installed, the version will be displayed as it starts up. At the Python prompt enter: quit()<ENTER> to exit Python and return to the command prompt. If you have the correct version of Python installed, skip down to the next section, on installing HUE. If you don’t have Python installed, you'll need to add the RHEL EPEL (Extra Packages for Enterprise Linux) repository. Open a terminal window and switch to the root account by using \"su -\" and then enter the following command to install the EPEL repository (if you are running Centos 6 (64-bit)): **      rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm** After adding the repository, you can install Python 2.6 from the command line. As root, enter this command:       **yum install python26**\n\n### **Install HUE**\n\nOnce you've verified that you have Python installed or have installed it,  you can add HUE to your Hadoop system. Open a terminal window, and switch to the root account. Entering the following command to start the HUE installation: **yum install hue** Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes. Once it completes installation, and provided you had no errors, enter at a command prompt: **service hue start** Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter http://HadoopTest:8888 in your browser. You’ll be prompted for a user ID and password and you'll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I'm assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don't need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: **/etc/hadoop/conf/hdfs-site.xml** and add this property before the final </configuration> tag: **<property>** **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done** Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file **/etc/hadoop/conf/core-site.xml** and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>\\*</value> </property>** **       <property>** **               <name>hadoop.proxyuser.hue.groups</name>** **               <value>\\*</value>** **       </property>** Now edit the file: **/etc/hue/conf/hue.ini**. Near the top is a section called \\[desktop\\] with the first parameter there being secret\\_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret\\_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for \\[\\[hdfs\\_clusters\\]\\] and look for a comment section below it that starts with \"**#Use WebHDFS/HttpFs as the communication mechanism**\". At the end of that section, add this: **webhdfs\\_url=http://<full server name with domain>:50070/webhdfs/v1/** substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. The Hue plugin that is used to communication with the JobTracker should already be installed as part of the dependencies. You can verify this by changing folders to /usr/lib/hadoop/lib and verifying there is a hue-plugins jar file present. Restart the HUE server **service hue start** and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-3.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - howto\nid: '1973'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-04-19 13:45:53\n---\n\n[![hue_logo_300dpi_huge](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png)](http://gethue.com)Welcome to part 3 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera's CDH distribution and HUE. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. Below I'll present the steps necessary to install the web-based management tool HUE for using with your cluster. (Hue is a trademark of Cloudera but is released under the Apache license version 2). This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n**Prerequisites** Before you can install HUE on your pseudo server, you need to see if Python 2.6 or Python 2.7 is installed. Open a terminal and enter: python <ENTER>. If Python is installed, the version will be displayed as it starts up. At the Python prompt enter: quit()<ENTER> to exit Python and return to the command prompt. If you have the correct version of Python installed, skip down to the next section, on installing HUE. If you don’t have Python installed, you'll need to add the RHEL EPEL (Extra Packages for Enterprise Linux) repository. Open a terminal window and switch to the root account by using \"su -\" and then enter the following command to install the EPEL repository (if you are running Centos 6 (64-bit)): **      rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm** After adding the repository, you can install Python 2.6 from the command line. As root, enter this command:       **yum install python26**\n\n### **Install HUE**\n\nOnce you've verified that you have Python installed or have installed it,  you can add HUE to your Hadoop system. Open a terminal window, and switch to the root account. Entering the following command to start the HUE installation: **yum install hue** Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes. Once it completes installation, and provided you had no errors, enter at a command prompt: **service hue start** Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter http://HadoopTest:8888 in your browser. You’ll be prompted for a user ID and password and you'll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I'm assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don't need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: **/etc/hadoop/conf/hdfs-site.xml** and add this property before the final </configuration> tag: **<property>** **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done** Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file **/etc/hadoop/conf/core-site.xml** and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>\\*</value> </property>** **       <property>** **               <name>hadoop.proxyuser.hue.groups</name>** **               <value>\\*</value>** **       </property>** Now edit the file: **/etc/hue/conf/hue.ini**. Near the top is a section called \\[desktop\\] with the first parameter there being secret\\_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret\\_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for \\[\\[hdfs\\_clusters\\]\\] and look for a comment section below it that starts with \"**#Use WebHDFS/HttpFs as the communication mechanism**\". At the end of that section, add this: **webhdfs\\_url=http://<full server name with domain>:50070/webhdfs/v1/** substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. The Hue plugin that is used to communication with the JobTracker should already be installed as part of the dependencies. You can verify this by changing folders to /usr/lib/hadoop/lib and verifying there is a hue-plugins jar file present. Restart the HUE server **service hue start** and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-3","published":1,"updated":"2020-08-23T20:54:34.822Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9h00alsdjxfr1z4d8c","content":"<p><a href=\"http://gethue.com/\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png\" alt=\"hue_logo_300dpi_huge\"></a>Welcome to part 3 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution and HUE. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. Below I’ll present the steps necessary to install the web-based management tool HUE for using with your cluster. (Hue is a trademark of Cloudera but is released under the Apache license version 2). This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<p><strong>Prerequisites</strong> Before you can install HUE on your pseudo server, you need to see if Python 2.6 or Python 2.7 is installed. Open a terminal and enter: python <ENTER>. If Python is installed, the version will be displayed as it starts up. At the Python prompt enter: quit()<ENTER> to exit Python and return to the command prompt. If you have the correct version of Python installed, skip down to the next section, on installing HUE. If you don’t have Python installed, you’ll need to add the RHEL EPEL (Extra Packages for Enterprise Linux) repository. Open a terminal window and switch to the root account by using “su -“ and then enter the following command to install the EPEL repository (if you are running Centos 6 (64-bit)): **      rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/x86/_64/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm</a>** After adding the repository, you can install Python 2.6 from the command line. As root, enter this command:       <strong>yum install python26</strong></p>\n<h3 id=\"Install-HUE\"><a href=\"#Install-HUE\" class=\"headerlink\" title=\"Install HUE\"></a><strong>Install HUE</strong></h3><p>Once you’ve verified that you have Python installed or have installed it,  you can add HUE to your Hadoop system. Open a terminal window, and switch to the root account. Entering the following command to start the HUE installation: <strong>yum install hue</strong> Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes. Once it completes installation, and provided you had no errors, enter at a command prompt: <strong>service hue start</strong> Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:8888/\">http://HadoopTest:8888</a> in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add this property before the final </configuration> tag: <strong><property></strong> **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong> Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file <strong>/etc/hadoop/conf/core-site.xml</strong> and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property>** **       <property>** **               <name>hadoop.proxyuser.hue.groups</name>** **               <value>*</value>** **       </property>** Now edit the file: <strong>/etc/hue/conf/hue.ini</strong>. Near the top is a section called [desktop] with the first parameter there being secret_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for [[hdfs_clusters]] and look for a comment section below it that starts with “<strong>#Use WebHDFS/HttpFs as the communication mechanism</strong>“. At the end of that section, add this: <strong>webhdfs_url=http://<full server name with domain>:50070/webhdfs/v1/</strong> substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. The Hue plugin that is used to communication with the JobTracker should already be installed as part of the dependencies. You can verify this by changing folders to /usr/lib/hadoop/lib and verifying there is a hue-plugins jar file present. Restart the HUE server <strong>service hue start</strong> and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://gethue.com/\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png\" alt=\"hue_logo_300dpi_huge\"></a>Welcome to part 3 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution and HUE. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. Below I’ll present the steps necessary to install the web-based management tool HUE for using with your cluster. (Hue is a trademark of Cloudera but is released under the Apache license version 2). This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<p><strong>Prerequisites</strong> Before you can install HUE on your pseudo server, you need to see if Python 2.6 or Python 2.7 is installed. Open a terminal and enter: python <ENTER>. If Python is installed, the version will be displayed as it starts up. At the Python prompt enter: quit()<ENTER> to exit Python and return to the command prompt. If you have the correct version of Python installed, skip down to the next section, on installing HUE. If you don’t have Python installed, you’ll need to add the RHEL EPEL (Extra Packages for Enterprise Linux) repository. Open a terminal window and switch to the root account by using “su -“ and then enter the following command to install the EPEL repository (if you are running Centos 6 (64-bit)): **      rpm -Uvh <a href=\"http://download.fedoraproject.org/pub/epel/6/x86/_64/epel-release-6-8.noarch.rpm\">http://download.fedoraproject.org/pub/epel/6/x86\\_64/epel-release-6-8.noarch.rpm</a>** After adding the repository, you can install Python 2.6 from the command line. As root, enter this command:       <strong>yum install python26</strong></p>\n<h3 id=\"Install-HUE\"><a href=\"#Install-HUE\" class=\"headerlink\" title=\"Install HUE\"></a><strong>Install HUE</strong></h3><p>Once you’ve verified that you have Python installed or have installed it,  you can add HUE to your Hadoop system. Open a terminal window, and switch to the root account. Entering the following command to start the HUE installation: <strong>yum install hue</strong> Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes. Once it completes installation, and provided you had no errors, enter at a command prompt: <strong>service hue start</strong> Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:8888/\">http://HadoopTest:8888</a> in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add this property before the final </configuration> tag: <strong><property></strong> **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong> Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file <strong>/etc/hadoop/conf/core-site.xml</strong> and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property>** **       <property>** **               <name>hadoop.proxyuser.hue.groups</name>** **               <value>*</value>** **       </property>** Now edit the file: <strong>/etc/hue/conf/hue.ini</strong>. Near the top is a section called [desktop] with the first parameter there being secret_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for [[hdfs_clusters]] and look for a comment section below it that starts with “<strong>#Use WebHDFS/HttpFs as the communication mechanism</strong>“. At the end of that section, add this: <strong>webhdfs_url=http://<full server name with domain>:50070/webhdfs/v1/</strong> substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. The Hue plugin that is used to communication with the JobTracker should already be installed as part of the dependencies. You can verify this by changing folders to /usr/lib/hadoop/lib and verifying there is a hue-plugins jar file present. Restart the HUE server <strong>service hue start</strong> and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4","id":"1985","comments":0,"date":"2014-04-21T19:43:43.000Z","_content":"\n[![oozie-derby](http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png)](http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png)This is part 4 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE - the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  In this installment, we'll set up Oozie - a job scheduler component for Hadoop, and fix a minor problem with the Hue setup. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### Install Oozie\n\n1.  Open a terminal window and switch to the root account.\n2.  Start the installation of the Oozie server and client tools with this command: **       yum install oozie**\n3.  By default, Oozie uses a Derby database. (If you aren't familiar with Derby, its a relational database management system from Apache, implemented in Java. It has very minimal operational requirements, making it ideal for implementing as part of other applications). Derby has some limitations, although as part of a  pseudo cluster it should be adequate.\n4.  We are using MRv1 with this setup, and no SSL because its a development/test system. To let Oozie know, enter the following command: **alternatives --set oozie-tomcat-conf /etc/oozie/tomcat-conf.http.mr1**\n5.  Oozie has a web console for management, but because we are using HUE for our setup, its not necessary, so we won't configure it.\n6.  Next, create the Oozie database in Derby by entering this command:      **sudo -u oozie /usr/lib/oozie/bin/ooziedb.sh create -run**\n7.  Create the Oozie user folder in your Hadoop file system , and then change the owner to Oozie: **     sudo -u hdfs hadoop fs -mkdir /user/oozie sudo -u hdfs hadoop fs -chown oozie:oozie /user/oozie**\n8.  Finally to install the shared library, enter this command: **oozie-setup sharelib create -fs hdfs://localhost -locallib /usr/lib/oozie/oozie-sharelib-mr1.tar.gz**\n9.  By default Oozie does not allow MapReduce jobs with uber jars. (An uber-jar is a \"super jar\", one that packages both your code package and all its dependencies into one single JAR file.) If you think you need the ability to use uber jar with your pseudo machine, then Edit the file: **/etc/oozie/conf/oozie-site.xml** and add this property at the top just after the <configuration> tab: **<property> <name>oozie.action.mapreduce.uber.jar.enable</name> <value>true</value>** **     </property>**\n10.  Start Oozie with the command: **service oozie start**\n11.  Open the HUE interface (**http://localhost:8888**), refresh it, and view the configuration issues. If everything was setup Ok with Ooze, you’ll see that the errors related to Oozie are now gone.\n12.  Open the Oozie web interface (**http://localhost:11000**) and you should see a message that the Oozie Web Console is disabled.\n\n**HUE Secret Key** Earlier in this series, we saw in the HUE.ini file a placeholder for entering a secret key. This is used as a salt value to generate a security HASH. You can leave this empty, but if you do, the HUE console will let you know about it every time you enter the HUE web interface. Its very easy to fix, so we'll walk through it now.\n\n1.  Open a terminal window, and switch to your root account. (Of course!)\n2.  Edit the Hue configuration file: **/etc/hue/conf.empty/hue.ini**\n3.  The first section of the file is the General configuration. Find the \\[desktop\\] subsection, and look for the secret\\_key parameter. Currently it should be: **secret\\_key=**\n4.  Enter a random string of characters after the equals sign (20-30 should be sufficient). Save the file and exit back to the command prompt.\n5.  Restart Hue with the service restart command: **service hue restart**\n6.  Switch back over to the Hue web interface and re-login. The error message about secret\\_key should now be gone.\n\nThat's all for this installment. Next time, we'll look at getting Hive2 up and running and integrated into HUE. After that is the Impala installation, Spark, and SQOOP.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-4.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\ntags:\n  - CDH\n  - Hadoop\n  - howto\nid: '1985'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-04-21 15:43:43\n---\n\n[![oozie-derby](http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png)](http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png)This is part 4 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE - the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  In this installment, we'll set up Oozie - a job scheduler component for Hadoop, and fix a minor problem with the Hue setup. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### Install Oozie\n\n1.  Open a terminal window and switch to the root account.\n2.  Start the installation of the Oozie server and client tools with this command: **       yum install oozie**\n3.  By default, Oozie uses a Derby database. (If you aren't familiar with Derby, its a relational database management system from Apache, implemented in Java. It has very minimal operational requirements, making it ideal for implementing as part of other applications). Derby has some limitations, although as part of a  pseudo cluster it should be adequate.\n4.  We are using MRv1 with this setup, and no SSL because its a development/test system. To let Oozie know, enter the following command: **alternatives --set oozie-tomcat-conf /etc/oozie/tomcat-conf.http.mr1**\n5.  Oozie has a web console for management, but because we are using HUE for our setup, its not necessary, so we won't configure it.\n6.  Next, create the Oozie database in Derby by entering this command:      **sudo -u oozie /usr/lib/oozie/bin/ooziedb.sh create -run**\n7.  Create the Oozie user folder in your Hadoop file system , and then change the owner to Oozie: **     sudo -u hdfs hadoop fs -mkdir /user/oozie sudo -u hdfs hadoop fs -chown oozie:oozie /user/oozie**\n8.  Finally to install the shared library, enter this command: **oozie-setup sharelib create -fs hdfs://localhost -locallib /usr/lib/oozie/oozie-sharelib-mr1.tar.gz**\n9.  By default Oozie does not allow MapReduce jobs with uber jars. (An uber-jar is a \"super jar\", one that packages both your code package and all its dependencies into one single JAR file.) If you think you need the ability to use uber jar with your pseudo machine, then Edit the file: **/etc/oozie/conf/oozie-site.xml** and add this property at the top just after the <configuration> tab: **<property> <name>oozie.action.mapreduce.uber.jar.enable</name> <value>true</value>** **     </property>**\n10.  Start Oozie with the command: **service oozie start**\n11.  Open the HUE interface (**http://localhost:8888**), refresh it, and view the configuration issues. If everything was setup Ok with Ooze, you’ll see that the errors related to Oozie are now gone.\n12.  Open the Oozie web interface (**http://localhost:11000**) and you should see a message that the Oozie Web Console is disabled.\n\n**HUE Secret Key** Earlier in this series, we saw in the HUE.ini file a placeholder for entering a secret key. This is used as a salt value to generate a security HASH. You can leave this empty, but if you do, the HUE console will let you know about it every time you enter the HUE web interface. Its very easy to fix, so we'll walk through it now.\n\n1.  Open a terminal window, and switch to your root account. (Of course!)\n2.  Edit the Hue configuration file: **/etc/hue/conf.empty/hue.ini**\n3.  The first section of the file is the General configuration. Find the \\[desktop\\] subsection, and look for the secret\\_key parameter. Currently it should be: **secret\\_key=**\n4.  Enter a random string of characters after the equals sign (20-30 should be sufficient). Save the file and exit back to the command prompt.\n5.  Restart Hue with the service restart command: **service hue restart**\n6.  Switch back over to the Hue web interface and re-login. The error message about secret\\_key should now be gone.\n\nThat's all for this installment. Next time, we'll look at getting Hive2 up and running and integrated into HUE. After that is the Impala installation, Spark, and SQOOP.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-4","published":1,"updated":"2020-08-23T20:54:34.826Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9k00aosdjxe20eh6lg","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png\" alt=\"oozie-derby\"></a>This is part 4 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE - the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  In this installment, we’ll set up Oozie - a job scheduler component for Hadoop, and fix a minor problem with the Hue setup. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<h3 id=\"Install-Oozie\"><a href=\"#Install-Oozie\" class=\"headerlink\" title=\"Install Oozie\"></a>Install Oozie</h3><ol>\n<li>Open a terminal window and switch to the root account.</li>\n<li>Start the installation of the Oozie server and client tools with this command: **       yum install oozie**</li>\n<li>By default, Oozie uses a Derby database. (If you aren’t familiar with Derby, its a relational database management system from Apache, implemented in Java. It has very minimal operational requirements, making it ideal for implementing as part of other applications). Derby has some limitations, although as part of a  pseudo cluster it should be adequate.</li>\n<li>We are using MRv1 with this setup, and no SSL because its a development/test system. To let Oozie know, enter the following command: <strong>alternatives –set oozie-tomcat-conf /etc/oozie/tomcat-conf.http.mr1</strong></li>\n<li>Oozie has a web console for management, but because we are using HUE for our setup, its not necessary, so we won’t configure it.</li>\n<li>Next, create the Oozie database in Derby by entering this command:      <strong>sudo -u oozie /usr/lib/oozie/bin/ooziedb.sh create -run</strong></li>\n<li>Create the Oozie user folder in your Hadoop file system , and then change the owner to Oozie: **     sudo -u hdfs hadoop fs -mkdir /user/oozie sudo -u hdfs hadoop fs -chown oozie:oozie /user/oozie**</li>\n<li>Finally to install the shared library, enter this command: <strong>oozie-setup sharelib create -fs hdfs://localhost -locallib /usr/lib/oozie/oozie-sharelib-mr1.tar.gz</strong></li>\n<li>By default Oozie does not allow MapReduce jobs with uber jars. (An uber-jar is a “super jar”, one that packages both your code package and all its dependencies into one single JAR file.) If you think you need the ability to use uber jar with your pseudo machine, then Edit the file: <strong>/etc/oozie/conf/oozie-site.xml</strong> and add this property at the top just after the <configuration> tab: <strong><property> <name>oozie.action.mapreduce.uber.jar.enable</name> <value>true</value></strong> **     </property>**</li>\n<li>Start Oozie with the command: <strong>service oozie start</strong></li>\n<li>Open the HUE interface (<strong><a href=\"http://localhost:8888/\">http://localhost:8888</a></strong>), refresh it, and view the configuration issues. If everything was setup Ok with Ooze, you’ll see that the errors related to Oozie are now gone.</li>\n<li>Open the Oozie web interface (<strong><a href=\"http://localhost:11000/\">http://localhost:11000</a></strong>) and you should see a message that the Oozie Web Console is disabled.</li>\n</ol>\n<p><strong>HUE Secret Key</strong> Earlier in this series, we saw in the HUE.ini file a placeholder for entering a secret key. This is used as a salt value to generate a security HASH. You can leave this empty, but if you do, the HUE console will let you know about it every time you enter the HUE web interface. Its very easy to fix, so we’ll walk through it now.</p>\n<ol>\n<li>Open a terminal window, and switch to your root account. (Of course!)</li>\n<li>Edit the Hue configuration file: <strong>/etc/hue/conf.empty/hue.ini</strong></li>\n<li>The first section of the file is the General configuration. Find the [desktop] subsection, and look for the secret_key parameter. Currently it should be: <strong>secret_key=</strong></li>\n<li>Enter a random string of characters after the equals sign (20-30 should be sufficient). Save the file and exit back to the command prompt.</li>\n<li>Restart Hue with the service restart command: <strong>service hue restart</strong></li>\n<li>Switch back over to the Hue web interface and re-login. The error message about secret_key should now be gone.</li>\n</ol>\n<p>That’s all for this installment. Next time, we’ll look at getting Hive2 up and running and integrated into HUE. After that is the Impala installation, Spark, and SQOOP.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/oozie-derby.png\" alt=\"oozie-derby\"></a>This is part 4 of my series on how to setup a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE - the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  In this installment, we’ll set up Oozie - a job scheduler component for Hadoop, and fix a minor problem with the Hue setup. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<h3 id=\"Install-Oozie\"><a href=\"#Install-Oozie\" class=\"headerlink\" title=\"Install Oozie\"></a>Install Oozie</h3><ol>\n<li>Open a terminal window and switch to the root account.</li>\n<li>Start the installation of the Oozie server and client tools with this command: **       yum install oozie**</li>\n<li>By default, Oozie uses a Derby database. (If you aren’t familiar with Derby, its a relational database management system from Apache, implemented in Java. It has very minimal operational requirements, making it ideal for implementing as part of other applications). Derby has some limitations, although as part of a  pseudo cluster it should be adequate.</li>\n<li>We are using MRv1 with this setup, and no SSL because its a development/test system. To let Oozie know, enter the following command: <strong>alternatives –set oozie-tomcat-conf /etc/oozie/tomcat-conf.http.mr1</strong></li>\n<li>Oozie has a web console for management, but because we are using HUE for our setup, its not necessary, so we won’t configure it.</li>\n<li>Next, create the Oozie database in Derby by entering this command:      <strong>sudo -u oozie /usr/lib/oozie/bin/ooziedb.sh create -run</strong></li>\n<li>Create the Oozie user folder in your Hadoop file system , and then change the owner to Oozie: **     sudo -u hdfs hadoop fs -mkdir /user/oozie sudo -u hdfs hadoop fs -chown oozie:oozie /user/oozie**</li>\n<li>Finally to install the shared library, enter this command: <strong>oozie-setup sharelib create -fs hdfs://localhost -locallib /usr/lib/oozie/oozie-sharelib-mr1.tar.gz</strong></li>\n<li>By default Oozie does not allow MapReduce jobs with uber jars. (An uber-jar is a “super jar”, one that packages both your code package and all its dependencies into one single JAR file.) If you think you need the ability to use uber jar with your pseudo machine, then Edit the file: <strong>/etc/oozie/conf/oozie-site.xml</strong> and add this property at the top just after the <configuration> tab: <strong><property> <name>oozie.action.mapreduce.uber.jar.enable</name> <value>true</value></strong> **     </property>**</li>\n<li>Start Oozie with the command: <strong>service oozie start</strong></li>\n<li>Open the HUE interface (<strong><a href=\"http://localhost:8888/\">http://localhost:8888</a></strong>), refresh it, and view the configuration issues. If everything was setup Ok with Ooze, you’ll see that the errors related to Oozie are now gone.</li>\n<li>Open the Oozie web interface (<strong><a href=\"http://localhost:11000/\">http://localhost:11000</a></strong>) and you should see a message that the Oozie Web Console is disabled.</li>\n</ol>\n<p><strong>HUE Secret Key</strong> Earlier in this series, we saw in the HUE.ini file a placeholder for entering a secret key. This is used as a salt value to generate a security HASH. You can leave this empty, but if you do, the HUE console will let you know about it every time you enter the HUE web interface. Its very easy to fix, so we’ll walk through it now.</p>\n<ol>\n<li>Open a terminal window, and switch to your root account. (Of course!)</li>\n<li>Edit the Hue configuration file: <strong>/etc/hue/conf.empty/hue.ini</strong></li>\n<li>The first section of the file is the General configuration. Find the [desktop] subsection, and look for the secret_key parameter. Currently it should be: <strong>secret_key=</strong></li>\n<li>Enter a random string of characters after the equals sign (20-30 should be sufficient). Save the file and exit back to the command prompt.</li>\n<li>Restart Hue with the service restart command: <strong>service hue restart</strong></li>\n<li>Switch back over to the Hue web interface and re-login. The error message about secret_key should now be gone.</li>\n</ol>\n<p>That’s all for this installment. Next time, we’ll look at getting Hive2 up and running and integrated into HUE. After that is the Impala installation, Spark, and SQOOP.</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5","id":"2003","comments":0,"date":"2014-04-25T19:28:11.000Z","_content":"\n[![hivemysql](http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png)](http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png)This is part 5 in my series on setting up our a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier. Last time in [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\"), I walked through the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key.\n\nIn this article, I'll walk through installing and configuring Hive2 and getting it to work with Cloudera's HUE web interface. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.\n\nUpdate: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### **CREATE A HIVE  FOLDER IN HDFS**\n\nIf you have followed along with this series, when you open the HUE web interface that is connected to your pseudo-cluster, you should see this configuration error:\n\n**Hive Editor - Failed to access Hive warehouse: /user/hive/warehouse**\n\nIts means that HUE can't get to the Hive warehouse folder in HDFS, and its a very easy problem to fix. As we've done previously, open up a terminal window, and switch over to the root account with: **su -**\n\n1.  Create the hive user folder in hdfs, and a warehouse subfolder, then change the ownership by running these three commands, one after another:  **sudo -u hdfs hadoop fs -mkdir /user/hive** **sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse** **           sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse**\n2.  Restart the HUE service and re-login:  **service hue restart**\n\n### **INSTALL HIVE SERVER 2**\n\nThe previous error will be gone, and it will be replaced with a new one: **Hive Editor - The application won't work without a running Hive Server2.**\n\nUnfortunately, the fix for this problem is a little more involved. Earlier in this series, one of the components we were dealing with was using a Derby database. While Hive Server 2 will also work with Derby, I've had better results using MySQL as a backend for Hive, so we'll install that and configure it for Hive.\n\n1.  In a terminal window as root, install MySQL:  **yum install mysql-server**\n2.  Start the service with this command:  **service mysqld start**\n3.  Now install the JDBC connector package for MySQL and copy the JAR file to the hive home folder:  **yum install mysql-connector-java**  **cp  /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar**\n4.  Configure MySQL to make it more secure by running the following command: **/usr/bin/mysql\\_secure\\_installation** At the prompts you should: set a new password, remove anonymous users, allow remote root login, remove test database, and reload privilege tables.\n5.  Configure your system to start MySQL at startup:  **/sbin/chkconfig mysqld on**\n6.  And verify that the settings are OK. The results of the following command should indicate that run-levels 2 through 5 are set to ON.  **/sbin/chkconfig --list mysqld**\n7.  Hive 2 requires a database to store some of its information in, so create it:  **mysql -u root -p** You'll be prompted to enter the password  you set in step 4 above. At the mysql> prompts, enter the following commands to create a database and switch to it: **CREATE DATABASE metastore;**  **USE metastore;**\n8.  Tell MySQL to create the structure of the database using this command at the mysql> prompt: **SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;**\n9.  Finally, add a local hive user to the database and assign permissions to it. This account will only be able to access the database from the local machine:  **CREATE USER 'hive'@'localhost' IDENTIFIED BY 'mypassword’;**  **REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'localhost';**  **GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.\\* TO 'hive’@‘localhost’; FLUSH PRIVILEGES;** **QUIT;**\n10.  MySQL is now installed with a database and user for Hive to use. Now let's setup Hive2.\n\n### HIVE 2 SETUP\n\n1.  CDH5 has replaced the original version of HIVE with HIVE2. In order to make it work with CDH5 and HUE, run the following commands to install the  components (as root in a terminal windows): **yum install hive-metastore** **service hive-metastore start** **yum install hive-server2** **service hive-server2 start** **     yum install hive-hbase**\n2.  Edit **/etc/hive/conf/hive-site.xml** to change connection info to mySQL. The first two properties should already be in the file, and will need to be modified: <property>     <name>javax.jdo.option.ConnectionURL</name>     <value>jdbc:mysql://localhost/metastore_?createDatabaseIfNotExist=true<_/value> <description>the URL of the MySQL database</description> </property><property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.jdbc.Driver</value> </property><property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password you created in step 9 for MySQL</value> </property>\n3.  Save the file, and restart your server.\n4.  After your server reboots, open a terminal window and switch to the root account. Check the hive metastore service:  **service hive-metastore status** If it responds dead, then you need to make sure the java-connector has been copied correctly to **/usr/lib/hive/lib/mysql-connector-java.jar**\n5.  Login to Hue, and the error about Hive Server 2 should be gone.\n\nAlmost complete! Next time I'll cover setting up Impala with our pseudo cluster, and talk about Spark.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-5.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\ntags:\n  - CDH\n  - Hadoop\n  - howto\n  - install\n  - MySQL\nid: '2003'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2014-04-25 15:28:11\n---\n\n[![hivemysql](http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png)](http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png)This is part 5 in my series on setting up our a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier. Last time in [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\"), I walked through the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key.\n\nIn this article, I'll walk through installing and configuring Hive2 and getting it to work with Cloudera's HUE web interface. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.\n\nUpdate: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### **CREATE A HIVE  FOLDER IN HDFS**\n\nIf you have followed along with this series, when you open the HUE web interface that is connected to your pseudo-cluster, you should see this configuration error:\n\n**Hive Editor - Failed to access Hive warehouse: /user/hive/warehouse**\n\nIts means that HUE can't get to the Hive warehouse folder in HDFS, and its a very easy problem to fix. As we've done previously, open up a terminal window, and switch over to the root account with: **su -**\n\n1.  Create the hive user folder in hdfs, and a warehouse subfolder, then change the ownership by running these three commands, one after another:  **sudo -u hdfs hadoop fs -mkdir /user/hive** **sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse** **           sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse**\n2.  Restart the HUE service and re-login:  **service hue restart**\n\n### **INSTALL HIVE SERVER 2**\n\nThe previous error will be gone, and it will be replaced with a new one: **Hive Editor - The application won't work without a running Hive Server2.**\n\nUnfortunately, the fix for this problem is a little more involved. Earlier in this series, one of the components we were dealing with was using a Derby database. While Hive Server 2 will also work with Derby, I've had better results using MySQL as a backend for Hive, so we'll install that and configure it for Hive.\n\n1.  In a terminal window as root, install MySQL:  **yum install mysql-server**\n2.  Start the service with this command:  **service mysqld start**\n3.  Now install the JDBC connector package for MySQL and copy the JAR file to the hive home folder:  **yum install mysql-connector-java**  **cp  /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar**\n4.  Configure MySQL to make it more secure by running the following command: **/usr/bin/mysql\\_secure\\_installation** At the prompts you should: set a new password, remove anonymous users, allow remote root login, remove test database, and reload privilege tables.\n5.  Configure your system to start MySQL at startup:  **/sbin/chkconfig mysqld on**\n6.  And verify that the settings are OK. The results of the following command should indicate that run-levels 2 through 5 are set to ON.  **/sbin/chkconfig --list mysqld**\n7.  Hive 2 requires a database to store some of its information in, so create it:  **mysql -u root -p** You'll be prompted to enter the password  you set in step 4 above. At the mysql> prompts, enter the following commands to create a database and switch to it: **CREATE DATABASE metastore;**  **USE metastore;**\n8.  Tell MySQL to create the structure of the database using this command at the mysql> prompt: **SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;**\n9.  Finally, add a local hive user to the database and assign permissions to it. This account will only be able to access the database from the local machine:  **CREATE USER 'hive'@'localhost' IDENTIFIED BY 'mypassword’;**  **REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'localhost';**  **GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.\\* TO 'hive’@‘localhost’; FLUSH PRIVILEGES;** **QUIT;**\n10.  MySQL is now installed with a database and user for Hive to use. Now let's setup Hive2.\n\n### HIVE 2 SETUP\n\n1.  CDH5 has replaced the original version of HIVE with HIVE2. In order to make it work with CDH5 and HUE, run the following commands to install the  components (as root in a terminal windows): **yum install hive-metastore** **service hive-metastore start** **yum install hive-server2** **service hive-server2 start** **     yum install hive-hbase**\n2.  Edit **/etc/hive/conf/hive-site.xml** to change connection info to mySQL. The first two properties should already be in the file, and will need to be modified: <property>     <name>javax.jdo.option.ConnectionURL</name>     <value>jdbc:mysql://localhost/metastore_?createDatabaseIfNotExist=true<_/value> <description>the URL of the MySQL database</description> </property><property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.jdbc.Driver</value> </property><property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password you created in step 9 for MySQL</value> </property>\n3.  Save the file, and restart your server.\n4.  After your server reboots, open a terminal window and switch to the root account. Check the hive metastore service:  **service hive-metastore status** If it responds dead, then you need to make sure the java-connector has been copied correctly to **/usr/lib/hive/lib/mysql-connector-java.jar**\n5.  Login to Hue, and the error about Hive Server 2 should be gone.\n\nAlmost complete! Next time I'll cover setting up Impala with our pseudo cluster, and talk about Spark.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-5","published":1,"updated":"2020-08-23T20:54:34.826Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9o00assdjxbgm129yc","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png\" alt=\"hivemysql\"></a>This is part 5 in my series on setting up our a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier. Last time in <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a>, I walked through the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key.</p>\n<p>In this article, I’ll walk through installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.</p>\n<p>Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<h3 id=\"CREATE-A-HIVE-FOLDER-IN-HDFS\"><a href=\"#CREATE-A-HIVE-FOLDER-IN-HDFS\" class=\"headerlink\" title=\"CREATE A HIVE  FOLDER IN HDFS\"></a><strong>CREATE A HIVE  FOLDER IN HDFS</strong></h3><p>If you have followed along with this series, when you open the HUE web interface that is connected to your pseudo-cluster, you should see this configuration error:</p>\n<p><strong>Hive Editor - Failed to access Hive warehouse: /user/hive/warehouse</strong></p>\n<p>Its means that HUE can’t get to the Hive warehouse folder in HDFS, and its a very easy problem to fix. As we’ve done previously, open up a terminal window, and switch over to the root account with: <strong>su -</strong></p>\n<ol>\n<li>Create the hive user folder in hdfs, and a warehouse subfolder, then change the ownership by running these three commands, one after another:  <strong>sudo -u hdfs hadoop fs -mkdir /user/hive</strong> <strong>sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse</strong> **           sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse**</li>\n<li>Restart the HUE service and re-login:  <strong>service hue restart</strong></li>\n</ol>\n<h3 id=\"INSTALL-HIVE-SERVER-2\"><a href=\"#INSTALL-HIVE-SERVER-2\" class=\"headerlink\" title=\"INSTALL HIVE SERVER 2\"></a><strong>INSTALL HIVE SERVER 2</strong></h3><p>The previous error will be gone, and it will be replaced with a new one: <strong>Hive Editor - The application won’t work without a running Hive Server2.</strong></p>\n<p>Unfortunately, the fix for this problem is a little more involved. Earlier in this series, one of the components we were dealing with was using a Derby database. While Hive Server 2 will also work with Derby, I’ve had better results using MySQL as a backend for Hive, so we’ll install that and configure it for Hive.</p>\n<ol>\n<li>In a terminal window as root, install MySQL:  <strong>yum install mysql-server</strong></li>\n<li>Start the service with this command:  <strong>service mysqld start</strong></li>\n<li>Now install the JDBC connector package for MySQL and copy the JAR file to the hive home folder:  <strong>yum install mysql-connector-java</strong>  <strong>cp  /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar</strong></li>\n<li>Configure MySQL to make it more secure by running the following command: <strong>/usr/bin/mysql_secure_installation</strong> At the prompts you should: set a new password, remove anonymous users, allow remote root login, remove test database, and reload privilege tables.</li>\n<li>Configure your system to start MySQL at startup:  <strong>/sbin/chkconfig mysqld on</strong></li>\n<li>And verify that the settings are OK. The results of the following command should indicate that run-levels 2 through 5 are set to ON.  <strong>/sbin/chkconfig –list mysqld</strong></li>\n<li>Hive 2 requires a database to store some of its information in, so create it:  <strong>mysql -u root -p</strong> You’ll be prompted to enter the password  you set in step 4 above. At the mysql&gt; prompts, enter the following commands to create a database and switch to it: <strong>CREATE DATABASE metastore;</strong>  <strong>USE metastore;</strong></li>\n<li>Tell MySQL to create the structure of the database using this command at the mysql&gt; prompt: <strong>SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;</strong></li>\n<li>Finally, add a local hive user to the database and assign permissions to it. This account will only be able to access the database from the local machine:  <strong>CREATE USER ‘hive’@’localhost’ IDENTIFIED BY ‘mypassword’;</strong>  <strong>REVOKE ALL PRIVILEGES, GRANT OPTION FROM ‘hive’@’localhost’;</strong>  <strong>GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.* TO ‘hive’@‘localhost’; FLUSH PRIVILEGES;</strong> <strong>QUIT;</strong></li>\n<li>MySQL is now installed with a database and user for Hive to use. Now let’s setup Hive2.</li>\n</ol>\n<h3 id=\"HIVE-2-SETUP\"><a href=\"#HIVE-2-SETUP\" class=\"headerlink\" title=\"HIVE 2 SETUP\"></a>HIVE 2 SETUP</h3><ol>\n<li>CDH5 has replaced the original version of HIVE with HIVE2. In order to make it work with CDH5 and HUE, run the following commands to install the  components (as root in a terminal windows): <strong>yum install hive-metastore</strong> <strong>service hive-metastore start</strong> <strong>yum install hive-server2</strong> <strong>service hive-server2 start</strong> **     yum install hive-hbase**</li>\n<li>Edit <strong>/etc/hive/conf/hive-site.xml</strong> to change connection info to mySQL. The first two properties should already be in the file, and will need to be modified: <property>     <name>javax.jdo.option.ConnectionURL</name>     <value>jdbc:mysql://localhost/metastore_?createDatabaseIfNotExist=true&lt;_/value&gt; <description>the URL of the MySQL database</description> </property><property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.jdbc.Driver</value> </property><property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password you created in step 9 for MySQL</value> </property></li>\n<li>Save the file, and restart your server.</li>\n<li>After your server reboots, open a terminal window and switch to the root account. Check the hive metastore service:  <strong>service hive-metastore status</strong> If it responds dead, then you need to make sure the java-connector has been copied correctly to <strong>/usr/lib/hive/lib/mysql-connector-java.jar</strong></li>\n<li>Login to Hue, and the error about Hive Server 2 should be gone.</li>\n</ol>\n<p>Almost complete! Next time I’ll cover setting up Impala with our pseudo cluster, and talk about Spark.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hivemysql.png\" alt=\"hivemysql\"></a>This is part 5 in my series on setting up our a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier. Last time in <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a>, I walked through the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key.</p>\n<p>In this article, I’ll walk through installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.</p>\n<p>Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<h3 id=\"CREATE-A-HIVE-FOLDER-IN-HDFS\"><a href=\"#CREATE-A-HIVE-FOLDER-IN-HDFS\" class=\"headerlink\" title=\"CREATE A HIVE  FOLDER IN HDFS\"></a><strong>CREATE A HIVE  FOLDER IN HDFS</strong></h3><p>If you have followed along with this series, when you open the HUE web interface that is connected to your pseudo-cluster, you should see this configuration error:</p>\n<p><strong>Hive Editor - Failed to access Hive warehouse: /user/hive/warehouse</strong></p>\n<p>Its means that HUE can’t get to the Hive warehouse folder in HDFS, and its a very easy problem to fix. As we’ve done previously, open up a terminal window, and switch over to the root account with: <strong>su -</strong></p>\n<ol>\n<li>Create the hive user folder in hdfs, and a warehouse subfolder, then change the ownership by running these three commands, one after another:  <strong>sudo -u hdfs hadoop fs -mkdir /user/hive</strong> <strong>sudo -u hdfs hadoop fs -mkdir /user/hive/warehouse</strong> **           sudo -u hdfs hadoop fs -chmod 1777 /user/hive/warehouse**</li>\n<li>Restart the HUE service and re-login:  <strong>service hue restart</strong></li>\n</ol>\n<h3 id=\"INSTALL-HIVE-SERVER-2\"><a href=\"#INSTALL-HIVE-SERVER-2\" class=\"headerlink\" title=\"INSTALL HIVE SERVER 2\"></a><strong>INSTALL HIVE SERVER 2</strong></h3><p>The previous error will be gone, and it will be replaced with a new one: <strong>Hive Editor - The application won’t work without a running Hive Server2.</strong></p>\n<p>Unfortunately, the fix for this problem is a little more involved. Earlier in this series, one of the components we were dealing with was using a Derby database. While Hive Server 2 will also work with Derby, I’ve had better results using MySQL as a backend for Hive, so we’ll install that and configure it for Hive.</p>\n<ol>\n<li>In a terminal window as root, install MySQL:  <strong>yum install mysql-server</strong></li>\n<li>Start the service with this command:  <strong>service mysqld start</strong></li>\n<li>Now install the JDBC connector package for MySQL and copy the JAR file to the hive home folder:  <strong>yum install mysql-connector-java</strong>  <strong>cp  /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar</strong></li>\n<li>Configure MySQL to make it more secure by running the following command: <strong>/usr/bin/mysql_secure_installation</strong> At the prompts you should: set a new password, remove anonymous users, allow remote root login, remove test database, and reload privilege tables.</li>\n<li>Configure your system to start MySQL at startup:  <strong>/sbin/chkconfig mysqld on</strong></li>\n<li>And verify that the settings are OK. The results of the following command should indicate that run-levels 2 through 5 are set to ON.  <strong>/sbin/chkconfig –list mysqld</strong></li>\n<li>Hive 2 requires a database to store some of its information in, so create it:  <strong>mysql -u root -p</strong> You’ll be prompted to enter the password  you set in step 4 above. At the mysql&gt; prompts, enter the following commands to create a database and switch to it: <strong>CREATE DATABASE metastore;</strong>  <strong>USE metastore;</strong></li>\n<li>Tell MySQL to create the structure of the database using this command at the mysql&gt; prompt: <strong>SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;</strong></li>\n<li>Finally, add a local hive user to the database and assign permissions to it. This account will only be able to access the database from the local machine:  <strong>CREATE USER ‘hive’@’localhost’ IDENTIFIED BY ‘mypassword’;</strong>  <strong>REVOKE ALL PRIVILEGES, GRANT OPTION FROM ‘hive’@’localhost’;</strong>  <strong>GRANT SELECT,INSERT,UPDATE,DELETE,LOCK TABLES,EXECUTE ON metastore.* TO ‘hive’@‘localhost’; FLUSH PRIVILEGES;</strong> <strong>QUIT;</strong></li>\n<li>MySQL is now installed with a database and user for Hive to use. Now let’s setup Hive2.</li>\n</ol>\n<h3 id=\"HIVE-2-SETUP\"><a href=\"#HIVE-2-SETUP\" class=\"headerlink\" title=\"HIVE 2 SETUP\"></a>HIVE 2 SETUP</h3><ol>\n<li>CDH5 has replaced the original version of HIVE with HIVE2. In order to make it work with CDH5 and HUE, run the following commands to install the  components (as root in a terminal windows): <strong>yum install hive-metastore</strong> <strong>service hive-metastore start</strong> <strong>yum install hive-server2</strong> <strong>service hive-server2 start</strong> **     yum install hive-hbase**</li>\n<li>Edit <strong>/etc/hive/conf/hive-site.xml</strong> to change connection info to mySQL. The first two properties should already be in the file, and will need to be modified: <property>     <name>javax.jdo.option.ConnectionURL</name>     <value>jdbc:mysql://localhost/metastore_?createDatabaseIfNotExist=true&lt;_/value&gt; <description>the URL of the MySQL database</description> </property><property> <name>javax.jdo.option.ConnectionDriverName</name> <value>com.mysql.jdbc.Driver</value> </property><property> <name>javax.jdo.option.ConnectionUserName</name> <value>hive</value> </property> <property> <name>javax.jdo.option.ConnectionPassword</name> <value>password you created in step 9 for MySQL</value> </property></li>\n<li>Save the file, and restart your server.</li>\n<li>After your server reboots, open a terminal window and switch to the root account. Check the hive metastore service:  <strong>service hive-metastore status</strong> If it responds dead, then you need to make sure the java-connector has been copied correctly to <strong>/usr/lib/hive/lib/mysql-connector-java.jar</strong></li>\n<li>Login to Hue, and the error about Hive Server 2 should be gone.</li>\n</ol>\n<p>Almost complete! Next time I’ll cover setting up Impala with our pseudo cluster, and talk about Spark.</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 6","id":"2024","comments":0,"date":"2014-04-28T19:35:14.000Z","_content":"\n[![Männliche Impala in der Serengeti](http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg)](http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg)This is part 6 in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\") as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Last time, in [Part 5](http://edpflager.com/?p=2003 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\"),  installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered.\n\nIn this article, I'll show the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera's query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.\n\nUpdate: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### INSTALLING IMPALA\n\nAccording to Cloudera's [website](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html), \"Impala is a massively parallel processing query engine that runs natively in Apache Hadoop.\" As more and more organizations are looking to use Hadoop to handle the vast amounts of data, tools that can be used that work with a SQL like syntax have become more readily available. Impala is one, HIVE is another, but they aren't the only ones: Hortonworks has open-sourced Stinger, Facebook  provides Presto, and  HAWQ from Pivotal are just a few.\n\n1.  Before installing Impala, you need to add the Impala Repository information to your system. As root at a terminal prompt, change to the **/etc/yum.repos.d** folder.\n2.  Download the Impala Repo information from Cloudera, with this command: **      wget http://archive.cloudera.com/impala/redhat/6/x86\\_64/impala/cloudera-impala.repo**\n3.  Now to install the various Impala components, run these commands: **     yum install impala** **     yum install impala-server** **     yum install impala-state-store** **     yum install impala-catalog**\n4.  Once the components are installed, edit **/etc/hadoop/conf/hdfs-site.xml** and add these properties to it: **     <property>          ** **<name>dfs.client.read.shortcircuit</name>** **<value>true</value>** **     </property>** **<property>** **<name>dfs.domain.socket.path</name>** **          <value>/var/run/hadoop-hdfs/dn.\\_PORT</value>  ** **     </property>** **<property>** **<name>dfs.client.file-block-storage-locations.timeout</name>** **<value>30000</value>** **          <--- Cloudera's website says 3000 but that caused my impala-server to shutdown because the value was too low. -->            ** **     </property>** **<property>** **          <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>          ** **<value>true</value>** **     </property>**\n5.  Save the file and exit. Now copy several config files to the **/etc/impala/conf** folder. (I have not tried using symbolic links for this, but it may be possible.) **cp /etc/hive/conf/hive-site.xml /etc/impala/conf** **cp /etc/hadoop/conf/core-site.xml /etc/impala/conf** **     cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf**\n6.  Change the group owner on **/var/run/hadoop-hdfs** to root: **     chgrp root /var/run/hadoop-hdfs**\n7.  Finally, install the Impala Shell application with this command: **yum install impala-shell**\n8.  Go ahead and restart the server (for me that is easier than restarting all of the various services). At this point you will not have any database connections defined for Impala. Please see Cloudera's website for information on setting up [ODBC](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_odbc.html) connections and [JDBC](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_jdbc.html) connections.\n9.  When your server comes back up, login to HUE and the error messages about Impala should be gone and you should be able to access Impala from the menu.\n\nINSTALLING Sqoop2 Apache Sqoop2 is useful when you have data that you need to interact with in a Hadoop environment that comes from or will be pushed to a relational database management (RDBMS) systems. In addition, because Sqoop2 moves the data in a bulk load fashion, it can save time when bringing a very large amount of data to a Hadoop cluster for processing, even with the addition of the movement of data.\n\n1.  Open a terminal window and change to root user.\n2.  Enter this command to install the sqoop2 server and client packages: **yum install sqoop2-server** \n3.  Sqoop2 can be used with MapReduce v1 or v2 (YARN), but not at the same time. Configuration files for use with the Tomcat webserver for both are provided as part of the installation, so you need to define which one to use. Add an alternatives entry for it (that's two dashes before the word SET): **   alternatives --set sqoop2-tomcat-conf /etc/sqoop2/tomcat-conf.mr1**\n4.  Start the Sqoop2 service: **     service sqoop2-server start**\n5.  Restart HUE: **service hue restart**\n\nSqoop2 should now be accessible from the menu in Hive as well. Next time, we'll finish up setting up the single node Hadoop server with CDH5 and HUE by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-6.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE – Part 6\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - howto\n  - impala\nid: '2024'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-04-28 15:35:14\n---\n\n[![Männliche Impala in der Serengeti](http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg)](http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg)This is part 6 in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\") as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Last time, in [Part 5](http://edpflager.com/?p=2003 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\"),  installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered.\n\nIn this article, I'll show the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera's query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.\n\nUpdate: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\n### INSTALLING IMPALA\n\nAccording to Cloudera's [website](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html), \"Impala is a massively parallel processing query engine that runs natively in Apache Hadoop.\" As more and more organizations are looking to use Hadoop to handle the vast amounts of data, tools that can be used that work with a SQL like syntax have become more readily available. Impala is one, HIVE is another, but they aren't the only ones: Hortonworks has open-sourced Stinger, Facebook  provides Presto, and  HAWQ from Pivotal are just a few.\n\n1.  Before installing Impala, you need to add the Impala Repository information to your system. As root at a terminal prompt, change to the **/etc/yum.repos.d** folder.\n2.  Download the Impala Repo information from Cloudera, with this command: **      wget http://archive.cloudera.com/impala/redhat/6/x86\\_64/impala/cloudera-impala.repo**\n3.  Now to install the various Impala components, run these commands: **     yum install impala** **     yum install impala-server** **     yum install impala-state-store** **     yum install impala-catalog**\n4.  Once the components are installed, edit **/etc/hadoop/conf/hdfs-site.xml** and add these properties to it: **     <property>          ** **<name>dfs.client.read.shortcircuit</name>** **<value>true</value>** **     </property>** **<property>** **<name>dfs.domain.socket.path</name>** **          <value>/var/run/hadoop-hdfs/dn.\\_PORT</value>  ** **     </property>** **<property>** **<name>dfs.client.file-block-storage-locations.timeout</name>** **<value>30000</value>** **          <--- Cloudera's website says 3000 but that caused my impala-server to shutdown because the value was too low. -->            ** **     </property>** **<property>** **          <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>          ** **<value>true</value>** **     </property>**\n5.  Save the file and exit. Now copy several config files to the **/etc/impala/conf** folder. (I have not tried using symbolic links for this, but it may be possible.) **cp /etc/hive/conf/hive-site.xml /etc/impala/conf** **cp /etc/hadoop/conf/core-site.xml /etc/impala/conf** **     cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf**\n6.  Change the group owner on **/var/run/hadoop-hdfs** to root: **     chgrp root /var/run/hadoop-hdfs**\n7.  Finally, install the Impala Shell application with this command: **yum install impala-shell**\n8.  Go ahead and restart the server (for me that is easier than restarting all of the various services). At this point you will not have any database connections defined for Impala. Please see Cloudera's website for information on setting up [ODBC](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_odbc.html) connections and [JDBC](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_jdbc.html) connections.\n9.  When your server comes back up, login to HUE and the error messages about Impala should be gone and you should be able to access Impala from the menu.\n\nINSTALLING Sqoop2 Apache Sqoop2 is useful when you have data that you need to interact with in a Hadoop environment that comes from or will be pushed to a relational database management (RDBMS) systems. In addition, because Sqoop2 moves the data in a bulk load fashion, it can save time when bringing a very large amount of data to a Hadoop cluster for processing, even with the addition of the movement of data.\n\n1.  Open a terminal window and change to root user.\n2.  Enter this command to install the sqoop2 server and client packages: **yum install sqoop2-server** \n3.  Sqoop2 can be used with MapReduce v1 or v2 (YARN), but not at the same time. Configuration files for use with the Tomcat webserver for both are provided as part of the installation, so you need to define which one to use. Add an alternatives entry for it (that's two dashes before the word SET): **   alternatives --set sqoop2-tomcat-conf /etc/sqoop2/tomcat-conf.mr1**\n4.  Start the Sqoop2 service: **     service sqoop2-server start**\n5.  Restart HUE: **service hue restart**\n\nSqoop2 should now be accessible from the menu in Hive as well. Next time, we'll finish up setting up the single node Hadoop server with CDH5 and HUE by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-6","published":1,"updated":"2020-08-23T20:54:34.830Z","layout":"post","photos":[],"link":"","_id":"ckeaq9a9t00avsdjx15ffe0ri","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg\" alt=\"Männliche Impala in der Serengeti\"></a>This is part 6 in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a> as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Last time, in <a href=\"http://edpflager.com/?p=2003\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\">Part 5</a>,  installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered.</p>\n<p>In this article, I’ll show the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera’s query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.</p>\n<p>Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<h3 id=\"INSTALLING-IMPALA\"><a href=\"#INSTALLING-IMPALA\" class=\"headerlink\" title=\"INSTALLING IMPALA\"></a>INSTALLING IMPALA</h3><p>According to Cloudera’s <a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html\">website</a>, “Impala is a massively parallel processing query engine that runs natively in Apache Hadoop.” As more and more organizations are looking to use Hadoop to handle the vast amounts of data, tools that can be used that work with a SQL like syntax have become more readily available. Impala is one, HIVE is another, but they aren’t the only ones: Hortonworks has open-sourced Stinger, Facebook  provides Presto, and  HAWQ from Pivotal are just a few.</p>\n<ol>\n<li>Before installing Impala, you need to add the Impala Repository information to your system. As root at a terminal prompt, change to the <strong>/etc/yum.repos.d</strong> folder.</li>\n<li>Download the Impala Repo information from Cloudera, with this command: **      wget <a href=\"http://archive.cloudera.com/impala/redhat/6/x86/_64/impala/cloudera-impala.repo\">http://archive.cloudera.com/impala/redhat/6/x86\\_64/impala/cloudera-impala.repo</a>**</li>\n<li>Now to install the various Impala components, run these commands: **     yum install impala** **     yum install impala-server** **     yum install impala-state-store** **     yum install impala-catalog**</li>\n<li>Once the components are installed, edit <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add these properties to it: **     <property>          ** <strong><name>dfs.client.read.shortcircuit</name></strong> <strong><value>true</value></strong> **     </property>** <strong><property></strong> <strong><name>dfs.domain.socket.path</name></strong> **          <value>/var/run/hadoop-hdfs/dn._PORT</value>  ** **     </property>** <strong><property></strong> <strong><name>dfs.client.file-block-storage-locations.timeout</name></strong> <strong><value>30000</value></strong> **          &lt;— Cloudera’s website says 3000 but that caused my impala-server to shutdown because the value was too low. –&gt;            ** **     </property>** <strong><property></strong> **          <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>          ** <strong><value>true</value></strong> **     </property>**</li>\n<li>Save the file and exit. Now copy several config files to the <strong>/etc/impala/conf</strong> folder. (I have not tried using symbolic links for this, but it may be possible.) <strong>cp /etc/hive/conf/hive-site.xml /etc/impala/conf</strong> <strong>cp /etc/hadoop/conf/core-site.xml /etc/impala/conf</strong> **     cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf**</li>\n<li>Change the group owner on <strong>/var/run/hadoop-hdfs</strong> to root: **     chgrp root /var/run/hadoop-hdfs**</li>\n<li>Finally, install the Impala Shell application with this command: <strong>yum install impala-shell</strong></li>\n<li>Go ahead and restart the server (for me that is easier than restarting all of the various services). At this point you will not have any database connections defined for Impala. Please see Cloudera’s website for information on setting up <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_odbc.html\">ODBC</a> connections and <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_jdbc.html\">JDBC</a> connections.</li>\n<li>When your server comes back up, login to HUE and the error messages about Impala should be gone and you should be able to access Impala from the menu.</li>\n</ol>\n<p>INSTALLING Sqoop2 Apache Sqoop2 is useful when you have data that you need to interact with in a Hadoop environment that comes from or will be pushed to a relational database management (RDBMS) systems. In addition, because Sqoop2 moves the data in a bulk load fashion, it can save time when bringing a very large amount of data to a Hadoop cluster for processing, even with the addition of the movement of data.</p>\n<ol>\n<li>Open a terminal window and change to root user.</li>\n<li>Enter this command to install the sqoop2 server and client packages: <strong>yum install sqoop2-server</strong> </li>\n<li>Sqoop2 can be used with MapReduce v1 or v2 (YARN), but not at the same time. Configuration files for use with the Tomcat webserver for both are provided as part of the installation, so you need to define which one to use. Add an alternatives entry for it (that’s two dashes before the word SET): **   alternatives –set sqoop2-tomcat-conf /etc/sqoop2/tomcat-conf.mr1**</li>\n<li>Start the Sqoop2 service: **     service sqoop2-server start**</li>\n<li>Restart HUE: <strong>service hue restart</strong></li>\n</ol>\n<p>Sqoop2 should now be accessible from the menu in Hive as well. Next time, we’ll finish up setting up the single node Hadoop server with CDH5 and HUE by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/Impala-Sqoop.jpg\" alt=\"Männliche Impala in der Serengeti\"></a>This is part 6 in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a> as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Last time, in <a href=\"http://edpflager.com/?p=2003\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\">Part 5</a>,  installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered.</p>\n<p>In this article, I’ll show the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera’s query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. This series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster.</p>\n<p>Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<h3 id=\"INSTALLING-IMPALA\"><a href=\"#INSTALLING-IMPALA\" class=\"headerlink\" title=\"INSTALLING IMPALA\"></a>INSTALLING IMPALA</h3><p>According to Cloudera’s <a href=\"http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html\">website</a>, “Impala is a massively parallel processing query engine that runs natively in Apache Hadoop.” As more and more organizations are looking to use Hadoop to handle the vast amounts of data, tools that can be used that work with a SQL like syntax have become more readily available. Impala is one, HIVE is another, but they aren’t the only ones: Hortonworks has open-sourced Stinger, Facebook  provides Presto, and  HAWQ from Pivotal are just a few.</p>\n<ol>\n<li>Before installing Impala, you need to add the Impala Repository information to your system. As root at a terminal prompt, change to the <strong>/etc/yum.repos.d</strong> folder.</li>\n<li>Download the Impala Repo information from Cloudera, with this command: **      wget <a href=\"http://archive.cloudera.com/impala/redhat/6/x86/_64/impala/cloudera-impala.repo\">http://archive.cloudera.com/impala/redhat/6/x86\\_64/impala/cloudera-impala.repo</a>**</li>\n<li>Now to install the various Impala components, run these commands: **     yum install impala** **     yum install impala-server** **     yum install impala-state-store** **     yum install impala-catalog**</li>\n<li>Once the components are installed, edit <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add these properties to it: **     <property>          ** <strong><name>dfs.client.read.shortcircuit</name></strong> <strong><value>true</value></strong> **     </property>** <strong><property></strong> <strong><name>dfs.domain.socket.path</name></strong> **          <value>/var/run/hadoop-hdfs/dn._PORT</value>  ** **     </property>** <strong><property></strong> <strong><name>dfs.client.file-block-storage-locations.timeout</name></strong> <strong><value>30000</value></strong> **          &lt;— Cloudera’s website says 3000 but that caused my impala-server to shutdown because the value was too low. –&gt;            ** **     </property>** <strong><property></strong> **          <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>          ** <strong><value>true</value></strong> **     </property>**</li>\n<li>Save the file and exit. Now copy several config files to the <strong>/etc/impala/conf</strong> folder. (I have not tried using symbolic links for this, but it may be possible.) <strong>cp /etc/hive/conf/hive-site.xml /etc/impala/conf</strong> <strong>cp /etc/hadoop/conf/core-site.xml /etc/impala/conf</strong> **     cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf**</li>\n<li>Change the group owner on <strong>/var/run/hadoop-hdfs</strong> to root: **     chgrp root /var/run/hadoop-hdfs**</li>\n<li>Finally, install the Impala Shell application with this command: <strong>yum install impala-shell</strong></li>\n<li>Go ahead and restart the server (for me that is easier than restarting all of the various services). At this point you will not have any database connections defined for Impala. Please see Cloudera’s website for information on setting up <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_odbc.html\">ODBC</a> connections and <a href=\"http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/Installing-and-Using-Impala/ciiu_impala_jdbc.html\">JDBC</a> connections.</li>\n<li>When your server comes back up, login to HUE and the error messages about Impala should be gone and you should be able to access Impala from the menu.</li>\n</ol>\n<p>INSTALLING Sqoop2 Apache Sqoop2 is useful when you have data that you need to interact with in a Hadoop environment that comes from or will be pushed to a relational database management (RDBMS) systems. In addition, because Sqoop2 moves the data in a bulk load fashion, it can save time when bringing a very large amount of data to a Hadoop cluster for processing, even with the addition of the movement of data.</p>\n<ol>\n<li>Open a terminal window and change to root user.</li>\n<li>Enter this command to install the sqoop2 server and client packages: <strong>yum install sqoop2-server</strong> </li>\n<li>Sqoop2 can be used with MapReduce v1 or v2 (YARN), but not at the same time. Configuration files for use with the Tomcat webserver for both are provided as part of the installation, so you need to define which one to use. Add an alternatives entry for it (that’s two dashes before the word SET): **   alternatives –set sqoop2-tomcat-conf /etc/sqoop2/tomcat-conf.mr1**</li>\n<li>Start the Sqoop2 service: **     service sqoop2-server start**</li>\n<li>Restart HUE: <strong>service hue restart</strong></li>\n</ol>\n<p>Sqoop2 should now be accessible from the menu in Hive as well. Next time, we’ll finish up setting up the single node Hadoop server with CDH5 and HUE by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce.</p>"},{"title":"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 7","id":"2058","comments":0,"date":"2014-05-02T19:51:55.000Z","_content":"\n[![nospark](http://edpflager.com/wp-content/uploads/2014/05/nospark.png)](http://edpflager.com/wp-content/uploads/2014/05/nospark.png)This is part 7 (the last part) in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\") as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered in [Part 5](http://edpflager.com/?p=2003 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\"). And last time in Part 6, I went through the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera's query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. With this article, we'll wrap up the series by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce. And my little disclaimer: this series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\nIf you've followed along with this series, at this point you should have a functioning Pseudo-cluster running on CentOS 6.5 with Cloudera's CDH5 Hadoop distribution. The only problem we still have to tackle is when you login to HUE, you see and a configuration error: \"Spark Editor - The app won't work without a running Job Server.\"\n\nI'm going to show you two ways to deal with this message. The first for me is the easier, and the one I opted for because I am not currently using Spark. I have disabled this functionality on my server.\n\nDISABLE SPARK\n\n1.  Open a terminal window and switch to the root account.\n2.  Navigate to **/etc/hue/conf.empty** and open the file **hue.ini** in your editor.\n3.  Search for the line that contains: **app\\_blacklist=**\n4.  Uncomment the line, and add **spark** at the end. It should now read: **       app\\_blacklist=spark**\n5.  Save the file and exit your editor.\n6.  Restart HUE with this command: **     service hue restart**\n7.  Login to the HUE inteface, and the warning message will be gone, and the Spark option will no longer appear in the menu either.\n\nHOW TO INSTALL SPARK\n\nIf you intend to work with Spark in your cluster, installation is pretty straightforward.\n\n1.  Open a terminal window and switch to the root user.\n2.  Install the various Spark packages by entering this command: **yum install spark-core spark-master spark-worker spark-python**\n3.  That will take a few minutes to pull all of the packages down from the Internet repository and install them. Once it completes you can start the services with these commands: **      service spark-master start** **      service spark-worker start**\n4.  Open a web browser and access port 18080 on your server (localhost) to see if the Spark Master console is displayed: **http://localhost:18080**\n5.  At this point you will still need to get Spark integrated with the Hue interface. Again in the terminal window as root, you'll need to download SBT (a Scala environment) with this command: **     wget [http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm](http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm)**\n6.  And install it: **     rpm -ivh sbt.rpm**\n7.  Next install GIT (a version control application): **     yum install git**\n8.  Copy the Spark repository to your system: **      git clone https://github.com/ooyala/spark-jobserver.git**\n9.  Once that completes, switch to the Spark Jobserver folder: **     cd spark-jobserver**\n10.  Start the SBT application: **     sbt (loads the SBT application)**\n11.  And start the Spark Job Server within SBT, which will take awhile: **     re-start**\n12.  This command will not exit back to a prompt, but stays open while Spark runs. Open another terminal window, and switch to root. Restart the HUE service: **     service hue restart**\n13.  And log back into HUE. The error message about Spark will be gone. To restart Spark if your system is rebooted, you will need to rerun steps 9-12 above to get it running again. I'm sure there is a way to automate it, but since I'm not using Spark, I haven't investigated it. I leave that as an exercise for the reader.\n\nSo that's the end for this series! If you've made it all the made through and everything is working well, congratulations to you! The process of deciphering various webpages instructions took me a couple of weeks to get through so this is definitely not a process for the timid. Hopefully this has made it a bit easier for you.","source":"_posts/setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-7.md","raw":"---\ntitle: Setup a Single-node Hadoop machine using CDH5 and HUE – Part 7\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - howto\n  - technical\nid: '2058'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-05-02 15:51:55\n---\n\n[![nospark](http://edpflager.com/wp-content/uploads/2014/05/nospark.png)](http://edpflager.com/wp-content/uploads/2014/05/nospark.png)This is part 7 (the last part) in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In [part 1](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"), I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In [part 2](http://edpflager.com/?p=1964 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\"), I walked through installing and configuring HBase, Zookeeper and Snappy. [Part 3](http://edpflager.com/?p=1973 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\") covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  [Part 4](http://edpflager.com/?p=1985 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\") as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered in [Part 5](http://edpflager.com/?p=2003 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\"). And last time in Part 6, I went through the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera's query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. With this article, we'll wrap up the series by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce. And my little disclaimer: this series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n<!-- more -->\nIf you've followed along with this series, at this point you should have a functioning Pseudo-cluster running on CentOS 6.5 with Cloudera's CDH5 Hadoop distribution. The only problem we still have to tackle is when you login to HUE, you see and a configuration error: \"Spark Editor - The app won't work without a running Job Server.\"\n\nI'm going to show you two ways to deal with this message. The first for me is the easier, and the one I opted for because I am not currently using Spark. I have disabled this functionality on my server.\n\nDISABLE SPARK\n\n1.  Open a terminal window and switch to the root account.\n2.  Navigate to **/etc/hue/conf.empty** and open the file **hue.ini** in your editor.\n3.  Search for the line that contains: **app\\_blacklist=**\n4.  Uncomment the line, and add **spark** at the end. It should now read: **       app\\_blacklist=spark**\n5.  Save the file and exit your editor.\n6.  Restart HUE with this command: **     service hue restart**\n7.  Login to the HUE inteface, and the warning message will be gone, and the Spark option will no longer appear in the menu either.\n\nHOW TO INSTALL SPARK\n\nIf you intend to work with Spark in your cluster, installation is pretty straightforward.\n\n1.  Open a terminal window and switch to the root user.\n2.  Install the various Spark packages by entering this command: **yum install spark-core spark-master spark-worker spark-python**\n3.  That will take a few minutes to pull all of the packages down from the Internet repository and install them. Once it completes you can start the services with these commands: **      service spark-master start** **      service spark-worker start**\n4.  Open a web browser and access port 18080 on your server (localhost) to see if the Spark Master console is displayed: **http://localhost:18080**\n5.  At this point you will still need to get Spark integrated with the Hue interface. Again in the terminal window as root, you'll need to download SBT (a Scala environment) with this command: **     wget [http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm](http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm)**\n6.  And install it: **     rpm -ivh sbt.rpm**\n7.  Next install GIT (a version control application): **     yum install git**\n8.  Copy the Spark repository to your system: **      git clone https://github.com/ooyala/spark-jobserver.git**\n9.  Once that completes, switch to the Spark Jobserver folder: **     cd spark-jobserver**\n10.  Start the SBT application: **     sbt (loads the SBT application)**\n11.  And start the Spark Job Server within SBT, which will take awhile: **     re-start**\n12.  This command will not exit back to a prompt, but stays open while Spark runs. Open another terminal window, and switch to root. Restart the HUE service: **     service hue restart**\n13.  And log back into HUE. The error message about Spark will be gone. To restart Spark if your system is rebooted, you will need to rerun steps 9-12 above to get it running again. I'm sure there is a way to automate it, but since I'm not using Spark, I haven't investigated it. I leave that as an exercise for the reader.\n\nSo that's the end for this series! If you've made it all the made through and everything is working well, congratulations to you! The process of deciphering various webpages instructions took me a couple of weeks to get through so this is definitely not a process for the timid. Hopefully this has made it a bit easier for you.","slug":"setup-a-single-node-hadoop-machine-using-cdh5-and-hue-part-7","published":1,"updated":"2020-08-23T20:54:34.834Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aa100azsdjx58wlgxeg","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/nospark.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/nospark.png\" alt=\"nospark\"></a>This is part 7 (the last part) in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a> as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered in <a href=\"http://edpflager.com/?p=2003\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\">Part 5</a>. And last time in Part 6, I went through the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera’s query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. With this article, we’ll wrap up the series by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce. And my little disclaimer: this series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<a id=\"more\"></a>\n<p>If you’ve followed along with this series, at this point you should have a functioning Pseudo-cluster running on CentOS 6.5 with Cloudera’s CDH5 Hadoop distribution. The only problem we still have to tackle is when you login to HUE, you see and a configuration error: “Spark Editor - The app won’t work without a running Job Server.”</p>\n<p>I’m going to show you two ways to deal with this message. The first for me is the easier, and the one I opted for because I am not currently using Spark. I have disabled this functionality on my server.</p>\n<p>DISABLE SPARK</p>\n<ol>\n<li>Open a terminal window and switch to the root account.</li>\n<li>Navigate to <strong>/etc/hue/conf.empty</strong> and open the file <strong>hue.ini</strong> in your editor.</li>\n<li>Search for the line that contains: <strong>app_blacklist=</strong></li>\n<li>Uncomment the line, and add <strong>spark</strong> at the end. It should now read: **       app_blacklist=spark**</li>\n<li>Save the file and exit your editor.</li>\n<li>Restart HUE with this command: **     service hue restart**</li>\n<li>Login to the HUE inteface, and the warning message will be gone, and the Spark option will no longer appear in the menu either.</li>\n</ol>\n<p>HOW TO INSTALL SPARK</p>\n<p>If you intend to work with Spark in your cluster, installation is pretty straightforward.</p>\n<ol>\n<li>Open a terminal window and switch to the root user.</li>\n<li>Install the various Spark packages by entering this command: <strong>yum install spark-core spark-master spark-worker spark-python</strong></li>\n<li>That will take a few minutes to pull all of the packages down from the Internet repository and install them. Once it completes you can start the services with these commands: **      service spark-master start** **      service spark-worker start**</li>\n<li>Open a web browser and access port 18080 on your server (localhost) to see if the Spark Master console is displayed: <strong><a href=\"http://localhost:18080/\">http://localhost:18080</a></strong></li>\n<li>At this point you will still need to get Spark integrated with the Hue interface. Again in the terminal window as root, you’ll need to download SBT (a Scala environment) with this command: **     wget <a href=\"http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm\">http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm</a>**</li>\n<li>And install it: **     rpm -ivh sbt.rpm**</li>\n<li>Next install GIT (a version control application): **     yum install git**</li>\n<li>Copy the Spark repository to your system: **      git clone <a href=\"https://github.com/ooyala/spark-jobserver.git\">https://github.com/ooyala/spark-jobserver.git</a>**</li>\n<li>Once that completes, switch to the Spark Jobserver folder: **     cd spark-jobserver**</li>\n<li>Start the SBT application: **     sbt (loads the SBT application)**</li>\n<li>And start the Spark Job Server within SBT, which will take awhile: **     re-start**</li>\n<li>This command will not exit back to a prompt, but stays open while Spark runs. Open another terminal window, and switch to root. Restart the HUE service: **     service hue restart**</li>\n<li>And log back into HUE. The error message about Spark will be gone. To restart Spark if your system is rebooted, you will need to rerun steps 9-12 above to get it running again. I’m sure there is a way to automate it, but since I’m not using Spark, I haven’t investigated it. I leave that as an exercise for the reader.</li>\n</ol>\n<p>So that’s the end for this series! If you’ve made it all the made through and everything is working well, congratulations to you! The process of deciphering various webpages instructions took me a couple of weeks to get through so this is definitely not a process for the timid. Hopefully this has made it a bit easier for you.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/05/nospark.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/05/nospark.png\" alt=\"nospark\"></a>This is part 7 (the last part) in my series on setting up a Hadoop single node, pseudo-cluster using Cloudera’s CDH distribution. In <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">part 1</a>, I walked through the beginning steps to configure a CentOS 6.5 system for use as a single-node Hadoop pseudo-cluster. In <a href=\"http://edpflager.com/?p=1964\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 2\">part 2</a>, I walked through installing and configuring HBase, Zookeeper and Snappy. <a href=\"http://edpflager.com/?p=1973\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 3\">Part 3</a> covered installing HUE – the graphical front end from Cloudera that makes interacting with the various components in our cluster easier.  <a href=\"http://edpflager.com/?p=1985\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 4\">Part 4</a> as about the set up of  Oozie – a job scheduler component for Hadoop, and how to fix a minor problem with the Hue setup involving a secret key. Installing and configuring Hive2 and getting it to work with Cloudera’s HUE web interface was covered in <a href=\"http://edpflager.com/?p=2003\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 5\">Part 5</a>. And last time in Part 6, I went through the steps necessary to integrate two tools for working with structured SQL data in a Hadoop pseudo-cluster environment: Impala - Cloudera’s query tool for Hadoop, and Sqoop2 - an ETL tool to move data between relational database systems and Hadoop. With this article, we’ll wrap up the series by looking at Apache Spark. Its a general purpose engine for running applications in a Hadoop environment and achieves speeds considerably faster than using native MapReduce. And my little disclaimer: this series is based on the Cloudera documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster. Update: Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>","more":"<p>If you’ve followed along with this series, at this point you should have a functioning Pseudo-cluster running on CentOS 6.5 with Cloudera’s CDH5 Hadoop distribution. The only problem we still have to tackle is when you login to HUE, you see and a configuration error: “Spark Editor - The app won’t work without a running Job Server.”</p>\n<p>I’m going to show you two ways to deal with this message. The first for me is the easier, and the one I opted for because I am not currently using Spark. I have disabled this functionality on my server.</p>\n<p>DISABLE SPARK</p>\n<ol>\n<li>Open a terminal window and switch to the root account.</li>\n<li>Navigate to <strong>/etc/hue/conf.empty</strong> and open the file <strong>hue.ini</strong> in your editor.</li>\n<li>Search for the line that contains: <strong>app_blacklist=</strong></li>\n<li>Uncomment the line, and add <strong>spark</strong> at the end. It should now read: **       app_blacklist=spark**</li>\n<li>Save the file and exit your editor.</li>\n<li>Restart HUE with this command: **     service hue restart**</li>\n<li>Login to the HUE inteface, and the warning message will be gone, and the Spark option will no longer appear in the menu either.</li>\n</ol>\n<p>HOW TO INSTALL SPARK</p>\n<p>If you intend to work with Spark in your cluster, installation is pretty straightforward.</p>\n<ol>\n<li>Open a terminal window and switch to the root user.</li>\n<li>Install the various Spark packages by entering this command: <strong>yum install spark-core spark-master spark-worker spark-python</strong></li>\n<li>That will take a few minutes to pull all of the packages down from the Internet repository and install them. Once it completes you can start the services with these commands: **      service spark-master start** **      service spark-worker start**</li>\n<li>Open a web browser and access port 18080 on your server (localhost) to see if the Spark Master console is displayed: <strong><a href=\"http://localhost:18080/\">http://localhost:18080</a></strong></li>\n<li>At this point you will still need to get Spark integrated with the Hue interface. Again in the terminal window as root, you’ll need to download SBT (a Scala environment) with this command: **     wget <a href=\"http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm\">http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm</a>**</li>\n<li>And install it: **     rpm -ivh sbt.rpm**</li>\n<li>Next install GIT (a version control application): **     yum install git**</li>\n<li>Copy the Spark repository to your system: **      git clone <a href=\"https://github.com/ooyala/spark-jobserver.git\">https://github.com/ooyala/spark-jobserver.git</a>**</li>\n<li>Once that completes, switch to the Spark Jobserver folder: **     cd spark-jobserver**</li>\n<li>Start the SBT application: **     sbt (loads the SBT application)**</li>\n<li>And start the Spark Job Server within SBT, which will take awhile: **     re-start**</li>\n<li>This command will not exit back to a prompt, but stays open while Spark runs. Open another terminal window, and switch to root. Restart the HUE service: **     service hue restart**</li>\n<li>And log back into HUE. The error message about Spark will be gone. To restart Spark if your system is rebooted, you will need to rerun steps 9-12 above to get it running again. I’m sure there is a way to automate it, but since I’m not using Spark, I haven’t investigated it. I leave that as an exercise for the reader.</li>\n</ol>\n<p>So that’s the end for this series! If you’ve made it all the made through and everything is working well, congratulations to you! The process of deciphering various webpages instructions took me a couple of weeks to get through so this is definitely not a process for the timid. Hopefully this has made it a bit easier for you.</p>"},{"title":"Setup a Single-node Hadoop Yarn machine using CDH5 - Part 2","id":"2490","comments":0,"date":"2014-10-08T18:08:26.000Z","_content":"\n[![hbase_zoo](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png)](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png)This is part 2 of setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), or for the series for using MapReduceV1, go here. I'm hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n\n##### Install HBase\n<!-- more -->\n1.  Open a terminal window on the CentOS server and switch to the root user account with:  **su -**\n2.  Enter this command: **yum install hbase** \n3.  Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.\n4.  HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the **/etc/security/limits.conf** file.\n5.  At the bottom of the file, before the #End of file line, add these two lines:  \n          **     hdfs             –       nofile  32768**  \n     **hbase           –       nofile  32768**\n6.  Restart the system to allow the changes to take effect.\n7.  Edit the **/etc/hadoop/conf/hdfs-site.xml** file to allow the data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum that should be used and xcievers is spelled correctly)  \n            **<property>**  \n     **<name>dfs.datanode.max.xcievers</name>**  \n     **<value>4096</value>**  \n     **</property>**\n8.  Restart HDFS by either restarting the server, or use this command:  \n          **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done**\n9.  At this point HBase is installed in standalone mode, and it should be switched to a pseudo-clustered mode. Install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:  \n     **yum install hbase-master**\n10.  Once the install completes, start the service with this command:  \n          **service hbase-master start**\n11.  Open a web browser and point it to the system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of the HBase system.\n12.  Scroll down on the HOME page, and there should be one entry under the Region Servers section. Click on the Region Server link, and the webpage will refresh with the Region Server status information. At the bottom of the page is a link to return to the HBase Master webpage.\n13.  Finally, in the terminal window, to access the HBase command line, enter the command: **    hbase shell.**\n14.  Check the version number of HBase with the command: **version**\n15.  Exit the HBase CLI with the command: **exit**\n\n##### INSTALL A THRIFT SERVER\n\n1.  Now install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **yum install hbase-thrift** and then start it with: **           service hbase-thrift start**\n2.  Install the REST server next. At the command line, enter: **yum install hbase-rest**\n3.  The HBase REST server by default uses port 8080, which is a commonly used one. It can be left as is, or changed. To alter the port, edit the config file /etc/hbase/conf/hbase-site.xml and add this property section between the configuration tags, using another port in the value section:  \n     **<property>**  \n     **<name>hbase.rest.port</name>**  \n     **<value>60050</value>**  \n     **</property>**\n4.  Still within the hbase-site.xml file, add these two additional properties:  \n     **<property>**  \n     **<name>hbase.cluster.distributed</name>**  \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>hbase.rootdir</name>**  \n     **<value>hdfs://localhost:8020/hbase</value>**   \n     **</property>**\n5.  **The value for hbase.rootdir's hostname must match the  hostname value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS property.**\n6.  Save the file.\n7.  Create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo – u hdfs portion to make sure the command is run by the correct Hadoop user):  \n             **sudo -u hdfs hadoop fs -mkdir /hbase**  \n             **sudo -u hdfs hadoop fs -chown hbase /hbase**\n\nINSTALL ZOOKEEPER\n\n1.  Before the HBase installation is complete, Zookeeper -Server needs to be installed and configured to run on a single server. Install the package by entering this command as root: **     yum install zookeeper-server**\n2.  Create a Zookeeper folder in the local Linux file system and change the ownership by entering the following:  \n             **mkdir -p /var/lib/zookeeper**  \n            ** chown -R zookeeper /var/lib/zookeeper/**\n3.  Since this is a first time installation of Zookeeper-server, initialize and start Zookeeper, by entering these commands as root:  \n     **service zookeeper-server init service zookeeper-server start** (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).\n4.  **Please Note**: For a production system Hadoop should be installed with an odd number of Zookeeper servers to maintain reliability. Because this is a pseudo-distributed mode installation primarily for testing and development, it is OK to use a single Zookeeper server.\n\n##### START HBASE-MASTER\n\n1.  Once the Zookeeper subsystem is running, start the HBase-Master service with this command: **     service hbase-master start**\n2.  A couple of subordinate services for HBase need to be installed now. To install the region server, type the following command as root:  \n     **yum install hbase-regionserver**\n3.  And start it with: **       service hbase-regionserver start**\n4.  At this point, nine services should be running as part of the Hadoop Yarn installation. To verify this, enter the following command in a terminal window as root: **jps**  A list of ten services with their PID should be returned. The tenth one is the Jps command.\n5.  Open a web browser and point it to the system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . Scroll down slightly to the Region Servers section on the HOME page, and there should now be two servers listed.\n\nAt this point, you have a basic Hadoop Yarn system configured with HBase and Zookeeper. You can access the various components using the command line tools. Next time, the configuration of the SNAPPY compression library and the first portion of installing and configuring HUE, Cloudera's web interface for working with a Hadoop cluster.","source":"_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-2.md","raw":"---\ntitle: Setup a Single-node Hadoop Yarn machine using CDH5 - Part 2\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - How-to\n  - howto\n  - install\n  - SysAdmin\n  - technical\n  - YARN\nid: '2490'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2014-10-08 14:08:26\n---\n\n[![hbase_zoo](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png)](http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png)This is part 2 of setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), or for the series for using MapReduceV1, go here. I'm hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.\n\n##### Install HBase\n<!-- more -->\n1.  Open a terminal window on the CentOS server and switch to the root user account with:  **su -**\n2.  Enter this command: **yum install hbase** \n3.  Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.\n4.  HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the **/etc/security/limits.conf** file.\n5.  At the bottom of the file, before the #End of file line, add these two lines:  \n          **     hdfs             –       nofile  32768**  \n     **hbase           –       nofile  32768**\n6.  Restart the system to allow the changes to take effect.\n7.  Edit the **/etc/hadoop/conf/hdfs-site.xml** file to allow the data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum that should be used and xcievers is spelled correctly)  \n            **<property>**  \n     **<name>dfs.datanode.max.xcievers</name>**  \n     **<value>4096</value>**  \n     **</property>**\n8.  Restart HDFS by either restarting the server, or use this command:  \n          **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done**\n9.  At this point HBase is installed in standalone mode, and it should be switched to a pseudo-clustered mode. Install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:  \n     **yum install hbase-master**\n10.  Once the install completes, start the service with this command:  \n          **service hbase-master start**\n11.  Open a web browser and point it to the system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of the HBase system.\n12.  Scroll down on the HOME page, and there should be one entry under the Region Servers section. Click on the Region Server link, and the webpage will refresh with the Region Server status information. At the bottom of the page is a link to return to the HBase Master webpage.\n13.  Finally, in the terminal window, to access the HBase command line, enter the command: **    hbase shell.**\n14.  Check the version number of HBase with the command: **version**\n15.  Exit the HBase CLI with the command: **exit**\n\n##### INSTALL A THRIFT SERVER\n\n1.  Now install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: **yum install hbase-thrift** and then start it with: **           service hbase-thrift start**\n2.  Install the REST server next. At the command line, enter: **yum install hbase-rest**\n3.  The HBase REST server by default uses port 8080, which is a commonly used one. It can be left as is, or changed. To alter the port, edit the config file /etc/hbase/conf/hbase-site.xml and add this property section between the configuration tags, using another port in the value section:  \n     **<property>**  \n     **<name>hbase.rest.port</name>**  \n     **<value>60050</value>**  \n     **</property>**\n4.  Still within the hbase-site.xml file, add these two additional properties:  \n     **<property>**  \n     **<name>hbase.cluster.distributed</name>**  \n     **<value>true</value>**  \n     **</property>**  \n     **<property>**  \n     **<name>hbase.rootdir</name>**  \n     **<value>hdfs://localhost:8020/hbase</value>**   \n     **</property>**\n5.  **The value for hbase.rootdir's hostname must match the  hostname value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS property.**\n6.  Save the file.\n7.  Create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo – u hdfs portion to make sure the command is run by the correct Hadoop user):  \n             **sudo -u hdfs hadoop fs -mkdir /hbase**  \n             **sudo -u hdfs hadoop fs -chown hbase /hbase**\n\nINSTALL ZOOKEEPER\n\n1.  Before the HBase installation is complete, Zookeeper -Server needs to be installed and configured to run on a single server. Install the package by entering this command as root: **     yum install zookeeper-server**\n2.  Create a Zookeeper folder in the local Linux file system and change the ownership by entering the following:  \n             **mkdir -p /var/lib/zookeeper**  \n            ** chown -R zookeeper /var/lib/zookeeper/**\n3.  Since this is a first time installation of Zookeeper-server, initialize and start Zookeeper, by entering these commands as root:  \n     **service zookeeper-server init service zookeeper-server start** (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).\n4.  **Please Note**: For a production system Hadoop should be installed with an odd number of Zookeeper servers to maintain reliability. Because this is a pseudo-distributed mode installation primarily for testing and development, it is OK to use a single Zookeeper server.\n\n##### START HBASE-MASTER\n\n1.  Once the Zookeeper subsystem is running, start the HBase-Master service with this command: **     service hbase-master start**\n2.  A couple of subordinate services for HBase need to be installed now. To install the region server, type the following command as root:  \n     **yum install hbase-regionserver**\n3.  And start it with: **       service hbase-regionserver start**\n4.  At this point, nine services should be running as part of the Hadoop Yarn installation. To verify this, enter the following command in a terminal window as root: **jps**  A list of ten services with their PID should be returned. The tenth one is the Jps command.\n5.  Open a web browser and point it to the system name and port 60010. For example [http://HadoopTest:60010](http://hadooptest:50070/) . Scroll down slightly to the Region Servers section on the HOME page, and there should now be two servers listed.\n\nAt this point, you have a basic Hadoop Yarn system configured with HBase and Zookeeper. You can access the various components using the command line tools. Next time, the configuration of the SNAPPY compression library and the first portion of installing and configuring HUE, Cloudera's web interface for working with a Hadoop cluster.","slug":"setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-2","published":1,"updated":"2020-08-23T20:54:34.902Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aa600b2sdjxajm8cqz3","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png\" alt=\"hbase_zoo\"></a>This is part 2 of setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, or for the series for using MapReduceV1, go here. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<h5 id=\"Install-HBase\"><a href=\"#Install-HBase\" class=\"headerlink\" title=\"Install HBase\"></a>Install HBase</h5><a id=\"more\"></a>\n<ol>\n<li>Open a terminal window on the CentOS server and switch to the root user account with:  <strong>su -</strong></li>\n<li>Enter this command: <strong>yum install hbase</strong> </li>\n<li>Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.</li>\n<li>HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the <strong>/etc/security/limits.conf</strong> file.</li>\n<li>At the bottom of the file, before the #End of file line, add these two lines:<br>      **     hdfs             –       nofile  32768**<br> <strong>hbase           –       nofile  32768</strong></li>\n<li>Restart the system to allow the changes to take effect.</li>\n<li>Edit the <strong>/etc/hadoop/conf/hdfs-site.xml</strong> file to allow the data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum that should be used and xcievers is spelled correctly)<br>        <strong><property></strong><br> <strong><name>dfs.datanode.max.xcievers</name></strong><br> <strong><value>4096</value></strong><br> <strong></property></strong></li>\n<li>Restart HDFS by either restarting the server, or use this command:<br>      <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong></li>\n<li>At this point HBase is installed in standalone mode, and it should be switched to a pseudo-clustered mode. Install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:<br> <strong>yum install hbase-master</strong></li>\n<li>Once the install completes, start the service with this command:<br>      <strong>service hbase-master start</strong></li>\n<li>Open a web browser and point it to the system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of the HBase system.</li>\n<li>Scroll down on the HOME page, and there should be one entry under the Region Servers section. Click on the Region Server link, and the webpage will refresh with the Region Server status information. At the bottom of the page is a link to return to the HBase Master webpage.</li>\n<li>Finally, in the terminal window, to access the HBase command line, enter the command: **    hbase shell.**</li>\n<li>Check the version number of HBase with the command: <strong>version</strong></li>\n<li>Exit the HBase CLI with the command: <strong>exit</strong></li>\n</ol>\n<h5 id=\"INSTALL-A-THRIFT-SERVER\"><a href=\"#INSTALL-A-THRIFT-SERVER\" class=\"headerlink\" title=\"INSTALL A THRIFT SERVER\"></a>INSTALL A THRIFT SERVER</h5><ol>\n<li>Now install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: <strong>yum install hbase-thrift</strong> and then start it with: **           service hbase-thrift start**</li>\n<li>Install the REST server next. At the command line, enter: <strong>yum install hbase-rest</strong></li>\n<li>The HBase REST server by default uses port 8080, which is a commonly used one. It can be left as is, or changed. To alter the port, edit the config file /etc/hbase/conf/hbase-site.xml and add this property section between the configuration tags, using another port in the value section:<br> <strong><property></strong><br> <strong><name>hbase.rest.port</name></strong><br> <strong><value>60050</value></strong><br> <strong></property></strong></li>\n<li>Still within the hbase-site.xml file, add these two additional properties:<br> <strong><property></strong><br> <strong><name>hbase.cluster.distributed</name></strong><br> <strong><value>true</value></strong><br> <strong></property></strong><br> <strong><property></strong><br> <strong><name>hbase.rootdir</name></strong><br> <strong><value>hdfs://localhost:8020/hbase</value></strong><br> <strong></property></strong></li>\n<li><strong>The value for hbase.rootdir’s hostname must match the  hostname value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS property.</strong></li>\n<li>Save the file.</li>\n<li>Create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo – u hdfs portion to make sure the command is run by the correct Hadoop user):<br>         <strong>sudo -u hdfs hadoop fs -mkdir /hbase</strong><br>         <strong>sudo -u hdfs hadoop fs -chown hbase /hbase</strong></li>\n</ol>\n<p>INSTALL ZOOKEEPER</p>\n<ol>\n<li>Before the HBase installation is complete, Zookeeper -Server needs to be installed and configured to run on a single server. Install the package by entering this command as root: **     yum install zookeeper-server**</li>\n<li>Create a Zookeeper folder in the local Linux file system and change the ownership by entering the following:<br>         <strong>mkdir -p /var/lib/zookeeper</strong><br>        ** chown -R zookeeper /var/lib/zookeeper/**</li>\n<li>Since this is a first time installation of Zookeeper-server, initialize and start Zookeeper, by entering these commands as root:<br> <strong>service zookeeper-server init service zookeeper-server start</strong> (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).</li>\n<li><strong>Please Note</strong>: For a production system Hadoop should be installed with an odd number of Zookeeper servers to maintain reliability. Because this is a pseudo-distributed mode installation primarily for testing and development, it is OK to use a single Zookeeper server.</li>\n</ol>\n<h5 id=\"START-HBASE-MASTER\"><a href=\"#START-HBASE-MASTER\" class=\"headerlink\" title=\"START HBASE-MASTER\"></a>START HBASE-MASTER</h5><ol>\n<li>Once the Zookeeper subsystem is running, start the HBase-Master service with this command: **     service hbase-master start**</li>\n<li>A couple of subordinate services for HBase need to be installed now. To install the region server, type the following command as root:<br> <strong>yum install hbase-regionserver</strong></li>\n<li>And start it with: **       service hbase-regionserver start**</li>\n<li>At this point, nine services should be running as part of the Hadoop Yarn installation. To verify this, enter the following command in a terminal window as root: <strong>jps</strong>  A list of ten services with their PID should be returned. The tenth one is the Jps command.</li>\n<li>Open a web browser and point it to the system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . Scroll down slightly to the Region Servers section on the HOME page, and there should now be two servers listed.</li>\n</ol>\n<p>At this point, you have a basic Hadoop Yarn system configured with HBase and Zookeeper. You can access the various components using the command line tools. Next time, the configuration of the SNAPPY compression library and the first portion of installing and configuring HUE, Cloudera’s web interface for working with a Hadoop cluster.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hbase_zoo-300x170.png\" alt=\"hbase_zoo\"></a>This is part 2 of setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, or for the series for using MapReduceV1, go here. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components.</p>\n<h5 id=\"Install-HBase\"><a href=\"#Install-HBase\" class=\"headerlink\" title=\"Install HBase\"></a>Install HBase</h5>","more":"<ol>\n<li>Open a terminal window on the CentOS server and switch to the root user account with:  <strong>su -</strong></li>\n<li>Enter this command: <strong>yum install hbase</strong> </li>\n<li>Type Y when prompted, and hit enter. After a few seconds the Hbase package will be downloaded and installed.</li>\n<li>HBase can use a large number of files at once which may conflict with a system configuration called ulimit that allows a default maximum of 1024 concurrent open files. To increase this limit, as the root user in the terminal window, edit the <strong>/etc/security/limits.conf</strong> file.</li>\n<li>At the bottom of the file, before the #End of file line, add these two lines:<br>      **     hdfs             –       nofile  32768**<br> <strong>hbase           –       nofile  32768</strong></li>\n<li>Restart the system to allow the changes to take effect.</li>\n<li>Edit the <strong>/etc/hadoop/conf/hdfs-site.xml</strong> file to allow the data node to serve a larger amount of files by adding the following property section to the file. (4096 is the minimum that should be used and xcievers is spelled correctly)<br>        <strong><property></strong><br> <strong><name>dfs.datanode.max.xcievers</name></strong><br> <strong><value>4096</value></strong><br> <strong></property></strong></li>\n<li>Restart HDFS by either restarting the server, or use this command:<br>      <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong></li>\n<li>At this point HBase is installed in standalone mode, and it should be switched to a pseudo-clustered mode. Install the HBase Master which controls the HBase system,and several other servers. Once again at the command line as root, enter the following:<br> <strong>yum install hbase-master</strong></li>\n<li>Once the install completes, start the service with this command:<br>      <strong>service hbase-master start</strong></li>\n<li>Open a web browser and point it to the system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of the HBase system.</li>\n<li>Scroll down on the HOME page, and there should be one entry under the Region Servers section. Click on the Region Server link, and the webpage will refresh with the Region Server status information. At the bottom of the page is a link to return to the HBase Master webpage.</li>\n<li>Finally, in the terminal window, to access the HBase command line, enter the command: **    hbase shell.**</li>\n<li>Check the version number of HBase with the command: <strong>version</strong></li>\n<li>Exit the HBase CLI with the command: <strong>exit</strong></li>\n</ol>\n<h5 id=\"INSTALL-A-THRIFT-SERVER\"><a href=\"#INSTALL-A-THRIFT-SERVER\" class=\"headerlink\" title=\"INSTALL A THRIFT SERVER\"></a>INSTALL A THRIFT SERVER</h5><ol>\n<li>Now install a Thrift server and a REST server to communicate with HBase. As root in a terminal window, enter this command: <strong>yum install hbase-thrift</strong> and then start it with: **           service hbase-thrift start**</li>\n<li>Install the REST server next. At the command line, enter: <strong>yum install hbase-rest</strong></li>\n<li>The HBase REST server by default uses port 8080, which is a commonly used one. It can be left as is, or changed. To alter the port, edit the config file /etc/hbase/conf/hbase-site.xml and add this property section between the configuration tags, using another port in the value section:<br> <strong><property></strong><br> <strong><name>hbase.rest.port</name></strong><br> <strong><value>60050</value></strong><br> <strong></property></strong></li>\n<li>Still within the hbase-site.xml file, add these two additional properties:<br> <strong><property></strong><br> <strong><name>hbase.cluster.distributed</name></strong><br> <strong><value>true</value></strong><br> <strong></property></strong><br> <strong><property></strong><br> <strong><name>hbase.rootdir</name></strong><br> <strong><value>hdfs://localhost:8020/hbase</value></strong><br> <strong></property></strong></li>\n<li><strong>The value for hbase.rootdir’s hostname must match the  hostname value in /etc/hadoop/conf/core-site.xml’s fs.default.name or fs.defaultFS property.</strong></li>\n<li>Save the file.</li>\n<li>Create the hbase directory in HDFS and give ownership to the HBase account, by entering these two lines (be sure to use the sudo – u hdfs portion to make sure the command is run by the correct Hadoop user):<br>         <strong>sudo -u hdfs hadoop fs -mkdir /hbase</strong><br>         <strong>sudo -u hdfs hadoop fs -chown hbase /hbase</strong></li>\n</ol>\n<p>INSTALL ZOOKEEPER</p>\n<ol>\n<li>Before the HBase installation is complete, Zookeeper -Server needs to be installed and configured to run on a single server. Install the package by entering this command as root: **     yum install zookeeper-server**</li>\n<li>Create a Zookeeper folder in the local Linux file system and change the ownership by entering the following:<br>         <strong>mkdir -p /var/lib/zookeeper</strong><br>        ** chown -R zookeeper /var/lib/zookeeper/**</li>\n<li>Since this is a first time installation of Zookeeper-server, initialize and start Zookeeper, by entering these commands as root:<br> <strong>service zookeeper-server init service zookeeper-server start</strong> (You’ll receive a message about specifying a myid if you are running in non-standalone mode. Its safe to ignore it).</li>\n<li><strong>Please Note</strong>: For a production system Hadoop should be installed with an odd number of Zookeeper servers to maintain reliability. Because this is a pseudo-distributed mode installation primarily for testing and development, it is OK to use a single Zookeeper server.</li>\n</ol>\n<h5 id=\"START-HBASE-MASTER\"><a href=\"#START-HBASE-MASTER\" class=\"headerlink\" title=\"START HBASE-MASTER\"></a>START HBASE-MASTER</h5><ol>\n<li>Once the Zookeeper subsystem is running, start the HBase-Master service with this command: **     service hbase-master start**</li>\n<li>A couple of subordinate services for HBase need to be installed now. To install the region server, type the following command as root:<br> <strong>yum install hbase-regionserver</strong></li>\n<li>And start it with: **       service hbase-regionserver start**</li>\n<li>At this point, nine services should be running as part of the Hadoop Yarn installation. To verify this, enter the following command in a terminal window as root: <strong>jps</strong>  A list of ten services with their PID should be returned. The tenth one is the Jps command.</li>\n<li>Open a web browser and point it to the system name and port 60010. For example <a href=\"http://hadooptest:50070/\">http://HadoopTest:60010</a> . Scroll down slightly to the Region Servers section on the HOME page, and there should now be two servers listed.</li>\n</ol>\n<p>At this point, you have a basic Hadoop Yarn system configured with HBase and Zookeeper. You can access the various components using the command line tools. Next time, the configuration of the SNAPPY compression library and the first portion of installing and configuring HUE, Cloudera’s web interface for working with a Hadoop cluster.</p>"},{"title":"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3","id":"2511","comments":0,"date":"2014-10-10T22:03:06.000Z","_content":"\nThis is part 3 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), and part 2 [here](http://edpflager.com/?p=2490 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\"). I have another series for using MapReduceV1, which is [here](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com/) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. This entry is fairly short, but next time I'll delve into installing HUE.\n<!-- more -->\n### **Snappy configuration**\n\nSnappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.\n\n1.  This is one of the deviations between setting up V1 and v2, To enable Snappy for MapReduce v2, from a command line as root, edit:**/etc/hadoop/conf/mapred-site.xml** and add these sections to the file before the final </configuration> tag:  \n    <property> <name>mapreduce.map.output.compress</name> <value>true</value> </property> <property> <name>mapred.map.output.compress.codec</name> <value>org.apache.hadoop.io.compress.SnappyCodec</value> </property>\n2.  Configurations for other components are enabled in the code to call the specific job.\n3.  Restart your server for Snappy to take effect.","source":"_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-3.md","raw":"---\ntitle: Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - How-to\n  - howto\n  - install\n  - technical\n  - YARN\nid: '2511'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-10-10 18:03:06\n---\n\nThis is part 3 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), and part 2 [here](http://edpflager.com/?p=2490 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\"). I have another series for using MapReduceV1, which is [here](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com/) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. This entry is fairly short, but next time I'll delve into installing HUE.\n<!-- more -->\n### **Snappy configuration**\n\nSnappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.\n\n1.  This is one of the deviations between setting up V1 and v2, To enable Snappy for MapReduce v2, from a command line as root, edit:**/etc/hadoop/conf/mapred-site.xml** and add these sections to the file before the final </configuration> tag:  \n    <property> <name>mapreduce.map.output.compress</name> <value>true</value> </property> <property> <name>mapred.map.output.compress.codec</name> <value>org.apache.hadoop.io.compress.SnappyCodec</value> </property>\n2.  Configurations for other components are enabled in the code to call the specific job.\n3.  Restart your server for Snappy to take effect.","slug":"setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-3","published":1,"updated":"2020-08-23T20:54:34.906Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aab00b6sdjxhb2284t9","content":"<p>This is part 3 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, and part 2 <a href=\"http://edpflager.com/?p=2490\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\">here</a>. I have another series for using MapReduceV1, which is <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">here</a>. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. This entry is fairly short, but next time I’ll delve into installing HUE.</p>\n<a id=\"more\"></a>\n<h3 id=\"Snappy-configuration\"><a href=\"#Snappy-configuration\" class=\"headerlink\" title=\"Snappy configuration\"></a><strong>Snappy configuration</strong></h3><p>Snappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.</p>\n<ol>\n<li>This is one of the deviations between setting up V1 and v2, To enable Snappy for MapReduce v2, from a command line as root, edit:<strong>/etc/hadoop/conf/mapred-site.xml</strong> and add these sections to the file before the final </configuration> tag:<br><property> <name>mapreduce.map.output.compress</name> <value>true</value> </property> <property> <name>mapred.map.output.compress.codec</name> <value>org.apache.hadoop.io.compress.SnappyCodec</value> </property></li>\n<li>Configurations for other components are enabled in the code to call the specific job.</li>\n<li>Restart your server for Snappy to take effect.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>This is part 3 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, and part 2 <a href=\"http://edpflager.com/?p=2490\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\">here</a>. I have another series for using MapReduceV1, which is <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">here</a>. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. This entry is fairly short, but next time I’ll delve into installing HUE.</p>","more":"<h3 id=\"Snappy-configuration\"><a href=\"#Snappy-configuration\" class=\"headerlink\" title=\"Snappy configuration\"></a><strong>Snappy configuration</strong></h3><p>Snappy is installed as part of the initial Hadoop installation. Its a compression library used by MapReduce, Hive, HBase, Sqoop and Pig and can make applications run faster without any changes to the application code.</p>\n<ol>\n<li>This is one of the deviations between setting up V1 and v2, To enable Snappy for MapReduce v2, from a command line as root, edit:<strong>/etc/hadoop/conf/mapred-site.xml</strong> and add these sections to the file before the final </configuration> tag:<br><property> <name>mapreduce.map.output.compress</name> <value>true</value> </property> <property> <name>mapred.map.output.compress.codec</name> <value>org.apache.hadoop.io.compress.SnappyCodec</value> </property></li>\n<li>Configurations for other components are enabled in the code to call the specific job.</li>\n<li>Restart your server for Snappy to take effect.</li>\n</ol>"},{"title":"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 4","id":"2515","comments":0,"date":"2014-10-16T22:16:50.000Z","_content":"\n[![hue_logo_300dpi_huge](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png)](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge.png)This is part 4 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), part 2 [here](http://edpflager.com/?p=2490 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\"), and part 3 [here](http://edpflager.com/?p=2511 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3\"). I have another series for using MapReduceV1, which is [here](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com/) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Before starting, make sure that Python 2.6 or 2.7 is installed on the server. This is easy to accomplish, by opening a terminal window, and from the command line, enter: **python** If Python is installed, it will load up and display the version of the software. On my test PC, it responded with Python 2.6.6. Return to the command line by entering at the Python prompt:  **quit()**\n<!-- more -->\nSwitch to superuser mode, and start the installation of hue with this command: **yum install hue** Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes to download and install 17 packages. (For MRv1 there is an additional package that is not needed with MRv2.) Once it completes installation, and provided you had no errors, enter at a command prompt: **service hue start** Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter http://HadoopTest:8888 in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: **/etc/hadoop/conf/hdfs-site.xml** and add this property before the final </configuration> tag: **<property>** **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done** Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file **/etc/hadoop/conf/core-site.xml** and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>\\*</value> </property>** **       <property>** **            <name>hadoop.proxyuser.hue.groups</name>** **            <value>\\*</value>** **       </property>** Now edit the file: **/etc/hue/conf/hue.ini**. Near the top is a section called \\[desktop\\] with the first parameter there being secret\\_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret\\_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for \\[\\[hdfs\\_clusters\\]\\] and look for a comment section below it that starts with \"**#Use WebHDFS/HttpFs as the communication mechanism**\". At the end of that section, add this: **webhdfs\\_url=http://<full server name with domain>:50070/webhdfs/v2/** substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. Restart the HUE server **service hue start** and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.","source":"_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-4.md","raw":"---\ntitle: Setup a Single-node Hadoop Yarn machine using CDH5 – Part 4\ntags:\n  - CDH\n  - Cloudera\n  - Hadoop\n  - How-to\n  - howto\n  - HUE\n  - install\n  - technical\n  - YARN\nid: '2515'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-10-16 18:16:50\n---\n\n[![hue_logo_300dpi_huge](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png)](http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge.png)This is part 4 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was [here](http://edpflager.com/?p=2475 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\"), part 2 [here](http://edpflager.com/?p=2490 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\"), and part 3 [here](http://edpflager.com/?p=2511 \"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3\"). I have another series for using MapReduceV1, which is [here](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the [Cloudera](http://www.cloudera.com/) documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Before starting, make sure that Python 2.6 or 2.7 is installed on the server. This is easy to accomplish, by opening a terminal window, and from the command line, enter: **python** If Python is installed, it will load up and display the version of the software. On my test PC, it responded with Python 2.6.6. Return to the command line by entering at the Python prompt:  **quit()**\n<!-- more -->\nSwitch to superuser mode, and start the installation of hue with this command: **yum install hue** Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes to download and install 17 packages. (For MRv1 there is an additional package that is not needed with MRv2.) Once it completes installation, and provided you had no errors, enter at a command prompt: **service hue start** Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter http://HadoopTest:8888 in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: **/etc/hadoop/conf/hdfs-site.xml** and add this property before the final </configuration> tag: **<property>** **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x restart ; done** Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file **/etc/hadoop/conf/core-site.xml** and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>\\*</value> </property>** **       <property>** **            <name>hadoop.proxyuser.hue.groups</name>** **            <value>\\*</value>** **       </property>** Now edit the file: **/etc/hue/conf/hue.ini**. Near the top is a section called \\[desktop\\] with the first parameter there being secret\\_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret\\_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for \\[\\[hdfs\\_clusters\\]\\] and look for a comment section below it that starts with \"**#Use WebHDFS/HttpFs as the communication mechanism**\". At the end of that section, add this: **webhdfs\\_url=http://<full server name with domain>:50070/webhdfs/v2/** substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. Restart the HUE server **service hue start** and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.","slug":"setup-a-single-node-hadoop-yarn-machine-using-cdh5-part-4","published":1,"updated":"2020-08-23T20:54:34.910Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aag00b9sdjx0zw8a1iq","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png\" alt=\"hue_logo_300dpi_huge\"></a>This is part 4 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, part 2 <a href=\"http://edpflager.com/?p=2490\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\">here</a>, and part 3 <a href=\"http://edpflager.com/?p=2511\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3\">here</a>. I have another series for using MapReduceV1, which is <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">here</a>. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Before starting, make sure that Python 2.6 or 2.7 is installed on the server. This is easy to accomplish, by opening a terminal window, and from the command line, enter: <strong>python</strong> If Python is installed, it will load up and display the version of the software. On my test PC, it responded with Python 2.6.6. Return to the command line by entering at the Python prompt:  <strong>quit()</strong></p>\n<a id=\"more\"></a>\n<p>Switch to superuser mode, and start the installation of hue with this command: <strong>yum install hue</strong> Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes to download and install 17 packages. (For MRv1 there is an additional package that is not needed with MRv2.) Once it completes installation, and provided you had no errors, enter at a command prompt: <strong>service hue start</strong> Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:8888/\">http://HadoopTest:8888</a> in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add this property before the final </configuration> tag: <strong><property></strong> **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong> Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file <strong>/etc/hadoop/conf/core-site.xml</strong> and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property>** **       <property>** **            <name>hadoop.proxyuser.hue.groups</name>** **            <value>*</value>** **       </property>** Now edit the file: <strong>/etc/hue/conf/hue.ini</strong>. Near the top is a section called [desktop] with the first parameter there being secret_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for [[hdfs_clusters]] and look for a comment section below it that starts with “<strong>#Use WebHDFS/HttpFs as the communication mechanism</strong>“. At the end of that section, add this: <strong>webhdfs_url=http://<full server name with domain>:50070/webhdfs/v2/</strong> substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. Restart the HUE server <strong>service hue start</strong> and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/04/hue_logo_300dpi_huge-300x75.png\" alt=\"hue_logo_300dpi_huge\"></a>This is part 4 of a series about setting up a single-node Hadoop Yarn system for sandbox use. Part 1 was <a href=\"http://edpflager.com/?p=2475\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 1\">here</a>, part 2 <a href=\"http://edpflager.com/?p=2490\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 2\">here</a>, and part 3 <a href=\"http://edpflager.com/?p=2511\" title=\"Setup a Single-node Hadoop Yarn machine using CDH5 – Part 3\">here</a>. I have another series for using MapReduceV1, which is <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">here</a>. I’m hoping to keep this series in a similar order as the original set of articles, and will deviate only when necessary. All the content here is based on the <a href=\"http://www.cloudera.com/\">Cloudera</a> documentation, but I’ve modified it to be easier to follow for setting up a pseudo cluster and added additional content where necessary. Please be careful when copying lines from these articles to paste into Hadoop config files or a terminal window. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Before starting, make sure that Python 2.6 or 2.7 is installed on the server. This is easy to accomplish, by opening a terminal window, and from the command line, enter: <strong>python</strong> If Python is installed, it will load up and display the version of the software. On my test PC, it responded with Python 2.6.6. Return to the command line by entering at the Python prompt:  <strong>quit()</strong></p>","more":"<p>Switch to superuser mode, and start the installation of hue with this command: <strong>yum install hue</strong> Any needed dependencies will be located and installed along with HUE, so the time it will take will vary according to that and on your Internet connection, but on my system it took less than ten minutes to download and install 17 packages. (For MRv1 there is an additional package that is not needed with MRv2.) Once it completes installation, and provided you had no errors, enter at a command prompt: <strong>service hue start</strong> Reopen a web browser and point it to your system name and port 8888 to access the HUE web interface. So for example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:8888/\">http://HadoopTest:8888</a> in your browser. You’ll be prompted for a user ID and password and you’ll be warned that this will become your administrator account, so be careful to remember what you enter. HUE will open up with the Quick Start window, and probably will be displaying a number of problems with your configuration. That’s all normal, so don’t worry about it. You’ll need to configure HUE and several other applications to get it working properly. Because we are setting up a pseudo-clustered machine, I’m assuming that you have direct access to the system and are not accessing the Hadoop machine from outside your local firewall. With that consideration, we don’t need to setup HttpFS on this machine. Once again using the terminal window as root, edit the file: <strong>/etc/hadoop/conf/hdfs-site.xml</strong> and add this property before the final </configuration> tag: <strong><property></strong> **               <name>dfs.webhdfs.enabled</name> <value>true</value> </property>** Save the file, exit the editor and restart your HDFS system with this command: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x restart ; done</strong> Now we need to configure the system to allow Hue to submit requests for any user or group with access to the system. As root from a terminal prompt, edit the file <strong>/etc/hadoop/conf/core-site.xml</strong> and add these two properties before the final </configuration> tag: **        <!-- Hue WebHDFS proxy user setting --> <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property>** **       <property>** **            <name>hadoop.proxyuser.hue.groups</name>** **            <value>*</value>** **       </property>** Now edit the file: <strong>/etc/hue/conf/hue.ini</strong>. Near the top is a section called [desktop] with the first parameter there being secret_key. Again, because this system is not meant to be a production server, you can leave it blank or add a random string of characters after the secret_key=. I recommend adding some characters, because not having any will cause HUE to generate a warning later. Now search for [[hdfs_clusters]] and look for a comment section below it that starts with “<strong>#Use WebHDFS/HttpFs as the communication mechanism</strong>“. At the end of that section, add this: <strong>webhdfs_url=http://<full server name with domain>:50070/webhdfs/v2/</strong> substituting the fully qualified domain name for your server where indicated. Save the file and exit back to a terminal prompt. Restart the HUE server <strong>service hue start</strong> and at this point the HUE web interface is available, and you can access HBase through it. However, you will see several error messages when you login. We need to work through these next time.</p>"},{"title":"Setup a Single-node Hadoop Yarn machine using CDH5 - Part 1","id":"2475","comments":0,"date":"2014-10-05T19:44:11.000Z","_content":"\n[![Balls of yarn](http://edpflager.com/wp-content/uploads/2014/10/yarn-300x195.jpg)](http://edpflager.com/wp-content/uploads/2014/10/yarn.jpg)Previously I posted a series of articles that walked through installing a single-node Hadoop machine using [version 1 of MapReduce](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). Since that  series went live, a good deal of development has moved on to MRv2 aka YARN. I won't go into the intricacies of the new architecture, suffice to say that it aims to be more efficient than the older MRv1. This article and any following it are based on the web documentation from Cloudera. While their information is very thorough, it often jumps around making it difficult for someone new to Hadoop to follow. Hopefully the instructions I will be providing here will make it easier for others to setup as well. Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Some may ask, \"What's the point?\" Cloudera, Hortonworks and other vendors offer Sandbox VM's of their distribution that you can download and play with so why install it yourself? My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I provide no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures!\n<!-- more -->\n### **Prepare your single node for Hadoop**\n\n1.  Install a CentOS 6.5 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.\n2.  Remove any unneeded applications and install any system updates.\n3.  Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.\n4.  Disable iptables and iptablesv6 from the Service Configuration window.\n5.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") and set it to disabled. Restart your server for all the config changes to take effect.\n6.  Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for \"OpenJDK\". Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.\n7.  Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   [http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html)\n8.  Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**\n9.  Once the installation completes repeat step 6, and it should report that java version 1.7.0\\_45 is installed.\n10.  You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**\n11.  On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.\n12.  Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.\n13.  Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.\n14.  Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.\n15.  Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.\n16.  While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**\n17.  Edit the **/etc/sysconfig/network** file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.\n18.  Still as root in the terminal window, edit the root .profile file (~/.bash\\_profile) and add these two lines to the end of the file, after \"export PATH\" line: **         export JAVA\\_HOME=/usr/java/latest** **         export PATH=$JAVA\\_HOME/bin:$PATH**\n19.  After you save the file, exit the root user account by typing Exit in the terminal.\n20.  With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java\\_home value is set correctly. Type: **        echo $JAVA\\_HOME**\n21.  The system should respond with: **/usr/java/latest**\n\nStop and take a breath.  At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.\n\n### **Install CDH5**\n\n1.  Open a web browser and download the CDH 5 package for Red Hat from this link: [http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86\\_64/cloudera-cdh-5-0.x86\\_64.rpm](http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm)\n2.  In the terminal window navigate to where the RPM file was downloaded. and enter this command (that's two dashes after YUM): **        yum --nogpgcheck localinstall cloudera-cdh-5-0.x86\\_64.rpm** The CDH5 quick start package will be installed.\n3.  Now add the Cloudera repository GPG key (that's two dashes after RPM): **       rpm --import  [http://archive.cloudera.com/cdh5/redhat/6/x86\\_64/cdh/RPM-GPG-KEY-cloudera](http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera)**\n4.  And finally install Hadoop with MRv1 with this command: **       sudo yum install hadoop-conf-pseudo** (At present 18 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)\n5.  Verify that Hadoop is installed correctly. At the command prompt enter **rpm -ql hadoop-conf-pseudo** and you should see a list of hadoop folders. No errors? All good.\n6.  Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.\n7.  With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x start ; done** (Watch the angle of the quotes - I have found that using normal single quotes errors out).\n8.  Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).\n9.  Now you need to create tmp, staging and log folders in HDFS and set permissions. With the terminal window open, enter: **   sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done\\_intermediate** **   sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging** **   sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** **   sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn** **   sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn**Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.\n10.  Once that is complete, you can start MapReduce with these commands: **   sudo service hadoop-yarn-resourcemanager start** **   sudo service hadoop-yarn-nodemanager start** **   sudo service hadoop-mapreduce-historyserver start**\n11.  Create the /user home directory, and then create a home directory for  yourself in the HDFS file system, and change the owner to yourself as well: **   sudo -u hdfs hadoop fs -mkdir /user** **   sudo -u hdfs hadoop fs -mkdir /user/<username>**\n12.  And finally change the ownership of the user's home directory, to the specific user: **sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name>**\n13.  Reopen a web browser and point it to your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top.\n14.  Click on the Datanodes tab and you should see your local machine listed along with the disk space available.\n15.  Click on the Utilities tab, and select the Browse the file system option, and you can see the various folders that were created in the steps above. Click on the User folder link, and you should see the various user folders and who the owners are.\n\nIf you've reached this point, you should now have a running Hadoop Yarn installation in pseudo-cluster mode. Next time, we'll start the installation of the various GUI tools to allow you to interact with your system via a web browser.","source":"_posts/setup-a-single-node-hadoop-yarn-machine-using-cdh5.md","raw":"---\ntitle: Setup a Single-node Hadoop Yarn machine using CDH5 - Part 1\ntags:\n  - CDH\n  - Cloudera\n  - install\n  - SysAdmin\n  - technical\n  - YARN\nid: '2475'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2014-10-05 15:44:11\n---\n\n[![Balls of yarn](http://edpflager.com/wp-content/uploads/2014/10/yarn-300x195.jpg)](http://edpflager.com/wp-content/uploads/2014/10/yarn.jpg)Previously I posted a series of articles that walked through installing a single-node Hadoop machine using [version 1 of MapReduce](http://edpflager.com/?p=1945 \"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\"). Since that  series went live, a good deal of development has moved on to MRv2 aka YARN. I won't go into the intricacies of the new architecture, suffice to say that it aims to be more efficient than the older MRv1. This article and any following it are based on the web documentation from Cloudera. While their information is very thorough, it often jumps around making it difficult for someone new to Hadoop to follow. Hopefully the instructions I will be providing here will make it easier for others to setup as well. Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Some may ask, \"What's the point?\" Cloudera, Hortonworks and other vendors offer Sandbox VM's of their distribution that you can download and play with so why install it yourself? My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I provide no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures!\n<!-- more -->\n### **Prepare your single node for Hadoop**\n\n1.  Install a CentOS 6.5 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.\n2.  Remove any unneeded applications and install any system updates.\n3.  Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.\n4.  Disable iptables and iptablesv6 from the Service Configuration window.\n5.  Edit [/etc/sysconfig/selinux](http://edpflager.com/?p=1866 \"Disable SELinux to install Cloudera\") and set it to disabled. Restart your server for all the config changes to take effect.\n6.  Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for \"OpenJDK\". Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.\n7.  Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   [http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html)\n8.  Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**\n9.  Once the installation completes repeat step 6, and it should report that java version 1.7.0\\_45 is installed.\n10.  You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**\n11.  On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.\n12.  Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.\n13.  Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.\n14.  Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.\n15.  Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.\n16.  While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**\n17.  Edit the **/etc/sysconfig/network** file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.\n18.  Still as root in the terminal window, edit the root .profile file (~/.bash\\_profile) and add these two lines to the end of the file, after \"export PATH\" line: **         export JAVA\\_HOME=/usr/java/latest** **         export PATH=$JAVA\\_HOME/bin:$PATH**\n19.  After you save the file, exit the root user account by typing Exit in the terminal.\n20.  With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java\\_home value is set correctly. Type: **        echo $JAVA\\_HOME**\n21.  The system should respond with: **/usr/java/latest**\n\nStop and take a breath.  At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.\n\n### **Install CDH5**\n\n1.  Open a web browser and download the CDH 5 package for Red Hat from this link: [http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86\\_64/cloudera-cdh-5-0.x86\\_64.rpm](http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm)\n2.  In the terminal window navigate to where the RPM file was downloaded. and enter this command (that's two dashes after YUM): **        yum --nogpgcheck localinstall cloudera-cdh-5-0.x86\\_64.rpm** The CDH5 quick start package will be installed.\n3.  Now add the Cloudera repository GPG key (that's two dashes after RPM): **       rpm --import  [http://archive.cloudera.com/cdh5/redhat/6/x86\\_64/cdh/RPM-GPG-KEY-cloudera](http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera)**\n4.  And finally install Hadoop with MRv1 with this command: **       sudo yum install hadoop-conf-pseudo** (At present 18 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)\n5.  Verify that Hadoop is installed correctly. At the command prompt enter **rpm -ql hadoop-conf-pseudo** and you should see a list of hadoop folders. No errors? All good.\n6.  Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.\n7.  With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: **for x in \\`cd /etc/init.d ; ls hadoop-hdfs-\\*\\` ; do sudo service $x start ; done** (Watch the angle of the quotes - I have found that using normal single quotes errors out).\n8.  Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).\n9.  Now you need to create tmp, staging and log folders in HDFS and set permissions. With the terminal window open, enter: **   sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done\\_intermediate** **   sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging** **   sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** **   sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn** **   sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn**Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.\n10.  Once that is complete, you can start MapReduce with these commands: **   sudo service hadoop-yarn-resourcemanager start** **   sudo service hadoop-yarn-nodemanager start** **   sudo service hadoop-mapreduce-historyserver start**\n11.  Create the /user home directory, and then create a home directory for  yourself in the HDFS file system, and change the owner to yourself as well: **   sudo -u hdfs hadoop fs -mkdir /user** **   sudo -u hdfs hadoop fs -mkdir /user/<username>**\n12.  And finally change the ownership of the user's home directory, to the specific user: **sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name>**\n13.  Reopen a web browser and point it to your system name and port 50070. For example, if your system was called HadoopTest, you would enter http://HadoopTest:50070 in your browser. In CDH version 5, you'll see a website with multiple tabs at the top.\n14.  Click on the Datanodes tab and you should see your local machine listed along with the disk space available.\n15.  Click on the Utilities tab, and select the Browse the file system option, and you can see the various folders that were created in the steps above. Click on the User folder link, and you should see the various user folders and who the owners are.\n\nIf you've reached this point, you should now have a running Hadoop Yarn installation in pseudo-cluster mode. Next time, we'll start the installation of the various GUI tools to allow you to interact with your system via a web browser.","slug":"setup-a-single-node-hadoop-yarn-machine-using-cdh5","published":1,"updated":"2020-08-23T20:54:34.898Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aaj00bcsdjx4djp4jwd","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/10/yarn.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/10/yarn-300x195.jpg\" alt=\"Balls of yarn\"></a>Previously I posted a series of articles that walked through installing a single-node Hadoop machine using <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">version 1 of MapReduce</a>. Since that  series went live, a good deal of development has moved on to MRv2 aka YARN. I won’t go into the intricacies of the new architecture, suffice to say that it aims to be more efficient than the older MRv1. This article and any following it are based on the web documentation from Cloudera. While their information is very thorough, it often jumps around making it difficult for someone new to Hadoop to follow. Hopefully the instructions I will be providing here will make it easier for others to setup as well. Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Some may ask, “What’s the point?” Cloudera, Hortonworks and other vendors offer Sandbox VM’s of their distribution that you can download and play with so why install it yourself? My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I provide no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures!</p>\n<a id=\"more\"></a>\n<h3 id=\"Prepare-your-single-node-for-Hadoop\"><a href=\"#Prepare-your-single-node-for-Hadoop\" class=\"headerlink\" title=\"Prepare your single node for Hadoop\"></a><strong>Prepare your single node for Hadoop</strong></h3><ol>\n<li>Install a CentOS 6.5 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.</li>\n<li>Disable iptables and iptablesv6 from the Service Configuration window.</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for “OpenJDK”. Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.</li>\n<li>Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a></li>\n<li>Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**</li>\n<li>Once the installation completes repeat step 6, and it should report that java version 1.7.0_45 is installed.</li>\n<li>You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**</li>\n<li>On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.</li>\n<li>Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.</li>\n<li>Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.</li>\n<li>Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.</li>\n<li>Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.</li>\n<li>While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**</li>\n<li>Edit the <strong>/etc/sysconfig/network</strong> file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.</li>\n<li>Still as root in the terminal window, edit the root .profile file (~/.bash_profile) and add these two lines to the end of the file, after “export PATH” line: **         export JAVA_HOME=/usr/java/latest** **         export PATH=$JAVA_HOME/bin:$PATH**</li>\n<li>After you save the file, exit the root user account by typing Exit in the terminal.</li>\n<li>With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java_home value is set correctly. Type: **        echo $JAVA_HOME**</li>\n<li>The system should respond with: <strong>/usr/java/latest</strong></li>\n</ol>\n<p>Stop and take a breath.  At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.</p>\n<h3 id=\"Install-CDH5\"><a href=\"#Install-CDH5\" class=\"headerlink\" title=\"Install CDH5\"></a><strong>Install CDH5</strong></h3><ol>\n<li>Open a web browser and download the CDH 5 package for Red Hat from this link: <a href=\"http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\">http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm</a></li>\n<li>In the terminal window navigate to where the RPM file was downloaded. and enter this command (that’s two dashes after YUM): **        yum –nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm** The CDH5 quick start package will be installed.</li>\n<li>Now add the Cloudera repository GPG key (that’s two dashes after RPM): **       rpm –import  <a href=\"http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera</a>**</li>\n<li>And finally install Hadoop with MRv1 with this command: **       sudo yum install hadoop-conf-pseudo** (At present 18 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)</li>\n<li>Verify that Hadoop is installed correctly. At the command prompt enter <strong>rpm -ql hadoop-conf-pseudo</strong> and you should see a list of hadoop folders. No errors? All good.</li>\n<li>Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.</li>\n<li>With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done</strong> (Watch the angle of the quotes - I have found that using normal single quotes errors out).</li>\n<li>Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).</li>\n<li>Now you need to create tmp, staging and log folders in HDFS and set permissions. With the terminal window open, enter: **   sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate** **   sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging** **   sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** **   sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn** **   sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn**Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.</li>\n<li>Once that is complete, you can start MapReduce with these commands: **   sudo service hadoop-yarn-resourcemanager start** **   sudo service hadoop-yarn-nodemanager start** **   sudo service hadoop-mapreduce-historyserver start**</li>\n<li>Create the /user home directory, and then create a home directory for  yourself in the HDFS file system, and change the owner to yourself as well: **   sudo -u hdfs hadoop fs -mkdir /user** **   sudo -u hdfs hadoop fs -mkdir /user/<username>**</li>\n<li>And finally change the ownership of the user’s home directory, to the specific user: <strong>sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name></strong></li>\n<li>Reopen a web browser and point it to your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top.</li>\n<li>Click on the Datanodes tab and you should see your local machine listed along with the disk space available.</li>\n<li>Click on the Utilities tab, and select the Browse the file system option, and you can see the various folders that were created in the steps above. Click on the User folder link, and you should see the various user folders and who the owners are.</li>\n</ol>\n<p>If you’ve reached this point, you should now have a running Hadoop Yarn installation in pseudo-cluster mode. Next time, we’ll start the installation of the various GUI tools to allow you to interact with your system via a web browser.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2014/10/yarn.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/10/yarn-300x195.jpg\" alt=\"Balls of yarn\"></a>Previously I posted a series of articles that walked through installing a single-node Hadoop machine using <a href=\"http://edpflager.com/?p=1945\" title=\"Setup a Single-node Hadoop machine using CDH5 and HUE – Part 1\">version 1 of MapReduce</a>. Since that  series went live, a good deal of development has moved on to MRv2 aka YARN. I won’t go into the intricacies of the new architecture, suffice to say that it aims to be more efficient than the older MRv1. This article and any following it are based on the web documentation from Cloudera. While their information is very thorough, it often jumps around making it difficult for someone new to Hadoop to follow. Hopefully the instructions I will be providing here will make it easier for others to setup as well. Please be careful if you are copying lines from these articles to paste into your Hadoop config files. I have found that the double hyphen characters used in the comment lines may copy over as a long hyphen instead. This is likely to cause issues when attempting to run the various components. Some may ask, “What’s the point?” Cloudera, Hortonworks and other vendors offer Sandbox VM’s of their distribution that you can download and play with so why install it yourself? My answer to that is, by installing it myself, I can learn more about how it all fits together, and gives me a better understanding of the whole Hadoop ecosystem. WARNING: This is not meant to be a production system, and I provide no warranties as to the usability of the system once you are complete. If you are setting up a production system and/or one that will exposed outside your firewall, please make sure you enable adequate security measures!</p>","more":"<h3 id=\"Prepare-your-single-node-for-Hadoop\"><a href=\"#Prepare-your-single-node-for-Hadoop\" class=\"headerlink\" title=\"Prepare your single node for Hadoop\"></a><strong>Prepare your single node for Hadoop</strong></h3><ol>\n<li>Install a CentOS 6.5 64-bit server with the standard Gnome desktop. A minimum of 2GB of RAM is necessary, although 4GB or more is better. Hard drive space should be a minimum of 20GB, considerably more if you are planning on experimenting with any sizable data sets.</li>\n<li>Remove any unneeded applications and install any system updates.</li>\n<li>Enable NTP (network time protocol daemon) via the Service Configuration window so your system time stays in sync with an external time source.</li>\n<li>Disable iptables and iptablesv6 from the Service Configuration window.</li>\n<li>Edit <a href=\"http://edpflager.com/?p=1866\" title=\"Disable SELinux to install Cloudera\">/etc/sysconfig/selinux</a> and set it to disabled. Restart your server for all the config changes to take effect.</li>\n<li>Remove the preinstalled Java runtime engines from the Add/Remove Applications tool by searching for “OpenJDK”. Once you have completed that, open a terminal window and type: **          java -version** You should receive a message that nothing was found.</li>\n<li>Download the Java Developers Kit 7u45 Linux-x64 RPM from this link (you may have to create a free Oracle account to download):   <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html\">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html</a></li>\n<li>Install the java sdk file by opening a terminal window and switching to the root user account: **          su -** Navigate to where the RPM file was downloaded to and then enter this command to install JAVA: **          rpm -ivh ./jdk-7u45-linux-x64.rpm**</li>\n<li>Once the installation completes repeat step 6, and it should report that java version 1.7.0_45 is installed.</li>\n<li>You now need to add a static IP address to your system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command: **           ifconfig**</li>\n<li>On the CentOS desktop, right click on the network icon in the top panel and choose Edit Connections. When the Network Connections window opens, click on the eth0 entry, and then the edit button.</li>\n<li>Make sure the Connect Automatically and Available to All Users boxes are checked, then go to the IPv4 tab. Change the Method drop down to Manual. Below that enter the Address, Net mask and Gateway that ifconfig displayed.</li>\n<li>Be sure to enter at least one value for DNS Server. I generally use the Gateway address here again because my router acts as a DNS cache. You may need to enter the DNS information provided by your ISP.</li>\n<li>Click over to the IPv6 tab and set it to “Ignore” in the Method drop down. Now click on the Apply button. You should be prompted to enter a password for the root account after which the change are made.</li>\n<li>Switch back over to your terminal window, and try pinging a website on the Internet to see if you get a response. If you get a series of messages back that a certain number of bytes were received, you are good. Hit Ctrl-C to stop the ping.</li>\n<li>While you are still logged in as root, edit the hosts file (/etc/hosts) to set your host name in the format: **        ipaddress     fully qualified domain name       short name** As an example: **        10.0.1.10     cloudera.mydomain.com           cloudera**</li>\n<li>Edit the <strong>/etc/sysconfig/network</strong> file to make sure your hostname is also set there. If you supplied a host name during the OS installation, it should already be defined.</li>\n<li>Still as root in the terminal window, edit the root .profile file (~/.bash_profile) and add these two lines to the end of the file, after “export PATH” line: **         export JAVA_HOME=/usr/java/latest** **         export PATH=$JAVA_HOME/bin:$PATH**</li>\n<li>After you save the file, exit the root user account by typing Exit in the terminal.</li>\n<li>With the terminal still open, re-enable root access to force the bash profile to be reloaded and check that the java_home value is set correctly. Type: **        echo $JAVA_HOME**</li>\n<li>The system should respond with: <strong>/usr/java/latest</strong></li>\n</ol>\n<p>Stop and take a breath.  At this point you have prepared your system for Hadoop. If you are working in a virtual machine, now would be a good time to make a backup or snapshot.</p>\n<h3 id=\"Install-CDH5\"><a href=\"#Install-CDH5\" class=\"headerlink\" title=\"Install CDH5\"></a><strong>Install CDH5</strong></h3><ol>\n<li>Open a web browser and download the CDH 5 package for Red Hat from this link: <a href=\"http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm\">http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm</a></li>\n<li>In the terminal window navigate to where the RPM file was downloaded. and enter this command (that’s two dashes after YUM): **        yum –nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm** The CDH5 quick start package will be installed.</li>\n<li>Now add the Cloudera repository GPG key (that’s two dashes after RPM): **       rpm –import  <a href=\"http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera</a>**</li>\n<li>And finally install Hadoop with MRv1 with this command: **       sudo yum install hadoop-conf-pseudo** (At present 18 packages will be downloaded and installed so the time involved may take a few minutes depending on your Internet connection.)</li>\n<li>Verify that Hadoop is installed correctly. At the command prompt enter <strong>rpm -ql hadoop-conf-pseudo</strong> and you should see a list of hadoop folders. No errors? All good.</li>\n<li>Format the HDFS filesystem with this command to make sure Hadoop is working correctly: **       sudo -u hdfs hdfs namenode -format** If you receive any error messages, go back through the instructions above to make sure you have down them correctly.</li>\n<li>With the HDFS file system formatted, you can start the various Hadoop services, by entering this command at the terminal prompt: <strong>for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done</strong> (Watch the angle of the quotes - I have found that using normal single quotes errors out).</li>\n<li>Open a web browser and point it to your your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top that you can click through to check out the status of your Hadoop box. (See the graphic at the top of this article).</li>\n<li>Now you need to create tmp, staging and log folders in HDFS and set permissions. With the terminal window open, enter: **   sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate** **   sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging** **   sudo -u hdfs hadoop fs -chmod -R 1777 /tmp** **   sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn** **   sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn**Be sure to use the sudo -u hdfs portion of the commands to ensure that the proper user creates the folders.</li>\n<li>Once that is complete, you can start MapReduce with these commands: **   sudo service hadoop-yarn-resourcemanager start** **   sudo service hadoop-yarn-nodemanager start** **   sudo service hadoop-mapreduce-historyserver start**</li>\n<li>Create the /user home directory, and then create a home directory for  yourself in the HDFS file system, and change the owner to yourself as well: **   sudo -u hdfs hadoop fs -mkdir /user** **   sudo -u hdfs hadoop fs -mkdir /user/<username>**</li>\n<li>And finally change the ownership of the user’s home directory, to the specific user: <strong>sudo -u hdfs hadoop fs -chown <your user name> /user/<your user name></strong></li>\n<li>Reopen a web browser and point it to your system name and port 50070. For example, if your system was called HadoopTest, you would enter <a href=\"http://hadooptest:50070/\">http://HadoopTest:50070</a> in your browser. In CDH version 5, you’ll see a website with multiple tabs at the top.</li>\n<li>Click on the Datanodes tab and you should see your local machine listed along with the disk space available.</li>\n<li>Click on the Utilities tab, and select the Browse the file system option, and you can see the various folders that were created in the steps above. Click on the User folder link, and you should see the various user folders and who the owners are.</li>\n</ol>\n<p>If you’ve reached this point, you should now have a running Hadoop Yarn installation in pseudo-cluster mode. Next time, we’ll start the installation of the various GUI tools to allow you to interact with your system via a web browser.</p>"},{"title":"Shell Script to control services on Linux Mint (especially SQL Server)","id":"3692","comments":0,"date":"2018-02-28T09:01:44.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/02/shell.png)](http://edpflager.com/?attachment_id=3694#main)I've posted before about how I don't run certain services on my Linux system all the time, but rather only when I am working with them. For example, Docker, Pentaho applications, and several database servers like MySQL, MariaDB and now Microsoft SQL Server.  The reasons are simple: because I experiment with a variety of technologies, I don't want to dedicate resources unnecessarily and there may often be conflicts such as web server interfaces using the same ports. So to alleviate some of those problems I generally disable services and start them when necessary. To do that I usually create Bash scripts to start and stop the services, and save those in a named location in /opt that is associated with the server application. I would have two scripts, one to start and one to stop, but since I've had some time lately I've worked up a method to do this in one script. Below is a script I drafted to start and stop Microsoft SQL Server on Linux. I saved this to the /opt folder where the SQL Server components are installed and then created a Launcher shortcut to add it to my menu. Now I just select that option in the menu, and I see the current status of the server, and I can start it or shut it down as need be. Its heavily commented to provide information, so use it as a source for yourself if you are running on Debian based systems.\n<!-- more -->\n#! /bin/bash\n# Previous line explicitly names which BASH shell to run. Some versions of Linux use a different \n# shell when running from GUI.\n\n# Check the status of mssql-server service. Redirects output to /dev/null to suppress printing \n# results because we are only interested in the return code.\nsystemctl status mssql-server | grep 'Active: inactive (dead)' &> /dev/null\n\n# Check return code. If result is zero then SQL Server Service is not running.\nif \\[ $? == 0 \\]; then \n echo \"SQL Server is not active.\"\n \n# Prompt use to start SQL Server. Loop through until user responses with a Yes or No. \n#   Result is stored in start variable.\n    while true\n    do\n    read -p \"Do you want to start it (Y/N)?\" start\n\n   # Case statements to check entered value\n     case $start in\n               # If y or Y entered, start service then exit.\n               \\[yY\\]\\* ) sudo systemctl enable mssql-server && sudo systemctl start mssql-server\n                break ;;\n \n               # If n or N entered, do nothing and exit.\n               \\[nN\\]\\* ) exit;;\n\n               # If Y or N not entered, prompt for correct entries\n               \\* ) echo \"Please enter Y or N\";;\n\n       # End case statement and loop again \n       esac\n       done\n# If return code from service check is not zero then SQL Server Service is running.\nelse \n echo \"SQL Server is active\"\n\n# Prompt use to stop SQL Server. Loop through until user responses with a Yes or No. \n#  Result is stored in start variable.\n   while true\n   do\n   read -p \"Do you want to stop it (Y/N)?\" stop\n\n   # Case statements to check entered value\n   case $stop in\n              # If y or Y entered, start service then exit.\n              \\[yY\\]\\* ) sudo systemctl stop mssql-server && sudo systemctl disable mssql-server\n              break ;;\n\n              # If n or N entered, do nothing and exit. \n              \\[nN\\]\\* ) exit;;\n\n              # If Y or N not entered, prompt for correct entries\n                  \\* ) echo \"Please enter Y or N\";;\n \n              # End case statement and loop again \n              esac\n              done\n\n# End IF statement\nfi\n\n#Exit script\nexit","source":"_posts/shell-script-to-control-services-on-linux-mint-especially-sql-server.md","raw":"---\ntitle: Shell Script to control services on Linux Mint (especially SQL Server)\ntags:\n  - How-to\n  - howto\n  - Mint\n  - SQL Server\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3692'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2018-02-28 04:01:44\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/02/shell.png)](http://edpflager.com/?attachment_id=3694#main)I've posted before about how I don't run certain services on my Linux system all the time, but rather only when I am working with them. For example, Docker, Pentaho applications, and several database servers like MySQL, MariaDB and now Microsoft SQL Server.  The reasons are simple: because I experiment with a variety of technologies, I don't want to dedicate resources unnecessarily and there may often be conflicts such as web server interfaces using the same ports. So to alleviate some of those problems I generally disable services and start them when necessary. To do that I usually create Bash scripts to start and stop the services, and save those in a named location in /opt that is associated with the server application. I would have two scripts, one to start and one to stop, but since I've had some time lately I've worked up a method to do this in one script. Below is a script I drafted to start and stop Microsoft SQL Server on Linux. I saved this to the /opt folder where the SQL Server components are installed and then created a Launcher shortcut to add it to my menu. Now I just select that option in the menu, and I see the current status of the server, and I can start it or shut it down as need be. Its heavily commented to provide information, so use it as a source for yourself if you are running on Debian based systems.\n<!-- more -->\n#! /bin/bash\n# Previous line explicitly names which BASH shell to run. Some versions of Linux use a different \n# shell when running from GUI.\n\n# Check the status of mssql-server service. Redirects output to /dev/null to suppress printing \n# results because we are only interested in the return code.\nsystemctl status mssql-server | grep 'Active: inactive (dead)' &> /dev/null\n\n# Check return code. If result is zero then SQL Server Service is not running.\nif \\[ $? == 0 \\]; then \n echo \"SQL Server is not active.\"\n \n# Prompt use to start SQL Server. Loop through until user responses with a Yes or No. \n#   Result is stored in start variable.\n    while true\n    do\n    read -p \"Do you want to start it (Y/N)?\" start\n\n   # Case statements to check entered value\n     case $start in\n               # If y or Y entered, start service then exit.\n               \\[yY\\]\\* ) sudo systemctl enable mssql-server && sudo systemctl start mssql-server\n                break ;;\n \n               # If n or N entered, do nothing and exit.\n               \\[nN\\]\\* ) exit;;\n\n               # If Y or N not entered, prompt for correct entries\n               \\* ) echo \"Please enter Y or N\";;\n\n       # End case statement and loop again \n       esac\n       done\n# If return code from service check is not zero then SQL Server Service is running.\nelse \n echo \"SQL Server is active\"\n\n# Prompt use to stop SQL Server. Loop through until user responses with a Yes or No. \n#  Result is stored in start variable.\n   while true\n   do\n   read -p \"Do you want to stop it (Y/N)?\" stop\n\n   # Case statements to check entered value\n   case $stop in\n              # If y or Y entered, start service then exit.\n              \\[yY\\]\\* ) sudo systemctl stop mssql-server && sudo systemctl disable mssql-server\n              break ;;\n\n              # If n or N entered, do nothing and exit. \n              \\[nN\\]\\* ) exit;;\n\n              # If Y or N not entered, prompt for correct entries\n                  \\* ) echo \"Please enter Y or N\";;\n \n              # End case statement and loop again \n              esac\n              done\n\n# End IF statement\nfi\n\n#Exit script\nexit","slug":"shell-script-to-control-services-on-linux-mint-especially-sql-server","published":1,"updated":"2020-08-23T20:54:35.126Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aao00bgsdjxhfig7i0t","content":"<p><a href=\"http://edpflager.com/?attachment_id=3694#main\"><img src=\"http://edpflager.com/wp-content/uploads/2018/02/shell.png\"></a>I’ve posted before about how I don’t run certain services on my Linux system all the time, but rather only when I am working with them. For example, Docker, Pentaho applications, and several database servers like MySQL, MariaDB and now Microsoft SQL Server.  The reasons are simple: because I experiment with a variety of technologies, I don’t want to dedicate resources unnecessarily and there may often be conflicts such as web server interfaces using the same ports. So to alleviate some of those problems I generally disable services and start them when necessary. To do that I usually create Bash scripts to start and stop the services, and save those in a named location in /opt that is associated with the server application. I would have two scripts, one to start and one to stop, but since I’ve had some time lately I’ve worked up a method to do this in one script. Below is a script I drafted to start and stop Microsoft SQL Server on Linux. I saved this to the /opt folder where the SQL Server components are installed and then created a Launcher shortcut to add it to my menu. Now I just select that option in the menu, and I see the current status of the server, and I can start it or shut it down as need be. Its heavily commented to provide information, so use it as a source for yourself if you are running on Debian based systems.</p>\n<a id=\"more\"></a>\n<p>#! /bin/bash</p>\n<h1 id=\"Previous-line-explicitly-names-which-BASH-shell-to-run-Some-versions-of-Linux-use-a-different\"><a href=\"#Previous-line-explicitly-names-which-BASH-shell-to-run-Some-versions-of-Linux-use-a-different\" class=\"headerlink\" title=\"Previous line explicitly names which BASH shell to run. Some versions of Linux use a different\"></a>Previous line explicitly names which BASH shell to run. Some versions of Linux use a different</h1><h1 id=\"shell-when-running-from-GUI\"><a href=\"#shell-when-running-from-GUI\" class=\"headerlink\" title=\"shell when running from GUI.\"></a>shell when running from GUI.</h1><h1 id=\"Check-the-status-of-mssql-server-service-Redirects-output-to-dev-null-to-suppress-printing\"><a href=\"#Check-the-status-of-mssql-server-service-Redirects-output-to-dev-null-to-suppress-printing\" class=\"headerlink\" title=\"Check the status of mssql-server service. Redirects output to /dev/null to suppress printing\"></a>Check the status of mssql-server service. Redirects output to /dev/null to suppress printing</h1><h1 id=\"results-because-we-are-only-interested-in-the-return-code\"><a href=\"#results-because-we-are-only-interested-in-the-return-code\" class=\"headerlink\" title=\"results because we are only interested in the return code.\"></a>results because we are only interested in the return code.</h1><p>systemctl status mssql-server | grep ‘Active: inactive (dead)’ &amp;&gt; /dev/null</p>\n<h1 id=\"Check-return-code-If-result-is-zero-then-SQL-Server-Service-is-not-running\"><a href=\"#Check-return-code-If-result-is-zero-then-SQL-Server-Service-is-not-running\" class=\"headerlink\" title=\"Check return code. If result is zero then SQL Server Service is not running.\"></a>Check return code. If result is zero then SQL Server Service is not running.</h1><p>if [ $? == 0 ]; then<br> echo “SQL Server is not active.”</p>\n<h1 id=\"Prompt-use-to-start-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\"><a href=\"#Prompt-use-to-start-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\" class=\"headerlink\" title=\"Prompt use to start SQL Server. Loop through until user responses with a Yes or No.\"></a>Prompt use to start SQL Server. Loop through until user responses with a Yes or No.</h1><h1 id=\"Result-is-stored-in-start-variable\"><a href=\"#Result-is-stored-in-start-variable\" class=\"headerlink\" title=\"Result is stored in start variable.\"></a>Result is stored in start variable.</h1><p>    while true<br>    do<br>    read -p “Do you want to start it (Y/N)?” start</p>\n<p>   # Case statements to check entered value<br>     case $start in<br>               # If y or Y entered, start service then exit.<br>               [yY]* ) sudo systemctl enable mssql-server &amp;&amp; sudo systemctl start mssql-server<br>                break ;;</p>\n<pre><code>           # If n or N entered, do nothing and exit.\n           \\[nN\\]\\* ) exit;;\n\n           # If Y or N not entered, prompt for correct entries\n           \\* ) echo &quot;Please enter Y or N&quot;;;\n\n   # End case statement and loop again \n   esac\n   done</code></pre>\n<h1 id=\"If-return-code-from-service-check-is-not-zero-then-SQL-Server-Service-is-running\"><a href=\"#If-return-code-from-service-check-is-not-zero-then-SQL-Server-Service-is-running\" class=\"headerlink\" title=\"If return code from service check is not zero then SQL Server Service is running.\"></a>If return code from service check is not zero then SQL Server Service is running.</h1><p>else<br> echo “SQL Server is active”</p>\n<h1 id=\"Prompt-use-to-stop-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\"><a href=\"#Prompt-use-to-stop-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\" class=\"headerlink\" title=\"Prompt use to stop SQL Server. Loop through until user responses with a Yes or No.\"></a>Prompt use to stop SQL Server. Loop through until user responses with a Yes or No.</h1><h1 id=\"Result-is-stored-in-start-variable-1\"><a href=\"#Result-is-stored-in-start-variable-1\" class=\"headerlink\" title=\"Result is stored in start variable.\"></a>Result is stored in start variable.</h1><p>   while true<br>   do<br>   read -p “Do you want to stop it (Y/N)?” stop</p>\n<h1 id=\"Case-statements-to-check-entered-value\"><a href=\"#Case-statements-to-check-entered-value\" class=\"headerlink\" title=\"Case statements to check entered value\"></a>Case statements to check entered value</h1><p>   case $stop in<br>              # If y or Y entered, start service then exit.<br>              [yY]* ) sudo systemctl stop mssql-server &amp;&amp; sudo systemctl disable mssql-server<br>              break ;;</p>\n<pre><code>          # If n or N entered, do nothing and exit. \n          \\[nN\\]\\* ) exit;;\n\n          # If Y or N not entered, prompt for correct entries\n              \\* ) echo &quot;Please enter Y or N&quot;;;\n\n          # End case statement and loop again \n          esac\n          done</code></pre>\n<h1 id=\"End-IF-statement\"><a href=\"#End-IF-statement\" class=\"headerlink\" title=\"End IF statement\"></a>End IF statement</h1><p>fi</p>\n<p>#Exit script<br>exit</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3694#main\"><img src=\"http://edpflager.com/wp-content/uploads/2018/02/shell.png\"></a>I’ve posted before about how I don’t run certain services on my Linux system all the time, but rather only when I am working with them. For example, Docker, Pentaho applications, and several database servers like MySQL, MariaDB and now Microsoft SQL Server.  The reasons are simple: because I experiment with a variety of technologies, I don’t want to dedicate resources unnecessarily and there may often be conflicts such as web server interfaces using the same ports. So to alleviate some of those problems I generally disable services and start them when necessary. To do that I usually create Bash scripts to start and stop the services, and save those in a named location in /opt that is associated with the server application. I would have two scripts, one to start and one to stop, but since I’ve had some time lately I’ve worked up a method to do this in one script. Below is a script I drafted to start and stop Microsoft SQL Server on Linux. I saved this to the /opt folder where the SQL Server components are installed and then created a Launcher shortcut to add it to my menu. Now I just select that option in the menu, and I see the current status of the server, and I can start it or shut it down as need be. Its heavily commented to provide information, so use it as a source for yourself if you are running on Debian based systems.</p>","more":"<p>#! /bin/bash</p>\n<h1 id=\"Previous-line-explicitly-names-which-BASH-shell-to-run-Some-versions-of-Linux-use-a-different\"><a href=\"#Previous-line-explicitly-names-which-BASH-shell-to-run-Some-versions-of-Linux-use-a-different\" class=\"headerlink\" title=\"Previous line explicitly names which BASH shell to run. Some versions of Linux use a different\"></a>Previous line explicitly names which BASH shell to run. Some versions of Linux use a different</h1><h1 id=\"shell-when-running-from-GUI\"><a href=\"#shell-when-running-from-GUI\" class=\"headerlink\" title=\"shell when running from GUI.\"></a>shell when running from GUI.</h1><h1 id=\"Check-the-status-of-mssql-server-service-Redirects-output-to-dev-null-to-suppress-printing\"><a href=\"#Check-the-status-of-mssql-server-service-Redirects-output-to-dev-null-to-suppress-printing\" class=\"headerlink\" title=\"Check the status of mssql-server service. Redirects output to /dev/null to suppress printing\"></a>Check the status of mssql-server service. Redirects output to /dev/null to suppress printing</h1><h1 id=\"results-because-we-are-only-interested-in-the-return-code\"><a href=\"#results-because-we-are-only-interested-in-the-return-code\" class=\"headerlink\" title=\"results because we are only interested in the return code.\"></a>results because we are only interested in the return code.</h1><p>systemctl status mssql-server | grep ‘Active: inactive (dead)’ &amp;&gt; /dev/null</p>\n<h1 id=\"Check-return-code-If-result-is-zero-then-SQL-Server-Service-is-not-running\"><a href=\"#Check-return-code-If-result-is-zero-then-SQL-Server-Service-is-not-running\" class=\"headerlink\" title=\"Check return code. If result is zero then SQL Server Service is not running.\"></a>Check return code. If result is zero then SQL Server Service is not running.</h1><p>if [ $? == 0 ]; then<br> echo “SQL Server is not active.”</p>\n<h1 id=\"Prompt-use-to-start-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\"><a href=\"#Prompt-use-to-start-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\" class=\"headerlink\" title=\"Prompt use to start SQL Server. Loop through until user responses with a Yes or No.\"></a>Prompt use to start SQL Server. Loop through until user responses with a Yes or No.</h1><h1 id=\"Result-is-stored-in-start-variable\"><a href=\"#Result-is-stored-in-start-variable\" class=\"headerlink\" title=\"Result is stored in start variable.\"></a>Result is stored in start variable.</h1><p>    while true<br>    do<br>    read -p “Do you want to start it (Y/N)?” start</p>\n<p>   # Case statements to check entered value<br>     case $start in<br>               # If y or Y entered, start service then exit.<br>               [yY]* ) sudo systemctl enable mssql-server &amp;&amp; sudo systemctl start mssql-server<br>                break ;;</p>\n<pre><code>           # If n or N entered, do nothing and exit.\n           \\[nN\\]\\* ) exit;;\n\n           # If Y or N not entered, prompt for correct entries\n           \\* ) echo &quot;Please enter Y or N&quot;;;\n\n   # End case statement and loop again \n   esac\n   done</code></pre>\n<h1 id=\"If-return-code-from-service-check-is-not-zero-then-SQL-Server-Service-is-running\"><a href=\"#If-return-code-from-service-check-is-not-zero-then-SQL-Server-Service-is-running\" class=\"headerlink\" title=\"If return code from service check is not zero then SQL Server Service is running.\"></a>If return code from service check is not zero then SQL Server Service is running.</h1><p>else<br> echo “SQL Server is active”</p>\n<h1 id=\"Prompt-use-to-stop-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\"><a href=\"#Prompt-use-to-stop-SQL-Server-Loop-through-until-user-responses-with-a-Yes-or-No\" class=\"headerlink\" title=\"Prompt use to stop SQL Server. Loop through until user responses with a Yes or No.\"></a>Prompt use to stop SQL Server. Loop through until user responses with a Yes or No.</h1><h1 id=\"Result-is-stored-in-start-variable-1\"><a href=\"#Result-is-stored-in-start-variable-1\" class=\"headerlink\" title=\"Result is stored in start variable.\"></a>Result is stored in start variable.</h1><p>   while true<br>   do<br>   read -p “Do you want to stop it (Y/N)?” stop</p>\n<h1 id=\"Case-statements-to-check-entered-value\"><a href=\"#Case-statements-to-check-entered-value\" class=\"headerlink\" title=\"Case statements to check entered value\"></a>Case statements to check entered value</h1><p>   case $stop in<br>              # If y or Y entered, start service then exit.<br>              [yY]* ) sudo systemctl stop mssql-server &amp;&amp; sudo systemctl disable mssql-server<br>              break ;;</p>\n<pre><code>          # If n or N entered, do nothing and exit. \n          \\[nN\\]\\* ) exit;;\n\n          # If Y or N not entered, prompt for correct entries\n              \\* ) echo &quot;Please enter Y or N&quot;;;\n\n          # End case statement and loop again \n          esac\n          done</code></pre>\n<h1 id=\"End-IF-statement\"><a href=\"#End-IF-statement\" class=\"headerlink\" title=\"End IF statement\"></a>End IF statement</h1><p>fi</p>\n<p>#Exit script<br>exit</p>"},{"title":"SmartDiagram in R Markdown","id":"4236","comments":0,"date":"2018-12-15T00:33:32.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)A useful plugin from the LaTeX environment that works with RMarkdown is [**smartdiagram**](http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf) developed by Claudio Fiandrino. Conceived as a way to create diagrams from a list of items, it will generate output as circular diagrams, flow diagrams, sequence diagrams and several others. Components within the diagrams can be different colors or a single uniform one. Combined these elements provide a visual layer to describe activities within your R Markdown diagrams and make what your are trying to convey more intuitive to the reader. I won't cover using all of the different diagrams in smartdiagrams with R Markdown, but this should be a good introduction to how to get it to work. To get started, in your Markdown document YAML header-includes line add a call to use the smartdiagram package:\n\nheader-includes: usepackage{smartdiagram}\n<!-- more -->\nSmartdiagram is built on the TikZ package and a call to that package is included so its not necessary to explicitly load it. You may however, need to include a tikz.sty file in your R Markdown directory if you don't already have one. The contents are very brief and are reproduced here:\n\n% Copyright 2006 by Till Tantau\n%\n% This file may be distributed and/or modified\n%\n% 1. under the LaTeX Project Public License and/or\n% 2. under the GNU Public License.\n%\n% See the file doc/generic/pgf/licenses/LICENSE for more details.\n\nRequirePackage{pgf,pgffor} % calc and xkeyval have been removed!\n\ninput{tikz.code.tex}\n\nendinput\n\n#### Sequence Diagrams\n\nWith the package call specified, we can define a simple sequence diagram. Our first step is the diagram set to specify characteristics of the sequence. To set a uniform color for the items in the diagram and specify the text color, we can use this:\n\nsmartdiagramset{uniform sequence color = true, sequence item text color = white}\n\nFollowing that define the diagram type (sequence) and the items in the sequence and knit the file to generate the diagram.\n\nsmartdiagram\\[sequence diagram\\]{Birth, Taxes, Death}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png) Adding color to the diagram is defined in the smartdiagramset command by replacing uniform sequence color with set color list. You then define a set of colors to match the elements in your list. One thing to be aware - if there are more items in the sequence than defined colors, any items that do not have a specific color defined will be displayed with the default color as shown in the graphic. The code below generates the diagram shown (with a nod to South Park):\n\nsmartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[sequence diagram\\]{Steal Underwear, ???, Profit, Success, Taxes}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png)\n\n#### Circular Diagrams\n\nCircular diagrams are useful for representing items that repeat in a cycle such as the seasons of the year (Spring, Summer, Fall, Winter). For this example though, I will use the software development lifecycle (SDLC). This is a repetitive five step process, where needs are identified, software is designed to meet those needs, code is implemented to process the design, the implemented software is tested, and the test results are evaluated to see if the software meets the original needs. If not all the needs have been met, or new needs are identified, the cycle repeats. With the same prerequisites in place as defined above for your R Markdown document, you can implement this circular diagram with one line of code and smartdiagram will use a predefined set of colors for your output:\n\nsmartdiagram\\[circular diagram:clockwise\\]{Needs, Design, Implement, Test, Evaluate}\n\nNotice that smartdiagram has a limited number of characters horizontally for items, so it hyphenates output text into multiple lines. With an odd number of elements for your diagram, smartdiagram does not put the first element at the top of the chart, but rather offsets it somewhat.  You can also define how the diagram is laid out - clockwise or counterclockwise. If you don't specify a rotation, smartdiagram will lay it out counterclockwise. [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png) You can also override the predefined colors by using a smartdiagramset command with your preferred color scheme. Unfortunately, for a uniform color you have to specify each color, as the uniform sequence color command does not work with circular diagrams. Here is an example:\n\nsmartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png)\n\n#### Flow diagrams\n\nSimilar to a circular diagram is a flow diagram which can be represented vertically (default) or horizontally. If no direction is specified in the command to generate the diagram it will default to vertical. For consistency, I prefer to include the direction for both. As with other smartdiagram examples, the colors will default to a predefined pallet unless specifically overwritten. Here are two examples:\n\nsmartdiagram\\[flow diagram:horizontal\\]{Beginning, Middle, End}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png)\n\nsmartdiagramset{set color list={green!50, blue, orange},sequence item text color = black}\nsmartdiagram\\[flow diagram:vertical\\]{Beginning, Middle, End}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png) So why use a flow diagram in place of a circular diagram? In the smartdiagram implementation in native LaTeX, the flow diagram has the ability to include other items that feed into the flow at specific parts of the flow. This involves the loading of the smartdiagram library{additions}, which I have not been able to get to work in R Markdown, as of yet.\n\n#### Other diagrams\n\nSmartdiagram includes a number of other diagram options that I have not found a use for, but below are a few examples. Be sure to check out the [smartdiagram](http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf) documentation for more. Bubble Diagrams   [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png) Priority Descriptive diagram [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png) Below is the R Markdown code I used to create this post:\n\n\\---\ntitle: \"SmartDiagrams\"\noutput:\n   pdf\\_document\nheader-includes: usepackage{smartdiagram}\n---\n\n\n\\`\\`\\`{r setup, include=FALSE}\n# usepackage{TikZ}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n\nSequence Diagrams\n\nsmartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[sequence diagram\\]{Steal Underwear, ???, Profit, Success, Taxes}\n\nsmartdiagramset{uniform sequence color = true, sequence item text color = white}\nsmartdiagram\\[sequence diagram\\]{Birth, Taxes, Death}\n\n\\*\\*\\*\nCircular diagrams\n\nsmartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}\nsmartdiagram\\[circular diagram:clockwise\\]{Needs, Design, Implement, Test, Evaluate}\n\nsmartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}\nsmartdiagram\\[circular diagram:counterclockwise\\]{Spring, Summer, Fall, Winter}\n\n\\*\\*\\* \nSequence diagrams\n\nsmartdiagramset{border color=none, uniform color list=teal!60 for 99 items}\nsmartdiagram\\[flow diagram:horizontal\\]{Spring, Summer, Fall, Winter}\n\n\nsmartdiagram\\[flow diagram:horizontal\\]{Beginning, Middle, End}\n\n\nsmartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}\nsmartdiagram\\[flow diagram:vertical\\]{Beginning, Middle, End}\n\n\n\\*\\*\\*\nBubble Diagram\nsmartdiagramset{set color list={blue!50, cyan, green, blue!50, cyan, green,blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[bubble diagram\\]{A,B,C,D,E,F,G,H,I,J }\n\n\n\\*\\*\\*\nPriority Diagram\nsmartdiagramset{\nset color list={blue!50!cyan,green!60!lime,orange!50!red},\npriority arrow width=2cm,\npriority arrow height advance=2.25cm\n}\nsmartdiagram\\[priority descriptive diagram\\]{Food, Shelter, Security }","source":"_posts/smartdiagram-in-r-markdown.md","raw":"---\ntitle: SmartDiagram in R Markdown\ntags:\n  - cookbook\n  - howto\n  - R Markdown\n  - technical\nid: '4236'\ncategories:\n  - - Business Intelligence\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-12-14 19:33:32\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)A useful plugin from the LaTeX environment that works with RMarkdown is [**smartdiagram**](http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf) developed by Claudio Fiandrino. Conceived as a way to create diagrams from a list of items, it will generate output as circular diagrams, flow diagrams, sequence diagrams and several others. Components within the diagrams can be different colors or a single uniform one. Combined these elements provide a visual layer to describe activities within your R Markdown diagrams and make what your are trying to convey more intuitive to the reader. I won't cover using all of the different diagrams in smartdiagrams with R Markdown, but this should be a good introduction to how to get it to work. To get started, in your Markdown document YAML header-includes line add a call to use the smartdiagram package:\n\nheader-includes: usepackage{smartdiagram}\n<!-- more -->\nSmartdiagram is built on the TikZ package and a call to that package is included so its not necessary to explicitly load it. You may however, need to include a tikz.sty file in your R Markdown directory if you don't already have one. The contents are very brief and are reproduced here:\n\n% Copyright 2006 by Till Tantau\n%\n% This file may be distributed and/or modified\n%\n% 1. under the LaTeX Project Public License and/or\n% 2. under the GNU Public License.\n%\n% See the file doc/generic/pgf/licenses/LICENSE for more details.\n\nRequirePackage{pgf,pgffor} % calc and xkeyval have been removed!\n\ninput{tikz.code.tex}\n\nendinput\n\n#### Sequence Diagrams\n\nWith the package call specified, we can define a simple sequence diagram. Our first step is the diagram set to specify characteristics of the sequence. To set a uniform color for the items in the diagram and specify the text color, we can use this:\n\nsmartdiagramset{uniform sequence color = true, sequence item text color = white}\n\nFollowing that define the diagram type (sequence) and the items in the sequence and knit the file to generate the diagram.\n\nsmartdiagram\\[sequence diagram\\]{Birth, Taxes, Death}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png) Adding color to the diagram is defined in the smartdiagramset command by replacing uniform sequence color with set color list. You then define a set of colors to match the elements in your list. One thing to be aware - if there are more items in the sequence than defined colors, any items that do not have a specific color defined will be displayed with the default color as shown in the graphic. The code below generates the diagram shown (with a nod to South Park):\n\nsmartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[sequence diagram\\]{Steal Underwear, ???, Profit, Success, Taxes}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png)\n\n#### Circular Diagrams\n\nCircular diagrams are useful for representing items that repeat in a cycle such as the seasons of the year (Spring, Summer, Fall, Winter). For this example though, I will use the software development lifecycle (SDLC). This is a repetitive five step process, where needs are identified, software is designed to meet those needs, code is implemented to process the design, the implemented software is tested, and the test results are evaluated to see if the software meets the original needs. If not all the needs have been met, or new needs are identified, the cycle repeats. With the same prerequisites in place as defined above for your R Markdown document, you can implement this circular diagram with one line of code and smartdiagram will use a predefined set of colors for your output:\n\nsmartdiagram\\[circular diagram:clockwise\\]{Needs, Design, Implement, Test, Evaluate}\n\nNotice that smartdiagram has a limited number of characters horizontally for items, so it hyphenates output text into multiple lines. With an odd number of elements for your diagram, smartdiagram does not put the first element at the top of the chart, but rather offsets it somewhat.  You can also define how the diagram is laid out - clockwise or counterclockwise. If you don't specify a rotation, smartdiagram will lay it out counterclockwise. [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png) You can also override the predefined colors by using a smartdiagramset command with your preferred color scheme. Unfortunately, for a uniform color you have to specify each color, as the uniform sequence color command does not work with circular diagrams. Here is an example:\n\nsmartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png)\n\n#### Flow diagrams\n\nSimilar to a circular diagram is a flow diagram which can be represented vertically (default) or horizontally. If no direction is specified in the command to generate the diagram it will default to vertical. For consistency, I prefer to include the direction for both. As with other smartdiagram examples, the colors will default to a predefined pallet unless specifically overwritten. Here are two examples:\n\nsmartdiagram\\[flow diagram:horizontal\\]{Beginning, Middle, End}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png)\n\nsmartdiagramset{set color list={green!50, blue, orange},sequence item text color = black}\nsmartdiagram\\[flow diagram:vertical\\]{Beginning, Middle, End}\n\n[![](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png) So why use a flow diagram in place of a circular diagram? In the smartdiagram implementation in native LaTeX, the flow diagram has the ability to include other items that feed into the flow at specific parts of the flow. This involves the loading of the smartdiagram library{additions}, which I have not been able to get to work in R Markdown, as of yet.\n\n#### Other diagrams\n\nSmartdiagram includes a number of other diagram options that I have not found a use for, but below are a few examples. Be sure to check out the [smartdiagram](http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf) documentation for more. Bubble Diagrams   [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png) Priority Descriptive diagram [![](http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png)](http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png) Below is the R Markdown code I used to create this post:\n\n\\---\ntitle: \"SmartDiagrams\"\noutput:\n   pdf\\_document\nheader-includes: usepackage{smartdiagram}\n---\n\n\n\\`\\`\\`{r setup, include=FALSE}\n# usepackage{TikZ}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n\nSequence Diagrams\n\nsmartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[sequence diagram\\]{Steal Underwear, ???, Profit, Success, Taxes}\n\nsmartdiagramset{uniform sequence color = true, sequence item text color = white}\nsmartdiagram\\[sequence diagram\\]{Birth, Taxes, Death}\n\n\\*\\*\\*\nCircular diagrams\n\nsmartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}\nsmartdiagram\\[circular diagram:clockwise\\]{Needs, Design, Implement, Test, Evaluate}\n\nsmartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}\nsmartdiagram\\[circular diagram:counterclockwise\\]{Spring, Summer, Fall, Winter}\n\n\\*\\*\\* \nSequence diagrams\n\nsmartdiagramset{border color=none, uniform color list=teal!60 for 99 items}\nsmartdiagram\\[flow diagram:horizontal\\]{Spring, Summer, Fall, Winter}\n\n\nsmartdiagram\\[flow diagram:horizontal\\]{Beginning, Middle, End}\n\n\nsmartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}\nsmartdiagram\\[flow diagram:vertical\\]{Beginning, Middle, End}\n\n\n\\*\\*\\*\nBubble Diagram\nsmartdiagramset{set color list={blue!50, cyan, green, blue!50, cyan, green,blue!50, cyan, green},sequence item text color = black}\nsmartdiagram\\[bubble diagram\\]{A,B,C,D,E,F,G,H,I,J }\n\n\n\\*\\*\\*\nPriority Diagram\nsmartdiagramset{\nset color list={blue!50!cyan,green!60!lime,orange!50!red},\npriority arrow width=2cm,\npriority arrow height advance=2.25cm\n}\nsmartdiagram\\[priority descriptive diagram\\]{Food, Shelter, Security }","slug":"smartdiagram-in-r-markdown","published":1,"updated":"2020-08-23T20:54:35.202Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aat00bjsdjxf4bi5ss2","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>A useful plugin from the LaTeX environment that works with RMarkdown is <a href=\"http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf\"><strong>smartdiagram</strong></a> developed by Claudio Fiandrino. Conceived as a way to create diagrams from a list of items, it will generate output as circular diagrams, flow diagrams, sequence diagrams and several others. Components within the diagrams can be different colors or a single uniform one. Combined these elements provide a visual layer to describe activities within your R Markdown diagrams and make what your are trying to convey more intuitive to the reader. I won’t cover using all of the different diagrams in smartdiagrams with R Markdown, but this should be a good introduction to how to get it to work. To get started, in your Markdown document YAML header-includes line add a call to use the smartdiagram package:</p>\n<p>header-includes: usepackage{smartdiagram}</p>\n<a id=\"more\"></a>\n<p>Smartdiagram is built on the TikZ package and a call to that package is included so its not necessary to explicitly load it. You may however, need to include a tikz.sty file in your R Markdown directory if you don’t already have one. The contents are very brief and are reproduced here:</p>\n<p>% Copyright 2006 by Till Tantau<br>%<br>% This file may be distributed and/or modified<br>%<br>% 1. under the LaTeX Project Public License and/or<br>% 2. under the GNU Public License.<br>%<br>% See the file doc/generic/pgf/licenses/LICENSE for more details.</p>\n<p>RequirePackage{pgf,pgffor} % calc and xkeyval have been removed!</p>\n<p>input{tikz.code.tex}</p>\n<p>endinput</p>\n<h4 id=\"Sequence-Diagrams\"><a href=\"#Sequence-Diagrams\" class=\"headerlink\" title=\"Sequence Diagrams\"></a>Sequence Diagrams</h4><p>With the package call specified, we can define a simple sequence diagram. Our first step is the diagram set to specify characteristics of the sequence. To set a uniform color for the items in the diagram and specify the text color, we can use this:</p>\n<p>smartdiagramset{uniform sequence color = true, sequence item text color = white}</p>\n<p>Following that define the diagram type (sequence) and the items in the sequence and knit the file to generate the diagram.</p>\n<p>smartdiagram[sequence diagram]{Birth, Taxes, Death}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png\"></a> Adding color to the diagram is defined in the smartdiagramset command by replacing uniform sequence color with set color list. You then define a set of colors to match the elements in your list. One thing to be aware - if there are more items in the sequence than defined colors, any items that do not have a specific color defined will be displayed with the default color as shown in the graphic. The code below generates the diagram shown (with a nod to South Park):</p>\n<p>smartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[sequence diagram]{Steal Underwear, ???, Profit, Success, Taxes}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png\"></a></p>\n<h4 id=\"Circular-Diagrams\"><a href=\"#Circular-Diagrams\" class=\"headerlink\" title=\"Circular Diagrams\"></a>Circular Diagrams</h4><p>Circular diagrams are useful for representing items that repeat in a cycle such as the seasons of the year (Spring, Summer, Fall, Winter). For this example though, I will use the software development lifecycle (SDLC). This is a repetitive five step process, where needs are identified, software is designed to meet those needs, code is implemented to process the design, the implemented software is tested, and the test results are evaluated to see if the software meets the original needs. If not all the needs have been met, or new needs are identified, the cycle repeats. With the same prerequisites in place as defined above for your R Markdown document, you can implement this circular diagram with one line of code and smartdiagram will use a predefined set of colors for your output:</p>\n<p>smartdiagram[circular diagram:clockwise]{Needs, Design, Implement, Test, Evaluate}</p>\n<p>Notice that smartdiagram has a limited number of characters horizontally for items, so it hyphenates output text into multiple lines. With an odd number of elements for your diagram, smartdiagram does not put the first element at the top of the chart, but rather offsets it somewhat.  You can also define how the diagram is laid out - clockwise or counterclockwise. If you don’t specify a rotation, smartdiagram will lay it out counterclockwise. <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png\"></a> You can also override the predefined colors by using a smartdiagramset command with your preferred color scheme. Unfortunately, for a uniform color you have to specify each color, as the uniform sequence color command does not work with circular diagrams. Here is an example:</p>\n<p>smartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png\"></a></p>\n<h4 id=\"Flow-diagrams\"><a href=\"#Flow-diagrams\" class=\"headerlink\" title=\"Flow diagrams\"></a>Flow diagrams</h4><p>Similar to a circular diagram is a flow diagram which can be represented vertically (default) or horizontally. If no direction is specified in the command to generate the diagram it will default to vertical. For consistency, I prefer to include the direction for both. As with other smartdiagram examples, the colors will default to a predefined pallet unless specifically overwritten. Here are two examples:</p>\n<p>smartdiagram[flow diagram:horizontal]{Beginning, Middle, End}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png\"></a></p>\n<p>smartdiagramset{set color list={green!50, blue, orange},sequence item text color = black}<br>smartdiagram[flow diagram:vertical]{Beginning, Middle, End}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png\"></a> So why use a flow diagram in place of a circular diagram? In the smartdiagram implementation in native LaTeX, the flow diagram has the ability to include other items that feed into the flow at specific parts of the flow. This involves the loading of the smartdiagram library{additions}, which I have not been able to get to work in R Markdown, as of yet.</p>\n<h4 id=\"Other-diagrams\"><a href=\"#Other-diagrams\" class=\"headerlink\" title=\"Other diagrams\"></a>Other diagrams</h4><p>Smartdiagram includes a number of other diagram options that I have not found a use for, but below are a few examples. Be sure to check out the <a href=\"http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf\">smartdiagram</a> documentation for more. Bubble Diagrams   <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png\"></a> Priority Descriptive diagram <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png\"></a> Below is the R Markdown code I used to create this post:</p>\n<p>-–<br>title: “SmartDiagrams”<br>output:<br>   pdf_document<br>header-includes: usepackage{smartdiagram}</p>\n<hr>\n<p>```{r setup, include=FALSE}</p>\n<h1 id=\"usepackage-TikZ\"><a href=\"#usepackage-TikZ\" class=\"headerlink\" title=\"usepackage{TikZ}\"></a>usepackage{TikZ}</h1><p>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>Sequence Diagrams</p>\n<p>smartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[sequence diagram]{Steal Underwear, ???, Profit, Success, Taxes}</p>\n<p>smartdiagramset{uniform sequence color = true, sequence item text color = white}<br>smartdiagram[sequence diagram]{Birth, Taxes, Death}</p>\n<p>***<br>Circular diagrams</p>\n<p>smartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}<br>smartdiagram[circular diagram:clockwise]{Needs, Design, Implement, Test, Evaluate}</p>\n<p>smartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}<br>smartdiagram[circular diagram:counterclockwise]{Spring, Summer, Fall, Winter}</p>\n<p>***<br>Sequence diagrams</p>\n<p>smartdiagramset{border color=none, uniform color list=teal!60 for 99 items}<br>smartdiagram[flow diagram:horizontal]{Spring, Summer, Fall, Winter}</p>\n<p>smartdiagram[flow diagram:horizontal]{Beginning, Middle, End}</p>\n<p>smartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}<br>smartdiagram[flow diagram:vertical]{Beginning, Middle, End}</p>\n<p>***<br>Bubble Diagram<br>smartdiagramset{set color list={blue!50, cyan, green, blue!50, cyan, green,blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[bubble diagram]{A,B,C,D,E,F,G,H,I,J }</p>\n<p>***<br>Priority Diagram<br>smartdiagramset{<br>set color list={blue!50!cyan,green!60!lime,orange!50!red},<br>priority arrow width=2cm,<br>priority arrow height advance=2.25cm<br>}<br>smartdiagram[priority descriptive diagram]{Food, Shelter, Security }</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>A useful plugin from the LaTeX environment that works with RMarkdown is <a href=\"http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf\"><strong>smartdiagram</strong></a> developed by Claudio Fiandrino. Conceived as a way to create diagrams from a list of items, it will generate output as circular diagrams, flow diagrams, sequence diagrams and several others. Components within the diagrams can be different colors or a single uniform one. Combined these elements provide a visual layer to describe activities within your R Markdown diagrams and make what your are trying to convey more intuitive to the reader. I won’t cover using all of the different diagrams in smartdiagrams with R Markdown, but this should be a good introduction to how to get it to work. To get started, in your Markdown document YAML header-includes line add a call to use the smartdiagram package:</p>\n<p>header-includes: usepackage{smartdiagram}</p>","more":"<p>Smartdiagram is built on the TikZ package and a call to that package is included so its not necessary to explicitly load it. You may however, need to include a tikz.sty file in your R Markdown directory if you don’t already have one. The contents are very brief and are reproduced here:</p>\n<p>% Copyright 2006 by Till Tantau<br>%<br>% This file may be distributed and/or modified<br>%<br>% 1. under the LaTeX Project Public License and/or<br>% 2. under the GNU Public License.<br>%<br>% See the file doc/generic/pgf/licenses/LICENSE for more details.</p>\n<p>RequirePackage{pgf,pgffor} % calc and xkeyval have been removed!</p>\n<p>input{tikz.code.tex}</p>\n<p>endinput</p>\n<h4 id=\"Sequence-Diagrams\"><a href=\"#Sequence-Diagrams\" class=\"headerlink\" title=\"Sequence Diagrams\"></a>Sequence Diagrams</h4><p>With the package call specified, we can define a simple sequence diagram. Our first step is the diagram set to specify characteristics of the sequence. To set a uniform color for the items in the diagram and specify the text color, we can use this:</p>\n<p>smartdiagramset{uniform sequence color = true, sequence item text color = white}</p>\n<p>Following that define the diagram type (sequence) and the items in the sequence and knit the file to generate the diagram.</p>\n<p>smartdiagram[sequence diagram]{Birth, Taxes, Death}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequenceuniform.png\"></a> Adding color to the diagram is defined in the smartdiagramset command by replacing uniform sequence color with set color list. You then define a set of colors to match the elements in your list. One thing to be aware - if there are more items in the sequence than defined colors, any items that do not have a specific color defined will be displayed with the default color as shown in the graphic. The code below generates the diagram shown (with a nod to South Park):</p>\n<p>smartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[sequence diagram]{Steal Underwear, ???, Profit, Success, Taxes}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-sequencecolor.png\"></a></p>\n<h4 id=\"Circular-Diagrams\"><a href=\"#Circular-Diagrams\" class=\"headerlink\" title=\"Circular Diagrams\"></a>Circular Diagrams</h4><p>Circular diagrams are useful for representing items that repeat in a cycle such as the seasons of the year (Spring, Summer, Fall, Winter). For this example though, I will use the software development lifecycle (SDLC). This is a repetitive five step process, where needs are identified, software is designed to meet those needs, code is implemented to process the design, the implemented software is tested, and the test results are evaluated to see if the software meets the original needs. If not all the needs have been met, or new needs are identified, the cycle repeats. With the same prerequisites in place as defined above for your R Markdown document, you can implement this circular diagram with one line of code and smartdiagram will use a predefined set of colors for your output:</p>\n<p>smartdiagram[circular diagram:clockwise]{Needs, Design, Implement, Test, Evaluate}</p>\n<p>Notice that smartdiagram has a limited number of characters horizontally for items, so it hyphenates output text into multiple lines. With an odd number of elements for your diagram, smartdiagram does not put the first element at the top of the chart, but rather offsets it somewhat.  You can also define how the diagram is laid out - clockwise or counterclockwise. If you don’t specify a rotation, smartdiagram will lay it out counterclockwise. <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularprocess.png\"></a> You can also override the predefined colors by using a smartdiagramset command with your preferred color scheme. Unfortunately, for a uniform color you have to specify each color, as the uniform sequence color command does not work with circular diagrams. Here is an example:</p>\n<p>smartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-circularuniform.png\"></a></p>\n<h4 id=\"Flow-diagrams\"><a href=\"#Flow-diagrams\" class=\"headerlink\" title=\"Flow diagrams\"></a>Flow diagrams</h4><p>Similar to a circular diagram is a flow diagram which can be represented vertically (default) or horizontally. If no direction is specified in the command to generate the diagram it will default to vertical. For consistency, I prefer to include the direction for both. As with other smartdiagram examples, the colors will default to a predefined pallet unless specifically overwritten. Here are two examples:</p>\n<p>smartdiagram[flow diagram:horizontal]{Beginning, Middle, End}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowdiagram.png\"></a></p>\n<p>smartdiagramset{set color list={green!50, blue, orange},sequence item text color = black}<br>smartdiagram[flow diagram:vertical]{Beginning, Middle, End}</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-flowvertical.png\"></a> So why use a flow diagram in place of a circular diagram? In the smartdiagram implementation in native LaTeX, the flow diagram has the ability to include other items that feed into the flow at specific parts of the flow. This involves the loading of the smartdiagram library{additions}, which I have not been able to get to work in R Markdown, as of yet.</p>\n<h4 id=\"Other-diagrams\"><a href=\"#Other-diagrams\" class=\"headerlink\" title=\"Other diagrams\"></a>Other diagrams</h4><p>Smartdiagram includes a number of other diagram options that I have not found a use for, but below are a few examples. Be sure to check out the <a href=\"http://texdoc.net/texmf-dist/doc/latex/smartdiagram/smartdiagram.pdf\">smartdiagram</a> documentation for more. Bubble Diagrams   <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-bubblediagram.png\"></a> Priority Descriptive diagram <a href=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/12/PDF-prioritydiagram.png\"></a> Below is the R Markdown code I used to create this post:</p>\n<p>-–<br>title: “SmartDiagrams”<br>output:<br>   pdf_document<br>header-includes: usepackage{smartdiagram}</p>\n<hr>\n<p>```{r setup, include=FALSE}</p>\n<h1 id=\"usepackage-TikZ\"><a href=\"#usepackage-TikZ\" class=\"headerlink\" title=\"usepackage{TikZ}\"></a>usepackage{TikZ}</h1><p>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<p>Sequence Diagrams</p>\n<p>smartdiagramset{set color list={blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[sequence diagram]{Steal Underwear, ???, Profit, Success, Taxes}</p>\n<p>smartdiagramset{uniform sequence color = true, sequence item text color = white}<br>smartdiagram[sequence diagram]{Birth, Taxes, Death}</p>\n<p>***<br>Circular diagrams</p>\n<p>smartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}<br>smartdiagram[circular diagram:clockwise]{Needs, Design, Implement, Test, Evaluate}</p>\n<p>smartdiagramset{set color list={gray, gray, gray, gray},sequence item text color = black}<br>smartdiagram[circular diagram:counterclockwise]{Spring, Summer, Fall, Winter}</p>\n<p>***<br>Sequence diagrams</p>\n<p>smartdiagramset{border color=none, uniform color list=teal!60 for 99 items}<br>smartdiagram[flow diagram:horizontal]{Spring, Summer, Fall, Winter}</p>\n<p>smartdiagram[flow diagram:horizontal]{Beginning, Middle, End}</p>\n<p>smartdiagramset{set color list={green!50, blue, orange, gray},sequence item text color = black}<br>smartdiagram[flow diagram:vertical]{Beginning, Middle, End}</p>\n<p>***<br>Bubble Diagram<br>smartdiagramset{set color list={blue!50, cyan, green, blue!50, cyan, green,blue!50, cyan, green},sequence item text color = black}<br>smartdiagram[bubble diagram]{A,B,C,D,E,F,G,H,I,J }</p>\n<p>***<br>Priority Diagram<br>smartdiagramset{<br>set color list={blue!50!cyan,green!60!lime,orange!50!red},<br>priority arrow width=2cm,<br>priority arrow height advance=2.25cm<br>}<br>smartdiagram[priority descriptive diagram]{Food, Shelter, Security }</p>"},{"title":"SQuirreL SQL Client for accessing different databases - Part 1","id":"2778","comments":0,"date":"2015-06-21T19:29:54.000Z","_content":"\n[![squirrel](http://edpflager.com/wp-content/uploads/2015/06/squirrel-164x300.jpg)](http://edpflager.com/wp-content/uploads/2015/06/squirrel.jpg)Its been my experience that if you work on ETL projects, you eventually accumulate client software for a number of database systems on your development PC. The reason is pretty straightforward - you need to be able to access the systems you are working with to determine data types, schema structures, and occasionally to check that a User account and Password you have been given actually works. One problem I've run into though is that not all operating systems are supported by different database vendors with their tools. While Windows has the largest installation base, Mac OS X, and Linux also are used for ETL development  but Microsoft's SQL Server management tool will only work on Windows machines. Apple's FileMaker software is similar, running on Mac OS X and Windows, but not Linux (since version 7). The examples go on and on. Also, because each tool is laid out differently, it can be difficult to find what you need quickly when you only work infrequently on a specific platform. Often times remembering where I need to go in a specific tool will take me longer than getting the actual information I was looking for. All of this leads to the point of this post - using a free open source product call SQuirreL SQL Client to access multiple database platforms via one application regardless of whether you are running Windows, Mac OS X or any of a large variety of Linux distributions.\n<!-- more -->\n##### **Installation of SQuirreL SQL Client**\n\n\\[caption id=\"attachment\\_2807\" align=\"aligncenter\" width=\"300\"\\][![squirrelscreen](http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen-300x164.png)](http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen.png) SQuirreL SQL Client connected to a test server running Microsoft's SQL Server 2012.\\[/caption\\]\n\n1.  To get started, make sure you have Oracle's JAVA client installed on your system. If you are running Linux, I've found that OpenJDK doesn't work right with this, so be sure you have the correct software. On most operating systems, you can open a terminal prompt and enter the following to see if you have it installed: **JAVA -version.**\n2.  Next go to this website: [http://www.squirrelsql.org/](http://www.squirrelsql.org/) and click the \"Download and Installation\" link in the toolbar and then click the link for your operating system. A small JAR file will be downloaded. If you receive a message that these types of files can be harmful to your system, click the option that lets you down the file anyway.\n3.  On Windows or Linux, open a terminal prompt and navigate to where you downloaded the installation file. Enter the command: **java -jar squirrel-sql-3.6-standard.jar<ENTER> ** to run the installation routine.\n4.  On Mac OS X, open the folder where you downloaded the file in Finder, and double click it.\n5.  An lzpack window will appear. Click NEXT on the welcome screen, and NEXT on the please read windows. You can decide where to install the application on the following screen (the default location is usually OK unless you have a compelling reason for installing it elsewhere). Note on Linux,  if you run the install as a normal user, the installation will default to a folder in the user's HOME folder. Installed as root, it defaults to /usr/local.\n6.  On the following screen, Choose the add-ins you would like to use such as any DB platforms you anticipate connecting to, the optional plugins for Smart Tools and SQL (located between PostgreSQL and Sybase in the list), and any translations plugins you may need. Some additional plugins you may want to select are:\n    *   Session Scripts - run SQL when opening an session\n    *   SQL Parametrisation - put variables into your SQL statements\n    *   SQL Replace - place environment variables into your SQL statements\n    *   SQL Validator - validate SQL against the ISO SQL-99 standard\n7.  Click Next to watch the installation progress, and then DONE when it finishes. Finally, you have the option to create shortcuts and allow all users to access the application. Select specific options according to your needs, and then finish with DONE.\n\n The program is now installed. To run it in Linux, run the squirrel-sql.sh bash file, in Windows, execute the squirrel-sql.bat file, and on a Mac, run the SquirreLSQL.app file.\n\n##### Configuration of SQuirreL SQL Client\n\nAt this point, you can run SquirreL but it won't be able to connect to much. In order to access different DB platforms, you'll need to download the appropriate JDBC drivers for the databases you want to use. Check with your database vendor for web locations where the driver can be found. Once you have the JDBC JAR files, just copy them to the /squirrel-sql-3.6/lib folder.  On Linux and Windows, the installation of the drivers is fairly straightforward. On Mac OS X, you'll need to open up the finder folder where the SQuirreL application has been installed. If you chose the default location, hit Shift-Command-G on your keyboard to open the Go to Folder window, and paste this in: /Applications/SquirreLSQL/Contents/Resources/Java/lib. Hit Enter and after a second or two the Lib folder will appear.\n\n1.  Once you have the JDBC JAR file installed, restart SQuirrel, and click the DRIVERS tab on the left side. The installed drivers should now appear with a check mark. If there is a red circle with an X, then the driver is not installed.\n2.  Click on the Aliases tab to add a connection to database.\n3.  Click the Plus icon to add a new alias. Enter a name in the appropriate box. Click the Driver drop down and choose the appropriate checked driver.\n4.  Update the URL to the appropriate one for your database. For example for a locally installed MySQL database, enter: **jdbc:mysql://localhost:3306** or for a specific database: ** jdbc:mysql://localhost:3306/databasename**\n5.  Enter a user name in the space for it, and LEAVE THE PASSWORD FIELD EMPTY. If you enter one in, and save it, it will be saved in clear text. (That's a BAD security breach.) Click OK to exit, and a new entry with the name you specified will appear in the Aliases panel.\n6.  Double click the entry you created to initiate a connection. A Connect To box will appear with the user name filled in. Enter your password, and click the Connect button.\n7.  If all goes well, you'll see a new window appear on the canvas with an object tree on the left. At the top, the catalog drop down will show the currently connected schema or database. You can only work in the database or schema that is selected here.\n\nNext time we'll look at how to navigate around in SquirreL SQL Client and access multiple DB platforms at the same time!","source":"_posts/squirrel-client-for-accessing-multiple-database-platforms-part-1.md","raw":"---\ntitle: SQuirreL SQL Client for accessing different databases - Part 1\ntags:\n  - centos\n  - ETL\n  - guides\n  - How-to\n  - howto\n  - install\n  - Mac\n  - MySQL\n  - SysAdmin\n  - technical\nid: '2778'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-06-21 15:29:54\n---\n\n[![squirrel](http://edpflager.com/wp-content/uploads/2015/06/squirrel-164x300.jpg)](http://edpflager.com/wp-content/uploads/2015/06/squirrel.jpg)Its been my experience that if you work on ETL projects, you eventually accumulate client software for a number of database systems on your development PC. The reason is pretty straightforward - you need to be able to access the systems you are working with to determine data types, schema structures, and occasionally to check that a User account and Password you have been given actually works. One problem I've run into though is that not all operating systems are supported by different database vendors with their tools. While Windows has the largest installation base, Mac OS X, and Linux also are used for ETL development  but Microsoft's SQL Server management tool will only work on Windows machines. Apple's FileMaker software is similar, running on Mac OS X and Windows, but not Linux (since version 7). The examples go on and on. Also, because each tool is laid out differently, it can be difficult to find what you need quickly when you only work infrequently on a specific platform. Often times remembering where I need to go in a specific tool will take me longer than getting the actual information I was looking for. All of this leads to the point of this post - using a free open source product call SQuirreL SQL Client to access multiple database platforms via one application regardless of whether you are running Windows, Mac OS X or any of a large variety of Linux distributions.\n<!-- more -->\n##### **Installation of SQuirreL SQL Client**\n\n\\[caption id=\"attachment\\_2807\" align=\"aligncenter\" width=\"300\"\\][![squirrelscreen](http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen-300x164.png)](http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen.png) SQuirreL SQL Client connected to a test server running Microsoft's SQL Server 2012.\\[/caption\\]\n\n1.  To get started, make sure you have Oracle's JAVA client installed on your system. If you are running Linux, I've found that OpenJDK doesn't work right with this, so be sure you have the correct software. On most operating systems, you can open a terminal prompt and enter the following to see if you have it installed: **JAVA -version.**\n2.  Next go to this website: [http://www.squirrelsql.org/](http://www.squirrelsql.org/) and click the \"Download and Installation\" link in the toolbar and then click the link for your operating system. A small JAR file will be downloaded. If you receive a message that these types of files can be harmful to your system, click the option that lets you down the file anyway.\n3.  On Windows or Linux, open a terminal prompt and navigate to where you downloaded the installation file. Enter the command: **java -jar squirrel-sql-3.6-standard.jar<ENTER> ** to run the installation routine.\n4.  On Mac OS X, open the folder where you downloaded the file in Finder, and double click it.\n5.  An lzpack window will appear. Click NEXT on the welcome screen, and NEXT on the please read windows. You can decide where to install the application on the following screen (the default location is usually OK unless you have a compelling reason for installing it elsewhere). Note on Linux,  if you run the install as a normal user, the installation will default to a folder in the user's HOME folder. Installed as root, it defaults to /usr/local.\n6.  On the following screen, Choose the add-ins you would like to use such as any DB platforms you anticipate connecting to, the optional plugins for Smart Tools and SQL (located between PostgreSQL and Sybase in the list), and any translations plugins you may need. Some additional plugins you may want to select are:\n    *   Session Scripts - run SQL when opening an session\n    *   SQL Parametrisation - put variables into your SQL statements\n    *   SQL Replace - place environment variables into your SQL statements\n    *   SQL Validator - validate SQL against the ISO SQL-99 standard\n7.  Click Next to watch the installation progress, and then DONE when it finishes. Finally, you have the option to create shortcuts and allow all users to access the application. Select specific options according to your needs, and then finish with DONE.\n\n The program is now installed. To run it in Linux, run the squirrel-sql.sh bash file, in Windows, execute the squirrel-sql.bat file, and on a Mac, run the SquirreLSQL.app file.\n\n##### Configuration of SQuirreL SQL Client\n\nAt this point, you can run SquirreL but it won't be able to connect to much. In order to access different DB platforms, you'll need to download the appropriate JDBC drivers for the databases you want to use. Check with your database vendor for web locations where the driver can be found. Once you have the JDBC JAR files, just copy them to the /squirrel-sql-3.6/lib folder.  On Linux and Windows, the installation of the drivers is fairly straightforward. On Mac OS X, you'll need to open up the finder folder where the SQuirreL application has been installed. If you chose the default location, hit Shift-Command-G on your keyboard to open the Go to Folder window, and paste this in: /Applications/SquirreLSQL/Contents/Resources/Java/lib. Hit Enter and after a second or two the Lib folder will appear.\n\n1.  Once you have the JDBC JAR file installed, restart SQuirrel, and click the DRIVERS tab on the left side. The installed drivers should now appear with a check mark. If there is a red circle with an X, then the driver is not installed.\n2.  Click on the Aliases tab to add a connection to database.\n3.  Click the Plus icon to add a new alias. Enter a name in the appropriate box. Click the Driver drop down and choose the appropriate checked driver.\n4.  Update the URL to the appropriate one for your database. For example for a locally installed MySQL database, enter: **jdbc:mysql://localhost:3306** or for a specific database: ** jdbc:mysql://localhost:3306/databasename**\n5.  Enter a user name in the space for it, and LEAVE THE PASSWORD FIELD EMPTY. If you enter one in, and save it, it will be saved in clear text. (That's a BAD security breach.) Click OK to exit, and a new entry with the name you specified will appear in the Aliases panel.\n6.  Double click the entry you created to initiate a connection. A Connect To box will appear with the user name filled in. Enter your password, and click the Connect button.\n7.  If all goes well, you'll see a new window appear on the canvas with an object tree on the left. At the top, the catalog drop down will show the currently connected schema or database. You can only work in the database or schema that is selected here.\n\nNext time we'll look at how to navigate around in SquirreL SQL Client and access multiple DB platforms at the same time!","slug":"squirrel-client-for-accessing-multiple-database-platforms-part-1","published":1,"updated":"2020-08-23T20:54:34.946Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aaw00bnsdjxawy2ddbc","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel-164x300.jpg\" alt=\"squirrel\"></a>Its been my experience that if you work on ETL projects, you eventually accumulate client software for a number of database systems on your development PC. The reason is pretty straightforward - you need to be able to access the systems you are working with to determine data types, schema structures, and occasionally to check that a User account and Password you have been given actually works. One problem I’ve run into though is that not all operating systems are supported by different database vendors with their tools. While Windows has the largest installation base, Mac OS X, and Linux also are used for ETL development  but Microsoft’s SQL Server management tool will only work on Windows machines. Apple’s FileMaker software is similar, running on Mac OS X and Windows, but not Linux (since version 7). The examples go on and on. Also, because each tool is laid out differently, it can be difficult to find what you need quickly when you only work infrequently on a specific platform. Often times remembering where I need to go in a specific tool will take me longer than getting the actual information I was looking for. All of this leads to the point of this post - using a free open source product call SQuirreL SQL Client to access multiple database platforms via one application regardless of whether you are running Windows, Mac OS X or any of a large variety of Linux distributions.</p>\n<a id=\"more\"></a>\n<h5 id=\"Installation-of-SQuirreL-SQL-Client\"><a href=\"#Installation-of-SQuirreL-SQL-Client\" class=\"headerlink\" title=\"Installation of SQuirreL SQL Client\"></a><strong>Installation of SQuirreL SQL Client</strong></h5><p>[caption id=”attachment_2807” align=”aligncenter” width=”300”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen-300x164.png\" alt=\"squirrelscreen\"></a> SQuirreL SQL Client connected to a test server running Microsoft’s SQL Server 2012.[/caption]</p>\n<ol>\n<li>To get started, make sure you have Oracle’s JAVA client installed on your system. If you are running Linux, I’ve found that OpenJDK doesn’t work right with this, so be sure you have the correct software. On most operating systems, you can open a terminal prompt and enter the following to see if you have it installed: <strong>JAVA -version.</strong></li>\n<li>Next go to this website: <a href=\"http://www.squirrelsql.org/\">http://www.squirrelsql.org/</a> and click the “Download and Installation” link in the toolbar and then click the link for your operating system. A small JAR file will be downloaded. If you receive a message that these types of files can be harmful to your system, click the option that lets you down the file anyway.</li>\n<li>On Windows or Linux, open a terminal prompt and navigate to where you downloaded the installation file. Enter the command: **java -jar squirrel-sql-3.6-standard.jar<ENTER> ** to run the installation routine.</li>\n<li>On Mac OS X, open the folder where you downloaded the file in Finder, and double click it.</li>\n<li>An lzpack window will appear. Click NEXT on the welcome screen, and NEXT on the please read windows. You can decide where to install the application on the following screen (the default location is usually OK unless you have a compelling reason for installing it elsewhere). Note on Linux,  if you run the install as a normal user, the installation will default to a folder in the user’s HOME folder. Installed as root, it defaults to /usr/local.</li>\n<li>On the following screen, Choose the add-ins you would like to use such as any DB platforms you anticipate connecting to, the optional plugins for Smart Tools and SQL (located between PostgreSQL and Sybase in the list), and any translations plugins you may need. Some additional plugins you may want to select are:<ul>\n<li>Session Scripts - run SQL when opening an session</li>\n<li>SQL Parametrisation - put variables into your SQL statements</li>\n<li>SQL Replace - place environment variables into your SQL statements</li>\n<li>SQL Validator - validate SQL against the ISO SQL-99 standard</li>\n</ul>\n</li>\n<li>Click Next to watch the installation progress, and then DONE when it finishes. Finally, you have the option to create shortcuts and allow all users to access the application. Select specific options according to your needs, and then finish with DONE.</li>\n</ol>\n<p> The program is now installed. To run it in Linux, run the squirrel-sql.sh bash file, in Windows, execute the squirrel-sql.bat file, and on a Mac, run the SquirreLSQL.app file.</p>\n<h5 id=\"Configuration-of-SQuirreL-SQL-Client\"><a href=\"#Configuration-of-SQuirreL-SQL-Client\" class=\"headerlink\" title=\"Configuration of SQuirreL SQL Client\"></a>Configuration of SQuirreL SQL Client</h5><p>At this point, you can run SquirreL but it won’t be able to connect to much. In order to access different DB platforms, you’ll need to download the appropriate JDBC drivers for the databases you want to use. Check with your database vendor for web locations where the driver can be found. Once you have the JDBC JAR files, just copy them to the /squirrel-sql-3.6/lib folder.  On Linux and Windows, the installation of the drivers is fairly straightforward. On Mac OS X, you’ll need to open up the finder folder where the SQuirreL application has been installed. If you chose the default location, hit Shift-Command-G on your keyboard to open the Go to Folder window, and paste this in: /Applications/SquirreLSQL/Contents/Resources/Java/lib. Hit Enter and after a second or two the Lib folder will appear.</p>\n<ol>\n<li>Once you have the JDBC JAR file installed, restart SQuirrel, and click the DRIVERS tab on the left side. The installed drivers should now appear with a check mark. If there is a red circle with an X, then the driver is not installed.</li>\n<li>Click on the Aliases tab to add a connection to database.</li>\n<li>Click the Plus icon to add a new alias. Enter a name in the appropriate box. Click the Driver drop down and choose the appropriate checked driver.</li>\n<li>Update the URL to the appropriate one for your database. For example for a locally installed MySQL database, enter: <strong>jdbc:mysql://localhost:3306</strong> or for a specific database: ** jdbc:mysql://localhost:3306/databasename**</li>\n<li>Enter a user name in the space for it, and LEAVE THE PASSWORD FIELD EMPTY. If you enter one in, and save it, it will be saved in clear text. (That’s a BAD security breach.) Click OK to exit, and a new entry with the name you specified will appear in the Aliases panel.</li>\n<li>Double click the entry you created to initiate a connection. A Connect To box will appear with the user name filled in. Enter your password, and click the Connect button.</li>\n<li>If all goes well, you’ll see a new window appear on the canvas with an object tree on the left. At the top, the catalog drop down will show the currently connected schema or database. You can only work in the database or schema that is selected here.</li>\n</ol>\n<p>Next time we’ll look at how to navigate around in SquirreL SQL Client and access multiple DB platforms at the same time!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel-164x300.jpg\" alt=\"squirrel\"></a>Its been my experience that if you work on ETL projects, you eventually accumulate client software for a number of database systems on your development PC. The reason is pretty straightforward - you need to be able to access the systems you are working with to determine data types, schema structures, and occasionally to check that a User account and Password you have been given actually works. One problem I’ve run into though is that not all operating systems are supported by different database vendors with their tools. While Windows has the largest installation base, Mac OS X, and Linux also are used for ETL development  but Microsoft’s SQL Server management tool will only work on Windows machines. Apple’s FileMaker software is similar, running on Mac OS X and Windows, but not Linux (since version 7). The examples go on and on. Also, because each tool is laid out differently, it can be difficult to find what you need quickly when you only work infrequently on a specific platform. Often times remembering where I need to go in a specific tool will take me longer than getting the actual information I was looking for. All of this leads to the point of this post - using a free open source product call SQuirreL SQL Client to access multiple database platforms via one application regardless of whether you are running Windows, Mac OS X or any of a large variety of Linux distributions.</p>","more":"<h5 id=\"Installation-of-SQuirreL-SQL-Client\"><a href=\"#Installation-of-SQuirreL-SQL-Client\" class=\"headerlink\" title=\"Installation of SQuirreL SQL Client\"></a><strong>Installation of SQuirreL SQL Client</strong></h5><p>[caption id=”attachment_2807” align=”aligncenter” width=”300”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrelscreen-300x164.png\" alt=\"squirrelscreen\"></a> SQuirreL SQL Client connected to a test server running Microsoft’s SQL Server 2012.[/caption]</p>\n<ol>\n<li>To get started, make sure you have Oracle’s JAVA client installed on your system. If you are running Linux, I’ve found that OpenJDK doesn’t work right with this, so be sure you have the correct software. On most operating systems, you can open a terminal prompt and enter the following to see if you have it installed: <strong>JAVA -version.</strong></li>\n<li>Next go to this website: <a href=\"http://www.squirrelsql.org/\">http://www.squirrelsql.org/</a> and click the “Download and Installation” link in the toolbar and then click the link for your operating system. A small JAR file will be downloaded. If you receive a message that these types of files can be harmful to your system, click the option that lets you down the file anyway.</li>\n<li>On Windows or Linux, open a terminal prompt and navigate to where you downloaded the installation file. Enter the command: **java -jar squirrel-sql-3.6-standard.jar<ENTER> ** to run the installation routine.</li>\n<li>On Mac OS X, open the folder where you downloaded the file in Finder, and double click it.</li>\n<li>An lzpack window will appear. Click NEXT on the welcome screen, and NEXT on the please read windows. You can decide where to install the application on the following screen (the default location is usually OK unless you have a compelling reason for installing it elsewhere). Note on Linux,  if you run the install as a normal user, the installation will default to a folder in the user’s HOME folder. Installed as root, it defaults to /usr/local.</li>\n<li>On the following screen, Choose the add-ins you would like to use such as any DB platforms you anticipate connecting to, the optional plugins for Smart Tools and SQL (located between PostgreSQL and Sybase in the list), and any translations plugins you may need. Some additional plugins you may want to select are:<ul>\n<li>Session Scripts - run SQL when opening an session</li>\n<li>SQL Parametrisation - put variables into your SQL statements</li>\n<li>SQL Replace - place environment variables into your SQL statements</li>\n<li>SQL Validator - validate SQL against the ISO SQL-99 standard</li>\n</ul>\n</li>\n<li>Click Next to watch the installation progress, and then DONE when it finishes. Finally, you have the option to create shortcuts and allow all users to access the application. Select specific options according to your needs, and then finish with DONE.</li>\n</ol>\n<p> The program is now installed. To run it in Linux, run the squirrel-sql.sh bash file, in Windows, execute the squirrel-sql.bat file, and on a Mac, run the SquirreLSQL.app file.</p>\n<h5 id=\"Configuration-of-SQuirreL-SQL-Client\"><a href=\"#Configuration-of-SQuirreL-SQL-Client\" class=\"headerlink\" title=\"Configuration of SQuirreL SQL Client\"></a>Configuration of SQuirreL SQL Client</h5><p>At this point, you can run SquirreL but it won’t be able to connect to much. In order to access different DB platforms, you’ll need to download the appropriate JDBC drivers for the databases you want to use. Check with your database vendor for web locations where the driver can be found. Once you have the JDBC JAR files, just copy them to the /squirrel-sql-3.6/lib folder.  On Linux and Windows, the installation of the drivers is fairly straightforward. On Mac OS X, you’ll need to open up the finder folder where the SQuirreL application has been installed. If you chose the default location, hit Shift-Command-G on your keyboard to open the Go to Folder window, and paste this in: /Applications/SquirreLSQL/Contents/Resources/Java/lib. Hit Enter and after a second or two the Lib folder will appear.</p>\n<ol>\n<li>Once you have the JDBC JAR file installed, restart SQuirrel, and click the DRIVERS tab on the left side. The installed drivers should now appear with a check mark. If there is a red circle with an X, then the driver is not installed.</li>\n<li>Click on the Aliases tab to add a connection to database.</li>\n<li>Click the Plus icon to add a new alias. Enter a name in the appropriate box. Click the Driver drop down and choose the appropriate checked driver.</li>\n<li>Update the URL to the appropriate one for your database. For example for a locally installed MySQL database, enter: <strong>jdbc:mysql://localhost:3306</strong> or for a specific database: ** jdbc:mysql://localhost:3306/databasename**</li>\n<li>Enter a user name in the space for it, and LEAVE THE PASSWORD FIELD EMPTY. If you enter one in, and save it, it will be saved in clear text. (That’s a BAD security breach.) Click OK to exit, and a new entry with the name you specified will appear in the Aliases panel.</li>\n<li>Double click the entry you created to initiate a connection. A Connect To box will appear with the user name filled in. Enter your password, and click the Connect button.</li>\n<li>If all goes well, you’ll see a new window appear on the canvas with an object tree on the left. At the top, the catalog drop down will show the currently connected schema or database. You can only work in the database or schema that is selected here.</li>\n</ol>\n<p>Next time we’ll look at how to navigate around in SquirreL SQL Client and access multiple DB platforms at the same time!</p>"},{"title":"SQuirreL SQL Client for accessing different databases - Part 2","id":"2825","comments":0,"date":"2015-06-29T06:38:31.000Z","_content":"\n[![squirrel2](http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg)](http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg)In my l[ast post on using SQuirrel SQL Client](http://edpflager.com/?p=2778) (aka Squirrel), I walked through downloading and installing it, and adding a connection to a locally hosted MySQL database. This time, I walk through using Squirrel to manipulate the MySQL database. I'm assuming you have the MySQL test database Sakila installed for this. If not, check out the [documentation](http://dev.mysql.com/doc/sakila/en/sakila-installation.html) at the MySQL website.\n\n##### OBJECTS DISPLAY\n\n1.  Open Squirrel, and from the Aliases tab on the left, double click the entry you setup in Part 1, for the MySQL database. The Connect to window will appear. In the URL field, make sure you have specified a database name. For my setup, the URL reads: **jdbc:mysql://localhost:3306/sakila [![mysql_login](http://edpflager.com/wp-content/uploads/2015/06/mysql_login-278x300.png)](http://edpflager.com/wp-content/uploads/2015/06/mysql_login.png)** \n2.  Enter your password in the appropriate spot and click Connect at the bottom.\n3.  On the main work area in the center of the screen, a new window will appear with A LOT going on. In the upper left portion will be a drop down list labeled Catalog. Currently the list should show \"sakila\" as the selected entry. (Even though all of the databases you have access to on the server will show up in the Object list, in order to switch to a different database, you'll need to select the new database from the Catalog drop down list.)![catalog](http://edpflager.com/wp-content/uploads/2015/06/catalog.png)\n4.  Below the Catalog list you'll see two tabs (on Mac OS X you-ll see buttons instead of tabs): **Objects** and **SQL.** If **Objects** is not selected, click on it to make it active. Below Objects on the left will be a search field, and then below that will be an object tree, showing the MySQL databases on the server you are connected to, and at the bottom, a USERS entry. To the right you'll see a set of tabs, a large number of them being reference ones:\n    *   FUNCTION names for numbers, strings and time/date and\n    *   KEYWORDS - reserved keywords for the particular database\n    *   TABLE and DATA TYPES -lists of the various kinds of tables and field types the particular database supports.\n5.  Depending on the RDBMS you are connected to you may see system maintenance tabs as well. \\[caption id=\"attachment\\_2832\" align=\"aligncenter\" width=\"500\"\\][![Objects](http://edpflager.com/wp-content/uploads/2015/06/Objects-1024x475.png)](http://edpflager.com/wp-content/uploads/2015/06/Objects.png) SQuirreL SQL Client connected to MySQL database. Objects tab active.\\[/caption\\]\n    \n    #### Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.\n    \n6.  Double click the entry for \"sakila\" in the tree, to toggle it open and display its sub-tree. The tabs displayed on the right will change and only a few will be shown. The Info tab will give you some very brief Name information about the database. Click on the MySQL Open tables and you will see a list of the tables in the \"sakila\" database and if the table is in use and/or locked. Click on the MySQL Table Status tab, and you'll get more information about each table and view in the database (MySQL Engine in use, how many rows in the table, etc).\n7.  Double click the entry labeled TABLE under \"sakila\" to see a list of tables within the database. The window on the right will clear, until you select a table, so click on the first one, labeled \"actor\". \\[caption id=\"attachment\\_2837\" align=\"aligncenter\" width=\"500\"\\][![actor_table](http://edpflager.com/wp-content/uploads/2015/06/actor_table-1024x478.png)](http://edpflager.com/wp-content/uploads/2015/06/actor_table.png) SQuirreL SQL Client connected to MySQL database with table selected\\[/caption\\]\n    *   The first tab, **Info** again provides some basic naming information about the table.\n    *   Click the second tab (**Content**) and you can see the first 100 records in the table.\n    *   **Row Count** shows you how many records are in the table.\n    *   **Columns** provides information on the fields in the table.\n    *   There are several tabs showing information about Primary Keys and Exported and Imported Keys (Foreign Key relationships).\n    *   Several other tabs are also present, covering various settings and configurations of the database. Depending on the RDBMS you are connected to, what you see may vary.\n8.  Below the TABLE entry in the Object list on the left, there is a VIEW entry. Double click that to see the VIEWS that are part of the database. In the \"sakila\" database there are seven of them, and clicking on each will update the window to the right with information about them. Of special interest is the last tab, labeled **Source.** Clicking on this will show the query that defines the view (very handy!)\n9.  Below the VIEW entry, you will see one labeled PROCEDURE. Double clicking on this will show you the six Stored Procedures in the \"sakila\" database. Clicking on the name of any of them will provide information to the right on them. As with the VIEW section, you can see the source code for the Stored Procedure.\n10.  Finally, below PROCEDURE, you will see an option labeled UDT for User Defined Types. If there are any UDTs defined in your database, they will display here. (Sakila has none).\n11.  That's the Objects side of the display!\n<!-- more -->\n##### SQL Display\n\nIn the first part of this article we walked through the OBJECTS side of Squirrel. The other tab (or button) available at the top of the MySQL window in the SQUirreL CLient is the **SQL** one.\n\n1.  Click on the **SQL** button or tab now. Compared to the **Objects** area, the work area here is spartan!\n2.  There are a row of icons across the top with tool tips. They are broken down into functional areas, such as dealing with the underlying database, file manipulation, query shortcuts, and result handling. Below that is a drop down list, that will be empty if this is the first time you've used SQuirreL SQL Client. This is the Query History drop down. Queries you enter into the builder window below it will be cached and when you exit SQuirreL they get written to a file for retrieval the the next time you start the client.![QueryWindow,png](http://edpflager.com/wp-content/uploads/2015/06/QueryWindowpng-1024x563.png)\n3.  To the right of the history drop down is a down arrow to copy the selected query to the query editor. Obviously if nothing has been entered, there is nothing to copy at this point! Next to that is the Open SQL History button, which will open a Window where you can search for a specific query if you have a lot of history. **You should note there is currently no option to delete your query history within the application. In order to remove history, delete the sql-history.xml file from the home/.squirrel-sql folder. It will be recreated the next time you close SQuirreL SQL Client.**\n4.  Positioned next (a bit awkwardly) is a checkbox. This applies to the Limit Rows drop down list. If you have **Limit Rows** selected, and the box checked, your query return at most the number of records specified in the field to  the right of the **Limit Rows** button. So as an example, if you are querying a table with 10,000 records, and have limit rows set to 100, it will only return 100 rows of data.\n5.  The **Limit Rows** drop down list has a second option: **Read On, Block Size.** With this option selected, the client will read a set of records in the number specified, and will pull in the next set of records when you scroll through the results. Lets try an example. Set the drop down to **Read On, Block Size**. and enter 30 in the text box. In the query window, enter this query: **SELECT \\* FROM actor;** and then click the button that looks like a runner to execute the query (Circled in red in the screen capture below).[![ReadOn](http://edpflager.com/wp-content/uploads/2015/06/ReadOn-1024x630.png)](http://edpflager.com/wp-content/uploads/2015/06/ReadOn.png)\n6.  The result set of 30 records will appear at the bottom of the window, and a message will show in the middle that the results are limited to 30 records. Scroll through the results past the 30th record, and the next 30 will appear. Neat!\n7.  Along with the results, you also have several other tabs that appear as well:\n    *   MetaData - shows information about each of the fields returned (data type, scale. precision, nullable, etc).\n    *   Info - displays the query along with the elapsed time to execute it.\n    *   Overview/Charts shows information on how the records are statistically distributed. Hover your mouse over each block and it will provide a tooltip with the information. You can change the number of groups the data is sliced into by changing the value in the Max Intervals drop down list. Click the Chart button to see several options for visualizing the distribution of the data, and click the Open Chart Window button once you have defined your visualization.[![overview](http://edpflager.com/wp-content/uploads/2015/06/overview-1024x329.png)](http://edpflager.com/wp-content/uploads/2015/06/overview.png)\n    *   Rotated table is the last option, and it shows a cross tab like view of your data with each column being one record. If you have a scroll wheel on your mouse, you can use that to scroll through the records, otherwise just use  the slider on the screen.\n\nThat's my brief tour of the SQuirreL SQL Client tool! If you need to connect to multiple database platforms, or the database platform doesn't provide a query tool for your operating system, be sure to give it a try. I will say that I have been able to connect to MySQL, SQL Server, and DB2 iSeries with this tool. If you have any questions on how to use it, please let me know, and I'll try to help.","source":"_posts/squirrel-sql-client-for-accessing-different-databases-part-2.md","raw":"---\ntitle: SQuirreL SQL Client for accessing different databases - Part 2\ntags:\n  - ETL\n  - How-to\n  - howto\n  - install\n  - Mac\n  - MySQL\n  - SysAdmin\n  - technical\nid: '2825'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2015-06-29 02:38:31\n---\n\n[![squirrel2](http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg)](http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg)In my l[ast post on using SQuirrel SQL Client](http://edpflager.com/?p=2778) (aka Squirrel), I walked through downloading and installing it, and adding a connection to a locally hosted MySQL database. This time, I walk through using Squirrel to manipulate the MySQL database. I'm assuming you have the MySQL test database Sakila installed for this. If not, check out the [documentation](http://dev.mysql.com/doc/sakila/en/sakila-installation.html) at the MySQL website.\n\n##### OBJECTS DISPLAY\n\n1.  Open Squirrel, and from the Aliases tab on the left, double click the entry you setup in Part 1, for the MySQL database. The Connect to window will appear. In the URL field, make sure you have specified a database name. For my setup, the URL reads: **jdbc:mysql://localhost:3306/sakila [![mysql_login](http://edpflager.com/wp-content/uploads/2015/06/mysql_login-278x300.png)](http://edpflager.com/wp-content/uploads/2015/06/mysql_login.png)** \n2.  Enter your password in the appropriate spot and click Connect at the bottom.\n3.  On the main work area in the center of the screen, a new window will appear with A LOT going on. In the upper left portion will be a drop down list labeled Catalog. Currently the list should show \"sakila\" as the selected entry. (Even though all of the databases you have access to on the server will show up in the Object list, in order to switch to a different database, you'll need to select the new database from the Catalog drop down list.)![catalog](http://edpflager.com/wp-content/uploads/2015/06/catalog.png)\n4.  Below the Catalog list you'll see two tabs (on Mac OS X you-ll see buttons instead of tabs): **Objects** and **SQL.** If **Objects** is not selected, click on it to make it active. Below Objects on the left will be a search field, and then below that will be an object tree, showing the MySQL databases on the server you are connected to, and at the bottom, a USERS entry. To the right you'll see a set of tabs, a large number of them being reference ones:\n    *   FUNCTION names for numbers, strings and time/date and\n    *   KEYWORDS - reserved keywords for the particular database\n    *   TABLE and DATA TYPES -lists of the various kinds of tables and field types the particular database supports.\n5.  Depending on the RDBMS you are connected to you may see system maintenance tabs as well. \\[caption id=\"attachment\\_2832\" align=\"aligncenter\" width=\"500\"\\][![Objects](http://edpflager.com/wp-content/uploads/2015/06/Objects-1024x475.png)](http://edpflager.com/wp-content/uploads/2015/06/Objects.png) SQuirreL SQL Client connected to MySQL database. Objects tab active.\\[/caption\\]\n    \n    #### Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.\n    \n6.  Double click the entry for \"sakila\" in the tree, to toggle it open and display its sub-tree. The tabs displayed on the right will change and only a few will be shown. The Info tab will give you some very brief Name information about the database. Click on the MySQL Open tables and you will see a list of the tables in the \"sakila\" database and if the table is in use and/or locked. Click on the MySQL Table Status tab, and you'll get more information about each table and view in the database (MySQL Engine in use, how many rows in the table, etc).\n7.  Double click the entry labeled TABLE under \"sakila\" to see a list of tables within the database. The window on the right will clear, until you select a table, so click on the first one, labeled \"actor\". \\[caption id=\"attachment\\_2837\" align=\"aligncenter\" width=\"500\"\\][![actor_table](http://edpflager.com/wp-content/uploads/2015/06/actor_table-1024x478.png)](http://edpflager.com/wp-content/uploads/2015/06/actor_table.png) SQuirreL SQL Client connected to MySQL database with table selected\\[/caption\\]\n    *   The first tab, **Info** again provides some basic naming information about the table.\n    *   Click the second tab (**Content**) and you can see the first 100 records in the table.\n    *   **Row Count** shows you how many records are in the table.\n    *   **Columns** provides information on the fields in the table.\n    *   There are several tabs showing information about Primary Keys and Exported and Imported Keys (Foreign Key relationships).\n    *   Several other tabs are also present, covering various settings and configurations of the database. Depending on the RDBMS you are connected to, what you see may vary.\n8.  Below the TABLE entry in the Object list on the left, there is a VIEW entry. Double click that to see the VIEWS that are part of the database. In the \"sakila\" database there are seven of them, and clicking on each will update the window to the right with information about them. Of special interest is the last tab, labeled **Source.** Clicking on this will show the query that defines the view (very handy!)\n9.  Below the VIEW entry, you will see one labeled PROCEDURE. Double clicking on this will show you the six Stored Procedures in the \"sakila\" database. Clicking on the name of any of them will provide information to the right on them. As with the VIEW section, you can see the source code for the Stored Procedure.\n10.  Finally, below PROCEDURE, you will see an option labeled UDT for User Defined Types. If there are any UDTs defined in your database, they will display here. (Sakila has none).\n11.  That's the Objects side of the display!\n<!-- more -->\n##### SQL Display\n\nIn the first part of this article we walked through the OBJECTS side of Squirrel. The other tab (or button) available at the top of the MySQL window in the SQUirreL CLient is the **SQL** one.\n\n1.  Click on the **SQL** button or tab now. Compared to the **Objects** area, the work area here is spartan!\n2.  There are a row of icons across the top with tool tips. They are broken down into functional areas, such as dealing with the underlying database, file manipulation, query shortcuts, and result handling. Below that is a drop down list, that will be empty if this is the first time you've used SQuirreL SQL Client. This is the Query History drop down. Queries you enter into the builder window below it will be cached and when you exit SQuirreL they get written to a file for retrieval the the next time you start the client.![QueryWindow,png](http://edpflager.com/wp-content/uploads/2015/06/QueryWindowpng-1024x563.png)\n3.  To the right of the history drop down is a down arrow to copy the selected query to the query editor. Obviously if nothing has been entered, there is nothing to copy at this point! Next to that is the Open SQL History button, which will open a Window where you can search for a specific query if you have a lot of history. **You should note there is currently no option to delete your query history within the application. In order to remove history, delete the sql-history.xml file from the home/.squirrel-sql folder. It will be recreated the next time you close SQuirreL SQL Client.**\n4.  Positioned next (a bit awkwardly) is a checkbox. This applies to the Limit Rows drop down list. If you have **Limit Rows** selected, and the box checked, your query return at most the number of records specified in the field to  the right of the **Limit Rows** button. So as an example, if you are querying a table with 10,000 records, and have limit rows set to 100, it will only return 100 rows of data.\n5.  The **Limit Rows** drop down list has a second option: **Read On, Block Size.** With this option selected, the client will read a set of records in the number specified, and will pull in the next set of records when you scroll through the results. Lets try an example. Set the drop down to **Read On, Block Size**. and enter 30 in the text box. In the query window, enter this query: **SELECT \\* FROM actor;** and then click the button that looks like a runner to execute the query (Circled in red in the screen capture below).[![ReadOn](http://edpflager.com/wp-content/uploads/2015/06/ReadOn-1024x630.png)](http://edpflager.com/wp-content/uploads/2015/06/ReadOn.png)\n6.  The result set of 30 records will appear at the bottom of the window, and a message will show in the middle that the results are limited to 30 records. Scroll through the results past the 30th record, and the next 30 will appear. Neat!\n7.  Along with the results, you also have several other tabs that appear as well:\n    *   MetaData - shows information about each of the fields returned (data type, scale. precision, nullable, etc).\n    *   Info - displays the query along with the elapsed time to execute it.\n    *   Overview/Charts shows information on how the records are statistically distributed. Hover your mouse over each block and it will provide a tooltip with the information. You can change the number of groups the data is sliced into by changing the value in the Max Intervals drop down list. Click the Chart button to see several options for visualizing the distribution of the data, and click the Open Chart Window button once you have defined your visualization.[![overview](http://edpflager.com/wp-content/uploads/2015/06/overview-1024x329.png)](http://edpflager.com/wp-content/uploads/2015/06/overview.png)\n    *   Rotated table is the last option, and it shows a cross tab like view of your data with each column being one record. If you have a scroll wheel on your mouse, you can use that to scroll through the records, otherwise just use  the slider on the screen.\n\nThat's my brief tour of the SQuirreL SQL Client tool! If you need to connect to multiple database platforms, or the database platform doesn't provide a query tool for your operating system, be sure to give it a try. I will say that I have been able to connect to MySQL, SQL Server, and DB2 iSeries with this tool. If you have any questions on how to use it, please let me know, and I'll try to help.","slug":"squirrel-sql-client-for-accessing-different-databases-part-2","published":1,"updated":"2020-08-23T20:54:34.958Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aaz00bqsdjx7qmj6dqu","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg\" alt=\"squirrel2\"></a>In my l<a href=\"http://edpflager.com/?p=2778\">ast post on using SQuirrel SQL Client</a> (aka Squirrel), I walked through downloading and installing it, and adding a connection to a locally hosted MySQL database. This time, I walk through using Squirrel to manipulate the MySQL database. I’m assuming you have the MySQL test database Sakila installed for this. If not, check out the <a href=\"http://dev.mysql.com/doc/sakila/en/sakila-installation.html\">documentation</a> at the MySQL website.</p>\n<h5 id=\"OBJECTS-DISPLAY\"><a href=\"#OBJECTS-DISPLAY\" class=\"headerlink\" title=\"OBJECTS DISPLAY\"></a>OBJECTS DISPLAY</h5><ol>\n<li><p>Open Squirrel, and from the Aliases tab on the left, double click the entry you setup in Part 1, for the MySQL database. The Connect to window will appear. In the URL field, make sure you have specified a database name. For my setup, the URL reads: <strong>jdbc:mysql://localhost:3306/sakila <a href=\"http://edpflager.com/wp-content/uploads/2015/06/mysql_login.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/mysql_login-278x300.png\" alt=\"mysql_login\"></a></strong> </p>\n</li>\n<li><p>Enter your password in the appropriate spot and click Connect at the bottom.</p>\n</li>\n<li><p>On the main work area in the center of the screen, a new window will appear with A LOT going on. In the upper left portion will be a drop down list labeled Catalog. Currently the list should show “sakila” as the selected entry. (Even though all of the databases you have access to on the server will show up in the Object list, in order to switch to a different database, you’ll need to select the new database from the Catalog drop down list.)<img src=\"http://edpflager.com/wp-content/uploads/2015/06/catalog.png\" alt=\"catalog\"></p>\n</li>\n<li><p>Below the Catalog list you’ll see two tabs (on Mac OS X you-ll see buttons instead of tabs): <strong>Objects</strong> and <strong>SQL.</strong> If <strong>Objects</strong> is not selected, click on it to make it active. Below Objects on the left will be a search field, and then below that will be an object tree, showing the MySQL databases on the server you are connected to, and at the bottom, a USERS entry. To the right you’ll see a set of tabs, a large number of them being reference ones:</p>\n<ul>\n<li>FUNCTION names for numbers, strings and time/date and</li>\n<li>KEYWORDS - reserved keywords for the particular database</li>\n<li>TABLE and DATA TYPES -lists of the various kinds of tables and field types the particular database supports.</li>\n</ul>\n</li>\n<li><p>Depending on the RDBMS you are connected to you may see system maintenance tabs as well. [caption id=”attachment_2832” align=”aligncenter” width=”500”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/Objects.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/Objects-1024x475.png\" alt=\"Objects\"></a> SQuirreL SQL Client connected to MySQL database. Objects tab active.[/caption]</p>\n<h4 id=\"Please-Note-None-of-the-information-displayed-in-the-Objects-area-is-editable-by-the-user-It-is-read-only\"><a href=\"#Please-Note-None-of-the-information-displayed-in-the-Objects-area-is-editable-by-the-user-It-is-read-only\" class=\"headerlink\" title=\"Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.\"></a>Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.</h4></li>\n<li><p>Double click the entry for “sakila” in the tree, to toggle it open and display its sub-tree. The tabs displayed on the right will change and only a few will be shown. The Info tab will give you some very brief Name information about the database. Click on the MySQL Open tables and you will see a list of the tables in the “sakila” database and if the table is in use and/or locked. Click on the MySQL Table Status tab, and you’ll get more information about each table and view in the database (MySQL Engine in use, how many rows in the table, etc).</p>\n</li>\n<li><p>Double click the entry labeled TABLE under “sakila” to see a list of tables within the database. The window on the right will clear, until you select a table, so click on the first one, labeled “actor”. [caption id=”attachment_2837” align=”aligncenter” width=”500”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/actor_table.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/actor_table-1024x478.png\" alt=\"actor_table\"></a> SQuirreL SQL Client connected to MySQL database with table selected[/caption]</p>\n<ul>\n<li>The first tab, <strong>Info</strong> again provides some basic naming information about the table.</li>\n<li>Click the second tab (<strong>Content</strong>) and you can see the first 100 records in the table.</li>\n<li><strong>Row Count</strong> shows you how many records are in the table.</li>\n<li><strong>Columns</strong> provides information on the fields in the table.</li>\n<li>There are several tabs showing information about Primary Keys and Exported and Imported Keys (Foreign Key relationships).</li>\n<li>Several other tabs are also present, covering various settings and configurations of the database. Depending on the RDBMS you are connected to, what you see may vary.</li>\n</ul>\n</li>\n<li><p>Below the TABLE entry in the Object list on the left, there is a VIEW entry. Double click that to see the VIEWS that are part of the database. In the “sakila” database there are seven of them, and clicking on each will update the window to the right with information about them. Of special interest is the last tab, labeled <strong>Source.</strong> Clicking on this will show the query that defines the view (very handy!)</p>\n</li>\n<li><p>Below the VIEW entry, you will see one labeled PROCEDURE. Double clicking on this will show you the six Stored Procedures in the “sakila” database. Clicking on the name of any of them will provide information to the right on them. As with the VIEW section, you can see the source code for the Stored Procedure.</p>\n</li>\n<li><p>Finally, below PROCEDURE, you will see an option labeled UDT for User Defined Types. If there are any UDTs defined in your database, they will display here. (Sakila has none).</p>\n</li>\n<li><p>That’s the Objects side of the display!</p>\n<a id=\"more\"></a>\n<h5 id=\"SQL-Display\"><a href=\"#SQL-Display\" class=\"headerlink\" title=\"SQL Display\"></a>SQL Display</h5></li>\n</ol>\n<p>In the first part of this article we walked through the OBJECTS side of Squirrel. The other tab (or button) available at the top of the MySQL window in the SQUirreL CLient is the <strong>SQL</strong> one.</p>\n<ol>\n<li>Click on the <strong>SQL</strong> button or tab now. Compared to the <strong>Objects</strong> area, the work area here is spartan!</li>\n<li>There are a row of icons across the top with tool tips. They are broken down into functional areas, such as dealing with the underlying database, file manipulation, query shortcuts, and result handling. Below that is a drop down list, that will be empty if this is the first time you’ve used SQuirreL SQL Client. This is the Query History drop down. Queries you enter into the builder window below it will be cached and when you exit SQuirreL they get written to a file for retrieval the the next time you start the client.<img src=\"http://edpflager.com/wp-content/uploads/2015/06/QueryWindowpng-1024x563.png\" alt=\"QueryWindow,png\"></li>\n<li>To the right of the history drop down is a down arrow to copy the selected query to the query editor. Obviously if nothing has been entered, there is nothing to copy at this point! Next to that is the Open SQL History button, which will open a Window where you can search for a specific query if you have a lot of history. <strong>You should note there is currently no option to delete your query history within the application. In order to remove history, delete the sql-history.xml file from the home/.squirrel-sql folder. It will be recreated the next time you close SQuirreL SQL Client.</strong></li>\n<li>Positioned next (a bit awkwardly) is a checkbox. This applies to the Limit Rows drop down list. If you have <strong>Limit Rows</strong> selected, and the box checked, your query return at most the number of records specified in the field to  the right of the <strong>Limit Rows</strong> button. So as an example, if you are querying a table with 10,000 records, and have limit rows set to 100, it will only return 100 rows of data.</li>\n<li>The <strong>Limit Rows</strong> drop down list has a second option: <strong>Read On, Block Size.</strong> With this option selected, the client will read a set of records in the number specified, and will pull in the next set of records when you scroll through the results. Lets try an example. Set the drop down to <strong>Read On, Block Size</strong>. and enter 30 in the text box. In the query window, enter this query: <strong>SELECT * FROM actor;</strong> and then click the button that looks like a runner to execute the query (Circled in red in the screen capture below).<a href=\"http://edpflager.com/wp-content/uploads/2015/06/ReadOn.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/ReadOn-1024x630.png\" alt=\"ReadOn\"></a></li>\n<li>The result set of 30 records will appear at the bottom of the window, and a message will show in the middle that the results are limited to 30 records. Scroll through the results past the 30th record, and the next 30 will appear. Neat!</li>\n<li>Along with the results, you also have several other tabs that appear as well:<ul>\n<li>MetaData - shows information about each of the fields returned (data type, scale. precision, nullable, etc).</li>\n<li>Info - displays the query along with the elapsed time to execute it.</li>\n<li>Overview/Charts shows information on how the records are statistically distributed. Hover your mouse over each block and it will provide a tooltip with the information. You can change the number of groups the data is sliced into by changing the value in the Max Intervals drop down list. Click the Chart button to see several options for visualizing the distribution of the data, and click the Open Chart Window button once you have defined your visualization.<a href=\"http://edpflager.com/wp-content/uploads/2015/06/overview.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/overview-1024x329.png\" alt=\"overview\"></a></li>\n<li>Rotated table is the last option, and it shows a cross tab like view of your data with each column being one record. If you have a scroll wheel on your mouse, you can use that to scroll through the records, otherwise just use  the slider on the screen.</li>\n</ul>\n</li>\n</ol>\n<p>That’s my brief tour of the SQuirreL SQL Client tool! If you need to connect to multiple database platforms, or the database platform doesn’t provide a query tool for your operating system, be sure to give it a try. I will say that I have been able to connect to MySQL, SQL Server, and DB2 iSeries with this tool. If you have any questions on how to use it, please let me know, and I’ll try to help.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/squirrel2.jpg\" alt=\"squirrel2\"></a>In my l<a href=\"http://edpflager.com/?p=2778\">ast post on using SQuirrel SQL Client</a> (aka Squirrel), I walked through downloading and installing it, and adding a connection to a locally hosted MySQL database. This time, I walk through using Squirrel to manipulate the MySQL database. I’m assuming you have the MySQL test database Sakila installed for this. If not, check out the <a href=\"http://dev.mysql.com/doc/sakila/en/sakila-installation.html\">documentation</a> at the MySQL website.</p>\n<h5 id=\"OBJECTS-DISPLAY\"><a href=\"#OBJECTS-DISPLAY\" class=\"headerlink\" title=\"OBJECTS DISPLAY\"></a>OBJECTS DISPLAY</h5><ol>\n<li><p>Open Squirrel, and from the Aliases tab on the left, double click the entry you setup in Part 1, for the MySQL database. The Connect to window will appear. In the URL field, make sure you have specified a database name. For my setup, the URL reads: <strong>jdbc:mysql://localhost:3306/sakila <a href=\"http://edpflager.com/wp-content/uploads/2015/06/mysql_login.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/mysql_login-278x300.png\" alt=\"mysql_login\"></a></strong> </p>\n</li>\n<li><p>Enter your password in the appropriate spot and click Connect at the bottom.</p>\n</li>\n<li><p>On the main work area in the center of the screen, a new window will appear with A LOT going on. In the upper left portion will be a drop down list labeled Catalog. Currently the list should show “sakila” as the selected entry. (Even though all of the databases you have access to on the server will show up in the Object list, in order to switch to a different database, you’ll need to select the new database from the Catalog drop down list.)<img src=\"http://edpflager.com/wp-content/uploads/2015/06/catalog.png\" alt=\"catalog\"></p>\n</li>\n<li><p>Below the Catalog list you’ll see two tabs (on Mac OS X you-ll see buttons instead of tabs): <strong>Objects</strong> and <strong>SQL.</strong> If <strong>Objects</strong> is not selected, click on it to make it active. Below Objects on the left will be a search field, and then below that will be an object tree, showing the MySQL databases on the server you are connected to, and at the bottom, a USERS entry. To the right you’ll see a set of tabs, a large number of them being reference ones:</p>\n<ul>\n<li>FUNCTION names for numbers, strings and time/date and</li>\n<li>KEYWORDS - reserved keywords for the particular database</li>\n<li>TABLE and DATA TYPES -lists of the various kinds of tables and field types the particular database supports.</li>\n</ul>\n</li>\n<li><p>Depending on the RDBMS you are connected to you may see system maintenance tabs as well. [caption id=”attachment_2832” align=”aligncenter” width=”500”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/Objects.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/Objects-1024x475.png\" alt=\"Objects\"></a> SQuirreL SQL Client connected to MySQL database. Objects tab active.[/caption]</p>\n<h4 id=\"Please-Note-None-of-the-information-displayed-in-the-Objects-area-is-editable-by-the-user-It-is-read-only\"><a href=\"#Please-Note-None-of-the-information-displayed-in-the-Objects-area-is-editable-by-the-user-It-is-read-only\" class=\"headerlink\" title=\"Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.\"></a>Please Note: None of the information displayed in the Objects area is editable by the user. It is read only.</h4></li>\n<li><p>Double click the entry for “sakila” in the tree, to toggle it open and display its sub-tree. The tabs displayed on the right will change and only a few will be shown. The Info tab will give you some very brief Name information about the database. Click on the MySQL Open tables and you will see a list of the tables in the “sakila” database and if the table is in use and/or locked. Click on the MySQL Table Status tab, and you’ll get more information about each table and view in the database (MySQL Engine in use, how many rows in the table, etc).</p>\n</li>\n<li><p>Double click the entry labeled TABLE under “sakila” to see a list of tables within the database. The window on the right will clear, until you select a table, so click on the first one, labeled “actor”. [caption id=”attachment_2837” align=”aligncenter” width=”500”]<a href=\"http://edpflager.com/wp-content/uploads/2015/06/actor_table.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/actor_table-1024x478.png\" alt=\"actor_table\"></a> SQuirreL SQL Client connected to MySQL database with table selected[/caption]</p>\n<ul>\n<li>The first tab, <strong>Info</strong> again provides some basic naming information about the table.</li>\n<li>Click the second tab (<strong>Content</strong>) and you can see the first 100 records in the table.</li>\n<li><strong>Row Count</strong> shows you how many records are in the table.</li>\n<li><strong>Columns</strong> provides information on the fields in the table.</li>\n<li>There are several tabs showing information about Primary Keys and Exported and Imported Keys (Foreign Key relationships).</li>\n<li>Several other tabs are also present, covering various settings and configurations of the database. Depending on the RDBMS you are connected to, what you see may vary.</li>\n</ul>\n</li>\n<li><p>Below the TABLE entry in the Object list on the left, there is a VIEW entry. Double click that to see the VIEWS that are part of the database. In the “sakila” database there are seven of them, and clicking on each will update the window to the right with information about them. Of special interest is the last tab, labeled <strong>Source.</strong> Clicking on this will show the query that defines the view (very handy!)</p>\n</li>\n<li><p>Below the VIEW entry, you will see one labeled PROCEDURE. Double clicking on this will show you the six Stored Procedures in the “sakila” database. Clicking on the name of any of them will provide information to the right on them. As with the VIEW section, you can see the source code for the Stored Procedure.</p>\n</li>\n<li><p>Finally, below PROCEDURE, you will see an option labeled UDT for User Defined Types. If there are any UDTs defined in your database, they will display here. (Sakila has none).</p>\n</li>\n<li><p>That’s the Objects side of the display!</p>","more":"<h5 id=\"SQL-Display\"><a href=\"#SQL-Display\" class=\"headerlink\" title=\"SQL Display\"></a>SQL Display</h5></li>\n</ol>\n<p>In the first part of this article we walked through the OBJECTS side of Squirrel. The other tab (or button) available at the top of the MySQL window in the SQUirreL CLient is the <strong>SQL</strong> one.</p>\n<ol>\n<li>Click on the <strong>SQL</strong> button or tab now. Compared to the <strong>Objects</strong> area, the work area here is spartan!</li>\n<li>There are a row of icons across the top with tool tips. They are broken down into functional areas, such as dealing with the underlying database, file manipulation, query shortcuts, and result handling. Below that is a drop down list, that will be empty if this is the first time you’ve used SQuirreL SQL Client. This is the Query History drop down. Queries you enter into the builder window below it will be cached and when you exit SQuirreL they get written to a file for retrieval the the next time you start the client.<img src=\"http://edpflager.com/wp-content/uploads/2015/06/QueryWindowpng-1024x563.png\" alt=\"QueryWindow,png\"></li>\n<li>To the right of the history drop down is a down arrow to copy the selected query to the query editor. Obviously if nothing has been entered, there is nothing to copy at this point! Next to that is the Open SQL History button, which will open a Window where you can search for a specific query if you have a lot of history. <strong>You should note there is currently no option to delete your query history within the application. In order to remove history, delete the sql-history.xml file from the home/.squirrel-sql folder. It will be recreated the next time you close SQuirreL SQL Client.</strong></li>\n<li>Positioned next (a bit awkwardly) is a checkbox. This applies to the Limit Rows drop down list. If you have <strong>Limit Rows</strong> selected, and the box checked, your query return at most the number of records specified in the field to  the right of the <strong>Limit Rows</strong> button. So as an example, if you are querying a table with 10,000 records, and have limit rows set to 100, it will only return 100 rows of data.</li>\n<li>The <strong>Limit Rows</strong> drop down list has a second option: <strong>Read On, Block Size.</strong> With this option selected, the client will read a set of records in the number specified, and will pull in the next set of records when you scroll through the results. Lets try an example. Set the drop down to <strong>Read On, Block Size</strong>. and enter 30 in the text box. In the query window, enter this query: <strong>SELECT * FROM actor;</strong> and then click the button that looks like a runner to execute the query (Circled in red in the screen capture below).<a href=\"http://edpflager.com/wp-content/uploads/2015/06/ReadOn.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/ReadOn-1024x630.png\" alt=\"ReadOn\"></a></li>\n<li>The result set of 30 records will appear at the bottom of the window, and a message will show in the middle that the results are limited to 30 records. Scroll through the results past the 30th record, and the next 30 will appear. Neat!</li>\n<li>Along with the results, you also have several other tabs that appear as well:<ul>\n<li>MetaData - shows information about each of the fields returned (data type, scale. precision, nullable, etc).</li>\n<li>Info - displays the query along with the elapsed time to execute it.</li>\n<li>Overview/Charts shows information on how the records are statistically distributed. Hover your mouse over each block and it will provide a tooltip with the information. You can change the number of groups the data is sliced into by changing the value in the Max Intervals drop down list. Click the Chart button to see several options for visualizing the distribution of the data, and click the Open Chart Window button once you have defined your visualization.<a href=\"http://edpflager.com/wp-content/uploads/2015/06/overview.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/06/overview-1024x329.png\" alt=\"overview\"></a></li>\n<li>Rotated table is the last option, and it shows a cross tab like view of your data with each column being one record. If you have a scroll wheel on your mouse, you can use that to scroll through the records, otherwise just use  the slider on the screen.</li>\n</ul>\n</li>\n</ol>\n<p>That’s my brief tour of the SQuirreL SQL Client tool! If you need to connect to multiple database platforms, or the database platform doesn’t provide a query tool for your operating system, be sure to give it a try. I will say that I have been able to connect to MySQL, SQL Server, and DB2 iSeries with this tool. If you have any questions on how to use it, please let me know, and I’ll try to help.</p>"},{"title":"UPDATE: Technical Difficulties - Resolved","id":"4711","comments":0,"date":"2018-04-07T14:27:15.000Z","_content":"\nUpdate: I never promised to be quick about restoring the contents, but here we are 3 months and change later, back up and running. :)  I have removed some of the older posts, mostly older photographs, and reviews of things I'm no longer interested in. The focus here now will probably be a bit narrower than its been in the past, and shifting a bit. Hopefully it remains interesting for some. I'm experiencing some technical difficulties with this website. If there is older content you are interested in seeing, please check the Internet Wayback Machine at [https://web.archive.org/web/\\*/edpflager.com.](https://web.archive.org/web/*/edpflager.com) Since this is a non-commercial site, I'll attempt to get my older content restored as soon as possible.","source":"_posts/technical-difficulties-please-stand-by.md","raw":"---\ntitle: 'UPDATE: Technical Difficulties - Resolved'\ntags: []\nid: '4711'\ncategories:\n  - - Blog\ncomments: false\ndate: 2018-04-07 10:27:15\n---\n\nUpdate: I never promised to be quick about restoring the contents, but here we are 3 months and change later, back up and running. :)  I have removed some of the older posts, mostly older photographs, and reviews of things I'm no longer interested in. The focus here now will probably be a bit narrower than its been in the past, and shifting a bit. Hopefully it remains interesting for some. I'm experiencing some technical difficulties with this website. If there is older content you are interested in seeing, please check the Internet Wayback Machine at [https://web.archive.org/web/\\*/edpflager.com.](https://web.archive.org/web/*/edpflager.com) Since this is a non-commercial site, I'll attempt to get my older content restored as soon as possible.","slug":"technical-difficulties-please-stand-by","published":1,"updated":"2020-08-23T20:54:35.186Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ab500busdjx4f7ufpzj","content":"<p>Update: I never promised to be quick about restoring the contents, but here we are 3 months and change later, back up and running. :)  I have removed some of the older posts, mostly older photographs, and reviews of things I’m no longer interested in. The focus here now will probably be a bit narrower than its been in the past, and shifting a bit. Hopefully it remains interesting for some. I’m experiencing some technical difficulties with this website. If there is older content you are interested in seeing, please check the Internet Wayback Machine at <a href=\"https://web.archive.org/web/*/edpflager.com\">https://web.archive.org/web/*/edpflager.com.</a> Since this is a non-commercial site, I’ll attempt to get my older content restored as soon as possible.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Update: I never promised to be quick about restoring the contents, but here we are 3 months and change later, back up and running. :)  I have removed some of the older posts, mostly older photographs, and reviews of things I’m no longer interested in. The focus here now will probably be a bit narrower than its been in the past, and shifting a bit. Hopefully it remains interesting for some. I’m experiencing some technical difficulties with this website. If there is older content you are interested in seeing, please check the Internet Wayback Machine at <a href=\"https://web.archive.org/web/*/edpflager.com\">https://web.archive.org/web/*/edpflager.com.</a> Since this is a non-commercial site, I’ll attempt to get my older content restored as soon as possible.</p>\n"},{"title":"The Vagaries of Software Updates","id":"2540","comments":0,"date":"2014-11-04T22:03:36.000Z","_content":"\n![crash](http://edpflager.com/wp-content/uploads/2014/11/crash-224x300.jpg)A short explanation for why the site has been quiet for a couple of weeks: In my last post I referenced updating to OS X Yosemite. While the operating system has been performing fairly well, one installed software package has not. For the most part, I use virtual machines to develop and test different things like Hadoop, SQL Server, Pentaho and various other items. This allows me to try out multiple packages and operating systems without the expense of a lot of physical boxes. For the host software, I have been using Fusion, the workstation software from VMWare that allows you to run multiple virtual systems on a Mac. The last time I upgraded my Mac's OS (less than a year ago to Mavericks), I had to fork over $45 to upgrade Fusion to overcome some issues with the new OS.\n<!-- more -->\nThis time, the new Mac OS X made all of my VMs virtually unusable and once again, VMWare wants users to shell out some coin to upgrade rather than providing a patch to overcome the issue. Rather than continue on this roller coaster, I'm attempting to recreate my VMs in Virtual Box, the free open source platform for VMs. I moved one of my VMs over to it, however, and it crashed, so it looks like I am going to have to prune my VMs down and reinstall  them by hand, a timely process indeed. Thanks for coming to visit, and as soon as I can rectify this situation, I'll have some new posts up. Stuff that's in the works includes: Pentaho Lookups and Fuzzy Lookups, possibly an intro to the HBase shell, some R and R studio articles, and a look at Tableau (which I am learning for my new job).","source":"_posts/the-vagaries-of-software-updates.md","raw":"---\ntitle: The Vagaries of Software Updates\ntags:\n  - Mac\n  - SysAdmin\n  - technical\nid: '2540'\ncategories:\n  - - Blog\ncomments: false\ndate: 2014-11-04 17:03:36\n---\n\n![crash](http://edpflager.com/wp-content/uploads/2014/11/crash-224x300.jpg)A short explanation for why the site has been quiet for a couple of weeks: In my last post I referenced updating to OS X Yosemite. While the operating system has been performing fairly well, one installed software package has not. For the most part, I use virtual machines to develop and test different things like Hadoop, SQL Server, Pentaho and various other items. This allows me to try out multiple packages and operating systems without the expense of a lot of physical boxes. For the host software, I have been using Fusion, the workstation software from VMWare that allows you to run multiple virtual systems on a Mac. The last time I upgraded my Mac's OS (less than a year ago to Mavericks), I had to fork over $45 to upgrade Fusion to overcome some issues with the new OS.\n<!-- more -->\nThis time, the new Mac OS X made all of my VMs virtually unusable and once again, VMWare wants users to shell out some coin to upgrade rather than providing a patch to overcome the issue. Rather than continue on this roller coaster, I'm attempting to recreate my VMs in Virtual Box, the free open source platform for VMs. I moved one of my VMs over to it, however, and it crashed, so it looks like I am going to have to prune my VMs down and reinstall  them by hand, a timely process indeed. Thanks for coming to visit, and as soon as I can rectify this situation, I'll have some new posts up. Stuff that's in the works includes: Pentaho Lookups and Fuzzy Lookups, possibly an intro to the HBase shell, some R and R studio articles, and a look at Tableau (which I am learning for my new job).","slug":"the-vagaries-of-software-updates","published":1,"updated":"2020-08-23T20:54:34.910Z","layout":"post","photos":[],"link":"","_id":"ckeaq9abj00bxsdjx0yjzajbp","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/11/crash-224x300.jpg\" alt=\"crash\">A short explanation for why the site has been quiet for a couple of weeks: In my last post I referenced updating to OS X Yosemite. While the operating system has been performing fairly well, one installed software package has not. For the most part, I use virtual machines to develop and test different things like Hadoop, SQL Server, Pentaho and various other items. This allows me to try out multiple packages and operating systems without the expense of a lot of physical boxes. For the host software, I have been using Fusion, the workstation software from VMWare that allows you to run multiple virtual systems on a Mac. The last time I upgraded my Mac’s OS (less than a year ago to Mavericks), I had to fork over $45 to upgrade Fusion to overcome some issues with the new OS.</p>\n<a id=\"more\"></a>\n<p>This time, the new Mac OS X made all of my VMs virtually unusable and once again, VMWare wants users to shell out some coin to upgrade rather than providing a patch to overcome the issue. Rather than continue on this roller coaster, I’m attempting to recreate my VMs in Virtual Box, the free open source platform for VMs. I moved one of my VMs over to it, however, and it crashed, so it looks like I am going to have to prune my VMs down and reinstall  them by hand, a timely process indeed. Thanks for coming to visit, and as soon as I can rectify this situation, I’ll have some new posts up. Stuff that’s in the works includes: Pentaho Lookups and Fuzzy Lookups, possibly an intro to the HBase shell, some R and R studio articles, and a look at Tableau (which I am learning for my new job).</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2014/11/crash-224x300.jpg\" alt=\"crash\">A short explanation for why the site has been quiet for a couple of weeks: In my last post I referenced updating to OS X Yosemite. While the operating system has been performing fairly well, one installed software package has not. For the most part, I use virtual machines to develop and test different things like Hadoop, SQL Server, Pentaho and various other items. This allows me to try out multiple packages and operating systems without the expense of a lot of physical boxes. For the host software, I have been using Fusion, the workstation software from VMWare that allows you to run multiple virtual systems on a Mac. The last time I upgraded my Mac’s OS (less than a year ago to Mavericks), I had to fork over $45 to upgrade Fusion to overcome some issues with the new OS.</p>","more":"<p>This time, the new Mac OS X made all of my VMs virtually unusable and once again, VMWare wants users to shell out some coin to upgrade rather than providing a patch to overcome the issue. Rather than continue on this roller coaster, I’m attempting to recreate my VMs in Virtual Box, the free open source platform for VMs. I moved one of my VMs over to it, however, and it crashed, so it looks like I am going to have to prune my VMs down and reinstall  them by hand, a timely process indeed. Thanks for coming to visit, and as soon as I can rectify this situation, I’ll have some new posts up. Stuff that’s in the works includes: Pentaho Lookups and Fuzzy Lookups, possibly an intro to the HBase shell, some R and R studio articles, and a look at Tableau (which I am learning for my new job).</p>"},{"title":"Tidyverse installation on Linux Mint","id":"4342","comments":0,"date":"2019-01-24T20:28:44.000Z","_content":"\nIf you've been using R for even a sh[![](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-139x150.png)](http://edpflager.com/wp-content/uploads/2019/01/tidyverse.png)ort time, you more than likely have heard of the Tidyverse from RStudio and originally developed by Hadley Wickham.  As explained on the [tidyverse website](https://www.tidyverse.org/) this set of packages works together with a common data representation methodology and API design.  They provide a host of functionality to clean and tidy your data to make it easier to analyze. Similar data representation between packages lets you define an entity for one operation, and use that entity through an entire workflow rather than creating intermittent entities along the way, resulting in more efficient code, and requiring fewer  system resources. The tidyverse methodology mimics good database design, with one row of data consisting of a single record or observation with variables assigned to columns in that row, and each value being assigned to its own cell. Having worked with databases for over a decade, the structure of tidy data makes sense. See Hadley Wickham's book [for more information](https://r4ds.had.co.nz/tidy-data.html). Currently there are 26 separate packages in the tidyverse, and even more that inter operate with them, so they can be a big part of your R workflow. The installation of the tidyverse is fairly easy in most situations, using the standard method  in R Studio or at the R command line:\n\ninstall.packages(\"tidyverse\")\n\nBut when setting up a new Linux Mint 19.1 system, that didn't work for me. The output included a long list of errors, and some helpful text to install this or that Debian package, buried in the text that was scrolling by. To make this process easier for Mint users, here are instructions on operating system requirements to install before attempting to install  the Tidyverse.\n<!-- more -->\nFirst make sure your apt-cache is up to date, then install the various packages I have strung together below.\n\n sudo apt-get update\n\n sudo apt-get install r-base-dev xml2 libxml2-dev libssl-dev libcurl4-openssl-dev unixodbc-dev \n\nOnce you have installed the Mint additions, start R-Studio and then run the standard command for installing R packages:\n\n    install.packages(\"tidyverse\")\n\nIn a few minutes it should indicate success. Try it out with this code and you should see results like the image below:\n\n    library(tidyverse)\n\n    tidyverse\\_packages\n\n[![](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png)](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png)","source":"_posts/tidyverse-installation-on-linux-mint.md","raw":"---\ntitle: Tidyverse installation on Linux Mint\ntags:\n  - howto\n  - Mint\n  - technical\n  - tidyverse\nid: '4342'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - R\ncomments: false\ndate: 2019-01-24 15:28:44\n---\n\nIf you've been using R for even a sh[![](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-139x150.png)](http://edpflager.com/wp-content/uploads/2019/01/tidyverse.png)ort time, you more than likely have heard of the Tidyverse from RStudio and originally developed by Hadley Wickham.  As explained on the [tidyverse website](https://www.tidyverse.org/) this set of packages works together with a common data representation methodology and API design.  They provide a host of functionality to clean and tidy your data to make it easier to analyze. Similar data representation between packages lets you define an entity for one operation, and use that entity through an entire workflow rather than creating intermittent entities along the way, resulting in more efficient code, and requiring fewer  system resources. The tidyverse methodology mimics good database design, with one row of data consisting of a single record or observation with variables assigned to columns in that row, and each value being assigned to its own cell. Having worked with databases for over a decade, the structure of tidy data makes sense. See Hadley Wickham's book [for more information](https://r4ds.had.co.nz/tidy-data.html). Currently there are 26 separate packages in the tidyverse, and even more that inter operate with them, so they can be a big part of your R workflow. The installation of the tidyverse is fairly easy in most situations, using the standard method  in R Studio or at the R command line:\n\ninstall.packages(\"tidyverse\")\n\nBut when setting up a new Linux Mint 19.1 system, that didn't work for me. The output included a long list of errors, and some helpful text to install this or that Debian package, buried in the text that was scrolling by. To make this process easier for Mint users, here are instructions on operating system requirements to install before attempting to install  the Tidyverse.\n<!-- more -->\nFirst make sure your apt-cache is up to date, then install the various packages I have strung together below.\n\n sudo apt-get update\n\n sudo apt-get install r-base-dev xml2 libxml2-dev libssl-dev libcurl4-openssl-dev unixodbc-dev \n\nOnce you have installed the Mint additions, start R-Studio and then run the standard command for installing R packages:\n\n    install.packages(\"tidyverse\")\n\nIn a few minutes it should indicate success. Try it out with this code and you should see results like the image below:\n\n    library(tidyverse)\n\n    tidyverse\\_packages\n\n[![](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png)](http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png)","slug":"tidyverse-installation-on-linux-mint","published":1,"updated":"2020-08-23T20:54:35.218Z","layout":"post","photos":[],"link":"","_id":"ckeaq9abo00c1sdjx1amk9lpj","content":"<p>If you’ve been using R for even a sh<a href=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-139x150.png\"></a>ort time, you more than likely have heard of the Tidyverse from RStudio and originally developed by Hadley Wickham.  As explained on the <a href=\"https://www.tidyverse.org/\">tidyverse website</a> this set of packages works together with a common data representation methodology and API design.  They provide a host of functionality to clean and tidy your data to make it easier to analyze. Similar data representation between packages lets you define an entity for one operation, and use that entity through an entire workflow rather than creating intermittent entities along the way, resulting in more efficient code, and requiring fewer  system resources. The tidyverse methodology mimics good database design, with one row of data consisting of a single record or observation with variables assigned to columns in that row, and each value being assigned to its own cell. Having worked with databases for over a decade, the structure of tidy data makes sense. See Hadley Wickham’s book <a href=\"https://r4ds.had.co.nz/tidy-data.html\">for more information</a>. Currently there are 26 separate packages in the tidyverse, and even more that inter operate with them, so they can be a big part of your R workflow. The installation of the tidyverse is fairly easy in most situations, using the standard method  in R Studio or at the R command line:</p>\n<p>install.packages(“tidyverse”)</p>\n<p>But when setting up a new Linux Mint 19.1 system, that didn’t work for me. The output included a long list of errors, and some helpful text to install this or that Debian package, buried in the text that was scrolling by. To make this process easier for Mint users, here are instructions on operating system requirements to install before attempting to install  the Tidyverse.</p>\n<a id=\"more\"></a>\n<p>First make sure your apt-cache is up to date, then install the various packages I have strung together below.</p>\n<p> sudo apt-get update</p>\n<p> sudo apt-get install r-base-dev xml2 libxml2-dev libssl-dev libcurl4-openssl-dev unixodbc-dev </p>\n<p>Once you have installed the Mint additions, start R-Studio and then run the standard command for installing R packages:</p>\n<pre><code>install.packages(&quot;tidyverse&quot;)</code></pre>\n<p>In a few minutes it should indicate success. Try it out with this code and you should see results like the image below:</p>\n<p>    library(tidyverse)</p>\n<p>    tidyverse_packages</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p>If you’ve been using R for even a sh<a href=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-139x150.png\"></a>ort time, you more than likely have heard of the Tidyverse from RStudio and originally developed by Hadley Wickham.  As explained on the <a href=\"https://www.tidyverse.org/\">tidyverse website</a> this set of packages works together with a common data representation methodology and API design.  They provide a host of functionality to clean and tidy your data to make it easier to analyze. Similar data representation between packages lets you define an entity for one operation, and use that entity through an entire workflow rather than creating intermittent entities along the way, resulting in more efficient code, and requiring fewer  system resources. The tidyverse methodology mimics good database design, with one row of data consisting of a single record or observation with variables assigned to columns in that row, and each value being assigned to its own cell. Having worked with databases for over a decade, the structure of tidy data makes sense. See Hadley Wickham’s book <a href=\"https://r4ds.had.co.nz/tidy-data.html\">for more information</a>. Currently there are 26 separate packages in the tidyverse, and even more that inter operate with them, so they can be a big part of your R workflow. The installation of the tidyverse is fairly easy in most situations, using the standard method  in R Studio or at the R command line:</p>\n<p>install.packages(“tidyverse”)</p>\n<p>But when setting up a new Linux Mint 19.1 system, that didn’t work for me. The output included a long list of errors, and some helpful text to install this or that Debian package, buried in the text that was scrolling by. To make this process easier for Mint users, here are instructions on operating system requirements to install before attempting to install  the Tidyverse.</p>","more":"<p>First make sure your apt-cache is up to date, then install the various packages I have strung together below.</p>\n<p> sudo apt-get update</p>\n<p> sudo apt-get install r-base-dev xml2 libxml2-dev libssl-dev libcurl4-openssl-dev unixodbc-dev </p>\n<p>Once you have installed the Mint additions, start R-Studio and then run the standard command for installing R packages:</p>\n<pre><code>install.packages(&quot;tidyverse&quot;)</code></pre>\n<p>In a few minutes it should indicate success. Try it out with this code and you should see results like the image below:</p>\n<p>    library(tidyverse)</p>\n<p>    tidyverse_packages</p>\n<p><a href=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png\"><img src=\"http://edpflager.com/wp-content/uploads/2019/01/tidyverse-screen.png\"></a></p>"},{"title":"Ubuntu 12.10 - Add PPA repositories","id":"1236","comments":0,"date":"2013-04-14T17:59:18.000Z","_content":"\n[![package](http://edpflager.com/wp-content/uploads/2013/04/package.jpg)](http://edpflager.com/wp-content/uploads/2013/04/package.jpg)If you are using Ubuntu, you are probably familiar with the concept of repositories - Internet software archives where additional software packages are available to be installed. There are official repositories, supported by Canonical but other vendors and publishers also provide software in their own repositories. Configuration information for accessing these archives is stored in a file at: /etc/apt/sources.list. To add additional repositories to this file, and be able to install software from them,\n<!-- more -->\ndepends on how the repository information is provided. If the publisher provides one or  several lines starting with \"deb\", then you can edit the sources.list file using your favorite text editor and add the information directly. Often though, you will see lines such as this: ppa:webupd8team/java. Add this information to your repositories list by using the command: sudo add-apt-repository ppa:webupd8team/java If you get a message that the command is not known, you will need to install a support package to get it to work. For versions of Ubuntu before 12.10, enter this command:\n\n*   sudo apt-get install python-software-properties\n\nFor Ubuntu 12.10 and after the command was relocated to another package. You can add it this way:\n\n*   sudo apt-get install software-properties-common\n\nOnce you have the ad-apt-repository command installed, you can add the ppa information to you sources.list file by executing the: sudo add-apt-repository ppa:<repository> and then execute: sudo apt-get update to update the system repository lists.","source":"_posts/ubuntu-12-10-add-ppa-repositories.md","raw":"---\ntitle: Ubuntu 12.10 - Add PPA repositories\ntags: []\nid: '1236'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2013-04-14 13:59:18\n---\n\n[![package](http://edpflager.com/wp-content/uploads/2013/04/package.jpg)](http://edpflager.com/wp-content/uploads/2013/04/package.jpg)If you are using Ubuntu, you are probably familiar with the concept of repositories - Internet software archives where additional software packages are available to be installed. There are official repositories, supported by Canonical but other vendors and publishers also provide software in their own repositories. Configuration information for accessing these archives is stored in a file at: /etc/apt/sources.list. To add additional repositories to this file, and be able to install software from them,\n<!-- more -->\ndepends on how the repository information is provided. If the publisher provides one or  several lines starting with \"deb\", then you can edit the sources.list file using your favorite text editor and add the information directly. Often though, you will see lines such as this: ppa:webupd8team/java. Add this information to your repositories list by using the command: sudo add-apt-repository ppa:webupd8team/java If you get a message that the command is not known, you will need to install a support package to get it to work. For versions of Ubuntu before 12.10, enter this command:\n\n*   sudo apt-get install python-software-properties\n\nFor Ubuntu 12.10 and after the command was relocated to another package. You can add it this way:\n\n*   sudo apt-get install software-properties-common\n\nOnce you have the ad-apt-repository command installed, you can add the ppa information to you sources.list file by executing the: sudo add-apt-repository ppa:<repository> and then execute: sudo apt-get update to update the system repository lists.","slug":"ubuntu-12-10-add-ppa-repositories","published":1,"updated":"2020-08-23T20:54:34.726Z","layout":"post","photos":[],"link":"","_id":"ckeaq9abu00c4sdjx03regp69","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/04/package.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/04/package.jpg\" alt=\"package\"></a>If you are using Ubuntu, you are probably familiar with the concept of repositories - Internet software archives where additional software packages are available to be installed. There are official repositories, supported by Canonical but other vendors and publishers also provide software in their own repositories. Configuration information for accessing these archives is stored in a file at: /etc/apt/sources.list. To add additional repositories to this file, and be able to install software from them,</p>\n<a id=\"more\"></a>\n<p>depends on how the repository information is provided. If the publisher provides one or  several lines starting with “deb”, then you can edit the sources.list file using your favorite text editor and add the information directly. Often though, you will see lines such as this: ppa:webupd8team/java. Add this information to your repositories list by using the command: sudo add-apt-repository ppa:webupd8team/java If you get a message that the command is not known, you will need to install a support package to get it to work. For versions of Ubuntu before 12.10, enter this command:</p>\n<ul>\n<li>sudo apt-get install python-software-properties</li>\n</ul>\n<p>For Ubuntu 12.10 and after the command was relocated to another package. You can add it this way:</p>\n<ul>\n<li>sudo apt-get install software-properties-common</li>\n</ul>\n<p>Once you have the ad-apt-repository command installed, you can add the ppa information to you sources.list file by executing the: sudo add-apt-repository ppa:<repository> and then execute: sudo apt-get update to update the system repository lists.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/04/package.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2013/04/package.jpg\" alt=\"package\"></a>If you are using Ubuntu, you are probably familiar with the concept of repositories - Internet software archives where additional software packages are available to be installed. There are official repositories, supported by Canonical but other vendors and publishers also provide software in their own repositories. Configuration information for accessing these archives is stored in a file at: /etc/apt/sources.list. To add additional repositories to this file, and be able to install software from them,</p>","more":"<p>depends on how the repository information is provided. If the publisher provides one or  several lines starting with “deb”, then you can edit the sources.list file using your favorite text editor and add the information directly. Often though, you will see lines such as this: ppa:webupd8team/java. Add this information to your repositories list by using the command: sudo add-apt-repository ppa:webupd8team/java If you get a message that the command is not known, you will need to install a support package to get it to work. For versions of Ubuntu before 12.10, enter this command:</p>\n<ul>\n<li>sudo apt-get install python-software-properties</li>\n</ul>\n<p>For Ubuntu 12.10 and after the command was relocated to another package. You can add it this way:</p>\n<ul>\n<li>sudo apt-get install software-properties-common</li>\n</ul>\n<p>Once you have the ad-apt-repository command installed, you can add the ppa information to you sources.list file by executing the: sudo add-apt-repository ppa:<repository> and then execute: sudo apt-get update to update the system repository lists.</p>"},{"title":"Use GMail with Pentaho BI-Server Community","id":"3157","comments":0,"date":"2015-12-08T20:15:51.000Z","_content":"\nEmail servers are fairly common in a lot of organizations, but many smaller companies elect to use outside hosting for their email. There are myriad reasons for doing so, and the choice obviously makes sense, otherwise they wouldn't do it. If your organization is using Google Gmailfor its email, you can set up Pentaho's BI Server to use it as your email server. In this brief article, I'll walk you through the settings you'll need to use to make it work. ![Home menu](http://edpflager.com/wp-content/uploads/2015/12/Home-menu-195x300.png)To access the email settings in the Pentaho User Console, login to your BI-Server website with an Administrator account. Click on the large HOME menu item, and at the bottom of the menu that appears, click on Administration. When the Administration screen appears, click on the Email Server option in the menu on the left. The screen will update with the Email Server settings fields. Below are instructions for what information to use to populate the fields depending on the hosting service you are using.\n<!-- more -->\nGMAIL [![GMail](http://edpflager.com/wp-content/uploads/2015/12/GMail1-143x300.png)](http://edpflager.com/wp-content/uploads/2015/12/GMail1.png) For Gmail, some of the settings are non-standard, but they are well documented. The settings are shown in the screen shot to the left. Just a few notes:\n\n*   For port use 587.\n*   From the Server Type drop down, choose SMTPS.\n*   Finally at the bottom of the screen check both boxes next to Use Start TLS and Use SSL.\n\nClick the Test Email Configuration after you are complete. If everything is correct, after a few seconds you'll see a message that the test was successful and in the email account you specified, you'll have a test email with the subject of Diagnostic Verification Email.","source":"_posts/use-gmailhotmail-with-pentaho-bi-server-community.md","raw":"---\ntitle: Use GMail with Pentaho BI-Server Community\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - SysAdmin\n  - technical\nid: '3157'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2015-12-08 15:15:51\n---\n\nEmail servers are fairly common in a lot of organizations, but many smaller companies elect to use outside hosting for their email. There are myriad reasons for doing so, and the choice obviously makes sense, otherwise they wouldn't do it. If your organization is using Google Gmailfor its email, you can set up Pentaho's BI Server to use it as your email server. In this brief article, I'll walk you through the settings you'll need to use to make it work. ![Home menu](http://edpflager.com/wp-content/uploads/2015/12/Home-menu-195x300.png)To access the email settings in the Pentaho User Console, login to your BI-Server website with an Administrator account. Click on the large HOME menu item, and at the bottom of the menu that appears, click on Administration. When the Administration screen appears, click on the Email Server option in the menu on the left. The screen will update with the Email Server settings fields. Below are instructions for what information to use to populate the fields depending on the hosting service you are using.\n<!-- more -->\nGMAIL [![GMail](http://edpflager.com/wp-content/uploads/2015/12/GMail1-143x300.png)](http://edpflager.com/wp-content/uploads/2015/12/GMail1.png) For Gmail, some of the settings are non-standard, but they are well documented. The settings are shown in the screen shot to the left. Just a few notes:\n\n*   For port use 587.\n*   From the Server Type drop down, choose SMTPS.\n*   Finally at the bottom of the screen check both boxes next to Use Start TLS and Use SSL.\n\nClick the Test Email Configuration after you are complete. If everything is correct, after a few seconds you'll see a message that the test was successful and in the email account you specified, you'll have a test email with the subject of Diagnostic Verification Email.","slug":"use-gmailhotmail-with-pentaho-bi-server-community","published":1,"updated":"2020-08-23T20:54:35.002Z","layout":"post","photos":[],"link":"","_id":"ckeaq9abz00c8sdjxf3qvfduc","content":"<p>Email servers are fairly common in a lot of organizations, but many smaller companies elect to use outside hosting for their email. There are myriad reasons for doing so, and the choice obviously makes sense, otherwise they wouldn’t do it. If your organization is using Google Gmailfor its email, you can set up Pentaho’s BI Server to use it as your email server. In this brief article, I’ll walk you through the settings you’ll need to use to make it work. <img src=\"http://edpflager.com/wp-content/uploads/2015/12/Home-menu-195x300.png\" alt=\"Home menu\">To access the email settings in the Pentaho User Console, login to your BI-Server website with an Administrator account. Click on the large HOME menu item, and at the bottom of the menu that appears, click on Administration. When the Administration screen appears, click on the Email Server option in the menu on the left. The screen will update with the Email Server settings fields. Below are instructions for what information to use to populate the fields depending on the hosting service you are using.</p>\n<a id=\"more\"></a>\n<p>GMAIL <a href=\"http://edpflager.com/wp-content/uploads/2015/12/GMail1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/GMail1-143x300.png\" alt=\"GMail\"></a> For Gmail, some of the settings are non-standard, but they are well documented. The settings are shown in the screen shot to the left. Just a few notes:</p>\n<ul>\n<li>For port use 587.</li>\n<li>From the Server Type drop down, choose SMTPS.</li>\n<li>Finally at the bottom of the screen check both boxes next to Use Start TLS and Use SSL.</li>\n</ul>\n<p>Click the Test Email Configuration after you are complete. If everything is correct, after a few seconds you’ll see a message that the test was successful and in the email account you specified, you’ll have a test email with the subject of Diagnostic Verification Email.</p>\n","site":{"data":{}},"excerpt":"<p>Email servers are fairly common in a lot of organizations, but many smaller companies elect to use outside hosting for their email. There are myriad reasons for doing so, and the choice obviously makes sense, otherwise they wouldn’t do it. If your organization is using Google Gmailfor its email, you can set up Pentaho’s BI Server to use it as your email server. In this brief article, I’ll walk you through the settings you’ll need to use to make it work. <img src=\"http://edpflager.com/wp-content/uploads/2015/12/Home-menu-195x300.png\" alt=\"Home menu\">To access the email settings in the Pentaho User Console, login to your BI-Server website with an Administrator account. Click on the large HOME menu item, and at the bottom of the menu that appears, click on Administration. When the Administration screen appears, click on the Email Server option in the menu on the left. The screen will update with the Email Server settings fields. Below are instructions for what information to use to populate the fields depending on the hosting service you are using.</p>","more":"<p>GMAIL <a href=\"http://edpflager.com/wp-content/uploads/2015/12/GMail1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/GMail1-143x300.png\" alt=\"GMail\"></a> For Gmail, some of the settings are non-standard, but they are well documented. The settings are shown in the screen shot to the left. Just a few notes:</p>\n<ul>\n<li>For port use 587.</li>\n<li>From the Server Type drop down, choose SMTPS.</li>\n<li>Finally at the bottom of the screen check both boxes next to Use Start TLS and Use SSL.</li>\n</ul>\n<p>Click the Test Email Configuration after you are complete. If everything is correct, after a few seconds you’ll see a message that the test was successful and in the email account you specified, you’ll have a test email with the subject of Diagnostic Verification Email.</p>"},{"title":"Use Linux Mint as a Server","id":"3173","comments":0,"date":"2015-12-15T00:53:39.000Z","_content":"\n[![mortar-mint](http://edpflager.com/wp-content/uploads/2015/12/mortar-mint-300x272.png)](http://edpflager.com/?attachment_id=3193#main)Linux Mint has been consistently at the top of the DistroWatch top ten list for more than a year due I am sure to its ease of use and solid performance. As a general purpose distribution, Mint is offered with several different desktop environments, but interestingly, no separate server distribution. However, it is possible to configure Mint as a server, and this article covers the basic setup I follow to configure a Mint system for **development server** purposes. As a warning however, be sure to lock your system down with appropriate security precautions before using one as a production server. Below are the standard steps I use while configuring Mint for **development systems**. Because I follow these steps pretty regularly, I have pulled them out as a separate article, and will link back to this as necessary for any future tutorials.\n<!-- more -->\nSET YOUR HOST NAME\n\n1.  After installing MINT from your ISO boot media, make sure your Linux Mint box is up to date. From a command prompt, run the below command:\n    \n    sudo apt-get update\n    \n2.  During installation you can provide a hostname, but I still prefer to validate it after the system is up to check, and rename your box as appropriate. To check your current hostname, enter:\n    \n    hostname\n    \n3.  To edit the host name, edit the **/etc/hostname** file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com.** Open nano to edit the file with this command:\n    \n    sudo nano /etc/hostname\n    \n    Save the file and close it when you are complete.\n\nSET A STATIC IP ADDRESS\n\n1.   You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:\n    \n    **ifconfig**\n    \n2.  From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.![](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png)\n3.  A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.![](http://edpflager.com/wp-content/uploads/2015/11/IPv4.png)\n4.  Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the **ifconfig** command from step 1. **Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.**\n5.  Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in **8.8.8.8** and/or **8.8.4.4.**Separate multiple server addresses by using a comma.\n6.  Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.\n7.  At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the **–** setting to reconnect. (you should get another onscreen notification that you are connected).![](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)\n8.  Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:\n    \n    ping www.linuxmint.com\n    \n    (or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.\n\n##### EDIT THE LOCAL HOST FILE\n\n1.  The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The **hosts file** is used by the operating system to map host names to IP addresses.\n2.  Open a terminal prompt again, and enter this command:\n    \n     sudo nano /etc/hosts\n    \n3.  The nano text editor will open and display the contents of your HOSTS file. It should look something like this:\n    \n    127.0.0.1    localhost\n    127.0.1.1    originalname\n    \n4.  Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:\n    \n    127.0.0.1         localhost\n    10.0.1.22         **ldap       ldap.test.com**\n    \n5.  Restart (just in case).\n\n##### CHECK YOUR JAVA VERSION\n\n1.  Open a terminal prompt and enter the command:\n    \n    java -version\n    \n    By default,  OpenJDK is installed in Mint, but you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation [how to](http://community.linuxmint.com/tutorial/view/1091). Or since Linux Mint is a derivative of Ubuntu, you can use [this tutorial](https://www.howtodojo.com/2015/07/how-to-install-jdk-8-on-ubuntu-15-04/) from www.howtodojo.com which is quite a bit simpler (substitute Java7 for Java8 if you need the older version).\n\n##### INSTALL OPENSSH-SERVER\n\n1.  The last thing I install on my new server is SSH. This allows me to open a secure terminal connection to the server from whatever system I am working on. Again from a terminal prompt, enter this command:\n    \n    sudo apt-get install openssh-server\n    \n2.  Then from another server, I open a terminal prompt and enter the command:\n    \n    ssh <ip-address of server> -l <user name>\n    \n3.  I'll be prompted to accept that the authenticity of the host I am trying to connect to cannot be established, and am I sure I want to connect. Enter yes and press ENTER, and then I am prompted to enter my password. Enter the password for the account being used to connect and press ENTER again.  [![ssh-login1](http://edpflager.com/wp-content/uploads/2015/12/ssh-login1-300x201.png)](http://edpflager.com/?attachment_id=3195#main)\n4.  If all is good, the terminal prompt will change to indicate you are connected to the new server.\n\nAt this point, my basic server is setup and configured, and I can continue on with whatever applications I need to install.","source":"_posts/use-linux-mint-as-a-server.md","raw":"---\ntitle: Use Linux Mint as a Server\ntags:\n  - guides\n  - How-to\n  - howto\n  - Mint\n  - SysAdmin\n  - technical\nid: '3173'\ncategories:\n  - - Blog\n  - - Linux\ncomments: false\ndate: 2015-12-14 19:53:39\n---\n\n[![mortar-mint](http://edpflager.com/wp-content/uploads/2015/12/mortar-mint-300x272.png)](http://edpflager.com/?attachment_id=3193#main)Linux Mint has been consistently at the top of the DistroWatch top ten list for more than a year due I am sure to its ease of use and solid performance. As a general purpose distribution, Mint is offered with several different desktop environments, but interestingly, no separate server distribution. However, it is possible to configure Mint as a server, and this article covers the basic setup I follow to configure a Mint system for **development server** purposes. As a warning however, be sure to lock your system down with appropriate security precautions before using one as a production server. Below are the standard steps I use while configuring Mint for **development systems**. Because I follow these steps pretty regularly, I have pulled them out as a separate article, and will link back to this as necessary for any future tutorials.\n<!-- more -->\nSET YOUR HOST NAME\n\n1.  After installing MINT from your ISO boot media, make sure your Linux Mint box is up to date. From a command prompt, run the below command:\n    \n    sudo apt-get update\n    \n2.  During installation you can provide a hostname, but I still prefer to validate it after the system is up to check, and rename your box as appropriate. To check your current hostname, enter:\n    \n    hostname\n    \n3.  To edit the host name, edit the **/etc/hostname** file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: **ldap.test.com.** Open nano to edit the file with this command:\n    \n    sudo nano /etc/hostname\n    \n    Save the file and close it when you are complete.\n\nSET A STATIC IP ADDRESS\n\n1.   You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:\n    \n    **ifconfig**\n    \n2.  From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.![](http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png)\n3.  A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.![](http://edpflager.com/wp-content/uploads/2015/11/IPv4.png)\n4.  Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the **ifconfig** command from step 1. **Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.**\n5.  Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in **8.8.8.8** and/or **8.8.4.4.**Separate multiple server addresses by using a comma.\n6.  Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.\n7.  At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the **–** setting to reconnect. (you should get another onscreen notification that you are connected).![](http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png)\n8.  Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:\n    \n    ping www.linuxmint.com\n    \n    (or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.\n\n##### EDIT THE LOCAL HOST FILE\n\n1.  The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The **hosts file** is used by the operating system to map host names to IP addresses.\n2.  Open a terminal prompt again, and enter this command:\n    \n     sudo nano /etc/hosts\n    \n3.  The nano text editor will open and display the contents of your HOSTS file. It should look something like this:\n    \n    127.0.0.1    localhost\n    127.0.1.1    originalname\n    \n4.  Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:\n    \n    127.0.0.1         localhost\n    10.0.1.22         **ldap       ldap.test.com**\n    \n5.  Restart (just in case).\n\n##### CHECK YOUR JAVA VERSION\n\n1.  Open a terminal prompt and enter the command:\n    \n    java -version\n    \n    By default,  OpenJDK is installed in Mint, but you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation [how to](http://community.linuxmint.com/tutorial/view/1091). Or since Linux Mint is a derivative of Ubuntu, you can use [this tutorial](https://www.howtodojo.com/2015/07/how-to-install-jdk-8-on-ubuntu-15-04/) from www.howtodojo.com which is quite a bit simpler (substitute Java7 for Java8 if you need the older version).\n\n##### INSTALL OPENSSH-SERVER\n\n1.  The last thing I install on my new server is SSH. This allows me to open a secure terminal connection to the server from whatever system I am working on. Again from a terminal prompt, enter this command:\n    \n    sudo apt-get install openssh-server\n    \n2.  Then from another server, I open a terminal prompt and enter the command:\n    \n    ssh <ip-address of server> -l <user name>\n    \n3.  I'll be prompted to accept that the authenticity of the host I am trying to connect to cannot be established, and am I sure I want to connect. Enter yes and press ENTER, and then I am prompted to enter my password. Enter the password for the account being used to connect and press ENTER again.  [![ssh-login1](http://edpflager.com/wp-content/uploads/2015/12/ssh-login1-300x201.png)](http://edpflager.com/?attachment_id=3195#main)\n4.  If all is good, the terminal prompt will change to indicate you are connected to the new server.\n\nAt this point, my basic server is setup and configured, and I can continue on with whatever applications I need to install.","slug":"use-linux-mint-as-a-server","published":1,"updated":"2020-08-23T20:54:35.006Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ac500cbsdjx5eti84ov","content":"<p><a href=\"http://edpflager.com/?attachment_id=3193#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/mortar-mint-300x272.png\" alt=\"mortar-mint\"></a>Linux Mint has been consistently at the top of the DistroWatch top ten list for more than a year due I am sure to its ease of use and solid performance. As a general purpose distribution, Mint is offered with several different desktop environments, but interestingly, no separate server distribution. However, it is possible to configure Mint as a server, and this article covers the basic setup I follow to configure a Mint system for <strong>development server</strong> purposes. As a warning however, be sure to lock your system down with appropriate security precautions before using one as a production server. Below are the standard steps I use while configuring Mint for <strong>development systems</strong>. Because I follow these steps pretty regularly, I have pulled them out as a separate article, and will link back to this as necessary for any future tutorials.</p>\n<a id=\"more\"></a>\n<p>SET YOUR HOST NAME</p>\n<ol>\n<li><p>After installing MINT from your ISO boot media, make sure your Linux Mint box is up to date. From a command prompt, run the below command:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>During installation you can provide a hostname, but I still prefer to validate it after the system is up to check, and rename your box as appropriate. To check your current hostname, enter:</p>\n<p>hostname</p>\n</li>\n<li><p>To edit the host name, edit the <strong>/etc/hostname</strong> file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: <strong>ldap.test.com.</strong> Open nano to edit the file with this command:</p>\n<p>sudo nano /etc/hostname</p>\n<p>Save the file and close it when you are complete.</p>\n</li>\n</ol>\n<p>SET A STATIC IP ADDRESS</p>\n<ol>\n<li><p> You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:</p>\n<p><strong>ifconfig</strong></p>\n</li>\n<li><p>From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png\"></p>\n</li>\n<li><p>A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4.png\"></p>\n</li>\n<li><p>Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the <strong>ifconfig</strong> command from step 1. <strong>Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.</strong></p>\n</li>\n<li><p>Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in <strong>8.8.8.8</strong> and/or **8.8.4.4.**Separate multiple server addresses by using a comma.</p>\n</li>\n<li><p>Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.</p>\n</li>\n<li><p>At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the <strong>–</strong> setting to reconnect. (you should get another onscreen notification that you are connected).<img src=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\"></p>\n</li>\n<li><p>Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:</p>\n<p>ping <a href=\"http://www.linuxmint.com/\">www.linuxmint.com</a></p>\n<p>(or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.</p>\n</li>\n</ol>\n<h5 id=\"EDIT-THE-LOCAL-HOST-FILE\"><a href=\"#EDIT-THE-LOCAL-HOST-FILE\" class=\"headerlink\" title=\"EDIT THE LOCAL HOST FILE\"></a>EDIT THE LOCAL HOST FILE</h5><ol>\n<li><p>The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The <strong>hosts file</strong> is used by the operating system to map host names to IP addresses.</p>\n</li>\n<li><p>Open a terminal prompt again, and enter this command:</p>\n<p> sudo nano /etc/hosts</p>\n</li>\n<li><p>The nano text editor will open and display the contents of your HOSTS file. It should look something like this:</p>\n<p>127.0.0.1    localhost<br>127.0.1.1    originalname</p>\n</li>\n<li><p>Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:</p>\n<p>127.0.0.1         localhost<br>10.0.1.22         <strong>ldap       ldap.test.com</strong></p>\n</li>\n<li><p>Restart (just in case).</p>\n</li>\n</ol>\n<h5 id=\"CHECK-YOUR-JAVA-VERSION\"><a href=\"#CHECK-YOUR-JAVA-VERSION\" class=\"headerlink\" title=\"CHECK YOUR JAVA VERSION\"></a>CHECK YOUR JAVA VERSION</h5><ol>\n<li><p>Open a terminal prompt and enter the command:</p>\n<p>java -version</p>\n<p>By default,  OpenJDK is installed in Mint, but you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation <a href=\"http://community.linuxmint.com/tutorial/view/1091\">how to</a>. Or since Linux Mint is a derivative of Ubuntu, you can use <a href=\"https://www.howtodojo.com/2015/07/how-to-install-jdk-8-on-ubuntu-15-04/\">this tutorial</a> from <a href=\"http://www.howtodojo.com/\">www.howtodojo.com</a> which is quite a bit simpler (substitute Java7 for Java8 if you need the older version).</p>\n</li>\n</ol>\n<h5 id=\"INSTALL-OPENSSH-SERVER\"><a href=\"#INSTALL-OPENSSH-SERVER\" class=\"headerlink\" title=\"INSTALL OPENSSH-SERVER\"></a>INSTALL OPENSSH-SERVER</h5><ol>\n<li><p>The last thing I install on my new server is SSH. This allows me to open a secure terminal connection to the server from whatever system I am working on. Again from a terminal prompt, enter this command:</p>\n<p>sudo apt-get install openssh-server</p>\n</li>\n<li><p>Then from another server, I open a terminal prompt and enter the command:</p>\n<p>ssh <ip-address of server> -l <user name></p>\n</li>\n<li><p>I’ll be prompted to accept that the authenticity of the host I am trying to connect to cannot be established, and am I sure I want to connect. Enter yes and press ENTER, and then I am prompted to enter my password. Enter the password for the account being used to connect and press ENTER again.  <a href=\"http://edpflager.com/?attachment_id=3195#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/ssh-login1-300x201.png\" alt=\"ssh-login1\"></a></p>\n</li>\n<li><p>If all is good, the terminal prompt will change to indicate you are connected to the new server.</p>\n</li>\n</ol>\n<p>At this point, my basic server is setup and configured, and I can continue on with whatever applications I need to install.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3193#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/mortar-mint-300x272.png\" alt=\"mortar-mint\"></a>Linux Mint has been consistently at the top of the DistroWatch top ten list for more than a year due I am sure to its ease of use and solid performance. As a general purpose distribution, Mint is offered with several different desktop environments, but interestingly, no separate server distribution. However, it is possible to configure Mint as a server, and this article covers the basic setup I follow to configure a Mint system for <strong>development server</strong> purposes. As a warning however, be sure to lock your system down with appropriate security precautions before using one as a production server. Below are the standard steps I use while configuring Mint for <strong>development systems</strong>. Because I follow these steps pretty regularly, I have pulled them out as a separate article, and will link back to this as necessary for any future tutorials.</p>","more":"<p>SET YOUR HOST NAME</p>\n<ol>\n<li><p>After installing MINT from your ISO boot media, make sure your Linux Mint box is up to date. From a command prompt, run the below command:</p>\n<p>sudo apt-get update</p>\n</li>\n<li><p>During installation you can provide a hostname, but I still prefer to validate it after the system is up to check, and rename your box as appropriate. To check your current hostname, enter:</p>\n<p>hostname</p>\n</li>\n<li><p>To edit the host name, edit the <strong>/etc/hostname</strong> file as root. If you have a local domain for your network, be sure to include it as part of the host name. For example, if your domain is test.com, you might make the host name: <strong>ldap.test.com.</strong> Open nano to edit the file with this command:</p>\n<p>sudo nano /etc/hostname</p>\n<p>Save the file and close it when you are complete.</p>\n</li>\n</ol>\n<p>SET A STATIC IP ADDRESS</p>\n<ol>\n<li><p> You now need to add a static IP address to your LDAP system. If your network administrator has provided one for you, use it. Otherwise to get the existing address for your PC, the gateway, the net mask and a lot of other info in the  terminal window, enter the command:</p>\n<p><strong>ifconfig</strong></p>\n</li>\n<li><p>From the Mint menu, access the Network Connections application, under Preferences. In the network connections window, click on Wired Connection 1, and then click the EDIT button on the right.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/NetworkConnection.png\"></p>\n</li>\n<li><p>A new window will appear with several tabs. Click on the one for IPv4. If the Method here is not set to MANUAL, click  the drop down list and select that option.<img src=\"http://edpflager.com/wp-content/uploads/2015/11/IPv4.png\"></p>\n</li>\n<li><p>Click the NEW button on the right, and enter your IP information. Here is where you will enter the IP address, Net Mask and Gateway information provided either by your network administrator, or from the <strong>ifconfig</strong> command from step 1. <strong>Keep in mind that if you use the already assigned DHCP address, you may encounter issues on your home network if that address is given out to another device by your router or DHCP server. Be sure to exclude it from the range of addresses that are handed out.</strong></p>\n</li>\n<li><p>Enter at least one DNS server (generally the first one will be the address of your gateway). You can also use Google’s public DNS servers, by adding in <strong>8.8.8.8</strong> and/or **8.8.4.4.**Separate multiple server addresses by using a comma.</p>\n</li>\n<li><p>Click over to the IPv6 tab. For our purposes, we don’t need to set a static address here, so set the Method drop down to IGNORE. Click the SAVE button to the Network Connection window and then click CLOSE in that window.</p>\n</li>\n<li><p>At this point, you should turn networking off and on using the network icon on your screen to ensure the new settings have taken effect. Click the icon once to bring up a control window. Toggle the switch at the top of the window to the O setting to turn it off (you should get an onscreen notification that you are disconnected). Toggle the switch back to the <strong>–</strong> setting to reconnect. (you should get another onscreen notification that you are connected).<img src=\"http://edpflager.com/wp-content/uploads/2015/11/restartnetwork.png\"></p>\n</li>\n<li><p>Finally, open a terminal prompt, and attempt to PING a website to make sure everything is working OK. Enter the command:</p>\n<p>ping <a href=\"http://www.linuxmint.com/\">www.linuxmint.com</a></p>\n<p>(or some other website) and hit ENTER. You should receive a number of reply messages that  64 bytes were received from the website in a certain amount of time. Hit CTRL-C to quit the PING application.</p>\n</li>\n</ol>\n<h5 id=\"EDIT-THE-LOCAL-HOST-FILE\"><a href=\"#EDIT-THE-LOCAL-HOST-FILE\" class=\"headerlink\" title=\"EDIT THE LOCAL HOST FILE\"></a>EDIT THE LOCAL HOST FILE</h5><ol>\n<li><p>The next step is to configure the local  HOST file to show the new server name and IP address for your Mint PC. The <strong>hosts file</strong> is used by the operating system to map host names to IP addresses.</p>\n</li>\n<li><p>Open a terminal prompt again, and enter this command:</p>\n<p> sudo nano /etc/hosts</p>\n</li>\n<li><p>The nano text editor will open and display the contents of your HOSTS file. It should look something like this:</p>\n<p>127.0.0.1    localhost<br>127.0.1.1    originalname</p>\n</li>\n<li><p>Change the second line to have your static IP address, the new Host Name you set previously, and the full host name with domain. After editing, it should look like:</p>\n<p>127.0.0.1         localhost<br>10.0.1.22         <strong>ldap       ldap.test.com</strong></p>\n</li>\n<li><p>Restart (just in case).</p>\n</li>\n</ol>\n<h5 id=\"CHECK-YOUR-JAVA-VERSION\"><a href=\"#CHECK-YOUR-JAVA-VERSION\" class=\"headerlink\" title=\"CHECK YOUR JAVA VERSION\"></a>CHECK YOUR JAVA VERSION</h5><ol>\n<li><p>Open a terminal prompt and enter the command:</p>\n<p>java -version</p>\n<p>By default,  OpenJDK is installed in Mint, but you can replace it with the official version of JAVA from Oracle. If you prefer the Oracle version, the Linux Mint website has a good installation <a href=\"http://community.linuxmint.com/tutorial/view/1091\">how to</a>. Or since Linux Mint is a derivative of Ubuntu, you can use <a href=\"https://www.howtodojo.com/2015/07/how-to-install-jdk-8-on-ubuntu-15-04/\">this tutorial</a> from <a href=\"http://www.howtodojo.com/\">www.howtodojo.com</a> which is quite a bit simpler (substitute Java7 for Java8 if you need the older version).</p>\n</li>\n</ol>\n<h5 id=\"INSTALL-OPENSSH-SERVER\"><a href=\"#INSTALL-OPENSSH-SERVER\" class=\"headerlink\" title=\"INSTALL OPENSSH-SERVER\"></a>INSTALL OPENSSH-SERVER</h5><ol>\n<li><p>The last thing I install on my new server is SSH. This allows me to open a secure terminal connection to the server from whatever system I am working on. Again from a terminal prompt, enter this command:</p>\n<p>sudo apt-get install openssh-server</p>\n</li>\n<li><p>Then from another server, I open a terminal prompt and enter the command:</p>\n<p>ssh <ip-address of server> -l <user name></p>\n</li>\n<li><p>I’ll be prompted to accept that the authenticity of the host I am trying to connect to cannot be established, and am I sure I want to connect. Enter yes and press ENTER, and then I am prompted to enter my password. Enter the password for the account being used to connect and press ENTER again.  <a href=\"http://edpflager.com/?attachment_id=3195#main\"><img src=\"http://edpflager.com/wp-content/uploads/2015/12/ssh-login1-300x201.png\" alt=\"ssh-login1\"></a></p>\n</li>\n<li><p>If all is good, the terminal prompt will change to indicate you are connected to the new server.</p>\n</li>\n</ol>\n<p>At this point, my basic server is setup and configured, and I can continue on with whatever applications I need to install.</p>"},{"title":"Use Linux SQL Server with R (JDBC)","id":"3537","comments":0,"date":"2017-04-29T08:30:00.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png)](http://edpflager.com/?attachment_id=3545#main)Over the past few months, I have been working to learn [R, a free software environment](https://www.r-project.org/) for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers and a Linux version of SQL Server is now available, its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R, using a JDBC connection. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). So now let's get started.\n<!-- more -->\n##### System preparation\n\nBefore you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft JDBC driver for SQL Server available at this [link](http://www.microsoft.com/downloads/en/details.aspx?FamilyID=a737000d-68d0-4531-b65d-da0f2a735707&displaylang=en). Extract to where ever you like. On my Ubuntu box, I put mine in /opt/jdbc and gave permissions to my user account /group to use it. Now open R (either command line or in RStudio) and install the RJDBC package with the command:\n\ninstall.packages(\"RJDBC\")\n\nIf you get an error like this:\n\n./configure: line 3736: /usr/lib/jvm/default-java/jre/bin/java: No such file or directory\n\nopen a terminal prompt and enter this command to install a distribution specific RJava package:\n\nsudo apt-get install r-cran-rjava\n\nThen retry the command: install.packages(\"RJDBC\")\n\n##### Use JDBC to connect to SQL Server\n\nNow that you have the JDBC software installed, lets turn to how to use it. In your R script file or in R Studio, call the JDBC library and define the driver variable to use. The following sample includes the path to where I installed the JDBC jar file. Your path may vary depending on where you installed to above:\n\nlibrary(RJDBC)\ndriver <- JDBC(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\", \"/opt/jdbc/sqljdbc4.jar\") \n\nNext you need to define your connection variable. Replace localhost with your server name if SQL Server is running somewhere else than your R machine. Substitute an appropriate user name and password for those values as well. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The result should look like this:\n\nconnect <- dbConnect(driver, \"jdbc:sqlserver://localhost\", \"username\",\"password\")\n\nIf you get an error like this:\n\nError in .jcall(drv@jdrv, \"Ljava/sql/Connection;\", \"connect\", as.character(url)\\[1\\], :\n com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host localhost, port 1433 has failed. Error: \"Connection refused (Connection refused).\n\nVerify the connection properties, check that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port, and that no firewall is blocking TCP connections to the port.\" Check to make sure the SQL Service is running on your Linux machine, and that the firewall port is not being blocked (by default its 1433). Next define a query variable to return the names of all of the databases on your server:\n\nquery <- 'SELECT name, database\\_id, create\\_date FROM sys.databases ;'\n\nFinally run your query and store the results:\n\nQueryResults <- dbGetQuery(connect, query)\n\nTo show the results just call the query results variable:\n\nQueryResults\n\nThe results will look like this: [![](http://edpflager.com/wp-content/uploads/2017/04/query-300x118.png)](http://edpflager.com/?attachment_id=3551#main)         FInally if you are finished with querying your database its considered good practice to close your connection. You can do that with the RJDBC package like this:\n\n dbDisconnect(connect)\n\nThat's it!","source":"_posts/use-linux-sql-server-with-r-jdbc.md","raw":"---\ntitle: Use Linux SQL Server with R (JDBC)\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - SQL Server\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3537'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2017-04-29 04:30:00\n---\n\n[![](http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png)](http://edpflager.com/?attachment_id=3545#main)Over the past few months, I have been working to learn [R, a free software environment](https://www.r-project.org/) for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers and a Linux version of SQL Server is now available, its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R, using a JDBC connection. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). So now let's get started.\n<!-- more -->\n##### System preparation\n\nBefore you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft JDBC driver for SQL Server available at this [link](http://www.microsoft.com/downloads/en/details.aspx?FamilyID=a737000d-68d0-4531-b65d-da0f2a735707&displaylang=en). Extract to where ever you like. On my Ubuntu box, I put mine in /opt/jdbc and gave permissions to my user account /group to use it. Now open R (either command line or in RStudio) and install the RJDBC package with the command:\n\ninstall.packages(\"RJDBC\")\n\nIf you get an error like this:\n\n./configure: line 3736: /usr/lib/jvm/default-java/jre/bin/java: No such file or directory\n\nopen a terminal prompt and enter this command to install a distribution specific RJava package:\n\nsudo apt-get install r-cran-rjava\n\nThen retry the command: install.packages(\"RJDBC\")\n\n##### Use JDBC to connect to SQL Server\n\nNow that you have the JDBC software installed, lets turn to how to use it. In your R script file or in R Studio, call the JDBC library and define the driver variable to use. The following sample includes the path to where I installed the JDBC jar file. Your path may vary depending on where you installed to above:\n\nlibrary(RJDBC)\ndriver <- JDBC(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\", \"/opt/jdbc/sqljdbc4.jar\") \n\nNext you need to define your connection variable. Replace localhost with your server name if SQL Server is running somewhere else than your R machine. Substitute an appropriate user name and password for those values as well. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The result should look like this:\n\nconnect <- dbConnect(driver, \"jdbc:sqlserver://localhost\", \"username\",\"password\")\n\nIf you get an error like this:\n\nError in .jcall(drv@jdrv, \"Ljava/sql/Connection;\", \"connect\", as.character(url)\\[1\\], :\n com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host localhost, port 1433 has failed. Error: \"Connection refused (Connection refused).\n\nVerify the connection properties, check that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port, and that no firewall is blocking TCP connections to the port.\" Check to make sure the SQL Service is running on your Linux machine, and that the firewall port is not being blocked (by default its 1433). Next define a query variable to return the names of all of the databases on your server:\n\nquery <- 'SELECT name, database\\_id, create\\_date FROM sys.databases ;'\n\nFinally run your query and store the results:\n\nQueryResults <- dbGetQuery(connect, query)\n\nTo show the results just call the query results variable:\n\nQueryResults\n\nThe results will look like this: [![](http://edpflager.com/wp-content/uploads/2017/04/query-300x118.png)](http://edpflager.com/?attachment_id=3551#main)         FInally if you are finished with querying your database its considered good practice to close your connection. You can do that with the RJDBC package like this:\n\n dbDisconnect(connect)\n\nThat's it!","slug":"use-linux-sql-server-with-r-jdbc","published":1,"updated":"2020-08-23T20:54:35.102Z","layout":"post","photos":[],"link":"","_id":"ckeaq9acb00cfsdjx6nwt9n7z","content":"<p><a href=\"http://edpflager.com/?attachment_id=3545#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png\"></a>Over the past few months, I have been working to learn <a href=\"https://www.r-project.org/\">R, a free software environment</a> for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers and a Linux version of SQL Server is now available, its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R, using a JDBC connection. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. So now let’s get started.</p>\n<a id=\"more\"></a>\n<h5 id=\"System-preparation\"><a href=\"#System-preparation\" class=\"headerlink\" title=\"System preparation\"></a>System preparation</h5><p>Before you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft JDBC driver for SQL Server available at this <a href=\"http://www.microsoft.com/downloads/en/details.aspx?FamilyID=a737000d-68d0-4531-b65d-da0f2a735707&displaylang=en\">link</a>. Extract to where ever you like. On my Ubuntu box, I put mine in /opt/jdbc and gave permissions to my user account /group to use it. Now open R (either command line or in RStudio) and install the RJDBC package with the command:</p>\n<p>install.packages(“RJDBC”)</p>\n<p>If you get an error like this:</p>\n<p>./configure: line 3736: /usr/lib/jvm/default-java/jre/bin/java: No such file or directory</p>\n<p>open a terminal prompt and enter this command to install a distribution specific RJava package:</p>\n<p>sudo apt-get install r-cran-rjava</p>\n<p>Then retry the command: install.packages(“RJDBC”)</p>\n<h5 id=\"Use-JDBC-to-connect-to-SQL-Server\"><a href=\"#Use-JDBC-to-connect-to-SQL-Server\" class=\"headerlink\" title=\"Use JDBC to connect to SQL Server\"></a>Use JDBC to connect to SQL Server</h5><p>Now that you have the JDBC software installed, lets turn to how to use it. In your R script file or in R Studio, call the JDBC library and define the driver variable to use. The following sample includes the path to where I installed the JDBC jar file. Your path may vary depending on where you installed to above:</p>\n<p>library(RJDBC)<br>driver &lt;- JDBC(“com.microsoft.sqlserver.jdbc.SQLServerDriver”, “/opt/jdbc/sqljdbc4.jar”) </p>\n<p>Next you need to define your connection variable. Replace localhost with your server name if SQL Server is running somewhere else than your R machine. Substitute an appropriate user name and password for those values as well. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The result should look like this:</p>\n<p>connect &lt;- dbConnect(driver, “jdbc:sqlserver://localhost”, “username”,”password”)</p>\n<p>If you get an error like this:</p>\n<p>Error in .jcall(drv@jdrv, “Ljava/sql/Connection;”, “connect”, as.character(url)[1], :<br> com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host localhost, port 1433 has failed. Error: “Connection refused (Connection refused).</p>\n<p>Verify the connection properties, check that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port, and that no firewall is blocking TCP connections to the port.” Check to make sure the SQL Service is running on your Linux machine, and that the firewall port is not being blocked (by default its 1433). Next define a query variable to return the names of all of the databases on your server:</p>\n<p>query &lt;- ‘SELECT name, database_id, create_date FROM sys.databases ;’</p>\n<p>Finally run your query and store the results:</p>\n<p>QueryResults &lt;- dbGetQuery(connect, query)</p>\n<p>To show the results just call the query results variable:</p>\n<p>QueryResults</p>\n<p>The results will look like this: <a href=\"http://edpflager.com/?attachment_id=3551#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/query-300x118.png\"></a>         FInally if you are finished with querying your database its considered good practice to close your connection. You can do that with the RJDBC package like this:</p>\n<p> dbDisconnect(connect)</p>\n<p>That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3545#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png\"></a>Over the past few months, I have been working to learn <a href=\"https://www.r-project.org/\">R, a free software environment</a> for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers and a Linux version of SQL Server is now available, its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R, using a JDBC connection. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. So now let’s get started.</p>","more":"<h5 id=\"System-preparation\"><a href=\"#System-preparation\" class=\"headerlink\" title=\"System preparation\"></a>System preparation</h5><p>Before you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft JDBC driver for SQL Server available at this <a href=\"http://www.microsoft.com/downloads/en/details.aspx?FamilyID=a737000d-68d0-4531-b65d-da0f2a735707&displaylang=en\">link</a>. Extract to where ever you like. On my Ubuntu box, I put mine in /opt/jdbc and gave permissions to my user account /group to use it. Now open R (either command line or in RStudio) and install the RJDBC package with the command:</p>\n<p>install.packages(“RJDBC”)</p>\n<p>If you get an error like this:</p>\n<p>./configure: line 3736: /usr/lib/jvm/default-java/jre/bin/java: No such file or directory</p>\n<p>open a terminal prompt and enter this command to install a distribution specific RJava package:</p>\n<p>sudo apt-get install r-cran-rjava</p>\n<p>Then retry the command: install.packages(“RJDBC”)</p>\n<h5 id=\"Use-JDBC-to-connect-to-SQL-Server\"><a href=\"#Use-JDBC-to-connect-to-SQL-Server\" class=\"headerlink\" title=\"Use JDBC to connect to SQL Server\"></a>Use JDBC to connect to SQL Server</h5><p>Now that you have the JDBC software installed, lets turn to how to use it. In your R script file or in R Studio, call the JDBC library and define the driver variable to use. The following sample includes the path to where I installed the JDBC jar file. Your path may vary depending on where you installed to above:</p>\n<p>library(RJDBC)<br>driver &lt;- JDBC(“com.microsoft.sqlserver.jdbc.SQLServerDriver”, “/opt/jdbc/sqljdbc4.jar”) </p>\n<p>Next you need to define your connection variable. Replace localhost with your server name if SQL Server is running somewhere else than your R machine. Substitute an appropriate user name and password for those values as well. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The result should look like this:</p>\n<p>connect &lt;- dbConnect(driver, “jdbc:sqlserver://localhost”, “username”,”password”)</p>\n<p>If you get an error like this:</p>\n<p>Error in .jcall(drv@jdrv, “Ljava/sql/Connection;”, “connect”, as.character(url)[1], :<br> com.microsoft.sqlserver.jdbc.SQLServerException: The TCP/IP connection to the host localhost, port 1433 has failed. Error: “Connection refused (Connection refused).</p>\n<p>Verify the connection properties, check that an instance of SQL Server is running on the host and accepting TCP/IP connections at the port, and that no firewall is blocking TCP connections to the port.” Check to make sure the SQL Service is running on your Linux machine, and that the firewall port is not being blocked (by default its 1433). Next define a query variable to return the names of all of the databases on your server:</p>\n<p>query &lt;- ‘SELECT name, database_id, create_date FROM sys.databases ;’</p>\n<p>Finally run your query and store the results:</p>\n<p>QueryResults &lt;- dbGetQuery(connect, query)</p>\n<p>To show the results just call the query results variable:</p>\n<p>QueryResults</p>\n<p>The results will look like this: <a href=\"http://edpflager.com/?attachment_id=3551#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/query-300x118.png\"></a>         FInally if you are finished with querying your database its considered good practice to close your connection. You can do that with the RJDBC package like this:</p>\n<p> dbDisconnect(connect)</p>\n<p>That’s it!</p>"},{"title":"Use Linux SQL Server with R (ODBC)","id":"3555","comments":0,"date":"2017-05-02T23:49:18.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png)](http://edpflager.com/?attachment_id=3545#main)This is my second article on using Microsoft's new Linux version of SQL Server with R. This time, I'll cover how to use RODBC to gather data from SQL Server. As a bit of background, over the past few months, I have been working to learn [R, a free software environment](https://www.r-project.org/) for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and in the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers  its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R using ODBC. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). So now let's get started.\n<!-- more -->\n##### SYSTEM PREPARATION\n\nBefore you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft 13.1 ODBC driver for SQL Server available at this [link](https://docs.microsoft.com/en-us/sql/connect/odbc/linux/microsoft-odbc-driver-for-sql-server-on-linux). Follow the instructions, and the driver will be installed automatically. On Ubuntu its written to a folder under /opt/microsoft/msodbcsql. With this version the ODBC driver no longer depends on custom packaging for the unixODBC driver manager as was required in prior versions. Now open R (either command line or in RStudio) and install the RODBC package with the command:\n\ninstall.packages(\"RODBC\")\n\n##### USE ODBC TO CONNECT TO SQL SERVER\n\nNow that you have the ODBC software installed, lets turn to how to use it. There are two methods for ODBC connections to be setup in Linux, one involves defining the connection in an .odbc.ini file. I prefer the second method which involves less setup, but is also less secure because your user name and password are included in the R script. In your R script file or in R Studio, call the RODBC library and define the driver variable to use:\n\nlibrary(RODBC)\ndriver <- \"ODBC Driver 13 for SQL Server\" \n\nNext you need to define your connection variables. Define a database you want to connect to, the hostname of the server (replace localhost with your server name if SQL Server is running somewhere else than your R machine), the port SQL Server is listening on  (1433 by default) and your user name and password. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The results should look like this:\n\ndb <- \"master\"\nhost <- \"localhost\"\nport <-\"1433\"\nuser <-\"username\"\npwd <- \"password\"\n\nNext we need to pull all of those variables together to make our connection string:\n\nconn <- paste(\"DRIVER=\",driver,\n              \";Database=\",db,\n              \";Server=\",host,\n              \";Port=\",port,\n              \";PROTOCOL=TCPIP\",\n              \";UID=\", user,\n              \";PWD=\",pwd,\n               sep=\"\")\n\nThis results in a connection string like this:\n\n\"DRIVER=ODBC Driver 13 for SQL Server;Database=master;Server=localhost;Port=1433;PROTOCOL=TCPIP;UID=username;PWD=password\n\nWith our connection built, we can pass it to the odbcDriverConnect function in R to open our connection:\n\ncon1 <- odbcDriverConnect(conn)\n\nIf all goes well, R will return to the next line prompt. Define a new variable to hold the results of your query and pass the query to the database using the sqlQuery function in R with the connection defined previously:\n\nres <- sqlQuery(con1, 'SELECT name, database\\_id, create\\_date FROM sys.databases ;')\n\nTo show the results just call the query results variable:\n\nres\n\nThe results will look like this: [![](http://edpflager.com/wp-content/uploads/2017/04/res-300x97.png)](http://edpflager.com/?attachment_id=3564#main)       Finally if we are finished with using this database, be sure to close the connection:\n\nodbcCloseAll()\n\nThat's it!","source":"_posts/use-linux-sql-server-with-r-odbc.md","raw":"---\ntitle: Use Linux SQL Server with R (ODBC)\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - howto\n  - install\n  - SQL Server\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3555'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\ncomments: false\ndate: 2017-05-02 19:49:18\n---\n\n[![](http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png)](http://edpflager.com/?attachment_id=3545#main)This is my second article on using Microsoft's new Linux version of SQL Server with R. This time, I'll cover how to use RODBC to gather data from SQL Server. As a bit of background, over the past few months, I have been working to learn [R, a free software environment](https://www.r-project.org/) for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and in the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers  its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R using ODBC. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a [write-up already](https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu). So now let's get started.\n<!-- more -->\n##### SYSTEM PREPARATION\n\nBefore you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft 13.1 ODBC driver for SQL Server available at this [link](https://docs.microsoft.com/en-us/sql/connect/odbc/linux/microsoft-odbc-driver-for-sql-server-on-linux). Follow the instructions, and the driver will be installed automatically. On Ubuntu its written to a folder under /opt/microsoft/msodbcsql. With this version the ODBC driver no longer depends on custom packaging for the unixODBC driver manager as was required in prior versions. Now open R (either command line or in RStudio) and install the RODBC package with the command:\n\ninstall.packages(\"RODBC\")\n\n##### USE ODBC TO CONNECT TO SQL SERVER\n\nNow that you have the ODBC software installed, lets turn to how to use it. There are two methods for ODBC connections to be setup in Linux, one involves defining the connection in an .odbc.ini file. I prefer the second method which involves less setup, but is also less secure because your user name and password are included in the R script. In your R script file or in R Studio, call the RODBC library and define the driver variable to use:\n\nlibrary(RODBC)\ndriver <- \"ODBC Driver 13 for SQL Server\" \n\nNext you need to define your connection variables. Define a database you want to connect to, the hostname of the server (replace localhost with your server name if SQL Server is running somewhere else than your R machine), the port SQL Server is listening on  (1433 by default) and your user name and password. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The results should look like this:\n\ndb <- \"master\"\nhost <- \"localhost\"\nport <-\"1433\"\nuser <-\"username\"\npwd <- \"password\"\n\nNext we need to pull all of those variables together to make our connection string:\n\nconn <- paste(\"DRIVER=\",driver,\n              \";Database=\",db,\n              \";Server=\",host,\n              \";Port=\",port,\n              \";PROTOCOL=TCPIP\",\n              \";UID=\", user,\n              \";PWD=\",pwd,\n               sep=\"\")\n\nThis results in a connection string like this:\n\n\"DRIVER=ODBC Driver 13 for SQL Server;Database=master;Server=localhost;Port=1433;PROTOCOL=TCPIP;UID=username;PWD=password\n\nWith our connection built, we can pass it to the odbcDriverConnect function in R to open our connection:\n\ncon1 <- odbcDriverConnect(conn)\n\nIf all goes well, R will return to the next line prompt. Define a new variable to hold the results of your query and pass the query to the database using the sqlQuery function in R with the connection defined previously:\n\nres <- sqlQuery(con1, 'SELECT name, database\\_id, create\\_date FROM sys.databases ;')\n\nTo show the results just call the query results variable:\n\nres\n\nThe results will look like this: [![](http://edpflager.com/wp-content/uploads/2017/04/res-300x97.png)](http://edpflager.com/?attachment_id=3564#main)       Finally if we are finished with using this database, be sure to close the connection:\n\nodbcCloseAll()\n\nThat's it!","slug":"use-linux-sql-server-with-r-odbc","published":1,"updated":"2020-08-23T20:54:35.106Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ad700cisdjx1ince5dp","content":"<p><a href=\"http://edpflager.com/?attachment_id=3545#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png\"></a>This is my second article on using Microsoft’s new Linux version of SQL Server with R. This time, I’ll cover how to use RODBC to gather data from SQL Server. As a bit of background, over the past few months, I have been working to learn <a href=\"https://www.r-project.org/\">R, a free software environment</a> for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and in the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers  its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R using ODBC. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. So now let’s get started.</p>\n<a id=\"more\"></a>\n<h5 id=\"SYSTEM-PREPARATION\"><a href=\"#SYSTEM-PREPARATION\" class=\"headerlink\" title=\"SYSTEM PREPARATION\"></a>SYSTEM PREPARATION</h5><p>Before you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft 13.1 ODBC driver for SQL Server available at this <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/linux/microsoft-odbc-driver-for-sql-server-on-linux\">link</a>. Follow the instructions, and the driver will be installed automatically. On Ubuntu its written to a folder under /opt/microsoft/msodbcsql. With this version the ODBC driver no longer depends on custom packaging for the unixODBC driver manager as was required in prior versions. Now open R (either command line or in RStudio) and install the RODBC package with the command:</p>\n<p>install.packages(“RODBC”)</p>\n<h5 id=\"USE-ODBC-TO-CONNECT-TO-SQL-SERVER\"><a href=\"#USE-ODBC-TO-CONNECT-TO-SQL-SERVER\" class=\"headerlink\" title=\"USE ODBC TO CONNECT TO SQL SERVER\"></a>USE ODBC TO CONNECT TO SQL SERVER</h5><p>Now that you have the ODBC software installed, lets turn to how to use it. There are two methods for ODBC connections to be setup in Linux, one involves defining the connection in an .odbc.ini file. I prefer the second method which involves less setup, but is also less secure because your user name and password are included in the R script. In your R script file or in R Studio, call the RODBC library and define the driver variable to use:</p>\n<p>library(RODBC)<br>driver &lt;- “ODBC Driver 13 for SQL Server” </p>\n<p>Next you need to define your connection variables. Define a database you want to connect to, the hostname of the server (replace localhost with your server name if SQL Server is running somewhere else than your R machine), the port SQL Server is listening on  (1433 by default) and your user name and password. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The results should look like this:</p>\n<p>db &lt;- “master”<br>host &lt;- “localhost”<br>port &lt;-“1433”<br>user &lt;-“username”<br>pwd &lt;- “password”</p>\n<p>Next we need to pull all of those variables together to make our connection string:</p>\n<p>conn &lt;- paste(“DRIVER=”,driver,<br>              “;Database=”,db,<br>              “;Server=”,host,<br>              “;Port=”,port,<br>              “;PROTOCOL=TCPIP”,<br>              “;UID=”, user,<br>              “;PWD=”,pwd,<br>               sep=””)</p>\n<p>This results in a connection string like this:</p>\n<p>“DRIVER=ODBC Driver 13 for SQL Server;Database=master;Server=localhost;Port=1433;PROTOCOL=TCPIP;UID=username;PWD=password</p>\n<p>With our connection built, we can pass it to the odbcDriverConnect function in R to open our connection:</p>\n<p>con1 &lt;- odbcDriverConnect(conn)</p>\n<p>If all goes well, R will return to the next line prompt. Define a new variable to hold the results of your query and pass the query to the database using the sqlQuery function in R with the connection defined previously:</p>\n<p>res &lt;- sqlQuery(con1, ‘SELECT name, database_id, create_date FROM sys.databases ;’)</p>\n<p>To show the results just call the query results variable:</p>\n<p>res</p>\n<p>The results will look like this: <a href=\"http://edpflager.com/?attachment_id=3564#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/res-300x97.png\"></a>       Finally if we are finished with using this database, be sure to close the connection:</p>\n<p>odbcCloseAll()</p>\n<p>That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3545#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/Rlogo-150x150.png\"></a>This is my second article on using Microsoft’s new Linux version of SQL Server with R. This time, I’ll cover how to use RODBC to gather data from SQL Server. As a bit of background, over the past few months, I have been working to learn <a href=\"https://www.r-project.org/\">R, a free software environment</a> for statistical computing. Its been gaining popularity over the past few years, and Microsoft just gave it a huge boost by integrating R into their Power BI visualization software and in the Windows version of SQL Server 2016. Since a good deal of my work involves connecting to Microsoft SQL Servers  its a good opportunity to show how to connect to a SQL Server installed on Ubuntu from R using ODBC. For this tutorial, I am going to assume that you already have R installed. For my purposes,  I am running R on the same Ubuntu machine as the SQL Server. If you need instructions for installing SQL Server on Linux, Microsoft has provided a <a href=\"https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu\">write-up already</a>. So now let’s get started.</p>","more":"<h5 id=\"SYSTEM-PREPARATION\"><a href=\"#SYSTEM-PREPARATION\" class=\"headerlink\" title=\"SYSTEM PREPARATION\"></a>SYSTEM PREPARATION</h5><p>Before you can connect to SQL Server in R, you need to do some system preparation. First download the Microsoft 13.1 ODBC driver for SQL Server available at this <a href=\"https://docs.microsoft.com/en-us/sql/connect/odbc/linux/microsoft-odbc-driver-for-sql-server-on-linux\">link</a>. Follow the instructions, and the driver will be installed automatically. On Ubuntu its written to a folder under /opt/microsoft/msodbcsql. With this version the ODBC driver no longer depends on custom packaging for the unixODBC driver manager as was required in prior versions. Now open R (either command line or in RStudio) and install the RODBC package with the command:</p>\n<p>install.packages(“RODBC”)</p>\n<h5 id=\"USE-ODBC-TO-CONNECT-TO-SQL-SERVER\"><a href=\"#USE-ODBC-TO-CONNECT-TO-SQL-SERVER\" class=\"headerlink\" title=\"USE ODBC TO CONNECT TO SQL SERVER\"></a>USE ODBC TO CONNECT TO SQL SERVER</h5><p>Now that you have the ODBC software installed, lets turn to how to use it. There are two methods for ODBC connections to be setup in Linux, one involves defining the connection in an .odbc.ini file. I prefer the second method which involves less setup, but is also less secure because your user name and password are included in the R script. In your R script file or in R Studio, call the RODBC library and define the driver variable to use:</p>\n<p>library(RODBC)<br>driver &lt;- “ODBC Driver 13 for SQL Server” </p>\n<p>Next you need to define your connection variables. Define a database you want to connect to, the hostname of the server (replace localhost with your server name if SQL Server is running somewhere else than your R machine), the port SQL Server is listening on  (1433 by default) and your user name and password. In this instance we are assuming that we are using a local SQL account and not an Active Directory account to connect to SQL Server. The results should look like this:</p>\n<p>db &lt;- “master”<br>host &lt;- “localhost”<br>port &lt;-“1433”<br>user &lt;-“username”<br>pwd &lt;- “password”</p>\n<p>Next we need to pull all of those variables together to make our connection string:</p>\n<p>conn &lt;- paste(“DRIVER=”,driver,<br>              “;Database=”,db,<br>              “;Server=”,host,<br>              “;Port=”,port,<br>              “;PROTOCOL=TCPIP”,<br>              “;UID=”, user,<br>              “;PWD=”,pwd,<br>               sep=””)</p>\n<p>This results in a connection string like this:</p>\n<p>“DRIVER=ODBC Driver 13 for SQL Server;Database=master;Server=localhost;Port=1433;PROTOCOL=TCPIP;UID=username;PWD=password</p>\n<p>With our connection built, we can pass it to the odbcDriverConnect function in R to open our connection:</p>\n<p>con1 &lt;- odbcDriverConnect(conn)</p>\n<p>If all goes well, R will return to the next line prompt. Define a new variable to hold the results of your query and pass the query to the database using the sqlQuery function in R with the connection defined previously:</p>\n<p>res &lt;- sqlQuery(con1, ‘SELECT name, database_id, create_date FROM sys.databases ;’)</p>\n<p>To show the results just call the query results variable:</p>\n<p>res</p>\n<p>The results will look like this: <a href=\"http://edpflager.com/?attachment_id=3564#main\"><img src=\"http://edpflager.com/wp-content/uploads/2017/04/res-300x97.png\"></a>       Finally if we are finished with using this database, be sure to close the connection:</p>\n<p>odbcCloseAll()</p>\n<p>That’s it!</p>"},{"title":"Use MS SQL Server with Pentaho Data Integrator","id":"2130","comments":0,"date":"2014-06-13T17:33:38.000Z","_content":"\n[![etl](http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png)](http://edpflager.com/wp-content/uploads/2013/08/etl.png)Being an open-source tool, I'm sure a large number of Pentaho Data Integrator (Kettle) users are working with open-source database systems like MySQL, MariaDB, Hadoop, MongoDB or PostgreSQL. But it also works well with with some of the Big Boys of the closed-source database world too. In my case, I often use Kettle to connect to Microsoft SQL Server and I know there are some gotchas you need to overcome to get it to work, especially if you are using Kettle on Mac OSX or Linux.\n<!-- more -->\n###### Gotchas\n\n1.  For better performance whenever possible use a JDBC (java database connector) driver instead of an ODBC (open database connector) driver. Because Kettle is a Java application, it works better with JDBC. So where do you get a JDBC driver for SQL Server? Currently the official Microsoft version is available [here](http://msdn.microsoft.com/en-us/data/aa937724.aspx). From the Microsoft wesbite, click the Download link, and get the sqljdbc\\_4.0.2206.100\\_enu.tar.gz file.Extract the contents, and look for the sqljdbc4.jar file in the root of the extracted folder.  On Mac OSX, copy that file to the data-integrationlib folder. On my Mac that path is under ApplicationsPentahodata-integrationlib. On Linux, it will depend on where you installed Kettle. In my case its currently under my home folder, and then data-integrationlib as well. If you are using the PDI server, you will also need to copy the jar file to the   /pentaho/server/data-integration-server/tomcat/webapps/pentaho-di/WEB-INF/lib/ folder.\n2.  If the Microsoft Windows server that SQL Server is running on has the Windows Firewall turned on, an incoming rule needs to be in place to allow connections to the SQL Server. Generally if the server is in production use, this will be  already done, but if not, it will need to be setup (for instance a new SQL installation or a development server). Microsoft has documentation for this on their [website](http://msdn.microsoft.com/en-us/library/ms175043(v=sql.110).aspx) if you are the server administrator, or contact your server administrator to set it up for you.\n3.  The account you use to connect to the database is dependent on how SQL Server authentication was configured. SQL Server can use:\n\n*   Active Directory (AD) authentication - user IDs and passwords are checked against an external repository of users tied to your network, or\n*   Mixed Mode - locally defined user accounts on the server and AD authentication. I will show you how to setup a connection to a SQL server setup in Mixed Mode in this article. Configuring for AD authentication is a more involved process, and will be covered in a future article.To follow this tutorial, have the SQL Administrator create a local user account on the SQL Server instance with read permissions to the database(s) that you need to access with Kettle.\n\n###### Configuration\n\n1.  To create your database connection in the Spoon GUI, start a new transformation (File ->New ->Transformation).\n2.  Switch to the View tab in the left panel and right click on Database Connections. Click New in the menu that appears.\n3.  The Database Connection window will appear. Enter a Connection Name in the top field.\n4.  Select MS SQL Server in the Connection Type area.\n5.  In the Access area select MS SQL Server. There are two options here.\n6.  Now you need to populate the various Settings fields.If your DNS is working correctly or you have entered your SQL Server's IP address and name in your Hosts file, you can enter the hostname in the Host Name field. Otherwise just enter the IP address.\n7.  Enter the database name you will be using in that field.\n8.  The Instance Name box can be left empty unless your SQL Server is setup with a distinct instance (your SQL DBA can tell you).\n9.  The port will default to MSSQL's normal one: 1433.\n10.  Finally enter your SQL Server user account and password.\n11.  Your connection screen should look similar to what I have pictured here.[![DBConnection](http://edpflager.com/wp-content/uploads/2014/06/DBConnection-300x279.png)](http://edpflager.com/wp-content/uploads/2014/06/DBConnection.png)\n12.  Click the TEST button and if everything is configured correctly, you should see a small window popup telling you that connection was successful!","source":"_posts/use-ms-sql-server-with-pentaho-data-integrator.md","raw":"---\ntitle: Use MS SQL Server with Pentaho Data Integrator\ntags:\n  - ETL\n  - howto\n  - install\n  - kettle\n  - PDI\n  - technical\nid: '2130'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2014-06-13 13:33:38\n---\n\n[![etl](http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png)](http://edpflager.com/wp-content/uploads/2013/08/etl.png)Being an open-source tool, I'm sure a large number of Pentaho Data Integrator (Kettle) users are working with open-source database systems like MySQL, MariaDB, Hadoop, MongoDB or PostgreSQL. But it also works well with with some of the Big Boys of the closed-source database world too. In my case, I often use Kettle to connect to Microsoft SQL Server and I know there are some gotchas you need to overcome to get it to work, especially if you are using Kettle on Mac OSX or Linux.\n<!-- more -->\n###### Gotchas\n\n1.  For better performance whenever possible use a JDBC (java database connector) driver instead of an ODBC (open database connector) driver. Because Kettle is a Java application, it works better with JDBC. So where do you get a JDBC driver for SQL Server? Currently the official Microsoft version is available [here](http://msdn.microsoft.com/en-us/data/aa937724.aspx). From the Microsoft wesbite, click the Download link, and get the sqljdbc\\_4.0.2206.100\\_enu.tar.gz file.Extract the contents, and look for the sqljdbc4.jar file in the root of the extracted folder.  On Mac OSX, copy that file to the data-integrationlib folder. On my Mac that path is under ApplicationsPentahodata-integrationlib. On Linux, it will depend on where you installed Kettle. In my case its currently under my home folder, and then data-integrationlib as well. If you are using the PDI server, you will also need to copy the jar file to the   /pentaho/server/data-integration-server/tomcat/webapps/pentaho-di/WEB-INF/lib/ folder.\n2.  If the Microsoft Windows server that SQL Server is running on has the Windows Firewall turned on, an incoming rule needs to be in place to allow connections to the SQL Server. Generally if the server is in production use, this will be  already done, but if not, it will need to be setup (for instance a new SQL installation or a development server). Microsoft has documentation for this on their [website](http://msdn.microsoft.com/en-us/library/ms175043(v=sql.110).aspx) if you are the server administrator, or contact your server administrator to set it up for you.\n3.  The account you use to connect to the database is dependent on how SQL Server authentication was configured. SQL Server can use:\n\n*   Active Directory (AD) authentication - user IDs and passwords are checked against an external repository of users tied to your network, or\n*   Mixed Mode - locally defined user accounts on the server and AD authentication. I will show you how to setup a connection to a SQL server setup in Mixed Mode in this article. Configuring for AD authentication is a more involved process, and will be covered in a future article.To follow this tutorial, have the SQL Administrator create a local user account on the SQL Server instance with read permissions to the database(s) that you need to access with Kettle.\n\n###### Configuration\n\n1.  To create your database connection in the Spoon GUI, start a new transformation (File ->New ->Transformation).\n2.  Switch to the View tab in the left panel and right click on Database Connections. Click New in the menu that appears.\n3.  The Database Connection window will appear. Enter a Connection Name in the top field.\n4.  Select MS SQL Server in the Connection Type area.\n5.  In the Access area select MS SQL Server. There are two options here.\n6.  Now you need to populate the various Settings fields.If your DNS is working correctly or you have entered your SQL Server's IP address and name in your Hosts file, you can enter the hostname in the Host Name field. Otherwise just enter the IP address.\n7.  Enter the database name you will be using in that field.\n8.  The Instance Name box can be left empty unless your SQL Server is setup with a distinct instance (your SQL DBA can tell you).\n9.  The port will default to MSSQL's normal one: 1433.\n10.  Finally enter your SQL Server user account and password.\n11.  Your connection screen should look similar to what I have pictured here.[![DBConnection](http://edpflager.com/wp-content/uploads/2014/06/DBConnection-300x279.png)](http://edpflager.com/wp-content/uploads/2014/06/DBConnection.png)\n12.  Click the TEST button and if everything is configured correctly, you should see a small window popup telling you that connection was successful!","slug":"use-ms-sql-server-with-pentaho-data-integrator","published":1,"updated":"2020-08-23T20:54:34.858Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ada00cmsdjx31mfg5j7","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/etl.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png\" alt=\"etl\"></a>Being an open-source tool, I’m sure a large number of Pentaho Data Integrator (Kettle) users are working with open-source database systems like MySQL, MariaDB, Hadoop, MongoDB or PostgreSQL. But it also works well with with some of the Big Boys of the closed-source database world too. In my case, I often use Kettle to connect to Microsoft SQL Server and I know there are some gotchas you need to overcome to get it to work, especially if you are using Kettle on Mac OSX or Linux.</p>\n<a id=\"more\"></a>\n<h6 id=\"Gotchas\"><a href=\"#Gotchas\" class=\"headerlink\" title=\"Gotchas\"></a>Gotchas</h6><ol>\n<li>For better performance whenever possible use a JDBC (java database connector) driver instead of an ODBC (open database connector) driver. Because Kettle is a Java application, it works better with JDBC. So where do you get a JDBC driver for SQL Server? Currently the official Microsoft version is available <a href=\"http://msdn.microsoft.com/en-us/data/aa937724.aspx\">here</a>. From the Microsoft wesbite, click the Download link, and get the sqljdbc_4.0.2206.100_enu.tar.gz file.Extract the contents, and look for the sqljdbc4.jar file in the root of the extracted folder.  On Mac OSX, copy that file to the data-integrationlib folder. On my Mac that path is under ApplicationsPentahodata-integrationlib. On Linux, it will depend on where you installed Kettle. In my case its currently under my home folder, and then data-integrationlib as well. If you are using the PDI server, you will also need to copy the jar file to the   /pentaho/server/data-integration-server/tomcat/webapps/pentaho-di/WEB-INF/lib/ folder.</li>\n<li>If the Microsoft Windows server that SQL Server is running on has the Windows Firewall turned on, an incoming rule needs to be in place to allow connections to the SQL Server. Generally if the server is in production use, this will be  already done, but if not, it will need to be setup (for instance a new SQL installation or a development server). Microsoft has documentation for this on their <a href=\"http://msdn.microsoft.com/en-us/library/ms175043(v=sql.110).aspx\">website</a> if you are the server administrator, or contact your server administrator to set it up for you.</li>\n<li>The account you use to connect to the database is dependent on how SQL Server authentication was configured. SQL Server can use:</li>\n</ol>\n<ul>\n<li>Active Directory (AD) authentication - user IDs and passwords are checked against an external repository of users tied to your network, or</li>\n<li>Mixed Mode - locally defined user accounts on the server and AD authentication. I will show you how to setup a connection to a SQL server setup in Mixed Mode in this article. Configuring for AD authentication is a more involved process, and will be covered in a future article.To follow this tutorial, have the SQL Administrator create a local user account on the SQL Server instance with read permissions to the database(s) that you need to access with Kettle.</li>\n</ul>\n<h6 id=\"Configuration\"><a href=\"#Configuration\" class=\"headerlink\" title=\"Configuration\"></a>Configuration</h6><ol>\n<li>To create your database connection in the Spoon GUI, start a new transformation (File -&gt;New -&gt;Transformation).</li>\n<li>Switch to the View tab in the left panel and right click on Database Connections. Click New in the menu that appears.</li>\n<li>The Database Connection window will appear. Enter a Connection Name in the top field.</li>\n<li>Select MS SQL Server in the Connection Type area.</li>\n<li>In the Access area select MS SQL Server. There are two options here.</li>\n<li>Now you need to populate the various Settings fields.If your DNS is working correctly or you have entered your SQL Server’s IP address and name in your Hosts file, you can enter the hostname in the Host Name field. Otherwise just enter the IP address.</li>\n<li>Enter the database name you will be using in that field.</li>\n<li>The Instance Name box can be left empty unless your SQL Server is setup with a distinct instance (your SQL DBA can tell you).</li>\n<li>The port will default to MSSQL’s normal one: 1433.</li>\n<li>Finally enter your SQL Server user account and password.</li>\n<li>Your connection screen should look similar to what I have pictured here.<a href=\"http://edpflager.com/wp-content/uploads/2014/06/DBConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/DBConnection-300x279.png\" alt=\"DBConnection\"></a></li>\n<li>Click the TEST button and if everything is configured correctly, you should see a small window popup telling you that connection was successful!</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2013/08/etl.png\"><img src=\"http://edpflager.com/wp-content/uploads/2013/08/etl-300x111.png\" alt=\"etl\"></a>Being an open-source tool, I’m sure a large number of Pentaho Data Integrator (Kettle) users are working with open-source database systems like MySQL, MariaDB, Hadoop, MongoDB or PostgreSQL. But it also works well with with some of the Big Boys of the closed-source database world too. In my case, I often use Kettle to connect to Microsoft SQL Server and I know there are some gotchas you need to overcome to get it to work, especially if you are using Kettle on Mac OSX or Linux.</p>","more":"<h6 id=\"Gotchas\"><a href=\"#Gotchas\" class=\"headerlink\" title=\"Gotchas\"></a>Gotchas</h6><ol>\n<li>For better performance whenever possible use a JDBC (java database connector) driver instead of an ODBC (open database connector) driver. Because Kettle is a Java application, it works better with JDBC. So where do you get a JDBC driver for SQL Server? Currently the official Microsoft version is available <a href=\"http://msdn.microsoft.com/en-us/data/aa937724.aspx\">here</a>. From the Microsoft wesbite, click the Download link, and get the sqljdbc_4.0.2206.100_enu.tar.gz file.Extract the contents, and look for the sqljdbc4.jar file in the root of the extracted folder.  On Mac OSX, copy that file to the data-integrationlib folder. On my Mac that path is under ApplicationsPentahodata-integrationlib. On Linux, it will depend on where you installed Kettle. In my case its currently under my home folder, and then data-integrationlib as well. If you are using the PDI server, you will also need to copy the jar file to the   /pentaho/server/data-integration-server/tomcat/webapps/pentaho-di/WEB-INF/lib/ folder.</li>\n<li>If the Microsoft Windows server that SQL Server is running on has the Windows Firewall turned on, an incoming rule needs to be in place to allow connections to the SQL Server. Generally if the server is in production use, this will be  already done, but if not, it will need to be setup (for instance a new SQL installation or a development server). Microsoft has documentation for this on their <a href=\"http://msdn.microsoft.com/en-us/library/ms175043(v=sql.110).aspx\">website</a> if you are the server administrator, or contact your server administrator to set it up for you.</li>\n<li>The account you use to connect to the database is dependent on how SQL Server authentication was configured. SQL Server can use:</li>\n</ol>\n<ul>\n<li>Active Directory (AD) authentication - user IDs and passwords are checked against an external repository of users tied to your network, or</li>\n<li>Mixed Mode - locally defined user accounts on the server and AD authentication. I will show you how to setup a connection to a SQL server setup in Mixed Mode in this article. Configuring for AD authentication is a more involved process, and will be covered in a future article.To follow this tutorial, have the SQL Administrator create a local user account on the SQL Server instance with read permissions to the database(s) that you need to access with Kettle.</li>\n</ul>\n<h6 id=\"Configuration\"><a href=\"#Configuration\" class=\"headerlink\" title=\"Configuration\"></a>Configuration</h6><ol>\n<li>To create your database connection in the Spoon GUI, start a new transformation (File -&gt;New -&gt;Transformation).</li>\n<li>Switch to the View tab in the left panel and right click on Database Connections. Click New in the menu that appears.</li>\n<li>The Database Connection window will appear. Enter a Connection Name in the top field.</li>\n<li>Select MS SQL Server in the Connection Type area.</li>\n<li>In the Access area select MS SQL Server. There are two options here.</li>\n<li>Now you need to populate the various Settings fields.If your DNS is working correctly or you have entered your SQL Server’s IP address and name in your Hosts file, you can enter the hostname in the Host Name field. Otherwise just enter the IP address.</li>\n<li>Enter the database name you will be using in that field.</li>\n<li>The Instance Name box can be left empty unless your SQL Server is setup with a distinct instance (your SQL DBA can tell you).</li>\n<li>The port will default to MSSQL’s normal one: 1433.</li>\n<li>Finally enter your SQL Server user account and password.</li>\n<li>Your connection screen should look similar to what I have pictured here.<a href=\"http://edpflager.com/wp-content/uploads/2014/06/DBConnection.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/06/DBConnection-300x279.png\" alt=\"DBConnection\"></a></li>\n<li>Click the TEST button and if everything is configured correctly, you should see a small window popup telling you that connection was successful!</li>\n</ol>"},{"title":"Using Chrome with Pentaho Report Designer 6","id":"3012","comments":0,"date":"2015-11-07T21:28:24.000Z","_content":"\n[![report](http://edpflager.com/wp-content/uploads/2015/11/report-300x251.jpg)](http://edpflager.com/wp-content/uploads/2015/11/report.jpg)Pentaho's Report Designer (PRD) is a full featured application that allows you to define reports that can be used within the Pentaho BI suite or as stand-alone documents. Output can be in a number of formats: PDF, Excel (XLS or XLSX versions), CSV/TXT, RTF or HTML.  If you would like to do a preview of your report in HTML format and you don't have one of the default supported browsers installed (like me on my Mint laptop), or you would like to use a different default browser, you can tell PRD which browser to use. Open Report Designer, and from the main menu, click on EDIT, and then click the Preferences option at the bottom of the screen.\n<!-- more -->\nThe SETTINGS window will open (not Preferences). On the left side, click the BROWSER button. The Settings window will update and show you browser options, as below. Switch the option from Default Browser to User defined Browser. In the program window you can enter the path to your browser. If you are not sure where exactly the browser is on your system, you can click the Browse button (looks like three dots) and navigate to the folder. On my system Chrome is installed under /usr/bin. Leave the parameters field as it is. On my system the results are pictured below. [![PRD-Browser](http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser-300x227.png)](http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser.png) Click the APPLY button at the bottom right and the window will close. You can test it out by going to the Help menu and choosing the Documentation option. If everything is OK, the browser you specified will open and the Pentaho Help page will be displayed.","source":"_posts/using-chrome-with-pentaho-report-designer-6.md","raw":"---\ntitle: Using Chrome with Pentaho Report Designer 6\ntags:\n  - guides\n  - How-to\n  - Linux\n  - technical\nid: '3012'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2015-11-07 16:28:24\n---\n\n[![report](http://edpflager.com/wp-content/uploads/2015/11/report-300x251.jpg)](http://edpflager.com/wp-content/uploads/2015/11/report.jpg)Pentaho's Report Designer (PRD) is a full featured application that allows you to define reports that can be used within the Pentaho BI suite or as stand-alone documents. Output can be in a number of formats: PDF, Excel (XLS or XLSX versions), CSV/TXT, RTF or HTML.  If you would like to do a preview of your report in HTML format and you don't have one of the default supported browsers installed (like me on my Mint laptop), or you would like to use a different default browser, you can tell PRD which browser to use. Open Report Designer, and from the main menu, click on EDIT, and then click the Preferences option at the bottom of the screen.\n<!-- more -->\nThe SETTINGS window will open (not Preferences). On the left side, click the BROWSER button. The Settings window will update and show you browser options, as below. Switch the option from Default Browser to User defined Browser. In the program window you can enter the path to your browser. If you are not sure where exactly the browser is on your system, you can click the Browse button (looks like three dots) and navigate to the folder. On my system Chrome is installed under /usr/bin. Leave the parameters field as it is. On my system the results are pictured below. [![PRD-Browser](http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser-300x227.png)](http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser.png) Click the APPLY button at the bottom right and the window will close. You can test it out by going to the Help menu and choosing the Documentation option. If everything is OK, the browser you specified will open and the Pentaho Help page will be displayed.","slug":"using-chrome-with-pentaho-report-designer-6","published":1,"updated":"2020-08-23T20:54:34.982Z","layout":"post","photos":[],"link":"","_id":"ckeaq9add00cpsdjxg4n496qj","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/report.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/report-300x251.jpg\" alt=\"report\"></a>Pentaho’s Report Designer (PRD) is a full featured application that allows you to define reports that can be used within the Pentaho BI suite or as stand-alone documents. Output can be in a number of formats: PDF, Excel (XLS or XLSX versions), CSV/TXT, RTF or HTML.  If you would like to do a preview of your report in HTML format and you don’t have one of the default supported browsers installed (like me on my Mint laptop), or you would like to use a different default browser, you can tell PRD which browser to use. Open Report Designer, and from the main menu, click on EDIT, and then click the Preferences option at the bottom of the screen.</p>\n<a id=\"more\"></a>\n<p>The SETTINGS window will open (not Preferences). On the left side, click the BROWSER button. The Settings window will update and show you browser options, as below. Switch the option from Default Browser to User defined Browser. In the program window you can enter the path to your browser. If you are not sure where exactly the browser is on your system, you can click the Browse button (looks like three dots) and navigate to the folder. On my system Chrome is installed under /usr/bin. Leave the parameters field as it is. On my system the results are pictured below. <a href=\"http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser-300x227.png\" alt=\"PRD-Browser\"></a> Click the APPLY button at the bottom right and the window will close. You can test it out by going to the Help menu and choosing the Documentation option. If everything is OK, the browser you specified will open and the Pentaho Help page will be displayed.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/11/report.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/report-300x251.jpg\" alt=\"report\"></a>Pentaho’s Report Designer (PRD) is a full featured application that allows you to define reports that can be used within the Pentaho BI suite or as stand-alone documents. Output can be in a number of formats: PDF, Excel (XLS or XLSX versions), CSV/TXT, RTF or HTML.  If you would like to do a preview of your report in HTML format and you don’t have one of the default supported browsers installed (like me on my Mint laptop), or you would like to use a different default browser, you can tell PRD which browser to use. Open Report Designer, and from the main menu, click on EDIT, and then click the Preferences option at the bottom of the screen.</p>","more":"<p>The SETTINGS window will open (not Preferences). On the left side, click the BROWSER button. The Settings window will update and show you browser options, as below. Switch the option from Default Browser to User defined Browser. In the program window you can enter the path to your browser. If you are not sure where exactly the browser is on your system, you can click the Browse button (looks like three dots) and navigate to the folder. On my system Chrome is installed under /usr/bin. Leave the parameters field as it is. On my system the results are pictured below. <a href=\"http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/11/PRD-Browser-300x227.png\" alt=\"PRD-Browser\"></a> Click the APPLY button at the bottom right and the window will close. You can test it out by going to the Help menu and choosing the Documentation option. If everything is OK, the browser you specified will open and the Pentaho Help page will be displayed.</p>"},{"title":"Using Docker on demand with Linux Mint","id":"3280","comments":0,"date":"2016-05-21T20:33:12.000Z","_content":"\n[![container_shipping](http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png)](http://edpflager.com/?attachment_id=3290#main)If you are like me and work on multiple things on your development system, you don't always want everything running when you start your PC. I've previously covered starting other services on demand, and this time around I'll cover running Docker as needed. Docker has essentially two separate components. There is the Docker daemon (or service) that is configured to start when the system is booted up and there is the Docker CLI that you interact with and your commands are passed to the daemon. The CLI only runs when you specifically call it from the terminal prompt with the DOCKER command. For my purposes, I didn't need or want the daemon running all the time because its a laptop. (If I was using a production system or even a full blown development box, I would prefer to have the daemon always running.) So after installing Docker, I needed to configure it to not start up every time the system starts, and then come up with an easy way to start it as needed.\n<!-- more -->\n##### Docker daemon configuration\n\n1.  Open a terminal prompt and navigate to the **/etc/init** directory.\n2.  Open the Nano editor as root:\n    \n    sudo nano ./docker.conf\n    \n3.  Near the top of the file, find the following line and comment it out by adding a couple of pound symbols (##) in front of it:\n    \n    start on (filesystem and net-device-up IFACE!=lo)\n    \n4.  Save the file and exit.\n5.  At this point if you restart your PC, your Docker daemon shouldn't start up when your system does anymore. But we still need a way to start it on demand.\n\n##### Desktop shortcuts  TO START AND STOP DOCKER\n\nThere are a number of ways to create a desktop shortcut to control the Docker daemon depending on your distro. You can also choose to just open a terminal prompt when you like and execute the start-up command and shutdown commands:\n\nsudo service docker start  OR sudo service docker stop\n\n[![launcher](http://edpflager.com/wp-content/uploads/2016/05/launcher-300x128.png)](http://edpflager.com/?attachment_id=3288#main)I prefer to add a desktop shortcut and to add the command to the menu system on Mint. In Mint 17.3, you can right click the desktop and click the option to **Create a new launcher here.** A Launcher Properties window will appear, like the one pictured here. Fill it in as I have. Before you click OK, you can assign your launcher a unique icon by clicking on the rocket button on the left. You'll be prompted to navigate to where the picture file is you would like to use is stored. Once you have selected it and returned to the Launcher properties, then click OK.\n\n###### For my icons, I created PNG files using the [Docker whale icon](https://www.docker.com/brand-guidelines) and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can't share those here.\n\nYou'll now receive a message asking if you want to add the item to the menu system as well, and informing you that initially it will be placed in an Other category. If you would like it added, click YES, otherwise click NO. If you would like to create an additional launcher to shutdown Docker when you don't need it, repeat the process above and change the name to Docker Down and the command to: **sudo service docker stop** That's it!","source":"_posts/using-docker-on-demand-with-linux-mint.md","raw":"---\ntitle: Using Docker on demand with Linux Mint\ntags:\n  - How-to\n  - howto\n  - Mint\n  - SysAdmin\n  - technical\n  - Ubuntu\nid: '3280'\ncategories:\n  - - Blog\n  - - Docker\n  - - Linux\ncomments: false\ndate: 2016-05-21 16:33:12\n---\n\n[![container_shipping](http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png)](http://edpflager.com/?attachment_id=3290#main)If you are like me and work on multiple things on your development system, you don't always want everything running when you start your PC. I've previously covered starting other services on demand, and this time around I'll cover running Docker as needed. Docker has essentially two separate components. There is the Docker daemon (or service) that is configured to start when the system is booted up and there is the Docker CLI that you interact with and your commands are passed to the daemon. The CLI only runs when you specifically call it from the terminal prompt with the DOCKER command. For my purposes, I didn't need or want the daemon running all the time because its a laptop. (If I was using a production system or even a full blown development box, I would prefer to have the daemon always running.) So after installing Docker, I needed to configure it to not start up every time the system starts, and then come up with an easy way to start it as needed.\n<!-- more -->\n##### Docker daemon configuration\n\n1.  Open a terminal prompt and navigate to the **/etc/init** directory.\n2.  Open the Nano editor as root:\n    \n    sudo nano ./docker.conf\n    \n3.  Near the top of the file, find the following line and comment it out by adding a couple of pound symbols (##) in front of it:\n    \n    start on (filesystem and net-device-up IFACE!=lo)\n    \n4.  Save the file and exit.\n5.  At this point if you restart your PC, your Docker daemon shouldn't start up when your system does anymore. But we still need a way to start it on demand.\n\n##### Desktop shortcuts  TO START AND STOP DOCKER\n\nThere are a number of ways to create a desktop shortcut to control the Docker daemon depending on your distro. You can also choose to just open a terminal prompt when you like and execute the start-up command and shutdown commands:\n\nsudo service docker start  OR sudo service docker stop\n\n[![launcher](http://edpflager.com/wp-content/uploads/2016/05/launcher-300x128.png)](http://edpflager.com/?attachment_id=3288#main)I prefer to add a desktop shortcut and to add the command to the menu system on Mint. In Mint 17.3, you can right click the desktop and click the option to **Create a new launcher here.** A Launcher Properties window will appear, like the one pictured here. Fill it in as I have. Before you click OK, you can assign your launcher a unique icon by clicking on the rocket button on the left. You'll be prompted to navigate to where the picture file is you would like to use is stored. Once you have selected it and returned to the Launcher properties, then click OK.\n\n###### For my icons, I created PNG files using the [Docker whale icon](https://www.docker.com/brand-guidelines) and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can't share those here.\n\nYou'll now receive a message asking if you want to add the item to the menu system as well, and informing you that initially it will be placed in an Other category. If you would like it added, click YES, otherwise click NO. If you would like to create an additional launcher to shutdown Docker when you don't need it, repeat the process above and change the name to Docker Down and the command to: **sudo service docker stop** That's it!","slug":"using-docker-on-demand-with-linux-mint","published":1,"updated":"2020-08-23T20:54:35.038Z","layout":"post","photos":[],"link":"","_id":"ckeaq9adi00ctsdjxgpupc4k9","content":"<p><a href=\"http://edpflager.com/?attachment_id=3290#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png\" alt=\"container_shipping\"></a>If you are like me and work on multiple things on your development system, you don’t always want everything running when you start your PC. I’ve previously covered starting other services on demand, and this time around I’ll cover running Docker as needed. Docker has essentially two separate components. There is the Docker daemon (or service) that is configured to start when the system is booted up and there is the Docker CLI that you interact with and your commands are passed to the daemon. The CLI only runs when you specifically call it from the terminal prompt with the DOCKER command. For my purposes, I didn’t need or want the daemon running all the time because its a laptop. (If I was using a production system or even a full blown development box, I would prefer to have the daemon always running.) So after installing Docker, I needed to configure it to not start up every time the system starts, and then come up with an easy way to start it as needed.</p>\n<a id=\"more\"></a>\n<h5 id=\"Docker-daemon-configuration\"><a href=\"#Docker-daemon-configuration\" class=\"headerlink\" title=\"Docker daemon configuration\"></a>Docker daemon configuration</h5><ol>\n<li><p>Open a terminal prompt and navigate to the <strong>/etc/init</strong> directory.</p>\n</li>\n<li><p>Open the Nano editor as root:</p>\n<p>sudo nano ./docker.conf</p>\n</li>\n<li><p>Near the top of the file, find the following line and comment it out by adding a couple of pound symbols (##) in front of it:</p>\n<p>start on (filesystem and net-device-up IFACE!=lo)</p>\n</li>\n<li><p>Save the file and exit.</p>\n</li>\n<li><p>At this point if you restart your PC, your Docker daemon shouldn’t start up when your system does anymore. But we still need a way to start it on demand.</p>\n</li>\n</ol>\n<h5 id=\"Desktop-shortcuts-TO-START-AND-STOP-DOCKER\"><a href=\"#Desktop-shortcuts-TO-START-AND-STOP-DOCKER\" class=\"headerlink\" title=\"Desktop shortcuts  TO START AND STOP DOCKER\"></a>Desktop shortcuts  TO START AND STOP DOCKER</h5><p>There are a number of ways to create a desktop shortcut to control the Docker daemon depending on your distro. You can also choose to just open a terminal prompt when you like and execute the start-up command and shutdown commands:</p>\n<p>sudo service docker start  OR sudo service docker stop</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3288#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/launcher-300x128.png\" alt=\"launcher\"></a>I prefer to add a desktop shortcut and to add the command to the menu system on Mint. In Mint 17.3, you can right click the desktop and click the option to <strong>Create a new launcher here.</strong> A Launcher Properties window will appear, like the one pictured here. Fill it in as I have. Before you click OK, you can assign your launcher a unique icon by clicking on the rocket button on the left. You’ll be prompted to navigate to where the picture file is you would like to use is stored. Once you have selected it and returned to the Launcher properties, then click OK.</p>\n<h6 id=\"For-my-icons-I-created-PNG-files-using-the-Docker-whale-icon-and-added-a-green-up-arrow-and-a-red-down-arrow-to-the-image-Due-to-Docker-usage-guidelines-I-can’t-share-those-here\"><a href=\"#For-my-icons-I-created-PNG-files-using-the-Docker-whale-icon-and-added-a-green-up-arrow-and-a-red-down-arrow-to-the-image-Due-to-Docker-usage-guidelines-I-can’t-share-those-here\" class=\"headerlink\" title=\"For my icons, I created PNG files using the Docker whale icon and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can’t share those here.\"></a>For my icons, I created PNG files using the <a href=\"https://www.docker.com/brand-guidelines\">Docker whale icon</a> and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can’t share those here.</h6><p>You’ll now receive a message asking if you want to add the item to the menu system as well, and informing you that initially it will be placed in an Other category. If you would like it added, click YES, otherwise click NO. If you would like to create an additional launcher to shutdown Docker when you don’t need it, repeat the process above and change the name to Docker Down and the command to: <strong>sudo service docker stop</strong> That’s it!</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/?attachment_id=3290#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/container_shipping-300x177.png\" alt=\"container_shipping\"></a>If you are like me and work on multiple things on your development system, you don’t always want everything running when you start your PC. I’ve previously covered starting other services on demand, and this time around I’ll cover running Docker as needed. Docker has essentially two separate components. There is the Docker daemon (or service) that is configured to start when the system is booted up and there is the Docker CLI that you interact with and your commands are passed to the daemon. The CLI only runs when you specifically call it from the terminal prompt with the DOCKER command. For my purposes, I didn’t need or want the daemon running all the time because its a laptop. (If I was using a production system or even a full blown development box, I would prefer to have the daemon always running.) So after installing Docker, I needed to configure it to not start up every time the system starts, and then come up with an easy way to start it as needed.</p>","more":"<h5 id=\"Docker-daemon-configuration\"><a href=\"#Docker-daemon-configuration\" class=\"headerlink\" title=\"Docker daemon configuration\"></a>Docker daemon configuration</h5><ol>\n<li><p>Open a terminal prompt and navigate to the <strong>/etc/init</strong> directory.</p>\n</li>\n<li><p>Open the Nano editor as root:</p>\n<p>sudo nano ./docker.conf</p>\n</li>\n<li><p>Near the top of the file, find the following line and comment it out by adding a couple of pound symbols (##) in front of it:</p>\n<p>start on (filesystem and net-device-up IFACE!=lo)</p>\n</li>\n<li><p>Save the file and exit.</p>\n</li>\n<li><p>At this point if you restart your PC, your Docker daemon shouldn’t start up when your system does anymore. But we still need a way to start it on demand.</p>\n</li>\n</ol>\n<h5 id=\"Desktop-shortcuts-TO-START-AND-STOP-DOCKER\"><a href=\"#Desktop-shortcuts-TO-START-AND-STOP-DOCKER\" class=\"headerlink\" title=\"Desktop shortcuts  TO START AND STOP DOCKER\"></a>Desktop shortcuts  TO START AND STOP DOCKER</h5><p>There are a number of ways to create a desktop shortcut to control the Docker daemon depending on your distro. You can also choose to just open a terminal prompt when you like and execute the start-up command and shutdown commands:</p>\n<p>sudo service docker start  OR sudo service docker stop</p>\n<p><a href=\"http://edpflager.com/?attachment_id=3288#main\"><img src=\"http://edpflager.com/wp-content/uploads/2016/05/launcher-300x128.png\" alt=\"launcher\"></a>I prefer to add a desktop shortcut and to add the command to the menu system on Mint. In Mint 17.3, you can right click the desktop and click the option to <strong>Create a new launcher here.</strong> A Launcher Properties window will appear, like the one pictured here. Fill it in as I have. Before you click OK, you can assign your launcher a unique icon by clicking on the rocket button on the left. You’ll be prompted to navigate to where the picture file is you would like to use is stored. Once you have selected it and returned to the Launcher properties, then click OK.</p>\n<h6 id=\"For-my-icons-I-created-PNG-files-using-the-Docker-whale-icon-and-added-a-green-up-arrow-and-a-red-down-arrow-to-the-image-Due-to-Docker-usage-guidelines-I-can’t-share-those-here\"><a href=\"#For-my-icons-I-created-PNG-files-using-the-Docker-whale-icon-and-added-a-green-up-arrow-and-a-red-down-arrow-to-the-image-Due-to-Docker-usage-guidelines-I-can’t-share-those-here\" class=\"headerlink\" title=\"For my icons, I created PNG files using the Docker whale icon and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can’t share those here.\"></a>For my icons, I created PNG files using the <a href=\"https://www.docker.com/brand-guidelines\">Docker whale icon</a> and added a green up arrow and a red down arrow to the image. Due to Docker usage guidelines, I can’t share those here.</h6><p>You’ll now receive a message asking if you want to add the item to the menu system as well, and informing you that initially it will be placed in an Other category. If you would like it added, click YES, otherwise click NO. If you would like to create an additional launcher to shutdown Docker when you don’t need it, repeat the process above and change the name to Docker Down and the command to: <strong>sudo service docker stop</strong> That’s it!</p>"},{"title":"Using MariaDB JDBC with Pentaho Kettle (PDI)","id":"2659","comments":0,"date":"2015-02-15T00:14:53.000Z","_content":"\n[![Mariadb-seal-shaded-browntext-alt](http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png)](http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png)A few weeks ago I got an email from reader Zachary Nielsen asking some questions about using the MariaDB JDBC driver with Pentaho Data Integration (aka PDI or Kettle). He had gotten it working as a JDNI option in PDI but wanted to have MariaDB listed as a database option in the database connection window. I looked into a bit, since I had not worked with the MariaDB JDBC connector, and here is what I found. For those unfamiliar with [MariaDB](http://www.mariadb.org), its a fork of MySQL by the original developers of MySQL who had concerns over the acquisition of MySQL by Oracle. MariaDB can be used as a drop-in replacement for MySQL, and works using the MySQL syntax, ports and tools (MySQL Workbench and MySQL JDBC drivers), but additional functionality is also available if you like. The MariaDB team also released a JDBC driver to work in place of the MySQL one that appears to process faster (although the benchmarks are almost two years old - you mileage may vary). In this part of the series, I'll walk through setting up Pentaho DI to use the MariaDB JDBC driver. I'm still working on implementing the driver on a Pentaho ETL server so that part of the series will come later.\n<!-- more -->\nAs initially installed Pentaho Data Integrator has built in support for a number of a different database platforms, from commercial products like Microsoft SQL Server and DB2 to open source platforms like PostgreSQL and of course MySQL. To actually get them to work, you may need to download a JDBC JAR file driver and install it.  Once you have that done, Pentaho will be able to create a connection to the particular database platform. I've covered previously how to get a [MySQL connection working](http://edpflager.com/?p=1622 \"Updated: Set up a Kettle repository using MySQL\"), but if you refer to use the MariaDB JDBC driver to connect to a MariaDB database instance, the process is a little different, although not difficult.\n\n1.  Download the MariaDB JDBC file from their website [here](https://downloads.mariadb.org/client-java/1.1/). Get the mariadb-java-client-1.1.8.jar file and copy it to the /data-integration/lib folder for Pentaho.\n2.  From the /data-integration/simple-jndi folder, open the jdbc.properties file with a text editor. At the end of the file append these five lines, editing them for your database instance. In my case, the database schema I created was called \"books\" and my server IP address was the one specified. My MariaDB instance was using the default port of 3306 : **        Books/type=javax.sql.DataSource** **        Books/driver=org.mariadb.jdbc.Driver** **        Books/url=jdbc:mariadb://192.168.255.134:3306/books** **        Books/user=edpflager** **        Books/password=password**\n3.  Once you have made those additions, save the file.\n4.  If you have PDI open, close it and restart it. Start a new transformation, and then start to create a new database connection from the View tab on the left side of the PDI window. Supply a name for the connection (Books in my case), and then scroll through the list of connection Types, choosing Generic database. At the bottom of the screen, switch the Access selection to JNDI. Finally click the Test button at the bottom of the screen and you should be rewarded with a successful connection message. You can also click the Explore button to look through the schema on the database server.[![connector](http://edpflager.com/wp-content/uploads/2015/02/connector-300x257.png)](http://edpflager.com/wp-content/uploads/2015/02/connector.png)\n5.  That is it. You can now use the MariaDB driver within your PDI jobs and transformations.\n\nUnfortunately, adding MariaDB as an option in the Connection Type selection is much more involved. From my understanding, each of the options in that panel is tied to underlying application code that defines how to make a connection to a particular database type. In order to add MariaDB as a listed database, the source code for PDI would need to be downloaded and altered to include MariaDB with the additional application code for connecting to MariaDB via ODBC and JDBC. Suggesting that to Pentaho may also get it included in a future version as well.","source":"_posts/using-mariadb-jdbc-with-pentaho-kettle-pdi.md","raw":"---\ntitle: Using MariaDB JDBC with Pentaho Kettle (PDI)\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - PDI\n  - technical\nid: '2659'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2015-02-14 19:14:53\n---\n\n[![Mariadb-seal-shaded-browntext-alt](http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png)](http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png)A few weeks ago I got an email from reader Zachary Nielsen asking some questions about using the MariaDB JDBC driver with Pentaho Data Integration (aka PDI or Kettle). He had gotten it working as a JDNI option in PDI but wanted to have MariaDB listed as a database option in the database connection window. I looked into a bit, since I had not worked with the MariaDB JDBC connector, and here is what I found. For those unfamiliar with [MariaDB](http://www.mariadb.org), its a fork of MySQL by the original developers of MySQL who had concerns over the acquisition of MySQL by Oracle. MariaDB can be used as a drop-in replacement for MySQL, and works using the MySQL syntax, ports and tools (MySQL Workbench and MySQL JDBC drivers), but additional functionality is also available if you like. The MariaDB team also released a JDBC driver to work in place of the MySQL one that appears to process faster (although the benchmarks are almost two years old - you mileage may vary). In this part of the series, I'll walk through setting up Pentaho DI to use the MariaDB JDBC driver. I'm still working on implementing the driver on a Pentaho ETL server so that part of the series will come later.\n<!-- more -->\nAs initially installed Pentaho Data Integrator has built in support for a number of a different database platforms, from commercial products like Microsoft SQL Server and DB2 to open source platforms like PostgreSQL and of course MySQL. To actually get them to work, you may need to download a JDBC JAR file driver and install it.  Once you have that done, Pentaho will be able to create a connection to the particular database platform. I've covered previously how to get a [MySQL connection working](http://edpflager.com/?p=1622 \"Updated: Set up a Kettle repository using MySQL\"), but if you refer to use the MariaDB JDBC driver to connect to a MariaDB database instance, the process is a little different, although not difficult.\n\n1.  Download the MariaDB JDBC file from their website [here](https://downloads.mariadb.org/client-java/1.1/). Get the mariadb-java-client-1.1.8.jar file and copy it to the /data-integration/lib folder for Pentaho.\n2.  From the /data-integration/simple-jndi folder, open the jdbc.properties file with a text editor. At the end of the file append these five lines, editing them for your database instance. In my case, the database schema I created was called \"books\" and my server IP address was the one specified. My MariaDB instance was using the default port of 3306 : **        Books/type=javax.sql.DataSource** **        Books/driver=org.mariadb.jdbc.Driver** **        Books/url=jdbc:mariadb://192.168.255.134:3306/books** **        Books/user=edpflager** **        Books/password=password**\n3.  Once you have made those additions, save the file.\n4.  If you have PDI open, close it and restart it. Start a new transformation, and then start to create a new database connection from the View tab on the left side of the PDI window. Supply a name for the connection (Books in my case), and then scroll through the list of connection Types, choosing Generic database. At the bottom of the screen, switch the Access selection to JNDI. Finally click the Test button at the bottom of the screen and you should be rewarded with a successful connection message. You can also click the Explore button to look through the schema on the database server.[![connector](http://edpflager.com/wp-content/uploads/2015/02/connector-300x257.png)](http://edpflager.com/wp-content/uploads/2015/02/connector.png)\n5.  That is it. You can now use the MariaDB driver within your PDI jobs and transformations.\n\nUnfortunately, adding MariaDB as an option in the Connection Type selection is much more involved. From my understanding, each of the options in that panel is tied to underlying application code that defines how to make a connection to a particular database type. In order to add MariaDB as a listed database, the source code for PDI would need to be downloaded and altered to include MariaDB with the additional application code for connecting to MariaDB via ODBC and JDBC. Suggesting that to Pentaho may also get it included in a future version as well.","slug":"using-mariadb-jdbc-with-pentaho-kettle-pdi","published":1,"updated":"2020-08-23T20:54:34.922Z","layout":"post","photos":[],"link":"","_id":"ckeaq9adj00cwsdjx9lvg0dcd","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png\" alt=\"Mariadb-seal-shaded-browntext-alt\"></a>A few weeks ago I got an email from reader Zachary Nielsen asking some questions about using the MariaDB JDBC driver with Pentaho Data Integration (aka PDI or Kettle). He had gotten it working as a JDNI option in PDI but wanted to have MariaDB listed as a database option in the database connection window. I looked into a bit, since I had not worked with the MariaDB JDBC connector, and here is what I found. For those unfamiliar with <a href=\"http://www.mariadb.org/\">MariaDB</a>, its a fork of MySQL by the original developers of MySQL who had concerns over the acquisition of MySQL by Oracle. MariaDB can be used as a drop-in replacement for MySQL, and works using the MySQL syntax, ports and tools (MySQL Workbench and MySQL JDBC drivers), but additional functionality is also available if you like. The MariaDB team also released a JDBC driver to work in place of the MySQL one that appears to process faster (although the benchmarks are almost two years old - you mileage may vary). In this part of the series, I’ll walk through setting up Pentaho DI to use the MariaDB JDBC driver. I’m still working on implementing the driver on a Pentaho ETL server so that part of the series will come later.</p>\n<a id=\"more\"></a>\n<p>As initially installed Pentaho Data Integrator has built in support for a number of a different database platforms, from commercial products like Microsoft SQL Server and DB2 to open source platforms like PostgreSQL and of course MySQL. To actually get them to work, you may need to download a JDBC JAR file driver and install it.  Once you have that done, Pentaho will be able to create a connection to the particular database platform. I’ve covered previously how to get a <a href=\"http://edpflager.com/?p=1622\" title=\"Updated: Set up a Kettle repository using MySQL\">MySQL connection working</a>, but if you refer to use the MariaDB JDBC driver to connect to a MariaDB database instance, the process is a little different, although not difficult.</p>\n<ol>\n<li>Download the MariaDB JDBC file from their website <a href=\"https://downloads.mariadb.org/client-java/1.1/\">here</a>. Get the mariadb-java-client-1.1.8.jar file and copy it to the /data-integration/lib folder for Pentaho.</li>\n<li>From the /data-integration/simple-jndi folder, open the jdbc.properties file with a text editor. At the end of the file append these five lines, editing them for your database instance. In my case, the database schema I created was called “books” and my server IP address was the one specified. My MariaDB instance was using the default port of 3306 : **        Books/type=javax.sql.DataSource** **        Books/driver=org.mariadb.jdbc.Driver** **        Books/url=jdbc:mariadb://192.168.255.134:3306/books** **        Books/user=edpflager** **        Books/password=password**</li>\n<li>Once you have made those additions, save the file.</li>\n<li>If you have PDI open, close it and restart it. Start a new transformation, and then start to create a new database connection from the View tab on the left side of the PDI window. Supply a name for the connection (Books in my case), and then scroll through the list of connection Types, choosing Generic database. At the bottom of the screen, switch the Access selection to JNDI. Finally click the Test button at the bottom of the screen and you should be rewarded with a successful connection message. You can also click the Explore button to look through the schema on the database server.<a href=\"http://edpflager.com/wp-content/uploads/2015/02/connector.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/02/connector-300x257.png\" alt=\"connector\"></a></li>\n<li>That is it. You can now use the MariaDB driver within your PDI jobs and transformations.</li>\n</ol>\n<p>Unfortunately, adding MariaDB as an option in the Connection Type selection is much more involved. From my understanding, each of the options in that panel is tied to underlying application code that defines how to make a connection to a particular database type. In order to add MariaDB as a listed database, the source code for PDI would need to be downloaded and altered to include MariaDB with the additional application code for connecting to MariaDB via ODBC and JDBC. Suggesting that to Pentaho may also get it included in a future version as well.</p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/02/Mariadb-seal-shaded-browntext-alt.png\" alt=\"Mariadb-seal-shaded-browntext-alt\"></a>A few weeks ago I got an email from reader Zachary Nielsen asking some questions about using the MariaDB JDBC driver with Pentaho Data Integration (aka PDI or Kettle). He had gotten it working as a JDNI option in PDI but wanted to have MariaDB listed as a database option in the database connection window. I looked into a bit, since I had not worked with the MariaDB JDBC connector, and here is what I found. For those unfamiliar with <a href=\"http://www.mariadb.org/\">MariaDB</a>, its a fork of MySQL by the original developers of MySQL who had concerns over the acquisition of MySQL by Oracle. MariaDB can be used as a drop-in replacement for MySQL, and works using the MySQL syntax, ports and tools (MySQL Workbench and MySQL JDBC drivers), but additional functionality is also available if you like. The MariaDB team also released a JDBC driver to work in place of the MySQL one that appears to process faster (although the benchmarks are almost two years old - you mileage may vary). In this part of the series, I’ll walk through setting up Pentaho DI to use the MariaDB JDBC driver. I’m still working on implementing the driver on a Pentaho ETL server so that part of the series will come later.</p>","more":"<p>As initially installed Pentaho Data Integrator has built in support for a number of a different database platforms, from commercial products like Microsoft SQL Server and DB2 to open source platforms like PostgreSQL and of course MySQL. To actually get them to work, you may need to download a JDBC JAR file driver and install it.  Once you have that done, Pentaho will be able to create a connection to the particular database platform. I’ve covered previously how to get a <a href=\"http://edpflager.com/?p=1622\" title=\"Updated: Set up a Kettle repository using MySQL\">MySQL connection working</a>, but if you refer to use the MariaDB JDBC driver to connect to a MariaDB database instance, the process is a little different, although not difficult.</p>\n<ol>\n<li>Download the MariaDB JDBC file from their website <a href=\"https://downloads.mariadb.org/client-java/1.1/\">here</a>. Get the mariadb-java-client-1.1.8.jar file and copy it to the /data-integration/lib folder for Pentaho.</li>\n<li>From the /data-integration/simple-jndi folder, open the jdbc.properties file with a text editor. At the end of the file append these five lines, editing them for your database instance. In my case, the database schema I created was called “books” and my server IP address was the one specified. My MariaDB instance was using the default port of 3306 : **        Books/type=javax.sql.DataSource** **        Books/driver=org.mariadb.jdbc.Driver** **        Books/url=jdbc:mariadb://192.168.255.134:3306/books** **        Books/user=edpflager** **        Books/password=password**</li>\n<li>Once you have made those additions, save the file.</li>\n<li>If you have PDI open, close it and restart it. Start a new transformation, and then start to create a new database connection from the View tab on the left side of the PDI window. Supply a name for the connection (Books in my case), and then scroll through the list of connection Types, choosing Generic database. At the bottom of the screen, switch the Access selection to JNDI. Finally click the Test button at the bottom of the screen and you should be rewarded with a successful connection message. You can also click the Explore button to look through the schema on the database server.<a href=\"http://edpflager.com/wp-content/uploads/2015/02/connector.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/02/connector-300x257.png\" alt=\"connector\"></a></li>\n<li>That is it. You can now use the MariaDB driver within your PDI jobs and transformations.</li>\n</ol>\n<p>Unfortunately, adding MariaDB as an option in the Connection Type selection is much more involved. From my understanding, each of the options in that panel is tied to underlying application code that defines how to make a connection to a particular database type. In order to add MariaDB as a listed database, the source code for PDI would need to be downloaded and altered to include MariaDB with the additional application code for connecting to MariaDB via ODBC and JDBC. Suggesting that to Pentaho may also get it included in a future version as well.</p>"},{"title":"Using Pentaho Table Output and Update together","id":"2449","comments":0,"date":"2014-09-27T14:38:05.000Z","_content":"\nSeveral weeks back, I posted a tutorial on how to use the [Update/Insert function](http://edpflager.com/?p=1818 \"Getting started with Kettle’s Insert/Update function\") in Pentaho Data Integration (PDI aka Kettle). Recently at work, I had an occasion to revisit the Update/Insert because a workflow using it was not getting all of the updated records. By switching it out with a workflow like that illustrated here, I was able to improve the runtime minutely. and also to rectify the problem.\n<!-- more -->\nFor a setup, I use the same starting database that the Update/Insert tutorial uses. The instructions are reproduced here for ease of use:\n\n##### **Assumptions and requirements**\n\nFor this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by [Fusheng Wang and Carlo Zaniolo](https://launchpad.net/test-db/) at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly). DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept\\_no CHAR(4) NOT NULL, dept\\_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept\\_no), UNIQUE KEY (dept\\_name) ); INSERT INTO departments VALUES (‘d001′,’Marketing’), (‘d002′,’Finance’), (‘d003′,’Human Resources’), (‘d004′,’Produktion’), (‘d005′,’Development’); Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department. dept\\_no;dept\\_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service\n\n##### USING TABLE OUTPUT AND UPDATE\n\n1.  Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.\n2.  Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.\n3.  Double click the Text file input step, and on the first tab – File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table.[![inputfilelocation](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg)](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg)\n4.  At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:[![showfile](http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg)](http://edpflager.com/wp-content/uploads/2014/02/showfile.jpg)\n5.  Close the Preview Data window and the Text File Input step.\n6.  From the Output node drag a Table Output step. Create a normal hop between the two steps.\n7.  Open the Table Output step and supply the connection information for the MySQL database that was created earlier in this tutorial. Leave the commit size set to the default value. Make sure the Truncate Table box is unchecked. Check the Specify database fields box.[![tableoutput1](http://edpflager.com/wp-content/uploads/2014/09/tableoutput1-300x251.png)](http://edpflager.com/wp-content/uploads/2014/09/tableoutput1.png)\n8.  Switch to the Database Fields tab, and click the Get Fields button on the right side. The Fields to Insert grid should populate with the table name and stream field information.[![tableoutput2](http://edpflager.com/wp-content/uploads/2014/09/tableoutput2-300x250.png)](http://edpflager.com/wp-content/uploads/2014/09/tableoutput2.png)\n9.  Click OK to exit the window.\n10.  Drag an Update step from the Output node. Start to create a hop between the Table Output step and the Output node, and when the output type menu appears, choose Error Handling of Step.[![stepmenu](http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png)](http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png)\n11.  The hop will be created between the two steps, but it will be a dotted red line with a red X appearing on the hop. This denotes its an error handling step.[![errorhop](http://edpflager.com/wp-content/uploads/2014/09/errorhop-300x69.png)](http://edpflager.com/wp-content/uploads/2014/09/errorhop.png)\n12.  Open the Update Step, and enter the connection information to the sample database and the departments table. Click the Get fields button next to the key(s) to look up the value(s) grid. Both of the fields from the departments table will be entered. Since the primary key is only the dept\\_no field, delete the dept\\_name line.\n13.  To the right of the Update fields grid, click the Get Update fields button. Again the grid will be populated with both columns from the departments table. Because the update should not be updating the PK, remove the dept\\_no field from the grid.[![update](http://edpflager.com/wp-content/uploads/2014/09/update-275x300.png)](http://edpflager.com/wp-content/uploads/2014/09/update.png)\n14.  Click OK to close the update window, and save the transformation. Click the RUN button.The transformation should process very quickly, and the metrics window should appear similar to this:[![metrics](http://edpflager.com/wp-content/uploads/2014/09/metrics-300x52.png)](http://edpflager.com/wp-content/uploads/2014/09/metrics.png)\n15.  Notice that the Output step shows 4 records were Output and 1 was Rejected. The four new records were added to the deparments table, and one record that was already in the database was passed to the Error handling step. There the record was updated.\n16.  Switching back to the MySQL console and doing a Select on the table will verify the results. Four new departments were added, and the spelling for the Production department was corrected.\n\nPentaho, Kettle and PDI are trademarks of Pentaho LLC.","source":"_posts/using-pentaho-table-output-and-update-together.md","raw":"---\ntitle: Using Pentaho Table Output and Update together\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - Linux\n  - PDI\n  - technical\nid: '2449'\ncategories:\n  - - Blog\n  - - Business Intelligence\n  - - Pentaho\ncomments: false\ndate: 2014-09-27 10:38:05\n---\n\nSeveral weeks back, I posted a tutorial on how to use the [Update/Insert function](http://edpflager.com/?p=1818 \"Getting started with Kettle’s Insert/Update function\") in Pentaho Data Integration (PDI aka Kettle). Recently at work, I had an occasion to revisit the Update/Insert because a workflow using it was not getting all of the updated records. By switching it out with a workflow like that illustrated here, I was able to improve the runtime minutely. and also to rectify the problem.\n<!-- more -->\nFor a setup, I use the same starting database that the Update/Insert tutorial uses. The instructions are reproduced here for ease of use:\n\n##### **Assumptions and requirements**\n\nFor this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by [Fusheng Wang and Carlo Zaniolo](https://launchpad.net/test-db/) at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly). DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept\\_no CHAR(4) NOT NULL, dept\\_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept\\_no), UNIQUE KEY (dept\\_name) ); INSERT INTO departments VALUES (‘d001′,’Marketing’), (‘d002′,’Finance’), (‘d003′,’Human Resources’), (‘d004′,’Produktion’), (‘d005′,’Development’); Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department. dept\\_no;dept\\_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service\n\n##### USING TABLE OUTPUT AND UPDATE\n\n1.  Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.\n2.  Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.\n3.  Double click the Text file input step, and on the first tab – File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table.[![inputfilelocation](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg)](http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg)\n4.  At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:[![showfile](http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg)](http://edpflager.com/wp-content/uploads/2014/02/showfile.jpg)\n5.  Close the Preview Data window and the Text File Input step.\n6.  From the Output node drag a Table Output step. Create a normal hop between the two steps.\n7.  Open the Table Output step and supply the connection information for the MySQL database that was created earlier in this tutorial. Leave the commit size set to the default value. Make sure the Truncate Table box is unchecked. Check the Specify database fields box.[![tableoutput1](http://edpflager.com/wp-content/uploads/2014/09/tableoutput1-300x251.png)](http://edpflager.com/wp-content/uploads/2014/09/tableoutput1.png)\n8.  Switch to the Database Fields tab, and click the Get Fields button on the right side. The Fields to Insert grid should populate with the table name and stream field information.[![tableoutput2](http://edpflager.com/wp-content/uploads/2014/09/tableoutput2-300x250.png)](http://edpflager.com/wp-content/uploads/2014/09/tableoutput2.png)\n9.  Click OK to exit the window.\n10.  Drag an Update step from the Output node. Start to create a hop between the Table Output step and the Output node, and when the output type menu appears, choose Error Handling of Step.[![stepmenu](http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png)](http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png)\n11.  The hop will be created between the two steps, but it will be a dotted red line with a red X appearing on the hop. This denotes its an error handling step.[![errorhop](http://edpflager.com/wp-content/uploads/2014/09/errorhop-300x69.png)](http://edpflager.com/wp-content/uploads/2014/09/errorhop.png)\n12.  Open the Update Step, and enter the connection information to the sample database and the departments table. Click the Get fields button next to the key(s) to look up the value(s) grid. Both of the fields from the departments table will be entered. Since the primary key is only the dept\\_no field, delete the dept\\_name line.\n13.  To the right of the Update fields grid, click the Get Update fields button. Again the grid will be populated with both columns from the departments table. Because the update should not be updating the PK, remove the dept\\_no field from the grid.[![update](http://edpflager.com/wp-content/uploads/2014/09/update-275x300.png)](http://edpflager.com/wp-content/uploads/2014/09/update.png)\n14.  Click OK to close the update window, and save the transformation. Click the RUN button.The transformation should process very quickly, and the metrics window should appear similar to this:[![metrics](http://edpflager.com/wp-content/uploads/2014/09/metrics-300x52.png)](http://edpflager.com/wp-content/uploads/2014/09/metrics.png)\n15.  Notice that the Output step shows 4 records were Output and 1 was Rejected. The four new records were added to the deparments table, and one record that was already in the database was passed to the Error handling step. There the record was updated.\n16.  Switching back to the MySQL console and doing a Select on the table will verify the results. Four new departments were added, and the spelling for the Production department was corrected.\n\nPentaho, Kettle and PDI are trademarks of Pentaho LLC.","slug":"using-pentaho-table-output-and-update-together","published":1,"updated":"2020-08-23T20:54:34.898Z","layout":"post","photos":[],"link":"","_id":"ckeaq9adt00d0sdjxd1rbfl99","content":"<p>Several weeks back, I posted a tutorial on how to use the <a href=\"http://edpflager.com/?p=1818\" title=\"Getting started with Kettle’s Insert/Update function\">Update/Insert function</a> in Pentaho Data Integration (PDI aka Kettle). Recently at work, I had an occasion to revisit the Update/Insert because a workflow using it was not getting all of the updated records. By switching it out with a workflow like that illustrated here, I was able to improve the runtime minutely. and also to rectify the problem.</p>\n<a id=\"more\"></a>\n<p>For a setup, I use the same starting database that the Update/Insert tutorial uses. The instructions are reproduced here for ease of use:</p>\n<h5 id=\"Assumptions-and-requirements\"><a href=\"#Assumptions-and-requirements\" class=\"headerlink\" title=\"Assumptions and requirements\"></a><strong>Assumptions and requirements</strong></h5><p>For this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by <a href=\"https://launchpad.net/test-db/\">Fusheng Wang and Carlo Zaniolo</a> at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly). DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept_no CHAR(4) NOT NULL, dept_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept_no), UNIQUE KEY (dept_name) ); INSERT INTO departments VALUES (‘d001′,’Marketing’), (‘d002′,’Finance’), (‘d003′,’Human Resources’), (‘d004′,’Produktion’), (‘d005′,’Development’); Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department. dept_no;dept_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service</p>\n<h5 id=\"USING-TABLE-OUTPUT-AND-UPDATE\"><a href=\"#USING-TABLE-OUTPUT-AND-UPDATE\" class=\"headerlink\" title=\"USING TABLE OUTPUT AND UPDATE\"></a>USING TABLE OUTPUT AND UPDATE</h5><ol>\n<li>Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.</li>\n<li>Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.</li>\n<li>Double click the Text file input step, and on the first tab – File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table.<a href=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg\" alt=\"inputfilelocation\"></a></li>\n<li>At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/02/showfile.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg\" alt=\"showfile\"></a></li>\n<li>Close the Preview Data window and the Text File Input step.</li>\n<li>From the Output node drag a Table Output step. Create a normal hop between the two steps.</li>\n<li>Open the Table Output step and supply the connection information for the MySQL database that was created earlier in this tutorial. Leave the commit size set to the default value. Make sure the Truncate Table box is unchecked. Check the Specify database fields box.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput1-300x251.png\" alt=\"tableoutput1\"></a></li>\n<li>Switch to the Database Fields tab, and click the Get Fields button on the right side. The Fields to Insert grid should populate with the table name and stream field information.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput2-300x250.png\" alt=\"tableoutput2\"></a></li>\n<li>Click OK to exit the window.</li>\n<li>Drag an Update step from the Output node. Start to create a hop between the Table Output step and the Output node, and when the output type menu appears, choose Error Handling of Step.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png\" alt=\"stepmenu\"></a></li>\n<li>The hop will be created between the two steps, but it will be a dotted red line with a red X appearing on the hop. This denotes its an error handling step.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/errorhop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/errorhop-300x69.png\" alt=\"errorhop\"></a></li>\n<li>Open the Update Step, and enter the connection information to the sample database and the departments table. Click the Get fields button next to the key(s) to look up the value(s) grid. Both of the fields from the departments table will be entered. Since the primary key is only the dept_no field, delete the dept_name line.</li>\n<li>To the right of the Update fields grid, click the Get Update fields button. Again the grid will be populated with both columns from the departments table. Because the update should not be updating the PK, remove the dept_no field from the grid.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/update.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/update-275x300.png\" alt=\"update\"></a></li>\n<li>Click OK to close the update window, and save the transformation. Click the RUN button.The transformation should process very quickly, and the metrics window should appear similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2014/09/metrics.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/metrics-300x52.png\" alt=\"metrics\"></a></li>\n<li>Notice that the Output step shows 4 records were Output and 1 was Rejected. The four new records were added to the deparments table, and one record that was already in the database was passed to the Error handling step. There the record was updated.</li>\n<li>Switching back to the MySQL console and doing a Select on the table will verify the results. Four new departments were added, and the spelling for the Production department was corrected.</li>\n</ol>\n<p>Pentaho, Kettle and PDI are trademarks of Pentaho LLC.</p>\n","site":{"data":{}},"excerpt":"<p>Several weeks back, I posted a tutorial on how to use the <a href=\"http://edpflager.com/?p=1818\" title=\"Getting started with Kettle’s Insert/Update function\">Update/Insert function</a> in Pentaho Data Integration (PDI aka Kettle). Recently at work, I had an occasion to revisit the Update/Insert because a workflow using it was not getting all of the updated records. By switching it out with a workflow like that illustrated here, I was able to improve the runtime minutely. and also to rectify the problem.</p>","more":"<p>For a setup, I use the same starting database that the Update/Insert tutorial uses. The instructions are reproduced here for ease of use:</p>\n<h5 id=\"Assumptions-and-requirements\"><a href=\"#Assumptions-and-requirements\" class=\"headerlink\" title=\"Assumptions and requirements\"></a><strong>Assumptions and requirements</strong></h5><p>For this tutorial, I am assuming you have access to a MySQL (or MariaDB) database server. We’ll be creating a a sample database based on one originally created by <a href=\"https://launchpad.net/test-db/\">Fusheng Wang and Carlo Zaniolo</a> at Siemens Corporate Research. For our purposes we only need one table with a small amount of data. Copy the script below and save it as a  SQL file on your system. Run it in MySQL to create the database and populate the table (Yes Production is spelling incorrectly). DROP DATABASE IF EXISTS sample; CREATE DATABASE IF NOT EXISTS sample; USE sample; CREATE TABLE departments ( dept_no CHAR(4) NOT NULL, dept_name VARCHAR(40) NOT NULL, PRIMARY KEY (dept_no), UNIQUE KEY (dept_name) ); INSERT INTO departments VALUES (‘d001′,’Marketing’), (‘d002′,’Finance’), (‘d003′,’Human Resources’), (‘d004′,’Produktion’), (‘d005′,’Development’); Once you have it loaded,  do a select on your departments table, and it should return five records. Now copy the lines below into a text file using a basic text editor (Notepad, TextEdit, Notepad++, Gedit, etc) and save it as changes.txt to a location you can reach from within Pentaho. Notice we are adding four departments and correcting the spelling on the Production department. dept_no;dept_name d006;Quality Management d007;Sales d008;Research d004;Production d009;Customer Service</p>\n<h5 id=\"USING-TABLE-OUTPUT-AND-UPDATE\"><a href=\"#USING-TABLE-OUTPUT-AND-UPDATE\" class=\"headerlink\" title=\"USING TABLE OUTPUT AND UPDATE\"></a>USING TABLE OUTPUT AND UPDATE</h5><ol>\n<li>Open up Pentaho and create a new database connection to your sample database. Be sure to test the connection to make sure you can access the data.</li>\n<li>Create a new transformation, and from the Design tab, under the Input node, drag a Text File Input step onto the canvas.</li>\n<li>Double click the Text file input step, and on the first tab – File, click the Browse button and navigate to where you saved the changes.txt file, choosing OK on the file location window to be returned to the File tab. Click the Add button to move the file down to the Selected Files table.<a href=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/inputfilelocation-300x57.jpg\" alt=\"inputfilelocation\"></a></li>\n<li>At this point, you can click the Show File Content button at the bottom to see what is in the text file. It should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2014/02/showfile.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2014/02/showfile-300x186.jpg\" alt=\"showfile\"></a></li>\n<li>Close the Preview Data window and the Text File Input step.</li>\n<li>From the Output node drag a Table Output step. Create a normal hop between the two steps.</li>\n<li>Open the Table Output step and supply the connection information for the MySQL database that was created earlier in this tutorial. Leave the commit size set to the default value. Make sure the Truncate Table box is unchecked. Check the Specify database fields box.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput1.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput1-300x251.png\" alt=\"tableoutput1\"></a></li>\n<li>Switch to the Database Fields tab, and click the Get Fields button on the right side. The Fields to Insert grid should populate with the table name and stream field information.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput2.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/tableoutput2-300x250.png\" alt=\"tableoutput2\"></a></li>\n<li>Click OK to exit the window.</li>\n<li>Drag an Update step from the Output node. Start to create a hop between the Table Output step and the Output node, and when the output type menu appears, choose Error Handling of Step.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/stepmenu.png\" alt=\"stepmenu\"></a></li>\n<li>The hop will be created between the two steps, but it will be a dotted red line with a red X appearing on the hop. This denotes its an error handling step.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/errorhop.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/errorhop-300x69.png\" alt=\"errorhop\"></a></li>\n<li>Open the Update Step, and enter the connection information to the sample database and the departments table. Click the Get fields button next to the key(s) to look up the value(s) grid. Both of the fields from the departments table will be entered. Since the primary key is only the dept_no field, delete the dept_name line.</li>\n<li>To the right of the Update fields grid, click the Get Update fields button. Again the grid will be populated with both columns from the departments table. Because the update should not be updating the PK, remove the dept_no field from the grid.<a href=\"http://edpflager.com/wp-content/uploads/2014/09/update.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/update-275x300.png\" alt=\"update\"></a></li>\n<li>Click OK to close the update window, and save the transformation. Click the RUN button.The transformation should process very quickly, and the metrics window should appear similar to this:<a href=\"http://edpflager.com/wp-content/uploads/2014/09/metrics.png\"><img src=\"http://edpflager.com/wp-content/uploads/2014/09/metrics-300x52.png\" alt=\"metrics\"></a></li>\n<li>Notice that the Output step shows 4 records were Output and 1 was Rejected. The four new records were added to the deparments table, and one record that was already in the database was passed to the Error handling step. There the record was updated.</li>\n<li>Switching back to the MySQL console and doing a Select on the table will verify the results. Four new departments were added, and the spelling for the Production department was corrected.</li>\n</ol>\n<p>Pentaho, Kettle and PDI are trademarks of Pentaho LLC.</p>"},{"title":"Wait for a File in Pentaho Kettle - Control Flow components - Part 2","id":"2727","comments":0,"date":"2015-05-02T23:07:51.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2015/04/clockwait-300x225.jpg)](http://edpflager.com/wp-content/uploads/2015/04/clockwait.jpg) In my [last post](http://edpflager.com/?p=2716) I wrote about checking for the existence of a file as a control mechanism in a Pentaho Data Integration (aka Kettle) job. This time around, I look at a similar component in Kettle, but this one waits for a file for a specified time. It will make repeated checks to see if the file has appeared, pausing between each check. While the two components have similar functionality, they also have some marked differences:\n\n*   \"File Exists\" is most useful at the beginning of a job flow, and the \"Wait for File\" process can be useful in multiple places within a job flow.\n*   \"File Exists\" can access multiple file systems such as HDFS, Amazon's S3 and the local computer and network and  \"Wait for File\" can  only access the local computer and network.\n<!-- more -->\nAs an example, you have an ETL process scheduled to run at a specific time and a file is needed about 75% of the way through to complete the job. Because the exact time when the file will be ready isn't known, you can include a Wait for File component in the job that does a periodic check. If the file isn't found, the process will sleep, and then check again, eventually succeeding or timing out. To create an example using the Wait for File component, follow these instructions:\n\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.\n3.  Open the Utility node, and drag an Abort job component to the canvas.\n4.  Finally, add a Wait for File component from the File Management node.\n5.  Add an unconditional hop between the Start component and Wait for File.\n6.  From Wait for File, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”\n7.  Add another hop from Wait for File to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.![WaitForFile](http://edpflager.com/wp-content/uploads/2015/04/WaitForFile-300x140.png)\n8.  Double click the Wait for File component to configure it. You can supply a name for the Job entry (I suggest using the file name you are looking for – i.e. Wait\\_for\\_file\\_name) in the first text box.\n9.  The second box is where you enter the file name you want to check for. If you want to access a file local to the machine you are working on, just enter the path to it, preceded by “file:///”  (for example “file:///C:/Users/root/file\\_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file\\_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder\\_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.(This assumes you already have a file in the location you will be checking).\n10.  The \"maximum timeout window\" setting allows you to specify in seconds how long you want the process to wait. Enter a zero if you want the process to keep cycling until the file appears, with no timeout. Otherwise for example purposes, enter 30.\n11.  \"Check cycle time\" allows you to set how frequently you want to check for the file, in seconds as well. For our example, lets enter 5.\n12.  Finally there are three checkboxes you can configure. If the job times out, and you want to treat the timeout as a success, check the first box.\n13.  After the file appears, you may want to wait to make sure that the process creating it has completed before your job starts to read it. Mark the File size check box to have the process wait the number of seconds entered in the Check cycle time field. As an example, if you select the File size check box, and have 60 in the check cycle time, once your job sees the file, it will wait an additional 60 seconds before it starts to read it.\n14.  The final checkbox is Add filename to result. This option allows you to add filenames to the internal result files of a transformation so that later job entries can use this information.\n15.  Once you have configured the various options, click on OK. Your job should look like this:[![WFFJob](http://edpflager.com/wp-content/uploads/2015/04/WFFJob-300x134.png)](http://edpflager.com/wp-content/uploads/2015/04/WFFJob.png)\n16.  Save it and run it.\n17.  If you specified a file that does not exist, the job will loop through every five seconds checking for the file. When it reaches your timeout value, it will Abort the job, unless you chose the box to treat a timeout as a success.\n18.  If you specified a file that does exist, the job should find it on the first iteration and proceed to the Success step.","source":"_posts/wait-for-a-file-in-pentaho-kettle-control-flow-components-part-2.md","raw":"---\ntitle: Wait for a File in Pentaho Kettle - Control Flow components - Part 2\ntags:\n  - ETL\n  - How-to\n  - howto\n  - kettle\n  - technical\nid: '2727'\ncategories:\n  - - Blog\n  - - Linux\n  - - Pentaho\ncomments: false\ndate: 2015-05-02 19:07:51\n---\n\n[![](http://edpflager.com/wp-content/uploads/2015/04/clockwait-300x225.jpg)](http://edpflager.com/wp-content/uploads/2015/04/clockwait.jpg) In my [last post](http://edpflager.com/?p=2716) I wrote about checking for the existence of a file as a control mechanism in a Pentaho Data Integration (aka Kettle) job. This time around, I look at a similar component in Kettle, but this one waits for a file for a specified time. It will make repeated checks to see if the file has appeared, pausing between each check. While the two components have similar functionality, they also have some marked differences:\n\n*   \"File Exists\" is most useful at the beginning of a job flow, and the \"Wait for File\" process can be useful in multiple places within a job flow.\n*   \"File Exists\" can access multiple file systems such as HDFS, Amazon's S3 and the local computer and network and  \"Wait for File\" can  only access the local computer and network.\n<!-- more -->\nAs an example, you have an ETL process scheduled to run at a specific time and a file is needed about 75% of the way through to complete the job. Because the exact time when the file will be ready isn't known, you can include a Wait for File component in the job that does a periodic check. If the file isn't found, the process will sleep, and then check again, eventually succeeding or timing out. To create an example using the Wait for File component, follow these instructions:\n\n1.  Start Spoon and create a new job.\n2.  From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.\n3.  Open the Utility node, and drag an Abort job component to the canvas.\n4.  Finally, add a Wait for File component from the File Management node.\n5.  Add an unconditional hop between the Start component and Wait for File.\n6.  From Wait for File, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”\n7.  Add another hop from Wait for File to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.![WaitForFile](http://edpflager.com/wp-content/uploads/2015/04/WaitForFile-300x140.png)\n8.  Double click the Wait for File component to configure it. You can supply a name for the Job entry (I suggest using the file name you are looking for – i.e. Wait\\_for\\_file\\_name) in the first text box.\n9.  The second box is where you enter the file name you want to check for. If you want to access a file local to the machine you are working on, just enter the path to it, preceded by “file:///”  (for example “file:///C:/Users/root/file\\_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file\\_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder\\_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.(This assumes you already have a file in the location you will be checking).\n10.  The \"maximum timeout window\" setting allows you to specify in seconds how long you want the process to wait. Enter a zero if you want the process to keep cycling until the file appears, with no timeout. Otherwise for example purposes, enter 30.\n11.  \"Check cycle time\" allows you to set how frequently you want to check for the file, in seconds as well. For our example, lets enter 5.\n12.  Finally there are three checkboxes you can configure. If the job times out, and you want to treat the timeout as a success, check the first box.\n13.  After the file appears, you may want to wait to make sure that the process creating it has completed before your job starts to read it. Mark the File size check box to have the process wait the number of seconds entered in the Check cycle time field. As an example, if you select the File size check box, and have 60 in the check cycle time, once your job sees the file, it will wait an additional 60 seconds before it starts to read it.\n14.  The final checkbox is Add filename to result. This option allows you to add filenames to the internal result files of a transformation so that later job entries can use this information.\n15.  Once you have configured the various options, click on OK. Your job should look like this:[![WFFJob](http://edpflager.com/wp-content/uploads/2015/04/WFFJob-300x134.png)](http://edpflager.com/wp-content/uploads/2015/04/WFFJob.png)\n16.  Save it and run it.\n17.  If you specified a file that does not exist, the job will loop through every five seconds checking for the file. When it reaches your timeout value, it will Abort the job, unless you chose the box to treat a timeout as a success.\n18.  If you specified a file that does exist, the job should find it on the first iteration and proceed to the Success step.","slug":"wait-for-a-file-in-pentaho-kettle-control-flow-components-part-2","published":1,"updated":"2020-08-23T20:54:34.938Z","layout":"post","photos":[],"link":"","_id":"ckeaq9adu00d3sdjx3wrsdedf","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/clockwait.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/clockwait-300x225.jpg\"></a> In my <a href=\"http://edpflager.com/?p=2716\">last post</a> I wrote about checking for the existence of a file as a control mechanism in a Pentaho Data Integration (aka Kettle) job. This time around, I look at a similar component in Kettle, but this one waits for a file for a specified time. It will make repeated checks to see if the file has appeared, pausing between each check. While the two components have similar functionality, they also have some marked differences:</p>\n<ul>\n<li>“File Exists” is most useful at the beginning of a job flow, and the “Wait for File” process can be useful in multiple places within a job flow.</li>\n<li>“File Exists” can access multiple file systems such as HDFS, Amazon’s S3 and the local computer and network and  “Wait for File” can  only access the local computer and network.<a id=\"more\"></a>\nAs an example, you have an ETL process scheduled to run at a specific time and a file is needed about 75% of the way through to complete the job. Because the exact time when the file will be ready isn’t known, you can include a Wait for File component in the job that does a periodic check. If the file isn’t found, the process will sleep, and then check again, eventually succeeding or timing out. To create an example using the Wait for File component, follow these instructions:</li>\n</ul>\n<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.</li>\n<li>Open the Utility node, and drag an Abort job component to the canvas.</li>\n<li>Finally, add a Wait for File component from the File Management node.</li>\n<li>Add an unconditional hop between the Start component and Wait for File.</li>\n<li>From Wait for File, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”</li>\n<li>Add another hop from Wait for File to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.<img src=\"http://edpflager.com/wp-content/uploads/2015/04/WaitForFile-300x140.png\" alt=\"WaitForFile\"></li>\n<li>Double click the Wait for File component to configure it. You can supply a name for the Job entry (I suggest using the file name you are looking for – i.e. Wait_for_file_name) in the first text box.</li>\n<li>The second box is where you enter the file name you want to check for. If you want to access a file local to the machine you are working on, just enter the path to it, preceded by “file:///”  (for example “file:///C:/Users/root/file_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.(This assumes you already have a file in the location you will be checking).</li>\n<li>The “maximum timeout window” setting allows you to specify in seconds how long you want the process to wait. Enter a zero if you want the process to keep cycling until the file appears, with no timeout. Otherwise for example purposes, enter 30.</li>\n<li>“Check cycle time” allows you to set how frequently you want to check for the file, in seconds as well. For our example, lets enter 5.</li>\n<li>Finally there are three checkboxes you can configure. If the job times out, and you want to treat the timeout as a success, check the first box.</li>\n<li>After the file appears, you may want to wait to make sure that the process creating it has completed before your job starts to read it. Mark the File size check box to have the process wait the number of seconds entered in the Check cycle time field. As an example, if you select the File size check box, and have 60 in the check cycle time, once your job sees the file, it will wait an additional 60 seconds before it starts to read it.</li>\n<li>The final checkbox is Add filename to result. This option allows you to add filenames to the internal result files of a transformation so that later job entries can use this information.</li>\n<li>Once you have configured the various options, click on OK. Your job should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2015/04/WFFJob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/WFFJob-300x134.png\" alt=\"WFFJob\"></a></li>\n<li>Save it and run it.</li>\n<li>If you specified a file that does not exist, the job will loop through every five seconds checking for the file. When it reaches your timeout value, it will Abort the job, unless you chose the box to treat a timeout as a success.</li>\n<li>If you specified a file that does exist, the job should find it on the first iteration and proceed to the Success step.</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2015/04/clockwait.jpg\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/clockwait-300x225.jpg\"></a> In my <a href=\"http://edpflager.com/?p=2716\">last post</a> I wrote about checking for the existence of a file as a control mechanism in a Pentaho Data Integration (aka Kettle) job. This time around, I look at a similar component in Kettle, but this one waits for a file for a specified time. It will make repeated checks to see if the file has appeared, pausing between each check. While the two components have similar functionality, they also have some marked differences:</p>\n<ul>\n<li>“File Exists” is most useful at the beginning of a job flow, and the “Wait for File” process can be useful in multiple places within a job flow.</li>\n<li>“File Exists” can access multiple file systems such as HDFS, Amazon’s S3 and the local computer and network and  “Wait for File” can  only access the local computer and network.","more":"As an example, you have an ETL process scheduled to run at a specific time and a file is needed about 75% of the way through to complete the job. Because the exact time when the file will be ready isn’t known, you can include a Wait for File component in the job that does a periodic check. If the file isn’t found, the process will sleep, and then check again, eventually succeeding or timing out. To create an example using the Wait for File component, follow these instructions:</li>\n</ul>\n<ol>\n<li>Start Spoon and create a new job.</li>\n<li>From the General node in the component panel (the left panel in Spoon), drag a Start and a Success component on to the canvas.</li>\n<li>Open the Utility node, and drag an Abort job component to the canvas.</li>\n<li>Finally, add a Wait for File component from the File Management node.</li>\n<li>Add an unconditional hop between the Start component and Wait for File.</li>\n<li>From Wait for File, add a hop to the Success component. Right click the hop, and a menu will appear. Click the Evaluation option to set  “Follow when result it true.”</li>\n<li>Add another hop from Wait for File to the Abort component. Right click the hop and from the Evaluation option, click on “Follow when result is false”.<img src=\"http://edpflager.com/wp-content/uploads/2015/04/WaitForFile-300x140.png\" alt=\"WaitForFile\"></li>\n<li>Double click the Wait for File component to configure it. You can supply a name for the Job entry (I suggest using the file name you are looking for – i.e. Wait_for_file_name) in the first text box.</li>\n<li>The second box is where you enter the file name you want to check for. If you want to access a file local to the machine you are working on, just enter the path to it, preceded by “file:///”  (for example “file:///C:/Users/root/file_name.txt” on a Windows machine, or on a Mac or Linux machine enter “file:///users/username/file_name.txt”). Checking for a file on your local network via a UNC is almost as easy, just enter file:////servername/folder_name/filename.csv). If you’d prefer to browse for the file or aren’t sure of the path to where the file will be stored, just click the Browse button to access the Open File window.(This assumes you already have a file in the location you will be checking).</li>\n<li>The “maximum timeout window” setting allows you to specify in seconds how long you want the process to wait. Enter a zero if you want the process to keep cycling until the file appears, with no timeout. Otherwise for example purposes, enter 30.</li>\n<li>“Check cycle time” allows you to set how frequently you want to check for the file, in seconds as well. For our example, lets enter 5.</li>\n<li>Finally there are three checkboxes you can configure. If the job times out, and you want to treat the timeout as a success, check the first box.</li>\n<li>After the file appears, you may want to wait to make sure that the process creating it has completed before your job starts to read it. Mark the File size check box to have the process wait the number of seconds entered in the Check cycle time field. As an example, if you select the File size check box, and have 60 in the check cycle time, once your job sees the file, it will wait an additional 60 seconds before it starts to read it.</li>\n<li>The final checkbox is Add filename to result. This option allows you to add filenames to the internal result files of a transformation so that later job entries can use this information.</li>\n<li>Once you have configured the various options, click on OK. Your job should look like this:<a href=\"http://edpflager.com/wp-content/uploads/2015/04/WFFJob.png\"><img src=\"http://edpflager.com/wp-content/uploads/2015/04/WFFJob-300x134.png\" alt=\"WFFJob\"></a></li>\n<li>Save it and run it.</li>\n<li>If you specified a file that does not exist, the job will loop through every five seconds checking for the file. When it reaches your timeout value, it will Abort the job, unless you chose the box to treat a timeout as a success.</li>\n<li>If you specified a file that does exist, the job should find it on the first iteration and proceed to the Success step.</li>\n</ol>"},{"title":"What is a data scientist?","id":"2112","comments":0,"date":"2014-05-30T15:49:31.000Z","_content":"\n##### **What is a data scientist?**\n\nQuick post today - crazy busy this week with the Memorial Day holiday, a major software upgrade at my job, and a lot of other things all coalescing at once.\n\n*   A data scientist is an individual, organization or application that performs statistical analysis, data mining and retrieval processes on a large amount of data to identify trends, figures and other relevant information. - [Technopedia](http://www.techopedia.com/definition/28177/data-scientist)\n\n*   A data scientist is a data analyst who lives in California - [Josh Wills](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCgQFjAA&url=https%3A%2F%2Ftwitter.com%2Fjosh_wills&ei=UNKIU5HLM8m2yATY54HADQ&usg=AFQjCNEpBTcqvMNPqJaJBkDhB7iBkvd6Cw&bvm=bv.67720277,d.aWw), Sr. Director of Data Science at Cloudera, at a Cloudera Users Group Meeting in Southfield, MI on 5/22/2014","source":"_posts/what-is-a-data-scientist.md","raw":"---\ntitle: What is a data scientist?\ntags:\n  - humor\n  - inspiration\nid: '2112'\ncategories:\n  - - Big Data\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2014-05-30 11:49:31\n---\n\n##### **What is a data scientist?**\n\nQuick post today - crazy busy this week with the Memorial Day holiday, a major software upgrade at my job, and a lot of other things all coalescing at once.\n\n*   A data scientist is an individual, organization or application that performs statistical analysis, data mining and retrieval processes on a large amount of data to identify trends, figures and other relevant information. - [Technopedia](http://www.techopedia.com/definition/28177/data-scientist)\n\n*   A data scientist is a data analyst who lives in California - [Josh Wills](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCgQFjAA&url=https%3A%2F%2Ftwitter.com%2Fjosh_wills&ei=UNKIU5HLM8m2yATY54HADQ&usg=AFQjCNEpBTcqvMNPqJaJBkDhB7iBkvd6Cw&bvm=bv.67720277,d.aWw), Sr. Director of Data Science at Cloudera, at a Cloudera Users Group Meeting in Southfield, MI on 5/22/2014","slug":"what-is-a-data-scientist","published":1,"updated":"2020-08-23T20:54:34.854Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ae300d7sdjxhrahhy7u","content":"<h5 id=\"What-is-a-data-scientist\"><a href=\"#What-is-a-data-scientist\" class=\"headerlink\" title=\"What is a data scientist?\"></a><strong>What is a data scientist?</strong></h5><p>Quick post today - crazy busy this week with the Memorial Day holiday, a major software upgrade at my job, and a lot of other things all coalescing at once.</p>\n<ul>\n<li><p>A data scientist is an individual, organization or application that performs statistical analysis, data mining and retrieval processes on a large amount of data to identify trends, figures and other relevant information. - <a href=\"http://www.techopedia.com/definition/28177/data-scientist\">Technopedia</a></p>\n</li>\n<li><p>A data scientist is a data analyst who lives in California - <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCgQFjAA&url=https://twitter.com/josh_wills&ei=UNKIU5HLM8m2yATY54HADQ&usg=AFQjCNEpBTcqvMNPqJaJBkDhB7iBkvd6Cw&bvm=bv.67720277,d.aWw\">Josh Wills</a>, Sr. Director of Data Science at Cloudera, at a Cloudera Users Group Meeting in Southfield, MI on 5/22/2014</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h5 id=\"What-is-a-data-scientist\"><a href=\"#What-is-a-data-scientist\" class=\"headerlink\" title=\"What is a data scientist?\"></a><strong>What is a data scientist?</strong></h5><p>Quick post today - crazy busy this week with the Memorial Day holiday, a major software upgrade at my job, and a lot of other things all coalescing at once.</p>\n<ul>\n<li><p>A data scientist is an individual, organization or application that performs statistical analysis, data mining and retrieval processes on a large amount of data to identify trends, figures and other relevant information. - <a href=\"http://www.techopedia.com/definition/28177/data-scientist\">Technopedia</a></p>\n</li>\n<li><p>A data scientist is a data analyst who lives in California - <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCgQFjAA&url=https://twitter.com/josh_wills&ei=UNKIU5HLM8m2yATY54HADQ&usg=AFQjCNEpBTcqvMNPqJaJBkDhB7iBkvd6Cw&bvm=bv.67720277,d.aWw\">Josh Wills</a>, Sr. Director of Data Science at Cloudera, at a Cloudera Users Group Meeting in Southfield, MI on 5/22/2014</p>\n</li>\n</ul>\n"},{"title":"What the Heck is BIG DATA?","id":"1473","comments":0,"date":"2013-04-19T10:26:58.000Z","_content":"\n![bigdata2](http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg) In the IT world over the past few years there has been a lot of talk / discussion / consternation / anticipation about the concept of \"BIG DATA\". (You have to put it in quotes and all caps). If you don't work or live with the technology sector, it may have passed you by, but lately its started to seep into the mainstream consciousness. Like many fashionable terms, what exactly is meant isn't often relayed. Its just assumed that people know. After all, its the next big thing. You should already know what it is. Its been stated in many articles I've seen that greater than 80% of the world's data has been created in the past couple of years. While that certainly sounds impressive, lets look a little closer at \"data\".\n<!-- more -->\nData isn't information, and its certainly not knowledge. A lot of data is just noise. If you post on Twitter what you had for lunch, that's data (technically its datum because its one piece of data), but its not useful information for most people. (Your spouse might find it useful in deciding what to do for dinner, but most people aren't going to care). If your BFF went to a movie last night without you and text-ed you a spoiler from the cinema, that's data as well. To garner information that raw data has to be evaluated and sifted through to develop facts, or concepts, or understanding of an idea. Newton being hit on the head by an apple falling from a tree in isolation is data. Newton taking multiple pieces of data, such as releasing a book in the air and watching it fall to earth and his observations about the noggin-thumping fruit and gaining an understanding of gravity, is information. Big data is taking huge amounts of what may be unrelated bits and pieces of data, combing through them and trying to find something useful or insightful. Take all of the Google searches for a given week and try to discern some kind of pattern from what people were searching on. You could find out pretty easily what TV shows were popular, or who was in the news that week and garnered their fifteen minutes of fame. And there could be a lot of data there that isn't relevant to anything. But there could be insights gleaned that we don't realize were there, as well. A few years ago Google announced that they had come up with an algorithm that showed users' queries in the search engine predicted the spread of the H1N1 flu virus. [Seriously](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CFUQFjAF&url=http%3A%2F%2Fstatic.googleusercontent.com%2Fexternal_content%2Funtrusted_dlcp%2Fresearch.google.com%2Fes%2F%2Farchive%2Fpapers%2Fdetecting-influenza-epidemics.pdf&ei=WGpxUcqOKqjr2AX5iIGADQ&usg=AFQjCNHwyWhmYVkhzLtDYTCrAWLnacv7lA&bvm=bv.45373924,d.b2I).  As people in different parts of the country became more aware of the flu, and started seeing initial symptoms of those around them,they started searching on it. Geographically, larger numbers of searches tended to coalesce around areas where the flu was becoming more prevalent. In other words, the searches coincided with the outbreaks in real-time, while the health organizations were getting data several days to a couple of weeks later. This example shows some of the benefit of big data. It takes large amounts of data that weren't previously available in a cost effective, and easily accessible form and allows us to digitize it. We can then analyze that data and look for trends and correlations that might not have been apparent to anyone. Sure there is a lot of noise in the data, but the insights that may be uncovered are mind boggling. (Image courtesy of [http://www.digital-delight.ch/](http://www.digital-delight.ch/))","source":"_posts/what-the-heck-is-big-data.md","raw":"---\ntitle: What the Heck is BIG DATA?\ntags: []\nid: '1473'\ncategories:\n  - - Blog\n  - - Business Intelligence\ncomments: false\ndate: 2013-04-19 06:26:58\n---\n\n![bigdata2](http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg) In the IT world over the past few years there has been a lot of talk / discussion / consternation / anticipation about the concept of \"BIG DATA\". (You have to put it in quotes and all caps). If you don't work or live with the technology sector, it may have passed you by, but lately its started to seep into the mainstream consciousness. Like many fashionable terms, what exactly is meant isn't often relayed. Its just assumed that people know. After all, its the next big thing. You should already know what it is. Its been stated in many articles I've seen that greater than 80% of the world's data has been created in the past couple of years. While that certainly sounds impressive, lets look a little closer at \"data\".\n<!-- more -->\nData isn't information, and its certainly not knowledge. A lot of data is just noise. If you post on Twitter what you had for lunch, that's data (technically its datum because its one piece of data), but its not useful information for most people. (Your spouse might find it useful in deciding what to do for dinner, but most people aren't going to care). If your BFF went to a movie last night without you and text-ed you a spoiler from the cinema, that's data as well. To garner information that raw data has to be evaluated and sifted through to develop facts, or concepts, or understanding of an idea. Newton being hit on the head by an apple falling from a tree in isolation is data. Newton taking multiple pieces of data, such as releasing a book in the air and watching it fall to earth and his observations about the noggin-thumping fruit and gaining an understanding of gravity, is information. Big data is taking huge amounts of what may be unrelated bits and pieces of data, combing through them and trying to find something useful or insightful. Take all of the Google searches for a given week and try to discern some kind of pattern from what people were searching on. You could find out pretty easily what TV shows were popular, or who was in the news that week and garnered their fifteen minutes of fame. And there could be a lot of data there that isn't relevant to anything. But there could be insights gleaned that we don't realize were there, as well. A few years ago Google announced that they had come up with an algorithm that showed users' queries in the search engine predicted the spread of the H1N1 flu virus. [Seriously](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CFUQFjAF&url=http%3A%2F%2Fstatic.googleusercontent.com%2Fexternal_content%2Funtrusted_dlcp%2Fresearch.google.com%2Fes%2F%2Farchive%2Fpapers%2Fdetecting-influenza-epidemics.pdf&ei=WGpxUcqOKqjr2AX5iIGADQ&usg=AFQjCNHwyWhmYVkhzLtDYTCrAWLnacv7lA&bvm=bv.45373924,d.b2I).  As people in different parts of the country became more aware of the flu, and started seeing initial symptoms of those around them,they started searching on it. Geographically, larger numbers of searches tended to coalesce around areas where the flu was becoming more prevalent. In other words, the searches coincided with the outbreaks in real-time, while the health organizations were getting data several days to a couple of weeks later. This example shows some of the benefit of big data. It takes large amounts of data that weren't previously available in a cost effective, and easily accessible form and allows us to digitize it. We can then analyze that data and look for trends and correlations that might not have been apparent to anyone. Sure there is a lot of noise in the data, but the insights that may be uncovered are mind boggling. (Image courtesy of [http://www.digital-delight.ch/](http://www.digital-delight.ch/))","slug":"what-the-heck-is-big-data","published":1,"updated":"2020-08-23T20:54:34.730Z","layout":"post","photos":[],"link":"","_id":"ckeaq9ae600dasdjx6dme68f0","content":"<p><img src=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg\" alt=\"bigdata2\"> In the IT world over the past few years there has been a lot of talk / discussion / consternation / anticipation about the concept of “BIG DATA”. (You have to put it in quotes and all caps). If you don’t work or live with the technology sector, it may have passed you by, but lately its started to seep into the mainstream consciousness. Like many fashionable terms, what exactly is meant isn’t often relayed. Its just assumed that people know. After all, its the next big thing. You should already know what it is. Its been stated in many articles I’ve seen that greater than 80% of the world’s data has been created in the past couple of years. While that certainly sounds impressive, lets look a little closer at “data”.</p>\n<a id=\"more\"></a>\n<p>Data isn’t information, and its certainly not knowledge. A lot of data is just noise. If you post on Twitter what you had for lunch, that’s data (technically its datum because its one piece of data), but its not useful information for most people. (Your spouse might find it useful in deciding what to do for dinner, but most people aren’t going to care). If your BFF went to a movie last night without you and text-ed you a spoiler from the cinema, that’s data as well. To garner information that raw data has to be evaluated and sifted through to develop facts, or concepts, or understanding of an idea. Newton being hit on the head by an apple falling from a tree in isolation is data. Newton taking multiple pieces of data, such as releasing a book in the air and watching it fall to earth and his observations about the noggin-thumping fruit and gaining an understanding of gravity, is information. Big data is taking huge amounts of what may be unrelated bits and pieces of data, combing through them and trying to find something useful or insightful. Take all of the Google searches for a given week and try to discern some kind of pattern from what people were searching on. You could find out pretty easily what TV shows were popular, or who was in the news that week and garnered their fifteen minutes of fame. And there could be a lot of data there that isn’t relevant to anything. But there could be insights gleaned that we don’t realize were there, as well. A few years ago Google announced that they had come up with an algorithm that showed users’ queries in the search engine predicted the spread of the H1N1 flu virus. <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CFUQFjAF&url=http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/es//archive/papers/detecting-influenza-epidemics.pdf&ei=WGpxUcqOKqjr2AX5iIGADQ&usg=AFQjCNHwyWhmYVkhzLtDYTCrAWLnacv7lA&bvm=bv.45373924,d.b2I\">Seriously</a>.  As people in different parts of the country became more aware of the flu, and started seeing initial symptoms of those around them,they started searching on it. Geographically, larger numbers of searches tended to coalesce around areas where the flu was becoming more prevalent. In other words, the searches coincided with the outbreaks in real-time, while the health organizations were getting data several days to a couple of weeks later. This example shows some of the benefit of big data. It takes large amounts of data that weren’t previously available in a cost effective, and easily accessible form and allows us to digitize it. We can then analyze that data and look for trends and correlations that might not have been apparent to anyone. Sure there is a lot of noise in the data, but the insights that may be uncovered are mind boggling. (Image courtesy of <a href=\"http://www.digital-delight.ch/\">http://www.digital-delight.ch/</a>)</p>\n","site":{"data":{}},"excerpt":"<p><img src=\"http://edpflager.com/wp-content/uploads/2013/04/bigdata2.jpg\" alt=\"bigdata2\"> In the IT world over the past few years there has been a lot of talk / discussion / consternation / anticipation about the concept of “BIG DATA”. (You have to put it in quotes and all caps). If you don’t work or live with the technology sector, it may have passed you by, but lately its started to seep into the mainstream consciousness. Like many fashionable terms, what exactly is meant isn’t often relayed. Its just assumed that people know. After all, its the next big thing. You should already know what it is. Its been stated in many articles I’ve seen that greater than 80% of the world’s data has been created in the past couple of years. While that certainly sounds impressive, lets look a little closer at “data”.</p>","more":"<p>Data isn’t information, and its certainly not knowledge. A lot of data is just noise. If you post on Twitter what you had for lunch, that’s data (technically its datum because its one piece of data), but its not useful information for most people. (Your spouse might find it useful in deciding what to do for dinner, but most people aren’t going to care). If your BFF went to a movie last night without you and text-ed you a spoiler from the cinema, that’s data as well. To garner information that raw data has to be evaluated and sifted through to develop facts, or concepts, or understanding of an idea. Newton being hit on the head by an apple falling from a tree in isolation is data. Newton taking multiple pieces of data, such as releasing a book in the air and watching it fall to earth and his observations about the noggin-thumping fruit and gaining an understanding of gravity, is information. Big data is taking huge amounts of what may be unrelated bits and pieces of data, combing through them and trying to find something useful or insightful. Take all of the Google searches for a given week and try to discern some kind of pattern from what people were searching on. You could find out pretty easily what TV shows were popular, or who was in the news that week and garnered their fifteen minutes of fame. And there could be a lot of data there that isn’t relevant to anything. But there could be insights gleaned that we don’t realize were there, as well. A few years ago Google announced that they had come up with an algorithm that showed users’ queries in the search engine predicted the spread of the H1N1 flu virus. <a href=\"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CFUQFjAF&url=http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/es//archive/papers/detecting-influenza-epidemics.pdf&ei=WGpxUcqOKqjr2AX5iIGADQ&usg=AFQjCNHwyWhmYVkhzLtDYTCrAWLnacv7lA&bvm=bv.45373924,d.b2I\">Seriously</a>.  As people in different parts of the country became more aware of the flu, and started seeing initial symptoms of those around them,they started searching on it. Geographically, larger numbers of searches tended to coalesce around areas where the flu was becoming more prevalent. In other words, the searches coincided with the outbreaks in real-time, while the health organizations were getting data several days to a couple of weeks later. This example shows some of the benefit of big data. It takes large amounts of data that weren’t previously available in a cost effective, and easily accessible form and allows us to digitize it. We can then analyze that data and look for trends and correlations that might not have been apparent to anyone. Sure there is a lot of noise in the data, but the insights that may be uncovered are mind boggling. (Image courtesy of <a href=\"http://www.digital-delight.ch/\">http://www.digital-delight.ch/</a>)</p>"},{"title":"Wrap text around Graphics","id":"4048","comments":0,"date":"2018-10-14T08:15:48.000Z","_content":"\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Building on my earlier posts about including graphics in your R Markdown output, this post will demonstrate how to wrap text around graphics in your R Markdown document. The technique will produce output like this where text is wrapped around the figure at the left. Wrapping text can provide a varied appearance to your document and minimizes white space. It can aid in readability but also tends to deemphasize the importance of the image, so you this technique sparingly.\n<!-- more -->\nTo wrap text around a graphical element, we need to include the wrapfig package as part of our YAML header. Including the packages specified previously for working with graphics in R Markdown documents, our header-includes section looks like this:\n\nheader-includes: usepackage{graphicx}\n                 usepackage{float}\n                 usepackage{wrapfig}\n                 usepackage{blindtext}\n\n(I am using the [blindtext option covered previously](http://edpflager.com/2018/09/27/r-markdown-cookbook-lorem-ipsum-text/) to produce some random text for this example). Within the document, we have inline code to define the start of the graphic wrapping sequence, and include several options. The sequence name must be wrapfigure. The options inclosed with brackets \\[ \\] are optional, while those within braces { } are required.\n\nbegin{wrapfigure}\\[12\\]{l}\\[2pt\\]{5cm}\n\n1.  The first option \\[12\\] in this example, tells R Markdown how many lines to wrap around your graphic. Be careful of setting this value too low if your image is large, because subsequent lines may over-lay the image.\n2.  The second option {l} above, indicates where to place the image. Allowed values are l or L for left, and r or R for right. In LaTeX, the upper case versions allow the image to float a bit, but in my experience I've seen no appreciable difference. LaTeX also allows i/I and o/O to place the image on the inside edge by the binding or the outside edge away from the binding. Again, I've not seen this work in R Markdown.\n3.  For the third option, you can indicate how far you want the image to hang out into the margin. You can use an explicit value as I have done in the example line \\[2pt\\] or you can use a calculated value like \\[0.25width\\] to put a quarter of the graphic into the margin. If you use just \\[width\\] the entire graphic will be placed in the margin, and may be cut off if its a larger image. The wrapped text will be indented regardless of which option you use.\n4.  Finally, the last option is used to indicate the width of the graphic element. This doesn't scale the image, but instead defined the \"window\" size around the graphic. If you specify a value that is too small here, the wrapped text may over-lay the image and if you specify a too large value, you may undo the wrapping you are trying to achieve.\n\nAfter the beginning sequence line, you then include your graphic element definition, and optionally a caption element. The includegraphics line can use the same options that were covered in a previous post.\n\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\ncaption{GIMP logo}\n\nFinally, you close the wrapfig sequence end{wrapfigure} Pulling all of this together, The R Markdown document looks like this:\n\n\\---\ntitle: \"GraphicsWrappingText\"\noutput: pdf\\_document\nheader-includes: usepackage{graphicx}\n                 usepackage{wrapfig}\n                 usepackage{blindtext}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n### Wrapfig\npar\nbegin{wrapfigure}\\[12\\]{l}\\[2pt\\]{5cm}\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\ncaption{GIMP logo}\nend{wrapfigure}\nBlindtext\n\nThe end result is this, reproduced from above. [![](http://edpflager.com/wp-content/uploads/2018/10/WrapFig-1024x545.png)](http://edpflager.com/wp-content/uploads/2018/10/WrapFig.png)","source":"_posts/wrap-text-around-graphics.md","raw":"---\ntitle: Wrap text around Graphics\ntags:\n  - cookbook\n  - guides\n  - How-to\n  - R Markdown\n  - technical\nid: '4048'\ncategories:\n  - - Blog\n  - - Misc\n  - - R\n  - - R Markdown Cookbook\ncomments: false\ndate: 2018-10-14 04:15:48\n---\n\n[![](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)](http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png)Building on my earlier posts about including graphics in your R Markdown output, this post will demonstrate how to wrap text around graphics in your R Markdown document. The technique will produce output like this where text is wrapped around the figure at the left. Wrapping text can provide a varied appearance to your document and minimizes white space. It can aid in readability but also tends to deemphasize the importance of the image, so you this technique sparingly.\n<!-- more -->\nTo wrap text around a graphical element, we need to include the wrapfig package as part of our YAML header. Including the packages specified previously for working with graphics in R Markdown documents, our header-includes section looks like this:\n\nheader-includes: usepackage{graphicx}\n                 usepackage{float}\n                 usepackage{wrapfig}\n                 usepackage{blindtext}\n\n(I am using the [blindtext option covered previously](http://edpflager.com/2018/09/27/r-markdown-cookbook-lorem-ipsum-text/) to produce some random text for this example). Within the document, we have inline code to define the start of the graphic wrapping sequence, and include several options. The sequence name must be wrapfigure. The options inclosed with brackets \\[ \\] are optional, while those within braces { } are required.\n\nbegin{wrapfigure}\\[12\\]{l}\\[2pt\\]{5cm}\n\n1.  The first option \\[12\\] in this example, tells R Markdown how many lines to wrap around your graphic. Be careful of setting this value too low if your image is large, because subsequent lines may over-lay the image.\n2.  The second option {l} above, indicates where to place the image. Allowed values are l or L for left, and r or R for right. In LaTeX, the upper case versions allow the image to float a bit, but in my experience I've seen no appreciable difference. LaTeX also allows i/I and o/O to place the image on the inside edge by the binding or the outside edge away from the binding. Again, I've not seen this work in R Markdown.\n3.  For the third option, you can indicate how far you want the image to hang out into the margin. You can use an explicit value as I have done in the example line \\[2pt\\] or you can use a calculated value like \\[0.25width\\] to put a quarter of the graphic into the margin. If you use just \\[width\\] the entire graphic will be placed in the margin, and may be cut off if its a larger image. The wrapped text will be indented regardless of which option you use.\n4.  Finally, the last option is used to indicate the width of the graphic element. This doesn't scale the image, but instead defined the \"window\" size around the graphic. If you specify a value that is too small here, the wrapped text may over-lay the image and if you specify a too large value, you may undo the wrapping you are trying to achieve.\n\nAfter the beginning sequence line, you then include your graphic element definition, and optionally a caption element. The includegraphics line can use the same options that were covered in a previous post.\n\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\ncaption{GIMP logo}\n\nFinally, you close the wrapfig sequence end{wrapfigure} Pulling all of this together, The R Markdown document looks like this:\n\n\\---\ntitle: \"GraphicsWrappingText\"\noutput: pdf\\_document\nheader-includes: usepackage{graphicx}\n                 usepackage{wrapfig}\n                 usepackage{blindtext}\n---\n\n\\`\\`\\`{r setup, include=FALSE}\nknitr::opts\\_chunk$set(echo = TRUE)\n\\`\\`\\`\n\n### Wrapfig\npar\nbegin{wrapfigure}\\[12\\]{l}\\[2pt\\]{5cm}\nincludegraphics\\[width=0.25textwidth\\]{gimp.png}\ncaption{GIMP logo}\nend{wrapfigure}\nBlindtext\n\nThe end result is this, reproduced from above. [![](http://edpflager.com/wp-content/uploads/2018/10/WrapFig-1024x545.png)](http://edpflager.com/wp-content/uploads/2018/10/WrapFig.png)","slug":"wrap-text-around-graphics","published":1,"updated":"2020-08-23T20:54:35.166Z","layout":"post","photos":[],"link":"","_id":"ckeaq9aeb00desdjx7xtbahjm","content":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Building on my earlier posts about including graphics in your R Markdown output, this post will demonstrate how to wrap text around graphics in your R Markdown document. The technique will produce output like this where text is wrapped around the figure at the left. Wrapping text can provide a varied appearance to your document and minimizes white space. It can aid in readability but also tends to deemphasize the importance of the image, so you this technique sparingly.</p>\n<a id=\"more\"></a>\n<p>To wrap text around a graphical element, we need to include the wrapfig package as part of our YAML header. Including the packages specified previously for working with graphics in R Markdown documents, our header-includes section looks like this:</p>\n<p>header-includes: usepackage{graphicx}<br>                 usepackage{float}<br>                 usepackage{wrapfig}<br>                 usepackage{blindtext}</p>\n<p>(I am using the <a href=\"http://edpflager.com/2018/09/27/r-markdown-cookbook-lorem-ipsum-text/\">blindtext option covered previously</a> to produce some random text for this example). Within the document, we have inline code to define the start of the graphic wrapping sequence, and include several options. The sequence name must be wrapfigure. The options inclosed with brackets [ ] are optional, while those within braces { } are required.</p>\n<p>begin{wrapfigure}[12]{l}[2pt]{5cm}</p>\n<ol>\n<li>The first option [12] in this example, tells R Markdown how many lines to wrap around your graphic. Be careful of setting this value too low if your image is large, because subsequent lines may over-lay the image.</li>\n<li>The second option {l} above, indicates where to place the image. Allowed values are l or L for left, and r or R for right. In LaTeX, the upper case versions allow the image to float a bit, but in my experience I’ve seen no appreciable difference. LaTeX also allows i/I and o/O to place the image on the inside edge by the binding or the outside edge away from the binding. Again, I’ve not seen this work in R Markdown.</li>\n<li>For the third option, you can indicate how far you want the image to hang out into the margin. You can use an explicit value as I have done in the example line [2pt] or you can use a calculated value like [0.25width] to put a quarter of the graphic into the margin. If you use just [width] the entire graphic will be placed in the margin, and may be cut off if its a larger image. The wrapped text will be indented regardless of which option you use.</li>\n<li>Finally, the last option is used to indicate the width of the graphic element. This doesn’t scale the image, but instead defined the “window” size around the graphic. If you specify a value that is too small here, the wrapped text may over-lay the image and if you specify a too large value, you may undo the wrapping you are trying to achieve.</li>\n</ol>\n<p>After the beginning sequence line, you then include your graphic element definition, and optionally a caption element. The includegraphics line can use the same options that were covered in a previous post.</p>\n<p>includegraphics[width=0.25textwidth]{gimp.png}<br>caption{GIMP logo}</p>\n<p>Finally, you close the wrapfig sequence end{wrapfigure} Pulling all of this together, The R Markdown document looks like this:</p>\n<p>-–<br>title: “GraphicsWrappingText”<br>output: pdf_document<br>header-includes: usepackage{graphicx}<br>                 usepackage{wrapfig}<br>                 usepackage{blindtext}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<h3 id=\"Wrapfig\"><a href=\"#Wrapfig\" class=\"headerlink\" title=\"Wrapfig\"></a>Wrapfig</h3><p>par<br>begin{wrapfigure}[12]{l}[2pt]{5cm}<br>includegraphics[width=0.25textwidth]{gimp.png}<br>caption{GIMP logo}<br>end{wrapfigure}<br>Blindtext</p>\n<p>The end result is this, reproduced from above. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/WrapFig.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/WrapFig-1024x545.png\"></a></p>\n","site":{"data":{}},"excerpt":"<p><a href=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/09/rmarkdown-e1538176415459.png\"></a>Building on my earlier posts about including graphics in your R Markdown output, this post will demonstrate how to wrap text around graphics in your R Markdown document. The technique will produce output like this where text is wrapped around the figure at the left. Wrapping text can provide a varied appearance to your document and minimizes white space. It can aid in readability but also tends to deemphasize the importance of the image, so you this technique sparingly.</p>","more":"<p>To wrap text around a graphical element, we need to include the wrapfig package as part of our YAML header. Including the packages specified previously for working with graphics in R Markdown documents, our header-includes section looks like this:</p>\n<p>header-includes: usepackage{graphicx}<br>                 usepackage{float}<br>                 usepackage{wrapfig}<br>                 usepackage{blindtext}</p>\n<p>(I am using the <a href=\"http://edpflager.com/2018/09/27/r-markdown-cookbook-lorem-ipsum-text/\">blindtext option covered previously</a> to produce some random text for this example). Within the document, we have inline code to define the start of the graphic wrapping sequence, and include several options. The sequence name must be wrapfigure. The options inclosed with brackets [ ] are optional, while those within braces { } are required.</p>\n<p>begin{wrapfigure}[12]{l}[2pt]{5cm}</p>\n<ol>\n<li>The first option [12] in this example, tells R Markdown how many lines to wrap around your graphic. Be careful of setting this value too low if your image is large, because subsequent lines may over-lay the image.</li>\n<li>The second option {l} above, indicates where to place the image. Allowed values are l or L for left, and r or R for right. In LaTeX, the upper case versions allow the image to float a bit, but in my experience I’ve seen no appreciable difference. LaTeX also allows i/I and o/O to place the image on the inside edge by the binding or the outside edge away from the binding. Again, I’ve not seen this work in R Markdown.</li>\n<li>For the third option, you can indicate how far you want the image to hang out into the margin. You can use an explicit value as I have done in the example line [2pt] or you can use a calculated value like [0.25width] to put a quarter of the graphic into the margin. If you use just [width] the entire graphic will be placed in the margin, and may be cut off if its a larger image. The wrapped text will be indented regardless of which option you use.</li>\n<li>Finally, the last option is used to indicate the width of the graphic element. This doesn’t scale the image, but instead defined the “window” size around the graphic. If you specify a value that is too small here, the wrapped text may over-lay the image and if you specify a too large value, you may undo the wrapping you are trying to achieve.</li>\n</ol>\n<p>After the beginning sequence line, you then include your graphic element definition, and optionally a caption element. The includegraphics line can use the same options that were covered in a previous post.</p>\n<p>includegraphics[width=0.25textwidth]{gimp.png}<br>caption{GIMP logo}</p>\n<p>Finally, you close the wrapfig sequence end{wrapfigure} Pulling all of this together, The R Markdown document looks like this:</p>\n<p>-–<br>title: “GraphicsWrappingText”<br>output: pdf_document<br>header-includes: usepackage{graphicx}<br>                 usepackage{wrapfig}<br>                 usepackage{blindtext}</p>\n<hr>\n<p>```{r setup, include=FALSE}<br>knitr::opts_chunk$set(echo = TRUE)<br>```</p>\n<h3 id=\"Wrapfig\"><a href=\"#Wrapfig\" class=\"headerlink\" title=\"Wrapfig\"></a>Wrapfig</h3><p>par<br>begin{wrapfigure}[12]{l}[2pt]{5cm}<br>includegraphics[width=0.25textwidth]{gimp.png}<br>caption{GIMP logo}<br>end{wrapfigure}<br>Blindtext</p>\n<p>The end result is this, reproduced from above. <a href=\"http://edpflager.com/wp-content/uploads/2018/10/WrapFig.png\"><img src=\"http://edpflager.com/wp-content/uploads/2018/10/WrapFig-1024x545.png\"></a></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckeaq99s40009sdjxei864dku","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99st000isdjxew0o7e5l"},{"post_id":"ckeaq99rr0003sdjxctj07si9","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq99te000ssdjx7f8k1gl1"},{"post_id":"ckeaq99rr0003sdjxctj07si9","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99ti000vsdjxh7qrf4ax"},{"post_id":"ckeaq99rr0003sdjxctj07si9","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99tl000ysdjx4c8d73gb"},{"post_id":"ckeaq99sx000jsdjxcjms1oci","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99tn0011sdjxg8gf8ddd"},{"post_id":"ckeaq99sx000jsdjxcjms1oci","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99ub0014sdjxg5klfo8i"},{"post_id":"ckeaq99t0000msdjx5qjh4dxx","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99ue0016sdjx9zrsf8es"},{"post_id":"ckeaq99t0000msdjx5qjh4dxx","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99ui001asdjxhloreyzo"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99uk001csdjx8edn5a9l"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99uq001fsdjxhzx4cn12"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99uu001isdjx5483asdd"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99v2001lsdjxgucx4j4v"},{"post_id":"ckeaq99s6000asdjx5pt97tsd","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99wa001osdjx5s72gh7t"},{"post_id":"ckeaq99s6000asdjx5pt97tsd","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99wf001rsdjx2ani5clo"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq99wk001usdjx7bnt6sx5"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99wq001ysdjx0dqx9y0n"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99wu0022sdjx0x0jgyb6"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99wy0026sdjxcw38fq7e"},{"post_id":"ckeaq99re0001sdjx2opubyx3","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99x2002asdjxemurf95p"},{"post_id":"ckeaq99re0001sdjx2opubyx3","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99x8002esdjxdopc3app"},{"post_id":"ckeaq99re0001sdjx2opubyx3","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq99xd002isdjx4sga1ill"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99xj002msdjxelabcxjw"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq99xo002qsdjxd57r27uj"},{"post_id":"ckeaq99tk000xsdjxgczwe103","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99xs002usdjx0g4625el"},{"post_id":"ckeaq99tk000xsdjxgczwe103","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99y1002xsdjxdtvw29hu"},{"post_id":"ckeaq99tk000xsdjxgczwe103","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq99ye0032sdjxd5tc2fck"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99yo0035sdjx33wcenxo"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99yx003asdjxejo1aajh"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99z0003dsdjxgp8p1adj"},{"post_id":"ckeaq99sc000esdjx59exegqq","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99z7003isdjx9jhs69lo"},{"post_id":"ckeaq99sc000esdjx59exegqq","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99zc003lsdjx2d0x4kla"},{"post_id":"ckeaq99sc000esdjx59exegqq","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99zg003qsdjx7rl3a50v"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99zm003tsdjx7mvbc7wq"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq99zs003ysdjx7kcj02sk"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq99zv0041sdjxex1h4jjs"},{"post_id":"ckeaq99um001esdjx82f6alcm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq99zx0046sdjxctw837rz"},{"post_id":"ckeaq99um001esdjx82f6alcm","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq99zz0049sdjxfmld9tig"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a08004dsdjxgw03a6wv"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a0f004hsdjxg1puex08"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9a0j004ksdjx6tfwdnnq"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9a0u004psdjx26yydsok"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a10004ssdjxgka62429"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a25004xsdjxdszj880z"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a280050sdjx3o79hsza"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9a2m0055sdjxhsa8ct6z"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a2t0058sdjxfw63g3kh"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a32005dsdjxfaqg65bx"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a38005gsdjxc9axfpdp"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a3f005ksdjx5bip3ayf"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a3i005nsdjxajljdbqk"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a3p005rsdjx4pqm5z1p"},{"post_id":"ckeaq99wp001xsdjxdp61c04i","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9a42005usdjx286u38fz"},{"post_id":"ckeaq99wp001xsdjxdp61c04i","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a46005ysdjxa85k67xy"},{"post_id":"ckeaq99wp001xsdjxdp61c04i","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a490061sdjxajvwhmgv"},{"post_id":"ckeaq99wt0021sdjx5lv9hmx0","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a4c0065sdjxf9dw1dy9"},{"post_id":"ckeaq99wt0021sdjx5lv9hmx0","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9a4g0068sdjx57rhf4dq"},{"post_id":"ckeaq99wt0021sdjx5lv9hmx0","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a4l006csdjx3t0r3sec"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a4p006fsdjxfj83ad8m"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9a4u006jsdjx4sgi7piw"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a4z006msdjx35bh2jkb"},{"post_id":"ckeaq99x5002dsdjxgfechhbu","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a52006qsdjxhvx15x9b"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a57006tsdjx2agsgpg9"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9a5e006xsdjx8ek0c7d9"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a5g0070sdjx23k5abxd"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a5j0074sdjx9k9e4psi"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9a5q0077sdjx5b379iil"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a69007bsdjxd5gq3wbh"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9a6g007esdjxcb666dsm"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a6w007isdjxf1l905m3"},{"post_id":"ckeaq99u60013sdjx87lx0yym","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a71007lsdjx8glx5kky"},{"post_id":"ckeaq99u60013sdjx87lx0yym","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9a74007psdjx26a041mm"},{"post_id":"ckeaq99u60013sdjx87lx0yym","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a78007ssdjx65sjdvea"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a7b007wsdjxgejgb7ei"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a7d007zsdjxa7smgkuy"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a7i0083sdjx5vlleisf"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9a7n0086sdjx4cvsh54o"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a7r008asdjx8u2q446n"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a7u008dsdjx3vg6euen"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a7w008hsdjxg4yi6xgi"},{"post_id":"ckeaq99us001hsdjxh1123mjy","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a7z008ksdjx656i962v"},{"post_id":"ckeaq99us001hsdjxh1123mjy","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a83008osdjx1v36hvb6"},{"post_id":"ckeaq99us001hsdjxh1123mjy","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9a85008rsdjx3y7qe4tr"},{"post_id":"ckeaq99us001hsdjxh1123mjy","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9a87008vsdjxbnmb3hds"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a89008ysdjxfex0ftj7"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a8b0092sdjxc4ro656f"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a8e0095sdjx69j56e7c"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a8g0099sdjx8ymb9bhf"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a8m009csdjxcsze3b3s"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9a8p009gsdjx0q3wcpmb"},{"post_id":"ckeaq99yj0034sdjx89gw45n3","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9a8r009jsdjx72jmevik"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a8v009nsdjx548dauaf"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9a8z009qsdjxcypz5kbb"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9a91009usdjx2lav9fld"},{"post_id":"ckeaq99wd001qsdjx8y705xah","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a93009xsdjx45zlhpwa"},{"post_id":"ckeaq99wd001qsdjx8y705xah","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a9400a1sdjx4lxb30c9"},{"post_id":"ckeaq99wd001qsdjx8y705xah","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9a9600a4sdjx8py2eq79"},{"post_id":"ckeaq99za003ksdjxfr9hfk5i","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a9800a8sdjx5xp17wd0"},{"post_id":"ckeaq99yz003csdjx70udhs2n","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a9a00absdjxh2lzfh5z"},{"post_id":"ckeaq99yz003csdjx70udhs2n","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a9d00afsdjx5opmdh2z"},{"post_id":"ckeaq99yz003csdjx70udhs2n","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9a9g00aisdjx72p35b48"},{"post_id":"ckeaq99zf003osdjx7h2j0kx2","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a9i00amsdjx3bp1c7si"},{"post_id":"ckeaq99zf003osdjx7h2j0kx2","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9a9m00apsdjxabubc3iu"},{"post_id":"ckeaq99wi001tsdjx92527s3n","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9a9q00atsdjxdebg2jug"},{"post_id":"ckeaq99wi001tsdjx92527s3n","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9a9u00awsdjx23yw5sfo"},{"post_id":"ckeaq99wi001tsdjx92527s3n","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aa300b0sdjxfpwh8c5i"},{"post_id":"ckeaq99wi001tsdjx92527s3n","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aa800b3sdjx3s717dqo"},{"post_id":"ckeaq99zu0040sdjxd58abysn","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aad00b7sdjx8r5l5wlg"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aai00basdjxf55uhilr"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aal00bdsdjx0lcodzvl"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aat00bhsdjxcmas920l"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9aau00bksdjx3i06ek04"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aax00bosdjx04h79olh"},{"post_id":"ckeaq99zj003ssdjx0kalcfbi","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ab200brsdjx9rrz0vnn"},{"post_id":"ckeaq99zj003ssdjx0kalcfbi","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9abf00bvsdjx56sd7bv5"},{"post_id":"ckeaq99zj003ssdjx0kalcfbi","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9abm00bysdjx95r3fd68"},{"post_id":"ckeaq99zj003ssdjx0kalcfbi","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9abp00c2sdjx94dj6rom"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9abw00c5sdjxff5wfpu6"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ac400c9sdjxgl3zecai"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ac700ccsdjx2zq26i72"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9acd00cgsdjx5zdeb2sb"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ad800cjsdjxd73nb3pt"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ada00cnsdjx8y8z2e7x"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ade00cqsdjxhhsz2l49"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9adj00cusdjx3ge491wr"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9adl00cxsdjx7h2738w9"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9adu00d1sdjx75hu4uye"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ady00d4sdjx2cixej9l"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ae500d8sdjxftg25vuz"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ae800dbsdjxa44d64hu"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9aee00dfsdjxgb2ubebw"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aer00dhsdjxgkjv0zuu"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aes00dksdjx3xahgiad"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aet00dmsdjxe4pleq29"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aeu00dpsdjxb05ge8js"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aeu00drsdjx20buhn42"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9aev00dusdjx9jhcclbu"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aew00dwsdjx4uxs1mr0"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9aew00dzsdjx7zt2ffv1"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aex00e1sdjx9tea4m9y"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aex00e4sdjx53amdz5i"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aey00e6sdjxhkl7hnws"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aez00e9sdjx4gnx27q0"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9aez00ebsdjx7jade8f1"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aez00eesdjx1a2a8ssf"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9af000egsdjx8mry3wzd"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9af100ejsdjxecl32rce"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9af100elsdjxd3i7f6um"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9af200eosdjx3um9deuy"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9af200eqsdjxfcr7hce7"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9af300etsdjxeurn6ypk"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9af400evsdjx8j98a4g0"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9af500eysdjx4il8dw95"},{"post_id":"ckeaq99s10006sdjxd42i1syq","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9af500f0sdjxet2dgh4k"},{"post_id":"ckeaq99s10006sdjxd42i1syq","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9af600f3sdjxa2i8chtw"},{"post_id":"ckeaq99s10006sdjxd42i1syq","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9af600f5sdjx75xje1fv"},{"post_id":"ckeaq99s10006sdjxd42i1syq","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9af700f8sdjx2z3eeuqj"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9af700fasdjxa7wa4cki"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9af700fcsdjx8g1ea24m"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9af900ffsdjx1py6gnd2"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9af900fhsdjx1mzzgvhf"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9afa00fksdjxf9sacxh2"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9afa00fmsdjxg1f24n96"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afb00fpsdjxa19l0qz1"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afb00frsdjxc6ib0s85"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afc00fusdjx5lmn6re0"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9afc00fwsdjx401l1rlb"},{"post_id":"ckeaq9a2w005csdjx6qxg58dl","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afc00fzsdjx340fbf61"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afd00g1sdjx6fbodnvp"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9afd00g4sdjx7sfye31k"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9afd00g6sdjx4n81ay3x"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aff00g9sdjxeeidcfm5"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afg00gbsdjxdhel6ffk"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afg00gesdjx1u49avd6"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9afh00ggsdjxarfv8w4e"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afh00gjsdjx7gi1c0w9"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afi00glsdjx8z4f5xrd"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9afi00gosdjxg8ap1cdv"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9afj00gqsdjx2aag3mvg"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afj00gtsdjxc7o7088z"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afk00gvsdjxcdldhisw"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afk00gysdjx7l36danl"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9afl00h0sdjx2dvz1954"},{"post_id":"ckeaq9a3h005msdjxextf1ons","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afl00h3sdjxdxa6fc8w"},{"post_id":"ckeaq9a3h005msdjxextf1ons","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afm00h5sdjx1uqg54h1"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afm00h8sdjxgip83ztx"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afn00hasdjx9afo6orp"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afn00hdsdjx9m7t9a0x"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afo00hfsdjx805t701d"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afp00hisdjxhr4bhbtq"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9afp00hksdjxghpz99cr"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afq00hnsdjxdy11brj7"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9afq00hpsdjxb4od9msi"},{"post_id":"ckeaq9a44005xsdjxgye25som","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afr00hssdjx6zfa0mlr"},{"post_id":"ckeaq9a44005xsdjxgye25som","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afr00husdjxegd9e2fy"},{"post_id":"ckeaq9a44005xsdjxgye25som","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afs00hxsdjxf4po2jmn"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afs00hzsdjx8saeckfo"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afw00i1sdjx8je8bk10"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9afx00i4sdjx5bg409cf"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9afx00i6sdjx6sj998ew"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9afy00i9sdjx02g5gv2y"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9afy00ibsdjxhuh60oof"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ag000iesdjx0qy7bqye"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag000igsdjxa7bghavm"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ag100ijsdjx7l0u2852"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9ag100ilsdjx4u3d5j46"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ag200iosdjxa3tabbfh"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag200iqsdjx2leoc8qb"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ag300itsdjx21ru87dl"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ag300ivsdjx05lve2ce"},{"post_id":"ckeaq9a4w006lsdjx9qn73o3h","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag500iysdjx18zkcskm"},{"post_id":"ckeaq9a4r006isdjx04r0cg0z","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag500j0sdjxf6wybqzc"},{"post_id":"ckeaq9a4r006isdjx04r0cg0z","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ag500j3sdjxflghfvp8"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag600j5sdjxdczzf8g2"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ag600j8sdjxdt0ca6yr"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9ag700jasdjxbjw3hx9d"},{"post_id":"ckeaq9a50006psdjx1bmedvym","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag800jdsdjxbk6n9tp7"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ag800jfsdjx35hadisc"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ag900jisdjxh3yu9ucp"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ag900jksdjxb7nx0pjo"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aga00jnsdjxhvdvg6f2"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aga00jpsdjx3onec76r"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agb00jssdjx4t62a8ur"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agb00jusdjx8xrd05ef"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agb00jxsdjx78f8dyce"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9agc00jzsdjx1vs1c2le"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agc00k2sdjx728qb6r4"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agd00k4sdjxb60v221d"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agd00k7sdjxdhclfgaf"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9age00k9sdjx4dbo3zlo"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9age00kcsdjxdzm94poj"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agf00kesdjxbkszho29"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agf00khsdjx20swbmpi"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9agg00kjsdjx67v81v0r"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agg00kmsdjxb98m5ra5"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agh00kosdjxbp3edmbc"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agh00krsdjxeq8fbwex"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agi00ktsdjxcdxp552h"},{"post_id":"ckeaq9a64007asdjxax46g6wd","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agj00kwsdjx991fb9ny"},{"post_id":"ckeaq9a64007asdjxax46g6wd","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agk00kysdjx0kmi1jve"},{"post_id":"ckeaq9a64007asdjxax46g6wd","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9agk00l1sdjx2oewaea2"},{"post_id":"ckeaq9a64007asdjxax46g6wd","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agl00l3sdjxdovpare9"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agl00l5sdjx5l8b4idx"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agm00l8sdjx7hju2921"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9agm00lasdjxbupmdidm"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agn00ldsdjx14thdojb"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agn00lfsdjxcbxmfoa3"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ago00lisdjxcslk6yur"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ago00lksdjx4e2u5mjy"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9agp00lnsdjxbooyajh8"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9agq00lpsdjxh91v1l6s"},{"post_id":"ckeaq9a72007osdjxe73j1oit","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agr00lssdjxeggs0mwr"},{"post_id":"ckeaq9a72007osdjxe73j1oit","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agr00lusdjx2yv7akam"},{"post_id":"ckeaq9a72007osdjxe73j1oit","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ags00lxsdjx1fqf5x90"},{"post_id":"ckeaq9a72007osdjxe73j1oit","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ags00lzsdjxe3rd6rk2"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9agu00m2sdjx1gy2c3ms"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agu00m4sdjx3ogu5jba"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agv00m7sdjxgxdu3vms"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agv00m9sdjxc3l6d1o3"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9agw00mcsdjxghok6d2b"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9agw00mesdjxa8lg9vnc"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9agx00mhsdjxbndkazx1"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9agx00mjsdjxahrq0w5q"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9agy00mmsdjx473h3jym"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9agy00mosdjx6jhv1wpq"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9agz00mrsdjx1ts0bs8t"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9agz00mtsdjx25nhgrzm"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ah000mwsdjx2ylkdslt"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ah000mysdjx75o96pcc"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ah100n0sdjxdr3r1pii"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ah200n3sdjx99us06mm"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ah200n5sdjx027h8mii"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ah200n8sdjx90pc4x9r"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ah300nasdjx0lc7dzzf"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ah300ndsdjxftsza5av"},{"post_id":"ckeaq9a7y008jsdjx8ocq01gl","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ah300nfsdjxdefpg79u"},{"post_id":"ckeaq9a7y008jsdjx8ocq01gl","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ah400nisdjxcexgd3nx"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ah400nksdjxgz1ghq4t"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ah500nnsdjx9k4u6qf2"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ah600npsdjx1f0o8c10"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ah700nssdjx8lec1pbr"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ah700nusdjx0zea7wy7"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ah800nxsdjxg2cjhtpx"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ah800nzsdjx68v1d52t"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aha00o2sdjxcqwrdk22"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9aha00o4sdjx2d1j47bt"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ahb00o7sdjx9j575fos"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9ahb00o9sdjx9ipa6zd8"},{"post_id":"ckeaq9a88008xsdjx1d1nf654","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahc00ocsdjx8pcyg1ol"},{"post_id":"ckeaq9a88008xsdjx1d1nf654","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahc00oesdjxax8se357"},{"post_id":"ckeaq9a86008usdjx458i40ly","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahd00ohsdjxao9l8x86"},{"post_id":"ckeaq9a86008usdjx458i40ly","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahd00ojsdjxetvrbbsy"},{"post_id":"ckeaq9a86008usdjx458i40ly","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahe00omsdjx6vra7jyz"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahe00oosdjxdbrk4xvu"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahf00orsdjx2n9ffyyv"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahg00otsdjx0xqld2ff"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ahg00owsdjx0npq7j8f"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahh00oysdjx2dd37l5e"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahh00p1sdjx3it42zuu"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahi00p3sdjxheyk5fl2"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahi00p6sdjx4vmfd0te"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ahk00p8sdjx3esdgyyc"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahl00pbsdjxe2p08f0u"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahl00pdsdjx1zqu1sk5"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9ahm00pgsdjxbqmv3lwb"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9ahn00pisdjxbata3n3i"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahn00plsdjxazlt9o5g"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aho00pnsdjx6pbi6k0k"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahp00pqsdjxdkjrbca0"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahp00pssdjxd93kh2jw"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahp00pusdjx8gy5fq5c"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ahq00pxsdjxb3l0gxjb"},{"post_id":"ckeaq9a8q009isdjxd9657doc","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahq00pzsdjx1zf1ecu0"},{"post_id":"ckeaq9a8q009isdjxd9657doc","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahr00q2sdjx2i3n2aqf"},{"post_id":"ckeaq9a8q009isdjxd9657doc","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ahr00q4sdjx8x5d237r"},{"post_id":"ckeaq9a8w009psdjxazxghrs5","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aht00q7sdjx8cnze0l6"},{"post_id":"ckeaq9a8w009psdjxazxghrs5","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aht00q9sdjxgq6p2r55"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aht00qcsdjxhzpag8a5"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahu00qesdjx40uu6wwr"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ahu00qhsdjx6guq6yxy"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ahu00qjsdjx7viv4g8a"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahw00qmsdjx72bv1qg2"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahw00qosdjx2t335raa"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahx00qrsdjx8ncsbmh4"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ahx00qtsdjxbzvb46f1"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ahy00qwsdjxea7026fy"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ahy00qysdjx49xbhkzu"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ahz00r1sdjx63xu55kj"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ai000r3sdjxbmz450ln"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ai100r6sdjxefwz9yvi"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ai100r8sdjx8rxw91mw"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ai300rbsdjx9976cc0a"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ai300rdsdjxbxnk7x19"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ai400rgsdjxeqa46hge"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ai400risdjxgmc60vpk"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ai500rlsdjxdbys6ivc"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ai500rnsdjx0u683j6s"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ai600rqsdjxecuwglaj"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ai700rssdjxhptjfh1k"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9ai800rvsdjx41r4a65r"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ai800rxsdjx733u8rj1"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ai900s0sdjx58e9aa49"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ai900s2sdjxbrwd3ted"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9ai900s4sdjxeb5212oh"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aia00s6sdjxglg93cg1"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aia00s8sdjx47qr336e"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aia00sasdjxe15ecv4k"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aia00scsdjxbir55rk7"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aia00sesdjx4z3a8m5c"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aib00sgsdjx25x93q2f"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aic00sisdjxgwgjcymw"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aic00sksdjxajspczem"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aid00smsdjxbiav0px2"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aid00sosdjxhclegt9q"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aid00sqsdjx6sv7f5ai"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aid00sssdjxhpyp4hdp"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aie00susdjx926ycoaq"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aie00swsdjx384ee8mg"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aie00sysdjx6befguum"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aif00t0sdjxbipxgel6"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aif00t2sdjx0ao66227"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aim00t4sdjx95epf780"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aim00t6sdjx9ffcad1m"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9ain00t8sdjx6k9fenar"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aio00tasdjx46z8e470"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aio00tcsdjx2srkb2s9"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aio00tesdjxffizht46"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aio00tgsdjx7dvvbk49"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aio00tisdjx69lndq25"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aio00tksdjxeg9a6919"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aio00tmsdjxgvns3rkk"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aip00tosdjx02dmebks"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aip00tqsdjx3p1n8qyr"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aip00tssdjx0j5b8589"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aip00tusdjx060c20ta"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aip00twsdjx9mcmbol6"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aip00tysdjx4wod2vac"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aiq00u0sdjx19jy8m8g"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aiq00u2sdjxfbvw0633"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aiq00u4sdjxbmhk2d12"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aiq00u6sdjx1e0r8z1b"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aiq00u8sdjx0fnwblmf"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ait00uasdjx5lpzf2rw"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aiv00ucsdjx2pn8aqgq"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aiv00uesdjx8gajatu8"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aiv00ugsdjx3al18jfn"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj000uisdjx9452dtce"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj000uksdjx4ezwflgg"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj000umsdjx8lmia68s"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aj000uosdjxgjcd1xf5"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9aj100uqsdjx3s2y1m7u"},{"post_id":"ckeaq9ab500busdjx4f7ufpzj","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj100ussdjx9w4qa48y"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj100uusdjx1vgx13oh"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj100uwsdjx9sbjfmp7"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj200uysdjxgc7qglbm"},{"post_id":"ckeaq9abj00bxsdjx0yjzajbp","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj200v0sdjxeh56fgna"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj200v2sdjxds2f0vmn"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj200v4sdjxaujk18bo"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj200v6sdjx5a5p1axd"},{"post_id":"ckeaq9abu00c4sdjx03regp69","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj300v8sdjx2io99u4i"},{"post_id":"ckeaq9abu00c4sdjx03regp69","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj300vasdjxhvg6csx9"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj300vcsdjxcxmzgyjq"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj300vesdjx6w3bbnxf"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj300vgsdjx6e1rb857"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aj400visdjx364j86gf"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj400vksdjxd8dpgj6j"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj400vmsdjx9dny5wx4"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj400vosdjxbfhq2e5k"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj500vqsdjx4h4whbr5"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj500vssdjxd7kg0blh"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj500vusdjx4d745ujj"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj500vwsdjxfldwfyyc"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj500vysdjxbbyu49g7"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj600w0sdjxdm0f1l11"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj600w2sdjxgetk2q8a"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj600w4sdjx0i0nh2u6"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj600w6sdjx6msmgc9k"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj600w8sdjxe1d708zd"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj700wasdjxdmbycfte"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj700wcsdjx45wn2jfo"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj700wesdjxghlp4c8o"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj700wgsdjx75wjb5zn"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj700wisdjx1tqhge1c"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj800wksdjx939pauko"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","category_id":"ckeaq99wv0024sdjxecebh2z8","_id":"ckeaq9aj800wmsdjx5lgg19d9"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj800wosdjxfwkz660z"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj800wqsdjx0p1a6ouk"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj800wssdjx8sffe9et"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj800wusdjxewz90ddr"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj800wwsdjx0tc24iog"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj800wysdjx5kkz4jbt"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj900x0sdjx2sxy8brp"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj900x2sdjx8x1i99i3"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj900x4sdjx94jn1xj9"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","category_id":"ckeaq99t5000osdjx4n4m8jtd","_id":"ckeaq9aj900x6sdjx6ofx511p"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","category_id":"ckeaq99tn0010sdjx1jcnebve","_id":"ckeaq9aj900x8sdjx8q7ygzw1"},{"post_id":"ckeaq9ae600dasdjx6dme68f0","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9aj900xasdjx4oru92ms"},{"post_id":"ckeaq9ae600dasdjx6dme68f0","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9aj900xcsdjxhuni71vj"},{"post_id":"ckeaq9ae300d7sdjxhrahhy7u","category_id":"ckeaq99s9000csdjx9rtz88dt","_id":"ckeaq9aj900xesdjxbqmkbwre"},{"post_id":"ckeaq9ae300d7sdjxhrahhy7u","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ajb00xgsdjx5rlh0cl5"},{"post_id":"ckeaq9ae300d7sdjxhrahhy7u","category_id":"ckeaq99rs0004sdjxe6r22zha","_id":"ckeaq9ajf00xisdjx3jf2efvt"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","category_id":"ckeaq99s30008sdjxgds0bl3v","_id":"ckeaq9ajg00xksdjx95hu3ag1"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","category_id":"ckeaq99wk001vsdjx49pl7hid","_id":"ckeaq9aji00xmsdjx6yvw80po"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","category_id":"ckeaq99ya0030sdjxfcbn4mxo","_id":"ckeaq9aji00xosdjxet6f7vto"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","category_id":"ckeaq9a08004csdjxcp2y48qd","_id":"ckeaq9aji00xqsdjxbap3e2ol"}],"PostTag":[{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq99wm001wsdjx70bkboqb"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq99wr001zsdjx80755gwv"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq99wv0023sdjx2hofa0rx"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq99wy0027sdjxeobze04p"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq99x4002bsdjxg6nfcb31"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq99x9002fsdjx6hgo0yqs"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq99xf002jsdjxhu9l0j60"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq99xk002nsdjxb28gajxj"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq99xp002rsdjx11mghk0t"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq99xt002vsdjx4j2e3mn0"},{"post_id":"ckeaq99qu0000sdjx5fandf4v","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq99y2002zsdjx0kxch5zp"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq99yi0033sdjxd3g87gs5"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq99yt0037sdjxafp88x7r"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq99yy003bsdjxgxfe2j36"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq99z1003fsdjx9s7rhjtx"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq99z8003jsdjx68xlh7ou"},{"post_id":"ckeaq99wz0029sdjx8orv9syd","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq99zd003nsdjx5xy8foh5"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq99zh003rsdjx5rhpeyry"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq99zn003usdjx0sd614e0"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq99zu003zsdjxew3h1xen"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq99zw0042sdjx71xwa9al"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq99zy0047sdjxde4ye10d"},{"post_id":"ckeaq99xb002hsdjx5kj26tbv","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a01004asdjx86388tvp"},{"post_id":"ckeaq99yz003csdjx70udhs2n","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a0c004fsdjx1vr61896"},{"post_id":"ckeaq99yz003csdjx70udhs2n","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a0f004isdjx29uohb2d"},{"post_id":"ckeaq99yz003csdjx70udhs2n","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9a0p004nsdjx463m8o6p"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99wf001ssdjxda6uhb8a","_id":"ckeaq9a0v004qsdjx04g29jmh"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a1x004vsdjx168ncpgp"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a26004ysdjx4c8hc5in"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a2f0053sdjx4xf5ez3n"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9a2o0056sdjx7nuk74zn"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a2u005bsdjxbfvuflnr"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9a33005esdjx8kam2j1z"},{"post_id":"ckeaq99re0001sdjx2opubyx3","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a39005isdjxd7g64gaa"},{"post_id":"ckeaq99rr0003sdjxctj07si9","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9a3g005lsdjxcvhf0yyc"},{"post_id":"ckeaq99rr0003sdjxctj07si9","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9a3l005psdjxb39209ie"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a3p005ssdjx8i3i5xma"},{"post_id":"ckeaq9a0g004jsdjxb95q4met","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a43005wsdjx285ehwky"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99wf001ssdjxda6uhb8a","_id":"ckeaq9a47005zsdjx6cke49qc"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a4a0063sdjxdqn49tpb"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a4d0066sdjx7uim33ye"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a4g0069sdjx7cieh0gy"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9a4l006dsdjxguvmeyqt"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq9a4q006gsdjxfcemc0h7"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9a4w006ksdjxa1xjhym3"},{"post_id":"ckeaq9a0r004osdjx22cqboc6","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a4z006nsdjx2qf9a4gq"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9a54006rsdjx6nnx1bbb"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9a57006usdjxeph56nz6"},{"post_id":"ckeaq99ru0005sdjx3oyz216e","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9a5f006ysdjxdbobb5n9"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9a5h0071sdjx5h35ab0r"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a5m0075sdjx88wh4e1l"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9a5s0078sdjxde695wmn"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a6b007csdjx2tsm8bcg"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a6h007fsdjxh608bp9j"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9a6z007jsdjxd13q7b2k"},{"post_id":"ckeaq9a21004wsdjxb9qm30lk","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a71007msdjx8ynmbltw"},{"post_id":"ckeaq99s10006sdjxd42i1syq","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a76007qsdjxbhbb8j0x"},{"post_id":"ckeaq99s10006sdjxd42i1syq","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9a79007tsdjx61as74uk"},{"post_id":"ckeaq99s10006sdjxd42i1syq","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a7c007xsdjxbqsm2lc5"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a7e0080sdjx9h74ai9h"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a7j0084sdjx4x7eax6c"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a7o0087sdjxb8zt4qhk"},{"post_id":"ckeaq9a34005fsdjx641tcwyo","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a7s008bsdjxdcvaa3qv"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a7u008esdjx3y7s52us"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a7x008isdjx8m167fxq"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a80008lsdjx3sw25p8q"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9a84008psdjx08ph3b4o"},{"post_id":"ckeaq9a3b005jsdjx8t2c3999","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a86008ssdjxbpkm81oc"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a88008wsdjx8xc96x5j"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a89008zsdjxa4r3a1gf"},{"post_id":"ckeaq9a4e0067sdjx1pvm99t8","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9a8c0093sdjx7420hyp5"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a8f0096sdjx3iv18r4t"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9a8i009asdjx80fq6xym"},{"post_id":"ckeaq9a4i006bsdjx2bw02r0z","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a8n009dsdjxafybedir"},{"post_id":"ckeaq9a4r006isdjx04r0cg0z","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a8q009hsdjx155z2qwg"},{"post_id":"ckeaq9a4r006isdjx04r0cg0z","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9a8s009ksdjxhl6dcmkv"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9a8w009osdjx19lkf5go"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9a8z009rsdjxc3dxg7ay"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9a91009vsdjx8jp023i0"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9a93009ysdjxas2qb9gt"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9a9500a2sdjxfu9q8tws"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq9a4a0062sdjxc06l7q8d","_id":"ckeaq9a9600a5sdjxdd7z90ru"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9a9900a9sdjx96hodi8f"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9a9a00acsdjxfssx6840"},{"post_id":"ckeaq99sa000dsdjxfs0edmxr","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9a9e00agsdjx36fmgnmu"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a9g00ajsdjxf40zhs9p"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9a9j00ansdjx4b4gb1na"},{"post_id":"ckeaq9a55006ssdjxbs8d52ng","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9a9n00aqsdjxf8ue96jj"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9a9s00ausdjx6mkz98kd"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aa000axsdjxci1g76i8"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aa500b1sdjx6hpubqrb"},{"post_id":"ckeaq9a5f006zsdjx4qke1yty","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aa900b4sdjx9oob3la2"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aaf00b8sdjx9dqoc8nf"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aaj00bbsdjxb2jf4vtt"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aan00bfsdjxe8ilel3m"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aat00bisdjxe02h6m9i"},{"post_id":"ckeaq9a5h0073sdjxgpg6ehpm","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aaw00bmsdjxez4yh7dt"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aaz00bpsdjx9ddccoqe"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ab400btsdjx029h0a3m"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9abg00bwsdjxflf86xhl"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9abn00c0sdjx125u8i90"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9abt00c3sdjx0c0w8dl9"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aby00c7sdjxg0micfaj"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ac400casdjxfra0a058"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ac900cesdjx38p28izo"},{"post_id":"ckeaq9a5n0076sdjx8j3ifjdz","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ad700chsdjx1rtpe2wd"},{"post_id":"ckeaq9a64007asdjxax46g6wd","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9ad900clsdjxbsj927wp"},{"post_id":"ckeaq9a64007asdjxax46g6wd","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9adc00cosdjxfyt982e4"},{"post_id":"ckeaq9a64007asdjxax46g6wd","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9adh00cssdjxa5yyeoe8"},{"post_id":"ckeaq9a64007asdjxax46g6wd","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9adj00cvsdjx1e785xat"},{"post_id":"ckeaq9a64007asdjxax46g6wd","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9adq00czsdjx5t8p51r6"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9adu00d2sdjx8ydia2v4"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ae200d6sdjxfl0whhzu"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ae500d9sdjx02uq9dtx"},{"post_id":"ckeaq9a6b007dsdjx76n6h8jv","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aea00ddsdjx9es58po4"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aeq00dgsdjxd8qqgc4z"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aes00djsdjx17fh4zc2"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aet00dlsdjx4dr24byn"},{"post_id":"ckeaq9a6z007ksdjx3xvzcsax","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aeu00dosdjxhrgv19ks"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aeu00dqsdjxf7vchih4"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aev00dtsdjx59lvb9mp"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aev00dvsdjxh2w58a7t"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aew00dysdjx72n72kde"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aex00e0sdjxc1fr88po"},{"post_id":"ckeaq9a72007osdjxe73j1oit","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aex00e3sdjx49xle54k"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aex00e5sdjx1xhnhdee"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aey00e8sdjx7t7maaf5"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aez00easdjx3uhn3uef"},{"post_id":"ckeaq9a7c007ysdjxe9zx4ses","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aez00ecsdjx9d1j8654"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9af000efsdjx9lso82ed"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9af000ehsdjxf0up8iw9"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9af100eksdjx4ynfe47y"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9af100emsdjxazgpdbv2"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9af200epsdjx1w2e7po7"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9af200ersdjx343x81b0"},{"post_id":"ckeaq9a7h0082sdjxcixv41qo","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9af400eusdjx5u2xbnlq"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9af400ewsdjx14pu6fuc"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9af500ezsdjx04e61j6c"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9af500f1sdjxdtrb12ee"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9af600f4sdjxfom91krl"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9af600f6sdjx012tgbo5"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq9a4a0062sdjxc06l7q8d","_id":"ckeaq9af700f9sdjx6hyld5b1"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9af700fbsdjxa42c881s"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9af900fesdjx2aq9bejl"},{"post_id":"ckeaq99sc000esdjx59exegqq","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9af900fgsdjx4v7x8gzz"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9afa00fjsdjxf9g3d8dq"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afa00flsdjxdrpndv73"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afb00fosdjx76lzge2q"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9afb00fqsdjx2xm86xwk"},{"post_id":"ckeaq9a7l0085sdjxgeeo0um9","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afb00ftsdjxa9a4hn8g"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9afc00fvsdjx6m122kxs"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9afc00fxsdjxcivhazgu"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afc00g0sdjx6sn3bsj6"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afd00g2sdjx4otpapok"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9afd00g5sdjx0nm235b8"},{"post_id":"ckeaq9a7q0089sdjx8v197vfm","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aff00g7sdjxczvee5gt"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aff00gasdjx36u34v2x"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9afg00gcsdjxfmgng2gk"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afh00gfsdjxgouad0xv"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afh00ghsdjx120x0hw8"},{"post_id":"ckeaq9a7t008csdjx6ir3haiv","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afi00gksdjx74xx88fa"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9afi00gmsdjxdq0w665f"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afj00gpsdjx4v4mgujp"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afj00grsdjx0wh66a10"},{"post_id":"ckeaq9a7v008gsdjxc7ema2ye","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afk00gusdjxblvw859o"},{"post_id":"ckeaq9a7y008jsdjx8ocq01gl","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afk00gwsdjx8q9fcb5w"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9afl00gzsdjxhljte5th"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afl00h1sdjxghvm7tq9"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afm00h4sdjxhlz962vb"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9afm00h6sdjx1rl96pj4"},{"post_id":"ckeaq9a81008nsdjx6qwy3sjh","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afn00h9sdjx9bcmd47z"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9afn00hbsdjxesm4dmxr"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afo00hesdjx50y31g1g"},{"post_id":"ckeaq9a84008qsdjx2uid9asx","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afo00hgsdjxdfuaew2n"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9afp00hjsdjx39wsfmqo"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afp00hlsdjx53mcelio"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9afq00hosdjx5hf276p7"},{"post_id":"ckeaq9a8d0094sdjxeikg9f2j","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afq00hqsdjxbuwxdg6t"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9afr00htsdjxaea433i7"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9afr00hvsdjx9w1wez1c"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9afs00hysdjx21lr77sm"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq9afs00i0sdjx4eb7gm8d"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9afx00i3sdjxd4d4fs5t"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9afx00i5sdjx998y3t3c"},{"post_id":"ckeaq9a8f0098sdjxe9go7ui7","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9afy00i8sdjx1gxe56ka"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9afy00iasdjxbmke8cpb"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9afz00idsdjxa22z4owd"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ag000ifsdjx717vgjea"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ag000iisdjx350oef3e"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ag100iksdjxdj41gpnu"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq9a4a0062sdjxc06l7q8d","_id":"ckeaq9ag100insdjx26zufo3e"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ag200ipsdjx0tl2d8yy"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ag200issdjx962d6lum"},{"post_id":"ckeaq99sh000hsdjxb6ag3mss","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ag300iusdjxbcci7s9y"},{"post_id":"ckeaq99sx000jsdjxcjms1oci","tag_id":"ckeaq99zw0043sdjx9nkbdfbu","_id":"ckeaq9ag400ixsdjxag6i2zw7"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ag500izsdjx6avy9apb"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ag500j1sdjx1lbf4g2c"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ag600j4sdjx1dsq1h9d"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ag600j6sdjx1aol71tw"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ag700j9sdjxdb6rgra2"},{"post_id":"ckeaq9a8t009msdjx02m62a6t","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ag700jbsdjxhdwk52ds"},{"post_id":"ckeaq9a8w009psdjxazxghrs5","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ag800jesdjx5komd0i4"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ag800jgsdjx6beh5gvx"},{"post_id":"ckeaq9a9500a3sdjxh9s508o8","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ag900jjsdjxfpuo0c6l"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ag900jlsdjx0tg6h8xo"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aga00josdjxbht53u7o"},{"post_id":"ckeaq9a9700a7sdjxgfjregzi","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aga00jqsdjx6dbaeiwa"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9agb00jtsdjx9p5ag8ti"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9agb00jvsdjx0orm1pki"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agc00jysdjx9s104he3"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9agc00k0sdjxd9ww5027"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9agd00k3sdjxgoumgf52"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9agd00k5sdjx8bu33wbe"},{"post_id":"ckeaq99t3000nsdjxdenk3vb1","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9age00k8sdjxb3d148pu"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9age00kasdjxbi35976q"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9agf00kdsdjx9uym8gnh"},{"post_id":"ckeaq9a9k00aosdjxe20eh6lg","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agf00kfsdjxhjqxc89e"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9agg00kisdjx5qfbe5iv"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9agg00kksdjx6qhbe9bu"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agh00knsdjxdezvaxh4"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9agh00kpsdjxe8pk797g"},{"post_id":"ckeaq9a9o00assdjxbgm129yc","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq9agi00kssdjxd4so53e7"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9agi00kusdjx84fwaqrd"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9agj00kxsdjx5bjngnwq"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9agk00kzsdjxca321g9h"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agl00l2sdjx14pwey17"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9agl00l4sdjxgpxhe0y8"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9agm00l7sdjx2hs84pby"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9agm00l9sdjx1itn04b6"},{"post_id":"ckeaq99t5000psdjx3dr1cbpm","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9agn00lcsdjx34571mjo"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9agn00lesdjxd4gq9x7a"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ago00lhsdjx74kxhcgx"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9ago00ljsdjxf2tn7avl"},{"post_id":"ckeaq9aat00bjsdjxf4bi5ss2","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9agp00lmsdjx12ak02j6"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","tag_id":"ckeaq9aaa00b5sdjxb41298ft","_id":"ckeaq9agp00losdjx21n1fglk"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9agq00lrsdjx0n8k34rn"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agr00ltsdjxbphx2hzd"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9agr00lwsdjx09xbaiad"},{"post_id":"ckeaq99tc000rsdjx9fxme6p6","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ags00lysdjxbtxnbnoj"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9agt00m1sdjxgswu2a90"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9agu00m3sdjx9uxsg9sa"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agu00m6sdjxgai2czp2"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9agv00m8sdjxhdu4gyih"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9agv00masdjx5n7heug8"},{"post_id":"ckeaq99tf000tsdjxeb2scz44","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9agw00mdsdjx7cf35mdm"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9agw00mfsdjxfwwg38tb"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9agx00misdjx9yhgdqgs"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9agx00mksdjxg2739636"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9agy00mnsdjx6e3chj5o"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9agy00mpsdjx34jsfnce"},{"post_id":"ckeaq9abz00c8sdjxf3qvfduc","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9agz00mssdjx5ama8ycu"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9agz00musdjx1w778g5n"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ah000mxsdjx39by1cqt"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ah100mzsdjx5baehfxp"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ah100n2sdjxgp36ec95"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ah200n4sdjx1kviek41"},{"post_id":"ckeaq9ac500cbsdjx5eti84ov","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ah200n7sdjxep927v44"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ah200n9sdjxdggnc4px"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ah300ncsdjx7fiqa4w1"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ah300nesdjxef71ctq3"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ah400nhsdjxfh8gdw1a"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ah400njsdjx7npqdp9z"},{"post_id":"ckeaq9ada00cmsdjx31mfg5j7","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ah400nlsdjx1qio2wur"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ah500nosdjx45vqeqzb"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ah600nqsdjx3dovfy6o"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ah700ntsdjxc9vr3dyb"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ah700nvsdjx9hom9s0m"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ah800nysdjxc1dl1caa"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ah900o0sdjxg7mg7m1i"},{"post_id":"ckeaq99tk000xsdjxgczwe103","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aha00o3sdjxh9xn96em"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aha00o5sdjx4oyg6821"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahb00o8sdjxhb6k9ylp"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ahb00oasdjxb3kk3qit"},{"post_id":"ckeaq9add00cpsdjxg4n496qj","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahc00odsdjx2snr6ofs"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahc00ofsdjx6ql19jvl"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ahd00oisdjx0xmx0ox0"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ahd00oksdjxgyqm4gbb"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ahe00onsdjxbso73khq"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahe00opsdjx5u4s7wbo"},{"post_id":"ckeaq9adi00ctsdjxgpupc4k9","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ahf00ossdjx7236h7bo"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ahg00ousdjx4gawd7ug"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahh00oxsdjx2or8g7yc"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ahh00ozsdjx90gbahka"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ahi00p2sdjx42lk5ybb"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ahi00p4sdjx13gj61jr"},{"post_id":"ckeaq9adj00cwsdjx9lvg0dcd","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahk00p7sdjx21xibnh3"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ahl00p9sdjx6rd86mum"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahl00pcsdjxaqcgcfo9"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ahl00pesdjx5j79e2xq"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ahn00phsdjx1ubfbr9a"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ahn00pjsdjx65w3c8ab"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aho00pmsdjxgyzneq55"},{"post_id":"ckeaq9adt00d0sdjxd1rbfl99","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aho00posdjx715ghhka"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ahp00prsdjx4qd94wdl"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahp00ptsdjxbxgbap6m"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ahq00pwsdjxd8fddcee"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ahq00pysdjxe7ol4jg4"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ahr00q1sdjxgbm2hlsq"},{"post_id":"ckeaq99tm000zsdjxgf95ax4u","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahr00q3sdjxdkwy8udz"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ahs00q6sdjx5mxmhfkj"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aht00q8sdjxd6x4fcos"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aht00qbsdjx3t159mwc"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ahu00qdsdjxbw2s9kgk"},{"post_id":"ckeaq9adu00d3sdjx3wrsdedf","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahu00qgsdjxf7qhas0d"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ahu00qisdjxbbhw1qxu"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ahw00qlsdjxhqtye13b"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahw00qnsdjx233b6a35"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9ahx00qqsdjx9s67hk6l"},{"post_id":"ckeaq9aeb00desdjx7xtbahjm","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ahx00qssdjxhc7h745o"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ahy00qvsdjx7hwkgwnd"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ahy00qxsdjxc2jb514t"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ahz00r0sdjxgu40bolq"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ai000r2sdjxfcv3b8tx"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ai100r5sdjx69kzcz2d"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ai100r7sdjx9itl06fm"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ai200rasdjx7jgjejbd"},{"post_id":"ckeaq99u60013sdjx87lx0yym","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ai300rcsdjxai889wlp"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ai300rfsdjxh3tihy7w"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ai400rhsdjx429o2bsb"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ai400rksdjx98bx2iil"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ai500rmsdjxfmdp1r71"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ai500rpsdjx6dzrccmn"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ai600rrsdjx6p0g9hb9"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq9aey00e7sdjx4xlz4hfe","_id":"ckeaq9ai700rusdjxfyg9f9td"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ai800rwsdjx653h2cep"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ai900rzsdjx24ld7bgc"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ai900s1sdjx26cu4ek7"},{"post_id":"ckeaq99ud0015sdjxe3bicarp","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ai900s3sdjx9pm709ii"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aia00s5sdjx8l9cfkzd"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aia00s7sdjx0od9e591"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aia00s9sdjx4uj47nvb"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aia00sbsdjx8bg16ab1"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aia00sdsdjxh9fw3q9m"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9aia00sfsdjxbiho7ms5"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aib00shsdjx2wxg02ue"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aic00sjsdjx36dy44p4"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aid00slsdjx74vj9ztl"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aid00snsdjxhhmyh5x0"},{"post_id":"ckeaq99uh0019sdjxah2t8k96","tag_id":"ckeaq9af600f7sdjx44so0ipa","_id":"ckeaq9aid00spsdjxgq2686zm"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aid00srsdjx9d39a7am"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aie00stsdjx0lhka14j"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9aie00svsdjxe6o80qou"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9aie00sxsdjx94dtdj2m"},{"post_id":"ckeaq99uj001bsdjx1f3mbsya","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9aie00szsdjxcppu8k1o"},{"post_id":"ckeaq99um001esdjx82f6alcm","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aif00t1sdjxfp2k9zmc"},{"post_id":"ckeaq99um001esdjx82f6alcm","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9aim00t3sdjxa9iud7cx"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq9afa00fnsdjxbbe21pyy","_id":"ckeaq9aim00t5sdjx9bvnej38"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aim00t7sdjxewhabu9q"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq9afb00fssdjx5om8da7c","_id":"ckeaq9ain00t9sdjx17q4azzq"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aio00tbsdjxgewn8868"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aio00tdsdjxdmlp26vh"},{"post_id":"ckeaq99us001hsdjxh1123mjy","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aio00tfsdjxh43b5l5f"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq9afd00g3sdjxfnn94wgd","_id":"ckeaq9aio00thsdjx9ilab6ry"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aio00tjsdjxgv0r1dew"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq9afb00fssdjx5om8da7c","_id":"ckeaq9aio00tlsdjx6y7x5ci6"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aip00tnsdjx4hfv07rw"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aip00tpsdjx7i7e726t"},{"post_id":"ckeaq99uy001ksdjx0kddbabw","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aip00trsdjxb6oi4nxa"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq9afd00g3sdjxfnn94wgd","_id":"ckeaq9aip00ttsdjxg23m22ga"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aip00tvsdjxbbb0hwnj"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aip00txsdjx2p601gc2"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq9afb00fssdjx5om8da7c","_id":"ckeaq9aiq00tzsdjxb9f9e5yq"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aiq00u1sdjxd6kh378d"},{"post_id":"ckeaq99w9001nsdjxekcseqh8","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aiq00u3sdjx3cy05oxk"},{"post_id":"ckeaq99wd001qsdjx8y705xah","tag_id":"ckeaq9afd00g3sdjxfnn94wgd","_id":"ckeaq9aiq00u5sdjx1y9026ez"},{"post_id":"ckeaq99wd001qsdjx8y705xah","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aiq00u7sdjx1i9c68p4"},{"post_id":"ckeaq99wd001qsdjx8y705xah","tag_id":"ckeaq9afb00fssdjx5om8da7c","_id":"ckeaq9ait00u9sdjxbpbwdavb"},{"post_id":"ckeaq99wd001qsdjx8y705xah","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ait00ubsdjx46utb9c5"},{"post_id":"ckeaq99wd001qsdjx8y705xah","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aiv00udsdjx3oaghvxe"},{"post_id":"ckeaq99wi001tsdjx92527s3n","tag_id":"ckeaq9afd00g3sdjxfnn94wgd","_id":"ckeaq9aiv00ufsdjxda2qbsxj"},{"post_id":"ckeaq99wi001tsdjx92527s3n","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj000uhsdjx506meyu2"},{"post_id":"ckeaq99wi001tsdjx92527s3n","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aj000ujsdjxdf067b4i"},{"post_id":"ckeaq99wi001tsdjx92527s3n","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aj000ulsdjx9frh5oqk"},{"post_id":"ckeaq99wi001tsdjx92527s3n","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj000unsdjx4l6kd71c"},{"post_id":"ckeaq99wp001xsdjxdp61c04i","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9aj000upsdjx7hax5mao"},{"post_id":"ckeaq99wp001xsdjxdp61c04i","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9aj100ursdjx6816hwof"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aj100utsdjx3vq55zu2"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj100uvsdjxaignco9w"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj200uxsdjxcljdaesb"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq9afr00hwsdjx0pcy6gu8","_id":"ckeaq9aj200uzsdjxen3o2tm4"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aj200v1sdjxh4b96g9b"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj200v3sdjxfz7f0la4"},{"post_id":"ckeaq99wx0025sdjx0s288ryh","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9aj200v5sdjx8nrfhyin"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aj200v7sdjx8ti99vfz"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj300v9sdjxh3wa4s5p"},{"post_id":"ckeaq99xh002lsdjx7ryn73g6","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj300vbsdjxej35co0i"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj300vdsdjx3lpz8d7b"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj300vfsdjxdvysep0f"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aj400vhsdjxes3hgz08"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9aj400vjsdjxcr6makqc"},{"post_id":"ckeaq99xm002psdjx5n5n4asm","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj400vlsdjxa2b8c0vy"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99wf001ssdjxda6uhb8a","_id":"ckeaq9aj400vnsdjxgmh2d57g"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aj400vpsdjx29g4bhjz"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj500vrsdjx0jzkhev7"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj500vtsdjxg5fjfxlb"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9aj500vvsdjxhv8370sr"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9aj500vxsdjxbtwi8xtv"},{"post_id":"ckeaq99xq002tsdjx1bw52tik","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj500vzsdjx8e0s75fr"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj600w1sdjxefdv1v5y"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj600w3sdjxazk150zt"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj600w5sdjx530g9u2k"},{"post_id":"ckeaq99xz002wsdjx9voi5s68","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj600w7sdjx6wr32kqd"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aj600w9sdjx55358stj"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aj700wbsdjxbh4y4zo9"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj700wdsdjx9zwe66jr"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj700wfsdjxekqqhykj"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj700whsdjx4c2af5lm"},{"post_id":"ckeaq99yb0031sdjx1k0y5gua","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj800wjsdjx7cii6i5s"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj800wlsdjxaf4lfdtc"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj800wnsdjx5mupf4ul"},{"post_id":"ckeaq99yu0038sdjxhnid1pl7","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj800wpsdjx5dfa9pfd"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","tag_id":"ckeaq9ag300iwsdjx1xdr8ehg","_id":"ckeaq9aj800wrsdjx85xzeawv"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","tag_id":"ckeaq9afd00g3sdjxfnn94wgd","_id":"ckeaq9aj800wtsdjxg7m51ufe"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj800wvsdjxe48568dt"},{"post_id":"ckeaq99z4003gsdjx9hz71dd0","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9aj800wxsdjxf57zgvwj"},{"post_id":"ckeaq99zj003ssdjx0kalcfbi","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9aj900wzsdjxe2347uxe"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aj900x1sdjxebuwa4x4"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aj900x3sdjx3o2wezbh"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj900x5sdjx7q3a60nf"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj900x7sdjx3st8cwh9"},{"post_id":"ckeaq99zo003wsdjx37fsgeys","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aj900x9sdjx0zone6iz"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aj900xbsdjxbazub9v9"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9aj900xdsdjxci8t2j1f"},{"post_id":"ckeaq99zw0044sdjx6devbh5o","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9aj900xfsdjx2ucvfz6n"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajb00xhsdjx0er1cnxu"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","tag_id":"ckeaq9aga00jrsdjxdfjc0et8","_id":"ckeaq9ajf00xjsdjx4aag24dm"},{"post_id":"ckeaq99zz0048sdjxd4lic37l","tag_id":"ckeaq9a280051sdjx0rctey0s","_id":"ckeaq9ajg00xlsdjx7vrd71os"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9aji00xnsdjxd9hd31wn"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","tag_id":"ckeaq9aaa00b5sdjxb41298ft","_id":"ckeaq9aji00xpsdjx81sidsx4"},{"post_id":"ckeaq9a02004bsdjx98b80r6n","tag_id":"ckeaq9afr00hwsdjx0pcy6gu8","_id":"ckeaq9aji00xrsdjxdcl33u0a"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9aji00xssdjx0a82d2ty"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajn00xtsdjxhp6fgzm4"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajo00xusdjx0dfa4ot5"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajo00xvsdjxds76a1cl"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajo00xwsdjxeywo2okz"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq9a4a0062sdjxc06l7q8d","_id":"ckeaq9ajo00xxsdjxcgdx216a"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ajo00xysdjx0eudd44o"},{"post_id":"ckeaq9a0e004gsdjxgbup1958","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajo00xzsdjx1i2pcbr1"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ajo00y0sdjxefyybf1e"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajo00y1sdjx4mx1ap9z"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajo00y2sdjxdp2e3c8a"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajo00y3sdjxf6aih4jm"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ajo00y4sdjx92ii316y"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ajo00y5sdjx3izh44f8"},{"post_id":"ckeaq9a0w004rsdjxeln8er3w","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajo00y6sdjx0pct3t2c"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajp00y7sdjxdtm38wcl"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajp00y8sdjxbu4ihrbk"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajp00y9sdjx7ec27c1y"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajp00yasdjx16xxgl5v"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ajp00ybsdjx3h9j43gx"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajp00ycsdjxd0ad4k9s"},{"post_id":"ckeaq9a26004zsdjxcjvy8hbj","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajp00ydsdjx5nhwaezu"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ajp00yesdjx496n9hc0"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ajp00yfsdjx4h27d6c3"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ajp00ygsdjx2m5s8hz8"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajp00yhsdjxca7v1r27"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajp00yisdjx6yvg8bs7"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajp00yjsdjxd6xienq6"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ajp00yksdjx78w69nyz"},{"post_id":"ckeaq9a2j0054sdjxgskz75wp","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajp00ylsdjxghv2aiz1"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","tag_id":"ckeaq9agk00l0sdjxgeu1d22z","_id":"ckeaq9ajp00ymsdjx3wq0cjog"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajp00ynsdjx1m3o80mu"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajq00yosdjxhl6uhk68"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajq00ypsdjxg7cwgejq"},{"post_id":"ckeaq9a2p0057sdjx2oa5e9rj","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajq00yqsdjxg9406z5f"},{"post_id":"ckeaq9a3h005msdjxextf1ons","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajq00yrsdjx7v1w89ge"},{"post_id":"ckeaq9a3h005msdjxextf1ons","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajq00yssdjx7c1u4j43"},{"post_id":"ckeaq9a3h005msdjxextf1ons","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ajq00ytsdjx78uvcadx"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajq00yusdjx1rg7579k"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajq00yvsdjxfb7e6095"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajq00ywsdjx74alfefw"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ajq00yxsdjx03fibk8q"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajq00yysdjxghvk91h6"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajq00yzsdjx5919bo3u"},{"post_id":"ckeaq9a3o005qsdjxexk20abe","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ajq00z0sdjxbuxqhuiy"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq9agk00l0sdjxgeu1d22z","_id":"ckeaq9ajq00z1sdjxbysi5i91"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajq00z2sdjx0s7s1ij3"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajq00z3sdjx1gp4fuzk"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq99yo0036sdjx4dyzbvmu","_id":"ckeaq9ajq00z4sdjx4oie3ktg"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq9agp00llsdjx6mhdfvm6","_id":"ckeaq9ajq00z5sdjx275eaz7b"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajr00z6sdjxdc2ag9hl"},{"post_id":"ckeaq9a3y005tsdjx1zrf6sfk","tag_id":"ckeaq9af600f7sdjx44so0ipa","_id":"ckeaq9ajr00z7sdjxbgf79p72"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ajr00z8sdjxav093p40"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajr00z9sdjxf37690ar"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajr00zasdjx0l0e7ej1"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajr00zbsdjxhse35qb0"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajr00zcsdjxfvg5f2gs"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ajr00zdsdjx5gr4dttb"},{"post_id":"ckeaq9a480060sdjx4kvnevnu","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajr00zesdjxbdn58wjm"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajr00zfsdjxc5m954i5"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajr00zgsdjxcx10ftel"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajr00zhsdjxbvngell1"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ajr00zisdjxcerj99e4"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajr00zjsdjxfsufdc12"},{"post_id":"ckeaq9a4b0064sdjxg3tq4sun","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajr00zksdjx3256diyr"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajr00zlsdjxbv9r3rxr"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajr00zmsdjxfc0w0u1s"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajr00znsdjxhc4dd9uu"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq9agu00m5sdjxcbtbh7vl","_id":"ckeaq9ajr00zosdjx2pw62fkw"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajs00zpsdjx6pj77w1n"},{"post_id":"ckeaq9a4m006esdjxck5kfsph","tag_id":"ckeaq9af600f7sdjx44so0ipa","_id":"ckeaq9ajs00zqsdjx6k3s65gt"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ajs00zrsdjx5ymc2ln3"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","tag_id":"ckeaq9agk00l0sdjxgeu1d22z","_id":"ckeaq9ajs00zssdjxhhvy5xub"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","tag_id":"ckeaq9agx00mlsdjxc5zmhhf6","_id":"ckeaq9ajs00ztsdjxbal7g4dd"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ajs00zusdjx8w2h63yh"},{"post_id":"ckeaq9a5a006wsdjxgkqcaymw","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajs00zvsdjx1wij7hye"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ajs00zwsdjx8bjj46s2"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ajs00zxsdjx38gy8yrv"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ajs00zysdjxch4k6n1l"},{"post_id":"ckeaq9a6v007hsdjxhaz3apjo","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ajs00zzsdjxfoayfciz"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajs0100sdjx2jq1cqzp"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","tag_id":"ckeaq9ah000mvsdjx08d567rz","_id":"ckeaq9ajs0101sdjxegeq5yl6"},{"post_id":"ckeaq9a77007rsdjx2yxfcols","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajs0102sdjxguh529o1"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","tag_id":"ckeaq9agk00l0sdjxgeu1d22z","_id":"ckeaq9ajs0103sdjx9ixha0p3"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","tag_id":"ckeaq9aaa00b5sdjxb41298ft","_id":"ckeaq9ajs0104sdjxgwl7btvu"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ajs0105sdjxcjyy3lme"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","tag_id":"ckeaq9ah300nbsdjx1tfq5wvt","_id":"ckeaq9ajt0106sdjxcaxjalwj"},{"post_id":"ckeaq9a7a007vsdjx8ym69hi1","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9aju0107sdjxhbjqhqj2"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9aju0108sdjx9gd37ud7"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9aju0109sdjxa5heaud8"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajv010asdjx40f2ckjc"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajv010bsdjxeyd6g67n"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ajv010csdjxhm5ff03u"},{"post_id":"ckeaq9a8a0091sdjxep3v8ugw","tag_id":"ckeaq9ah300ngsdjx3yp37ha6","_id":"ckeaq9ajv010dsdjx0t078sgo"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ajv010esdjx8ukhgsl1"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajv010fsdjx5se872v8"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajv010gsdjxdwxp3uxg"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajv010hsdjxgw5nbd2o"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ajv010isdjx13if614f"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ajv010jsdjxhkpl3omq"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajv010ksdjxe8fma95b"},{"post_id":"ckeaq9a8j009bsdjx4sw84go9","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ajv010lsdjxfod6e34j"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ajv010msdjx8wr3c5tp"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ajv010nsdjx2i819rpy"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajv010osdjx2zww99ys"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajv010psdjxe2mlfq33"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajw010qsdjx7h5nd4dq"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ajw010rsdjx24d935zq"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ajw010ssdjx66gx9rqj"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99to0012sdjx6zmkgqne","_id":"ckeaq9ajw010tsdjxbjllchr0"},{"post_id":"ckeaq9a8o009fsdjx6w9s1bj2","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajw010usdjxbz1kgb3p"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ajw010vsdjxa9cfb3xh"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ajw010wsdjxhrfi8t00"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ajw010xsdjxasd4g5j7"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ajw010ysdjx3l0o9lhs"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ajw010zsdjx28ue6imo"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99t8000qsdjxf0n43sto","_id":"ckeaq9ajw0110sdjx0tl0hqz2"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ajw0111sdjxe1sub2ig"},{"post_id":"ckeaq9a8q009isdjxd9657doc","tag_id":"ckeaq9af600f7sdjx44so0ipa","_id":"ckeaq9ajw0112sdjx5odd2vei"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ajw0113sdjxbim99zoi"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ajw0114sdjx55469yu6"},{"post_id":"ckeaq9a90009tsdjx5cc6hzzt","tag_id":"ckeaq9agx00mlsdjxc5zmhhf6","_id":"ckeaq9ajx0115sdjx7y5w26lt"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ajx0116sdjxfo6fgmd3"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak00117sdjx31xjbx35"},{"post_id":"ckeaq9a92009wsdjxbohpa8na","tag_id":"ckeaq9agx00mlsdjxc5zmhhf6","_id":"ckeaq9ak00118sdjxdycl5y8v"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak00119sdjx60xaeslf"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak0011asdjx6qj4em2z"},{"post_id":"ckeaq9a9300a0sdjx8jzq6mav","tag_id":"ckeaq9agx00mlsdjxc5zmhhf6","_id":"ckeaq9ak0011bsdjxgafd4th5"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak0011csdjxcgmz96ft"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak0011dsdjxg38tfd69"},{"post_id":"ckeaq9a9900aasdjxfj9v52cs","tag_id":"ckeaq9agx00mlsdjxc5zmhhf6","_id":"ckeaq9ak0011esdjx798yfxwe"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak0011fsdjx1062d3vi"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak1011gsdjx5gh08ikk"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak1011hsdjxc8xxeczy"},{"post_id":"ckeaq9a9b00aesdjx0p0kcxsf","tag_id":"ckeaq9ahf00oqsdjxbnec8flo","_id":"ckeaq9ak1011isdjxhki90dd2"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak1011jsdjx84h23old"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak2011ksdjx9sw7b6rb"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak2011lsdjxanffal7y"},{"post_id":"ckeaq9a9f00ahsdjxgw4zfq7h","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak2011msdjx4txu7a4q"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak2011nsdjxay9n7ju5"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak2011osdjx4bf07rrq"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak2011psdjx6tgg1p9v"},{"post_id":"ckeaq9a9h00alsdjxfr1z4d8c","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak2011qsdjx0t7le8yr"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak2011rsdjxbpj3h0p9"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak2011ssdjxc8ajfxnf"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak2011tsdjxeliu5fgv"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak2011usdjxef82ajts"},{"post_id":"ckeaq9a9t00avsdjx15ffe0ri","tag_id":"ckeaq9aey00e7sdjx4xlz4hfe","_id":"ckeaq9ak2011vsdjx9whreffn"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak2011wsdjxdhxigpz0"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak2011xsdjxa7os5ja6"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak2011ysdjxb1d91ybj"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak2011zsdjx0a2v0hlb"},{"post_id":"ckeaq9aa100azsdjx58wlgxeg","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak20120sdjx39t2ay1o"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak30121sdjxh7bhh1qy"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak30122sdjxh2i752oi"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak30123sdjx02t9811i"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak30124sdjx93qz696t"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak30125sdjx7mjj2lb0"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak30126sdjx7g8q7xgd"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak40127sdjx3rntbuxv"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak40128sdjxa2d9h9uc"},{"post_id":"ckeaq9aa600b2sdjxajm8cqz3","tag_id":"ckeaq9aho00ppsdjxgvnvacg5","_id":"ckeaq9ak40129sdjx0gz49kz7"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak4012asdjxe4ahb5b6"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak5012bsdjx0sih7py0"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak5012csdjxa0tbatus"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak5012dsdjxalrehwtp"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak5012esdjxdv3dede0"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak5012fsdjx34y640ln"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak5012gsdjxfjyidooo"},{"post_id":"ckeaq9aab00b6sdjxhb2284t9","tag_id":"ckeaq9aho00ppsdjxgvnvacg5","_id":"ckeaq9ak5012hsdjxhckb1abm"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak5012isdjxhq7kg2il"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak6012jsdjxgn70cyvt"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99zn003vsdjx5r7n172h","_id":"ckeaq9ak6012ksdjxa2ke6pdb"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak6012lsdjx6x745v1i"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak6012msdjx6kyt9xg1"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq9ahf00oqsdjxbnec8flo","_id":"ckeaq9ak6012nsdjx7qm8598c"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak6012osdjxbir1bwxa"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak6012psdjxgbschl26"},{"post_id":"ckeaq9aag00b9sdjx0zw8a1iq","tag_id":"ckeaq9aho00ppsdjxgvnvacg5","_id":"ckeaq9ak6012qsdjx7g573su7"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq99zc003msdjx2gpocfgu","_id":"ckeaq9ak6012rsdjxayc39duy"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq9aew00dxsdjx3dnccdqj","_id":"ckeaq9ak6012ssdjxhi207qof"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak6012tsdjx6yj07i5p"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak6012usdjxgrppbmac"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak6012vsdjx2d3ncpf0"},{"post_id":"ckeaq9aaj00bcsdjx4djp4jwd","tag_id":"ckeaq9aho00ppsdjxgvnvacg5","_id":"ckeaq9ak6012wsdjxdtqj7100"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak6012xsdjxepzq4hoo"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak6012ysdjxcb0nc89a"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ak6012zsdjx282u0kiy"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ak60130sdjxbkj88q11"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak60131sdjx0s4f7jcw"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak60132sdjx2fdf0ewv"},{"post_id":"ckeaq9aao00bgsdjxhfig7i0t","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ak60133sdjxhqj0gbtl"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99wf001ssdjxda6uhb8a","_id":"ckeaq9ak60134sdjx76sh5zrj"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ak60135sdjx1ib028iy"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ak70136sdjx9bk4f3lt"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak70137sdjx06zd0nva"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak70138sdjx1ao6fnar"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak70139sdjx5ha1669p"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ak7013asdjx6cba371u"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq9ak7013bsdjx4qcl75qd"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak7013csdjxffbv5bp4"},{"post_id":"ckeaq9aaw00bnsdjxawy2ddbc","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak7013dsdjx4pfyaz63"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99s30007sdjx40srbwf8","_id":"ckeaq9ak7013esdjx6z4689fs"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak7013fsdjx38yf15xa"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak7013gsdjxg4qt976i"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak7013hsdjx88dm05br"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ak7013isdjxd3lghqmk"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99tj000wsdjxb1ap1voa","_id":"ckeaq9ak7013jsdjx95786s95"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak7013ksdjx6ugcf9tq"},{"post_id":"ckeaq9aaz00bqsdjx7qmj6dqu","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak7013lsdjx0g5i29ll"},{"post_id":"ckeaq9abj00bxsdjx0yjzajbp","tag_id":"ckeaq9afz00icsdjxccsphlvn","_id":"ckeaq9ak7013msdjxf3hqcpv4"},{"post_id":"ckeaq9abj00bxsdjx0yjzajbp","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak7013nsdjx1nmdfor7"},{"post_id":"ckeaq9abj00bxsdjx0yjzajbp","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak7013osdjxfwdzcsbs"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak7013psdjx6t94gslg"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","tag_id":"ckeaq9a4h006asdjx9sqg9ozr","_id":"ckeaq9ak7013qsdjxf8kwdgki"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak8013rsdjxay2obn7p"},{"post_id":"ckeaq9abo00c1sdjx1amk9lpj","tag_id":"ckeaq9ai300resdjxfr7qh4p6","_id":"ckeaq9ak8013ssdjx2igoannc"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ak8013tsdjx6wwrapki"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ak8013usdjx2syqgwz0"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak8013vsdjx4y8ka4kn"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak8013wsdjx1ty5h701"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ak8013xsdjxguymcrjy"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak8013ysdjxfmda2uaw"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak8013zsdjxe4751ldy"},{"post_id":"ckeaq9acb00cfsdjx6nwt9n7z","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ak80140sdjx3ypsgsgr"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99rn0002sdjxcusz1ycg","_id":"ckeaq9ak80141sdjx66kugtbj"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99s7000bsdjx077kfxg4","_id":"ckeaq9ak80142sdjx969d9z2m"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99sf000fsdjx16ypandi","_id":"ckeaq9ak80143sdjxeilugjk1"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99sz000lsdjxebz55lbg","_id":"ckeaq9ak80144sdjx8ekf5xmb"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99xl002osdjx6td09z8q","_id":"ckeaq9ak80145sdjxfcm34vqe"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq9af200essdjxdd8mcy5t","_id":"ckeaq9ak80146sdjxbgmc0vdf"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99ug0018sdjxeqep0vkv","_id":"ckeaq9ak80147sdjx3zm6escp"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99uq001gsdjx7psy5cxh","_id":"ckeaq9ak80148sdjx4bfj1ww8"},{"post_id":"ckeaq9ad700cisdjx1ince5dp","tag_id":"ckeaq99w7001msdjx9rzyh71f","_id":"ckeaq9ak80149sdjx6ds45usj"},{"post_id":"ckeaq9ae300d7sdjxhrahhy7u","tag_id":"ckeaq9afr00hwsdjx0pcy6gu8","_id":"ckeaq9ak8014asdjx06ndb0xd"},{"post_id":"ckeaq9ae300d7sdjxhrahhy7u","tag_id":"ckeaq9ah300nbsdjx1tfq5wvt","_id":"ckeaq9ak8014bsdjxglsgh2gg"}],"Tag":[{"name":"cookbook","_id":"ckeaq99rn0002sdjxcusz1ycg"},{"name":"ETL","_id":"ckeaq99s30007sdjx40srbwf8"},{"name":"guides","_id":"ckeaq99s7000bsdjx077kfxg4"},{"name":"How-to","_id":"ckeaq99sf000fsdjx16ypandi"},{"name":"howto","_id":"ckeaq99sz000lsdjxebz55lbg"},{"name":"kettle","_id":"ckeaq99t8000qsdjxf0n43sto"},{"name":"MySQL","_id":"ckeaq99tj000wsdjxb1ap1voa"},{"name":"PDI","_id":"ckeaq99to0012sdjx6zmkgqne"},{"name":"SysAdmin","_id":"ckeaq99ug0018sdjxeqep0vkv"},{"name":"technical","_id":"ckeaq99uq001gsdjx7psy5cxh"},{"name":"Ubuntu","_id":"ckeaq99w7001msdjx9rzyh71f"},{"name":"centos","_id":"ckeaq99wf001ssdjxda6uhb8a"},{"name":"install","_id":"ckeaq99xl002osdjx6td09z8q"},{"name":"Linux","_id":"ckeaq99yo0036sdjx4dyzbvmu"},{"name":"CDH","_id":"ckeaq99zc003msdjx2gpocfgu"},{"name":"Hadoop","_id":"ckeaq99zn003vsdjx5r7n172h"},{"name":"Big Data","_id":"ckeaq99zw0043sdjx9nkbdfbu"},{"name":"R Markdown","_id":"ckeaq9a280051sdjx0rctey0s"},{"name":"LDAP","_id":"ckeaq9a4a0062sdjxc06l7q8d"},{"name":"Mint","_id":"ckeaq9a4h006asdjx9sqg9ozr"},{"name":"goofy","_id":"ckeaq9aaa00b5sdjxb41298ft"},{"name":"Cloudera","_id":"ckeaq9aew00dxsdjx3dnccdqj"},{"name":"impala","_id":"ckeaq9aey00e7sdjx4xlz4hfe"},{"name":"SQL Server","_id":"ckeaq9af200essdjxdd8mcy5t"},{"name":"Windows","_id":"ckeaq9af600f7sdjx44so0ipa"},{"name":"Graph Databases","_id":"ckeaq9afa00fnsdjxbbe21pyy"},{"name":"metadata","_id":"ckeaq9afb00fssdjx5om8da7c"},{"name":"graph database","_id":"ckeaq9afd00g3sdjxfnn94wgd"},{"name":"humor","_id":"ckeaq9afr00hwsdjx0pcy6gu8"},{"name":"Mac","_id":"ckeaq9afz00icsdjxccsphlvn"},{"name":"ggplot2","_id":"ckeaq9ag300iwsdjx1xdr8ehg"},{"name":"Pandoc","_id":"ckeaq9aga00jrsdjxdfjc0et8"},{"name":"external article","_id":"ckeaq9agk00l0sdjxgeu1d22z"},{"name":"nginx","_id":"ckeaq9agp00llsdjx6mhdfvm6"},{"name":"nat: HNS failed","_id":"ckeaq9agu00m5sdjxcbtbh7vl"},{"name":"HortonWorks","_id":"ckeaq9agx00mlsdjxc5zmhhf6"},{"name":"rvest","_id":"ckeaq9ah000mvsdjx08d567rz"},{"name":"inspiration","_id":"ckeaq9ah300nbsdjx1tfq5wvt"},{"name":"RStudio","_id":"ckeaq9ah300ngsdjx3yp37ha6"},{"name":"HUE","_id":"ckeaq9ahf00oqsdjxbnec8flo"},{"name":"YARN","_id":"ckeaq9aho00ppsdjxgvnvacg5"},{"name":"tidyverse","_id":"ckeaq9ai300resdjxfr7qh4p6"}]}}